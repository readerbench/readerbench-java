When designing a mechanism there are several key properties that are desirable to maintain. Some of the more important ones are individual rationality (IR) - to make it worthwhile for all players to participate, incentive compatibility (IC) - to give incentive to players to report their true value to the mechanism and budget balance (BB) - not to run the mechanism on a loss. In many of the mechanisms the goal function that a mechanism designer attempts to maximize is the social welfare1 - the total benefit to society.
However, it is well known from [15] that any mechanism that maximizes social welfare while maintaining individual rationality and incentive compatibility runs a deficit perforce, i.e., is not budget balanced.
Of course, for many applications of practical importance we lack the will and the capability to allow the mechanism to run a deficit and hence one must balance the payments made by the mechanism. To maintain the BB property in an IR and IC mechanism it is necessary to compromise on the optimality of the social welfare.
There have been several attempts to design budget balanced mechanisms for particular domains2 . For instance, for double-sided auctions where both the buyers and sellers are strategic and the goods are homogeneous [13] (or when the goods are heterogeneous [5]). [13] developed a mechanism that given valuations of buyers and sellers produces an allocation (which are the trading players) and a matching between buyers and sellers such that the mechanism is IR,
IC, and BB while retaining most of the social welfare. In the distributed markets problem (and closely related problems) goods are transported between geographic locations while incurring some constant cost for transportation. [16, 9, 3] present mechanisms that approximate the social welfare while achieving an IR, IC and BB mechanism. For supply chain problems [2, 4] bounds the loss of social welfare that is necessary to inflict on the mechanism in order to achieve the desired combination of IR, IC, and BB.
Despite the works discussed above, the question of how to design a general mechanism that achieves IR, IC, and BB independently of the problem domain remains open.
Furthermore, there are several domains where the question of how to design an IR, IC and BB mechanism which approx1 Social Welfare is also referred to as efficiency in the economics literature. 2 A brief reminder of all of the problems used in this paper can be found in Appendix B 20 imates the social welfare remains an open problem. For example, in the important domain of combinatorial doublesided auctions there is no known result that bounds the loss of social welfare needed to achieve budget balance. Another interesting example is the open question left by [3]:How can one bound the loss in social welfare that is needed to achieve budget balance in an IR and IC distributed market where the transportation edges are strategic. Naturally an answer to the BB distributed market with strategic edges has vast practical implications, for example to transportation networks.
In this paper we unify all the problems discussed above (both the solved as well as the open ones) into one solution concept procedure. The solution procedure called the Generalized Trade Reduction (GTR). GTR accepts an IR and IC mechanism for single-valued players and outputs an IR,
IC and BB mechanism. The output mechanism may suffer some welfare loss as a tradeoff of achieving BB. There are problem instances in which no welfare loss is necessary but by [15] there are problem instances in which there is welfare loss. Nevertheless for a wide class of problems we are able to bound the loss in welfare. A particularly interesting case is one in which the input mechanism is an efficient allocation.
In addition to unifying many of the BB problems under a single solution concept, the GTR procedure improves on existing results and solves several open problems in the literature. The existing solutions our GTR procedure improves are homogeneous double-sided auctions, distributed markets [3], and supply chain [2, 4]. For the homogeneous doublesided auctions the GTR solution procedure improves on the well known solution [13] by allowing for some cases of no trade reduction at all. For the distributed markets [3] and the supply chain [2, 4] the GTR solution procedure improves on the welfare losses" bound, i.e., allows one to achieve an IR, IC and BB mechanism with smaller loss on the social welfare. Recently we also learned that the GTR procedure allows one to turn the model newly presented [6] into a BB mechanism. The open problems that are answered by GTR are distributed markets with strategic transportation edges and bounded paths, combinatorial double-sided auctions with bounded size of the trading group i.e., a buyer and its bundle goods" sellers, combinatorial double-sided auctions with bounded number of possible trading groups.
In addition to the main contribution described above, this paper also defines an important classification of problem domains. We define class based domain and procurement class based domains. The above definitions build on the different competition powers of players in a mechanisms called internal and external competition. Most of the studied problem domains are of the more restrictive procurement class domains and we believe that the more general setting will inspire more research.
In this paper we design a method which given any IR and IC mechanism outputs a mechanism that maintains the IC and IR properties while achieving BB. For some classes of mechanisms we bound the competitive approximation of welfare.
In our model there are N players divided into sets of trade.
The sets of trade are called procurement sets and are defined (following [2]) as follows: Definition 2.1. A procurement set s is the smallest set of players that is required for trade to occur.
For example, in a double-sided auction, a procurement set is a pair consisting of a buyer and a seller. In a combinatorial double-sided auction a procurement set can consist of a buyer and several sellers. We mark the set of all procurement sets as S and assume that any allocation is a disjoint union of procurement sets.
Each player i, 1 ≤ i ≤ n, assigns a real value vi(s) to each possible procurement set s ∈ S. Namely, vi(s) is the valuation of player i on procurement set s. We assume that for each player i vi(s) is i"s private value and that i is a single value player, meaning that if vi(sj) > 0 then for every other sk, k = j, either vi(sk) = vi(sj) or vi(sk) = 0. For the ease of notation we will mark by vi the value of player i for any procurement set s such that vi(s) > 0. The set Vi ⊆ R is the set of all possible valuations vi. The set of all possible valuations of all the players is denoted by V = V1 × ... × Vn. Let v−i = (v1, ..., vi−1, vi+1, ..., vn) be the vector of valuations of all the players besides player i, and let V−i be the set of all possible vectors v−i.
We denote by W(s) the value of a procurement set s ∈ S such that W(s) = i∈s vi(s) + F(s), where F is some function that assigns a constant to procurement sets. For example, F can be a (non-strategic) transportation cost in a distributed market problem. Let the size of a procurement set s be denoted as |s|.
It is assumed that any allocation is a disjoint union of procurement sets and therefore one can say that an allocation partitions the players into two sets; a set of players that trade and a set of players that do not trade.
The paper denotes by O the set of possible partitions of an allocation A into procurement sets. The value W(A) of an allocation A is the sum of the values of its most efficient partition to procurement sets, that is W(A) = maxS∈O s∈S W(s).
This means that W(A) = i∈A vi +maxS∈O s∈S F(s). In the case where F is identically zero, then W(A) = i∈A vi.
An optimal partition S∗ (A) is a partition that maximizes the above sum for an allocation A. Let the value of A be W(S∗ (A)) (note that the value can depend on F). We say that the allocation A is efficient if there is no other allocation with a higher value. The efficiency of the allocation ˆA is W ( ˆA) W (A) , where A is a maximal valued allocation. We assume w.l.o.g. that there are no two allocations with the same value3 .
A mechanism M defines an allocation and payment rules,
M = (R, P). A payment rule P decides i"s payment pi where P is a function P : V → RN . We work with mechanisms 3 Ties can be broken using the identities of the players. 21 in which players are required to report their values. An example of such a mechanism is the VCG mechanism [17, 8, 10]. The reported value bi ∈ Vi of player i is called a bid and might be different from his private value vi. Let b ∈ V be the bids of all players. An allocation rule R decides the allocation according to the reported values b ∈ V . We make the standard assumption that players have quasi-linear utility so that when player i trades and pays pi then his utility is ui(vi, b−i) = vi − pi, ui : V ⇒ R. We also assume that players are rational utility maximizers.
Mechanism M is Budget Balanced (BB) if i∈N pi ≥ 0 for any bids b ∈ V . M is Incentive-Compatible (IC) in dominant strategies if for any player i, value vi and any b−i ∈ V−i, ui(vi, b−i) ≥ ui(b) meaning that for any player i, bidding vi maximized i"s utility over all possible bids of the other players. M is (ex-post) Individually Rational (IR) if for any player i value vi, and any b−i ∈ V−i ui(vi, b−i) ≥ 0 meaning that for all possible bids of the other players, player"s i utility is non-negative. Note that since our mechanisms are normalized IR, if a player does not trade then the player pays 0 and has utility 0.
Our algorithm presented in the next section employs a commonly used payment scheme, the critical value payment scheme.
Definition 2.2. Critical value payment scheme: A mechanism uses a critical value payment scheme if given an allocation it charges players the minimum value they need to report to the mechanism in order to remain allocated.
We denote by Ci the critical value price computed for player i.
In this paper we present two generalized trade reduction algorithms. The two algorithms are such that given an IR and IC mechanism M that solves a problem in some domain (different domains are formally defined below), turns M into IR, IC and BB mechanism. The algorithm presented finds procurement sets and removes them in iterations until the right conditions are fulfilled and the mechanism M is turned into a BB one. The right conditions that need to be met are conditions of competition among the players in the given problem. The following definitions leads us to the competition conditions we are looking for.
Definition 2.3. For any player i ∈ N, we say that the set Ri ⊆ N \ {i} is a replacing set of i, if for any procurement set s ∈ S such that i ∈ s and Ri∩s = ∅, s\{i}∪Ri ∈ S.
For example, in a (homogeneous) double-sided auction (see problem B.1) the replacement set for any buyer is simply any other buyer. In an auction for transportation slots (see problem B.7), the replacement set of an edge is a path between the endpoints of the edge. Note that a set can replace a single player. Furthermore, this relationship is transitive but not necessarily symmetric. If i is a replacement set for j, it is not necessarily true that j is a replacement set for i.
Definition 2.4. For any allocation A, procurement set s ⊆ A, and any i ∈ s we say Ri(A, s) is an internal competition for i with respect to A and s, if Ri(A, s) ⊆ N \ A is a replacement set for i s.t. T = s \ {i} ∪ Ri(A, s) ∈ S and W(T) ≥ 0.
Definition 2.5. For any allocation A and procurement set s ⊆ A and any i ∈ s we say that Ei(A, s) is an external competition for i with respect to A and s, if Ei(A, s) ⊆ N \ A is a set s.t., T = {i} ∪ Ei(A, s) ∈ S and W(T) ≥ 0.
We will assume, without loss of generality, that there are no ties between the values of any allocations, and in particular there are no ties between values of procurement sets.
In case of ties, these can be broken by using the identities of the players4 . So for any allocation A, procurement set s and player i with external competition Ei(A, s), there exists exactly one set representing the maximally valued external competition.
Definition 2.6. A set X ⊂ N is closed under replacement if ∀i ∈ X then Ri ⊂ X The following defines the required competition needed to maintain IC, IR and BB. The set X5 denotes this competition and is closed under replacement. In the remainder of the paper we will assume that all of our sets which define competition in a mechanism are closed under replacement.
Definition 2.7. Let X ⊂ N be a set that is closed under replacement, we say that the mechanism is an X-external mechanism, if
Ri1 (A, s), . . . , Rit (A, s) such that for every iz = iq, Riz (A, s) ∩ Riq (A, s) = ∅
For general domains the choice of X can be crucial. In fact even for the same domain the welfare (and revenue) can vary widely depending on how X is defined. In appendix C we give an example where two possible choices of X yield greatly different results. Although we show that X should be chosen as small as possible we do not give any characterization of the optimality of X and this is an important open problem.
Our two generalized trade reduction algorithms will ensure that for any allocation we have the desired types of competition. So given a mechanism M that is IC and IR with allocation A, the goal of the algorithms is to turn M into an X-external mechanism. The two generalized trade reduction algorithms utilize a dividing function D which divides allocation A into disjoint procurement sets. The algorithms order the procurements sets defined by D in order of increasing value. For any procurement set there is a desired type of competition that depends only on the players who compose the procurement set. The generalized trade reduction algorithms go over the procurement sets in order (from the smallest to the largest) and remove any procurement set that does not have the desired competition when the set is reached. The reduction of procurement sets will also be referred to as a trade reduction.
Formally, 4 The details of how to break ties in allocations are standard and are omitted. 5 We present some tradeoffs between the different possible sets in Appendix C. 22 Definition 2.8. D is a dividing function if for any allocation A and the players" value vector v, D divides the allocation into disjoint procurements sets s1, . . . , sk s.t. ∪sj = A and for any player i with value vi if i ∈ sj1 and t ∈ sj2 s.t. j1 ≥ j2 then for any value vi > vi of player i and division by D into s1, . . . , sk such that i ∈ sj1 and t ∈ sj2 then j1 > j2.
The two generalized trade reduction algorithms presented accept problems in different domains. The formal domain definitions follow: Definition 2.9. A domain is a class domain if for all i ∈ N and all replacement sets of i, Ri, |Ri| = 1 and for all i, j, i = j if j = Ri then i = Rj.
Intuitively, this means that replacement sets are of size 1 and the replacing relationship is symmetric.
We define the class of a player i as the set of the player"s replacement sets and denote the class of player i by [i]. It is important to note that since replacement sets are transitive relations and since class domains also impose symmetric relations on the replacement sets, the class of a player i, [i] is actually an equivalence class for i.
Definition 2.10. A domain is a procurement-class domain if the domain is a class-based domain and if for any player i such that there exists two procurement sets s1, s2 (not necessarily trading simultaneously in any allocation) such that i ∈ s1 and i ∈ s2 then there exists a bijection f : s1 → s2 such that for any j ∈ s1, f(j) is a replacement set for j in s2.
Example 2.1. A (homogeneous) double-sided auction (see problem B.1) is a procurement-class based domain. For the (homogeneous) double-sided auction each procurement set consists of a buyer and a seller.
The double sided combinatorial auction consisting of a single multi-minded buyer and multiple sellers of heterogenous goods (see problem B.9), is a class based domain (as we have a single buyer) but not a procurement-class based domain.
In this case, the buyer is a class and each set of sellers of the same good is a class. However, for a buyer there is no bijection between the different the procurement sets of the bundles of goods the buyer is interested in.
The spatial-distributed market with strategic edges (see problem B.6) is not a class-based domain (and therefore not a procurement-class domain). For example, even for a fixed buyer and a fixed seller there are two different procurement sets consisting of different paths between the buyers and sellers.
The next sections present two algorithms GTR-1 and GTR2. GTR-1 accepts problems in procurement-class based domains, its properties are proved with a general dividing function D. The GTR-2 algorithm accepts problems in any domain. We prove the GTR-2"s properties with specific dividing function D0. The function will be defined in section
impact on welfare (and revenue) the generality of GTR − 1 (albeit in special domains) can be an important practical consideration.
DOMAINS This section focuses on the problems that are procurementclass based domains. For this domain, we present an algorithm called GTR-1, which given a mechanism that is IR and IC outputs a mechanism with reduced welfare which is IR, IC and budget balanced.
Although procurement class domains appear to be a relatively restricted model, in fact many domains studied in the literature are procurement class domains.
Example 3.1. The following domains are procurement class domains: • double-sided auctions with homogenous goods [13](problem B.1). In this domain there are two classes. The class of buyers and the class of sellers. Each procurement set consists of a single buyer and a single seller.
Since every pair of (buyer, seller) is a valid procurement set (albeit possible with negative value) this is a procurement class domain. In this domain the constant assigned to the procurement sets is F = 0. • Spatially distributed markets with non strategic edges [3, 9](problem B.3). Like the double-sided auctions with homogenous goods, their are two classes in the domain. Class of buyers and class of sellers with procurement sets consisting of single buyer and single seller.
The sellers and buyers are nodes in a graph and the function F is the distance of two nodes (length of the edge) which represent transport costs. These costs differ between different (buyer, seller) pairs. • Supply chains [2, 4] (problem B.5). The assumption of unique manufactory by [2, 4] can best be understood as turning general supply chains (which need not be a procurement class domain) into a procurement class domain. • Single minded combinatorial auctions [11] (problem B.8).
In this context each seller sells a single good and each buyer wants a set of goods. The classes are the sets of sellers selling the same good as well as the buyers who desire the same bundle. A procurement set consists of a single buyer as well as a set of sellers who can satisfy that buyer.
A definition of the mechanism follows: Definition 3.1. The GTR-1 algorithm - given a mechanism M, a set X ⊂ N which is closed under replacement, a dividing function D, and allocation A, GTR-1 operates as follows:
procurement sets s1, . . . , sk ∈ S.
set: If for every i ∈ sj ∩ X there is external competition and every i ∈ sj \ X there is internal competition then 23 keep sj. Otherwise reduce the trade sj (i.e., remove every i ∈ sj from the allocation).6
trading. All non trading players are charged nothing.
Remark 3.1. The special case where X = N has received attention under different guises in various special cases, such as ([13, 3, 4]).
Mechanism that is IR, IC and BB In this subsection we prove that the GTR-1 algorithm produces an X-external mechanism that is IR, IC and BB.
To prove GTR-1"s properties we make use of theorem 3.1 which is a well known result (e.g., [14, 11]). Theorem 3.1 characterizes necessary and sufficient conditions for a mechanism for single value players to be IR and IC: Definition 3.2. An allocation rule R is Bid Monotonic if for any player i, any bids of the other players b−i ∈ V−i, and any two possible bids of i, ˆbi > bi, if i trades under the allocation rule R when reporting bi, then i also trades when reporting ˆbi.
Intuitively, a bid monotonic allocation rule ensures that no trading player can become a non-trading player by improving his bid.
Theorem 3.1. An IR mechanism M with allocation rule R is IC if and only if R is Bid Monotonic and each trading player i pays his critical value Ci (pi = Ci).
So for normalized IR7 and IC mechanisms, the allocation rule which is bid monotonic uniquely defines the critical values for all the players and thus the payments.
Observation 3.1. Let M1 and M2 be two IR and IC mechanisms with the same allocation rule. Then M1 and M2 must have the same payment rule.
In the following we prove that the X-external GTR-1 algorithm produces a IR, IC and BB mechanism, but first a subsidiary lemma is shown.
Lemma 3.1. For procurement class domains if there exists a procurement set sj s.t. i ∈ sj and i has external competition than all t = i t ∈ sj, t has internal competition.
Proof. This follows from the definition of procurement class domains. Suppose that i has external competition, then there exists a set of players Ei(A, s) such that {i} ∪ Ei(A, s) ∈ S. Let us denote by sj = {i} ∪ Ei(A, s). Since the domain is a procurement-class domain there exists a bijection function f between sj and sj. f defines the required internal competition.
We start by proving IR and IC: 6 Although the definition of an X-external mechanism requires that X intersects every procurement set, this is not strictly necessary. It is possible to define an X that does not intersect every possible procurement set. In this case, any procurement set s ∈ S s.t. s ∩ X = ∅ will be reduced. 7 Note that this is not true for mechanisms which are not normalized e.g., [7, 12] Lemma 3.2. For any X, the X-external mechanism with a critical value pricing scheme produced by the GTR-1 algorithm is an IR and IC mechanism.
Proof. By the definition of a critical value pricing scheme
trading player i, vi ≥ 0. By the GTR-1 algorithm 3.1 nontrading players i have a payment of zero. Thus for every player i, value vi, and any b−i ∈ V−i ui(vi, b−i) ≥ 0, meaning the produced X-external mechanism is IR.
As the X-external GTR-1 algorithm is IR and applies the critical value payment scheme according to theorem 3.1, in order to show that the produced X-external mechanism with the critical value payment scheme is IC, it remains to show that the produced mechanism"s allocation rule is bid monotonic.
Since GTR-1 orders the procurement sets according to increasing value, if player i increases his bid from bi to bi > bi then for any division function D of procurement sets, the procurement set s containing i always appears later with the bid bi than with the bid bi. So the likelihood of competition can only increase if i appears in later procurement sets. This follows as GTR-1 can reduce more of the lower value procurement sets which will result in more non-trading players.
Therefore if s has the required competition and is not reduced with bi then it will have the required competition with bi and will not be reduced.
Finally we prove BB: Lemma 3.3. For any X, the X-external mechanism with critical value pricing scheme produced by the GTR-1 algorithm is a BB mechanism.
Proof. In order to show that the produced mechanism is BB we show that each procurement set that is not reduced has a positive budget (i.e., the sum of payments is positive).
Let s ∈ S be a procurement set that is not reduced. Let i ∈ s ∩ X then according to the definition of X-external definition 2.7 and the GTR-1 algorithm 3.1 i has an external competition. Assume w.l.o.g.8 that i is the only player with external competition in s and all other players j = i, j ∈ s have internal competition.
Let A be the allocation after the procurement sets reduction by the GTR-1 algorithm. According to the definition of external competition 2.5, there exists a set Ei(A, s) ⊂ N \A such that i ∪ Ei(A, s) ∈ S and W(i ∪ Ei(A, s)) ≥ 0. Since W(i∪Ei(A, s)) = vi +W(Ei(A, s)) then vi ≥ −W(Ei(A, s)).
By the critical value pricing scheme definition 2.2 it means that if player i bids any less than −W(Ei(A, s)) he will not have external competition and therefore will be removed from trading. Thus i pays no less than min −W(Ei(A, s)).
Since all other players j ∈ s have internal competition their critical price can not be less than their maximal value internal competitor (set) i.e., max W(Rj(A, s)). If any player j ∈ s bids less then its maximal internal competitor (set) then he will not be in s but his maximal internal competitor (set) will.
As a possible Ei(A, s) is ∪j∈sRj(A, s) one can bound the maximal value of i"s external competition W(Ei(A, s)) by the sum of the maximal values of the rest of the players in s 8 since the domain is a procurement class domain we can use lemma 3.1 24 internal competition i.e., j∈s max W(Rj(A, s)). Therefore min −W(Ei(A, s)) = −( j∈s max W(Rj(A, s))). As the F function is defined to be a positive constant we get that W(s) = min −W(Ei(A, s))+( j∈s max W(Rj(A, s)))+F(s) ≥ 0 and thus s is at least budget balanced. As each procurement set that is not reduced is at least budget balanced, it follows that the produced X-external mechanism is BB.
The above two lemmas yield the following theorem: Theorem 3.2. For procurement class domains for any X, the X-external mechanism with critical value pricing scheme produced by the GTR-1 algorithm is an IR, IC and BB mechanism.
Remark 3.2. The proof of the theorem yields bounds on the payments any player has to make to the mechanism.
DOMAINS The main reason that GTR-1 works for the procurementclass domains is that each player"s possibility of being reduced is monotonic. By the definition of a dividing function if a player i ∈ sj increases his value, i can only appear in later procurement set sj and hence has a higher chance of having the desired competition.
Therefore, the chance of i lacking the requisite competition is decreased. Since the domain is a procurement class domain, all other players t = i,t ∈ sj are also more likely to have competition since members of their class continue to appear before i and hence the likelihood that i will be reduced is decreased. Since by theorem 3.1 a necessary and sufficient condition for the mechanism to be IC is monotonicity. GTR-1 is IC for procurement-class domains.
However, for domains that are not procurement class domains this does not suffice even if the domain is a class based domain. Although, all members of sj continue to have the required competition it is possible that there are members of sj who do not have analogues in sj who do not have competition. Hence i might be reduced after increasing his value which by lemma 3.1 means the mechanism is not IC.
We therefore define a different algorithm for non procurement class domains.
Our modified algorithm requires a special dividing function in order to maintain the IC property. Although our restriction to this special dividing function appears stringent, the dividing function we use is a generalization of the way that procurement sets are chosen in procurement-class based domains e.g., [13, 16, 9, 3, 2, 4].
For ease of presentation in this section we assume that F = 0.
The dividing function for general domains is defined by looking at all possible dividing functions. For each dividing function Di and each set of bids, the GTR-1 algorithm yields a welfare that is a function of the bids and the dividing function9 . We denote by D0 the dividing function that divides the players into sets s.t. the welfare that GTR-1 finds is maximal10 . 9 Note that for any particular Di this might not be IC as GTR-1 is IC only for procurement class domains and not for general domains 10 In Appendix A we show how to calculate D0 in polynoFormally,
Let D be the set of all dividing functions D. Denote the welfare achieved by the mechanism produced by GTR1 when using dividing function D and a set of bids ¯b by GTR1(D,¯b). Denote by D0(¯b) = argmaxD∈D(GTR1(D,¯b)).
For ease of presentation we denote D0(¯b) by D0 when the dependence on b is clear from the context.
Remark 4.1. D0 is an element of the set of dividing functions, and therefore is a dividing function.
The second generalized trade reduction algorithm GTR-2 follows.
Definition 4.1. The GTR-2 algorithm - Given mechanism M, allocation A, and a set X ⊂ N closed under replacement, GTR-2 operates as follows:
procurement sets s1, . . . , sk ∈ S.
procurement set, do the following: If for i ∈ sj ∩ X there is an external competition and there is at most one i ∈ sj that does not have an internal competition then keep sj. Otherwise, reduce the trade sj.
trading. All non trading players are charged zero. 11 We will prove that the mechanism produced by GTR-2 maintains the desired properties of IR, IC, and BB. The following lemma shows that the GTR-2 produced mechanism is IR, and IC.
Lemma 4.1. For any X, the X-external mechanism with critical value pricing scheme produced by the GTR-2 algorithm is an IR and IC mechanism.
Proof. By theorem 3.1 it suffices to prove that the produced mechanism by the GTR-2 algorithm is bid monotonic for every player i. Suppose that i was not reduced when bidding bi we need to prove that i will not be reduced when bidding bi > bi. Denote by D1 = D0(b) the dividing function used by GTR-2 when i reported bi and the rest of the players reported b−i. Denote by D1 = D0(bi, b−i) the dividing function used by GTR-2 when i reported bi and the rest of the players reported b−i. Denote by ¯D1(b) a maximal dividing function that results in GTR-1 reducing i when reporting bi. Assume to the contrary that GTR-2 reduced i from the trade when i reported bi then GTR1(D1, (bi, b−i)) = GTR1( ¯D1, b). Since D1 ∈ D it follows that GTR1(D1, b) > GTR1( ¯D1, b) and therefore GTR1(D1, b) > GTR1(D1, (bi, b−i)). However according to the definition D1 ∈ D, GTR-2 should not have reduced i mial time for procurement-class domains. Calculating D0 in polynomial time for general domains is an important open problem. 11 In the full version GTR-2 is extend such that it suffices that there exists some time in which the third step holds.
That extension is omitted from current version due to lack of space. 25 with the dividing function D1 and gained a greater welfare than GTR1(D1, b). Thus a contradiction arises and and GTR-2 does not reduce i from the trade when i reports bi > bi.
Lemma 4.2. For any X, the X-external mechanism with critical value pricing scheme produced by the GTR-2 algorithm is a BB mechanism.
Proof. This proof is similar to the proof of lemma 3.3.
Combining the two lemmas above we get: Theorem 4.1. For any X closed under replacement, the X-external mechanism with critical value pricing scheme produced by the GTR-2 algorithm is an IR, IC and BB mechanism.
Appendix A shows how to calculate D0 for procurement class domains in polynomial time, it is not generally known how to easily calculate D0. Creating a general method for calculating the needed dividing function in polynomial time remains as an open question.
ProcurementClass Based Domains and other General Domains Cases This section shows that in addition to producing a mechanism with the desired properties, GTR-2 also produces a mechanism that maintains high welfare. Since the GTR-2 algorithm finds a budget balanced mechanism in arbitrary domains we are unable to bound the welfare for general cases. However we can bound the welfare for procurementclass based domain and a wide variety of cases in general domains which includes many cases previously studied.
Definition 4.2. Denote freqk([i], sj) to indicate that a class [i] appears in a procurement set sj, k times and there are k members of [i] in sj.
Definition 4.3. Denote by freqk([i], S) the maximal k s.t. there are k members of [i] in sj. I.e., freqk([i], S) = maxsj ∈S freqk([i], sj).
Let the set of equivalence classes in procurement class based domain mechanism be ec and |ec| be the number of those equivalence classes.
Using the definition of class appearance frequency we can bound the welfare achieved by the GTR-2 produced mechanism for procurement class domains12 : Lemma 4.3. For procurement class domains with F = 0, the number of procurement sets that are reduced by GTR-213 is at most |ec| times the maximal frequency of each class.
Formally, the maximal number of procurement sets that is reduced is O( [i]∈ec freqk([i], S)) Proof. Let D be an arbitrary dividing function. We note that by definition any procurement set sj will not be reduced if every i ∈ sj has both internal competition and external competition. 12 The welfare achieved by GTR-1 can also be bounded for the cases presented in this section. However, we focus on GTR-2 as it always achieves better welfare. 13 or GTR-1 Every procurement set s that is reduced has at least one player i who has no competition. Once s is reduced all players of [i] have internal competition. So by reducing the number of equivalence classes |ec| procurement sets we cover all the remaining players with internal competition.
If the maximal frequency of every equivalence classes was one then each remaining player t in procurement set sk also have external competition as all the internal competitors of players ¯t = t, ¯t ∈ sk are an external competition for t.
If we have freqk([t], S) players from class [t] who were reduced then there is sufficient external competition for all players in sk.
Therefore it suffices to reduce O( [i]∈ec freqk([i], S)) procurement sets in order to ensure that both the requisite internal and external competition exists.
The next theorem follows as an immediate corollary for lemma 4.3: Theorem 4.2. Given procurement-class based domain mechanisms with H procurement sets, the efficiency is at least a 1 − O( O( [i]∈ec freqk([i],S)) H ) fraction of the optimal welfare.
The following corollaries are direct results of theorem 4.2.
All of these corollaries either improve prior results or achieve the same welfare as prior results.
Corollary 4.1. Using GTR-2 for homogenous doublesided auctions (problem B.1) at most14 one procurement set must be reduced.
Similarly, for spatially distributed markets without strategic edges (problem B.3) using GTR-2 improves the result of [3] where a minimum cycle including a buyer and seller is reduced.
Corollary 4.2. Using GTR-2 for spatially distributed markets without strategic edges at most one cycle per connected component15 will be reduced.
For supply chains (problem B.5) using GTR-2 improves the result of [2, 4] similar to corollary 4.2.
Corollary 4.3. Using GTR-2 for supply chains at most one cycle per connected component16 will be reduced.
The following corollary solves the open problem at [3].
Corollary 4.4. For distributed markets on n nodes with strategic agents and paths of bounded length K (problem B.6) it suffices to remove at most K ∗ n procurements sets.
Proof. Sketch: These will create at least K spanning trees, hence we can disjointly cover every remaining procurement set. This improves the naive algorithm of reducing n2 procurement sets.
We provide results for two special cases of double sided CA with single value players (problem B.8). 14 It is possible that no reductions will be made, for instance when there is a non-trading player who is the requisite external competition. 15 Similar to the double-sided auctions, sometimes there will be enough competition without a reduction. 16 Similar to double-sided auctions, sometimes there will be enough competition without a reduction. 26 Corollary 4.5. if there are at most M different kinds of procurement sets it suffices to remove M procurement sets.
Corollary 4.6. If there are K types of goods and each procurement set consists of at most one of each type it suffices to remove at most K procurement sets.
In this paper we presented a general solution procedure called the Generalized Trade Reduction (GTR). GTR accepts an IR and IC mechanism as an input and outputs mechanisms that are IR, IC and BB. The output mechanisms achieves welfare that is close to optimal for a wide range of domains.
The GTR procedure improves on existing results such as homogeneous double-sided auctions, distributed markets, and supply chains, and solves several open problems such as distributed markets with strategic transportation edges and bounded paths, combinatorial double-sided auctions with bounded size procurements sets, and combinatorial doublesided auctions with a bounded number of procurement sets.
The question of the quality of welfare approximation both in general and in class domains that are not procurement class domains is an important and interesting open question. We also leave open the question of upper bounds for the quality of approximation of welfare. Although we know that it is impossible to have IR, IC and BB in an efficient mechanism it would be interesting to have an upper bound on the approximation to welfare achievable in an IR, IC and BB mechanism.
The GTR procedure outputs a mechanism which depends on a set X ⊂ N. Another interesting question is what the quality of approximation is when X is chosen randomly from N before valuations are declared.
Acknowledgements The authors wish to thank Eva Tardos et al for sharing their results with us. The authors also wish to express their gratitude to the helpful comments of the anonymous reviewers.
[1] A. Archer and E. Tardos. Frugal path mechanisms.
Symposium on Discrete Algorithms, Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms,2002. [2] M. Babaioff and N. Nisan. Concurrent Auctions Across the Supply Chain. Journal of Artificial Intelligence Research,2004. [3] Babaioff M., Nisan N. and Pavlov E. Mechanisms for a Spatially Distributed Market. In proceedings of the 5th ACM Conference on Electronic Commerce,2004. [4] M. Babaioff and W. E. Walsh. Incentive-Compatible,
Budget-Balanced, yet Highly Efficient Auctions for Supply Chain Formation. In proceedings of Fourth ACM Conference on Electronic Commerce,2003. [5] Y. Bartal, R. Gonen and P. La Mura.
Negotiation-range mechanisms: exploring the limits of truthful efficient markets. EC "04: Proceedings of the 5th ACM conference on Electronic commerce, 2004. [6] Blume L, Easley D., Kleinberg J. and Tardos E.
Trading Networks with Price-Setting Agents. In proceedings of the 8th ACM conference on Electronic commerce,2007. [7] Cavallo R. Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments.
In Proc. 5th Int. Conf. on Auton. Agents and Multi-Agent Systems (AAMAS06). [8] E. H. Clarke Multipart Pricing of Public Goods. In journal Public Choice 1971, vol. 2, pp. 17-33. [9] Chu L. Y. and Shen Zuo-Jun M. Agent Competition Double Auction Mechanism. Management Science,vol 52(8),2006. [10] T. Groves Incentives in teams. In journal Econometrica 1973, vol. 41, pp. 617-631. [11] D. Lehmann, L. I. O"Callaghan, and Y. Shoham.
Truth Revelation in Approximately Efficient Combinatorial Auctions. In Journal of ACM 2002, vol. 49(5), pp. 577-602. [12] Leonard H. Elicitation of Honest Preferences for the Assignment of Individuals to Positions. Journal of political econ,1983. [13] McAfee R. P. A Dominant Strategy Double Auction.
Journal of Economic Theory,vol 56, 434-450, 1992. [14] A. Mu"alem, and N. Nisan. Truthful Approximation Mechanisms for Restricted Combinatorial Auctions.
Proceeding of AAAI 2002. [15] Myerson R. B. and Satterthwaite M. A. Efficient Mechanisms for Bilateral Trading. Journal of Economic Theory,vol 29, 265-281, 1983. [16] Roundy R., Chen R., Janakriraman G. and Zhang R.
Q. Efficient Auction Mechanisms for Supply Chain Procurement. School of Operations Research and Industrial Engineering, Cornell University,2001. [17] W. Vickrey Counterspeculation, Auctions and Competitive Sealed Tenders. In Journal of Finance 1961, vol. 16, pp. 8-37.
APPENDIX A. CALCULATING THE OPTIMAL DIVIDING FUNCTION IN PROCUREMENT CLASS DOMAINS IN POLYNOMIAL TIME In this section we show how to calculate the optimal dividing function for procurement class domains in polynomial time. We first define a special dividing function D0 which is easy to calculate: We define the dividing function D0 recursively as follows: At stage j, D0 divides the trading players into two sets Aj and Aj s.t. • Aj is a procurement set • Aj can be divided into a disjoint union of procurement sets. • Aj has minimal value from all possible such partitions.
Define sj = Aj and recursively invoke D0 and Aj until Aj = ∅.
We now prove that D0 is the required dividing function.
Lemma A.1. For procurement class domains D0 = D0.
Proof. Since the domain is a procurement class domain, for every reduced procurement set the set of players which achieve competition (either internal or external) is fixed. 27 Therefore, the number of procurement sets which are reduced is independent of the dividing function D. Since the goal is to optimize welfare by reducing procurement sets with the least value we can optimize welfare. This is achieved by D0.
B. PROBLEMS AND EXAMPLES For completeness we present in this section the formal definitions of the problems that we use to illustrate our mechanism.
The first problem that we define is the double-sided auction with homogeneous goods.
Problem B.1. Double-sided auction with homogeneous goods: There are m sellers each of which have a single good (all goods are identical) and n buyers each of which are interested in receiving a good. We denote the set of sellers by S and the set of buyers by B. Every player i ∈ S ∪ B (both buyers and sellers) has a value vi for the good. In this model a procurement set consists of a single buyer and a single seller, i.e., |s| = 2. The value of a procurement set is W(s) = vj − vi where j ∈ B and i ∈ S, i.e., the gain from trade.
If procurement sets are created by matching the highest value buyer to the lowest value seller then [13]"s deterministic trade reduction mechanism17 reduces the lowest value procurement set.
A related model is the pair related costs [9] model.
Problem B.2. The pair related costs: A double-sided auction B.1 in which every pair of players i ∈ S and j ∈ B has a related cost F(i, j) ≥ 0 in order to trade. F(i, j) is a friction cost which should be minimized in order to maximize welfare. [9] defines two budget-balanced mechanisms for this case.
One of [9]"s mechanisms has the set of buyers B as the X set for the X-external mechanism and the other has the set of sellers S as the X set for the X-external mechanism.
A similar model is the spatially distributed markets (SDM) model [3] in which there is a graph imposing relationships on the cost.
Problem B.3. Spatially distributed markets: there is a graph G = (V, E) such that each v ∈ V has a set of sellers Sv and a set of buyers Bv . Each edge e ∈ E has an associated cost which is the cost to transport a single unit of good along the edge. The edges are non strategic but all players are strategic. [3] defines a budget balanced mechanism for this case. Our paper improves on [3] result.
Another graph model is the model defined in [6].
Problem B.4. Trading Networks: Given a graph and buyers and sellers who are situated on nodes of the graph.
All trade must pass through a trader. In this case procurement sets are of the form (buyer, seller, trader) where the possible sets of this form are defined by a graph.
The supply chain model [2, 4] can be seen as a generalization of [6] in which procurement sets consist of the form (producer, consumer, trader1, . . . , traderk). 17 It is also possible to randomize the reduction of procurements sets so as to achieve an expected budget of zero similar to [13], details are obvious and omitted.
Problem B.5. Supply Chain: There is a set D of agents and a set G of goods and a graph G = (V, E) which defines possible trading relationships. Agents can require an input of multiple quantities of goods in order to output a single good.
The producer type of player can produce goods out of nothing, the consumer has a valuation and an entire chain of interim traders is necessary to create a viable procurement set. [2, 4] consider unique manufacturing technology in which the graph defining possible relationships is a tree.
All of the above problems are procurement-class domains.
We also consider several problems which are not procurement class domains and generally the questions of budget balance have been left as open problems.
An open problem raised in [3] is the SDM model in which edges are strategic.
Problem B.6. Spatially distributed markets with strategic edges: there is a graph G = (V, E) such that each v ∈ V has a set of sellers Sv and a set of buyers Bv . Each edge e ∈ E has an associated cost which is the cost to transport a single unit of good along the edge. Each buyer,seller and edge has a value for the trade, i.e., all entities are strategic. [2, 4] left open the question of budget balanced mechanisms for supply chains where there is no unique manufacturing technology. It is easy to see that this problem is not a procurement class domain.
Another interesting problem is transport networks.
Problem B.7. Transport networks: A graph G = (V, E) where the edges are strategic players with costs and the goal is to find a minimum cost transportation route between a pair of privileged nodes Source, Target ∈ V .
It was shown in [1] that the efficient allocation can have a budget deficit that is linear in the number of players.
Clearly, this problem is not a procurement class domain and [1] left the question of a budget balanced mechanism open.
Another non procurement-class based domain mechanism is the double-sided combinatorial auction (CA) with singlevalue players.
Problem B.8. Double-sided combinatorial auction (CA) with single value players: There exists a set S of sellers each selling a single good. There also exists a set B of buyers each interested in bundles of 2S18 .
There are two variants of this problem. In the single minded case each buyer has a positive value for only a single subset whereas in the multi minded case each buyer can have multiple bundles with positive valuation but all of the values are the same. In both cases we assume free disposal so that all bundles containing the desired bundle have the same value for the buyer.
We also consider problems that are non class domains.
Problem B.9. Double-sided combinatorial auction (CA) with general multi-minded players: same as B.8 but each buyer can have multiple bundles with positive valuation which are not necessarily the same. 18 We abuse notation and identify the seller with the good. 28 C. COMPARING DIFFERENT CHOICES OF X The choice of X can have a large impact on the welfare (and revenue) of the reduced mechanism and therefore the question arises of how one should choose the set X.
As the X-external mechanism is required to maintain IC clearly the choice of X can not depend on the value of the players as otherwise the reduced mechanism will not be truthful.
In this section we motivate the choice of small X sets for procurement class domains and give intuition that it may also be the case for some other domains.
We start by illustrating the effect of the set X over the welfare and revenue in the double-sided auction with homogeneous goods problem B.1. Similar examples can be constructed for the other problems defined is B.
The following example shows an effect on the welfare.
Example C.1. There are two buyers and two sellers and two non intersecting (incomparable) sets X = {buyers} and Y = {sellers}. If the values of the buyers are 101, 100 and the sellers are 150, 1 then the X-external mechanism will yield a gain from trade of 0 and the Y -external mechanism will yield a gain from trade of 100.
Conversely, if the buyers values are 100, 1 and the sellers are 2, 3 the X-external mechanism will yield a gain from trade of 98 and and the Y -external mechanism will yield a gain from trade of zero.
The example clearly shows that the difference between the X-external and the Y -external mechanism is unbounded although as shown above the fraction each of them reduces can be bound and therefore the multiplicative ratio between them can be bound (as a function of the number of trades).
On the revenue side we can not even bound the ratio as seen from the following example: Example C.2. Consider k buyers with value 100 and k+ 1 sellers with value 1.
If X = {buyers} then there is no need to reduce any trade and all of the buyer receive the good and pay 1. k + 1 of the sellers sell and each of them receive 1. This yields a net revenue of zero.
If Y = {sellers} then one must reduce a trade! This means that all of the buyers pay 100 while all of the sellers still receive 1. the revenue is then 99k.
Similarly, an example can be constructed that yields much higher revenue for the X-external mechanism as compared to the Y -external mechanism.
The above examples refer to sets X and Y which do not intersect and are incomparable. The following theorem compares the X-external and Y -external mechanisms for procurement class domains where X is a subset of Y .
Theorem C.1. For procurement class domains, if X ⊂ Y and for any s ∈ S, s ∩ X ∩ Y = ∅ then:
(and hence GTR-2) is at least that of the Y -external mechanism.
and Y -external mechanisms pays no less in the Y -external than in the X-external and therefore the ratio of budget to welfare is no worse in the Y external then the X-external.
Proof. 1. For any dividing function D if there is a procurement set sj that is reduced in the X-external mechanism there are two possible reasons: (a) sj lacks external competition in the X-external mechanism. In this case sj lacks external competition in the internal mechanism. (b) sj has all required external competitions in X-external.
In this case sj has all required internal competitions in Y -external by lemma 3.1 but might lack some external competition for sj ∪ {Y \ X} and be reduced,
procurement set s that is reduced in the X-external mechanism is also reduced in the Y -external mechanism. Therefore, the critical value is no less in the Yexternal mechanism than the X-external mechanism.
Remark C.1. For any two sets X, Y it is easy to build an example in which the X-external and Y -external mechanisms reduce the same procurement sets so the inequality is weak.

The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.
Numerous empirical studies [10, 15, 13, 5] show that buyers seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.
Recent analysis, however, raises important questions regarding the ability of existing forums to reflect the real quality of a product. In the absence of clear incentives, users with a moderate outlook will not bother to voice their opinions, which leads to an unrepresentative sample of reviews.
For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped distributions where most of the ratings are either very good, or very bad. Controlled experiments, on the other hand, reveal opinions on the same items that are normally distributed. Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.
Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users. Hu et al. [12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval. The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.
In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback. Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users. The first is simple linguistic evidence from the textual review that usually accompanies the numerical ratings. We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text. We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that particular feature. Intuitively, lengthy comments reveal the importance of the feature to the user. Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.
Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel. Name of hotel and advertisements were deliberatively erased. and the reviews that preceded it. A perusal of online reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.
One may see, for example, users who make an effort to contradict, or vehemently agree with, the remarks of previous users. By analyzing the time sequence of reports, we conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of service. The subjective perception of the user is influenced by the gap between the prior expectation and the actual performance of the service [17, 18, 16, 21] which will later reflect in the user"s rating. We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.
Both results can be used to improve the way reputation mechanisms aggregate the information from individual reviews. Our first result can be used to determine a featureby-feature estimate of quality, where for each feature, a different subset of reviews (i.e., those with lengthy comments of that feature) is considered. The second leads to an algorithm that outputs a more precise estimate of the real quality.
We use in this paper real hotel reviews collected from the popular travel site TripAdvisor. TripAdvisor indexes hotels from cities across the world, along with reviews written by travelers. Users can search the site by giving the hotel"s name and location (optional). The reviews for a given hotel are displayed as a list (ordered from the most recent to the oldest), with 5 reviews per page. The reviews contain: • information about the author of the review (e.g., dates of stay, username of the reviewer, location of the reviewer); • the overall rating (from 1, lowest, to 5, highest); • a textual review containing a title for the review, free comments, and the main things the reviewer liked and disliked; • numerical ratings (from 1, lowest, to 5, highest) for different features (e.g., cleanliness, service, location, etc.) Below the name of the hotel, TripAdvisor displays the address of the hotel, general information (number of rooms, number of stars, short description, etc), the average overall rating, the TripAdvisor ranking, and an average rating for each feature. Figure 1 shows the page for a popular Boston hotel whose name (along with advertisements) was explicitly erased.
We selected three cities for this study: Boston, Sydney and Las Vegas. For each city we considered all hotels that had at least 10 reviews, and recorded all reviews. Table 1 presents the number of hotels considered in each city, the total number of reviews recorded for each city, and the distribution of hotels with respect to the star-rating (as available on the TripAdvisor site). Note that not all hotels have a star-rating.
Table 1: A summary of the data set.
City # Reviews # Hotels # of Hotels with 1,2,3,4 & 5 stars Boston 3993 58 1+3+17+15+2 Sydney 1371 47 0+0+9+13+10 Las Vegas 5593 40 0+3+10+9+6 For each review we recorded the overall rating, the textual review (title and body of the review) and the numerical rating on 7 features: Rooms(R), Service(S), Cleanliness(C),
Value(V), Food(F), Location(L) and Noise(N).
TripAdvisor does not require users to submit anything other than the overall rating, hence a typical review rates few additional features, regardless of the discussion in the textual comment. Only the features Rooms(R), Service(S),
Cleanliness(C) and Value(V) are rated by a significant number of users. However, we also selected the features Food(F),
Location(L) and Noise(N) because they are referred to in a significant number of textual comments. For each feature we record the numerical rating given by the user, or 0 when the rating is missing. The typical length of the textual comment amounts to approximately 200 words. All data was collected by crawling the TripAdvisor site in September 2006.
We will formally refer to a review by a tuple (r, T) where: • r = (rf ) is a vector containing the ratings rf ∈ {0, 1, . . . 5} for the features f ∈ F = {O, R, S, C, V, F, L, N}; note that the overall rating, rO, is abusively recorded as the rating for the feature Overall(O); • T is the textual comment that accompanies the review. 135 Reviews are indexed according to the variable i, such that (ri , Ti ) is the ith review in our database. Since we don"t record the username of the reviewer, we will also say that the ith review in our data set was submitted by user i. When we need to consider only the reviews of a given hotel, h, we will use (ri(h) , Ti(h) ) to denote the ith review about the hotel h.
COMMENTS The free textual comments associated to online reviews are a valuable source of information for understanding the reasons behind the numerical ratings left by the reviewers.
The text may, for example, reveal concrete examples of aspects that the user liked or disliked, thus justifying some of the high, respectively low ratings for certain features. The text may also offer guidelines for understanding the preferences of the reviewer, and the weights of different features when computing an overall rating.
The problem, however, is that free textual comments are difficult to read. Users are required to scroll through many reviews and read mostly repetitive information. Significant improvements would be obtained if the reviews were automatically interpreted and aggregated. Unfortunately, this seems a difficult task for computers since human users often use witty language, abbreviations, cultural specific phrases, and the figurative style.
Nevertheless, several important results use the textual comments of online reviews in an automated way. Using well established natural language techniques, reviews or parts of reviews can be classified as having a positive or negative semantic orientation. Pang et al. [2] classify movie reviews into positive/negative by training three different classifiers (Naive Bayes, Maximum Entropy and SVM) using classification features based on unigrams, bigrams or part-of-speech tags.
Dave et al. [4] analyze reviews from CNet and Amazon, and surprisingly show that classification features based on unigrams or bigrams perform better than higher-order n-grams. This result is challenged by Cui et al. [3] who look at large collections of reviews crawled from the web.
They show that the size of the data set is important, and that bigger training sets allow classifiers to successfully use more complex classification features based on n-grams.
Hu and Liu [11] also crawl the web for product reviews and automatically identify product attributes that have been discussed by reviewers. They use Wordnet to compute the semantic orientation of product evaluations and summarize user reviews by extracting positive and negative evaluations of different product features. Popescu and Etzioni [20] analyze a similar setting, but use search engine hit-counts to identify product attributes; the semantic orientation is assigned through the relaxation labeling technique.
Ghose et al. [7, 8] analyze seller reviews from the Amazon secondary market to identify the different dimensions (e.g., delivery, packaging, customer support, etc.) of reputation.
They parse the text, and tag the part-of-speech for each word. Frequent nouns, noun phrases and verbal phrases are identified as dimensions of reputation, while the corresponding modifiers (i.e., adjectives and adverbs) are used to derive numerical scores for each dimension. The enhanced reputation measure correlates better with the pricing information observed in the market. Pavlou and Dimoka [19] analyze eBay reviews and find that textual comments have an important impact on reputation premiums.
Our approach is similar to the previously mentioned works, in the sense that we identify the aspects (i.e., hotel features) discussed by the users in the textual reviews.
However, we do not compute the semantic orientation of the text, nor attempt to infer missing ratings.
We define the weight, wi f , of feature f ∈ F in the text Ti associated with the review (ri , Ti ), as the fraction of Ti dedicated to discussing aspects (both positive and negative) related to feature f. We propose an elementary method to approximate the values of these weights. For each feature we manually construct the word list Lf containing approximately 50 words that are most commonly associated to the feature f. The initial words were selected from reading some of the reviews, and seeing what words coincide with discussion of which features. The list was then extended by adding all thesaurus entries that were related to the initial words.
Finally, we brainstormed for missing words that would normally be associated with each of the features.
Let Lf ∩Ti be the list of terms common to both Lf and Ti.
Each term of Lf is counted the number of times it appears in Ti , with two exceptions: • in cases where the user submits a title to the review, we account for the title text by appending it three times to the review text Ti . The intuitive assumption is that the user"s opinion is more strongly reflected in the title, rather than in the body of the review. For example, many reviews are accurately summarized by titles such as Excellent service, terrible location or Bad value for money; • certain words that occur only once in the text are counted multiple times if their relevance to that feature is particularly strong. These were "root" words for each feature (e.g., "staff" is a root word for the feature Service), and were weighted either 2 or 3. Each feature was assigned up to 3 such root words, so almost all words are counted only once.
The list of words for the feature Rooms is given for reference in Appendix A.
The weight wi f is computed as: wi f = |Lf ∩ Ti| f∈F |Lf ∩ Ti| (1) where |Lf ∩Ti | is the number of terms common to Lf and Ti .
The weight for the feature Overall was set to min{ |T i | 5000 , 1} where |Ti | is the number of character in Ti .
The following is a TripAdvisor review for a Boston hotel (the name of the hotel is omitted): I"ll start by saying that I"m more of a Holiday Inn person than a *** type. So I get frustrated when I pay double the room rate and get half the amenities that I"d get at a Hampton Inn or Holiday Inn. The location was definitely the main asset of this place. It was only a few blocks from the Hynes Center subway stop and it was easy to walk to some good restaurants in the Back Bay area. Boylston isn"t far off at all. So I had no trouble with foregoing a rental car and taking the subway from the airport to the hotel and using the subway for any other travel.
Otherwise, they make you pay for anything and everything. 136 And when you"ve already dropped $215/night on the room, that gets frustrating.The room itself was decent, about what I would expect. Staff was also average, not bad and not excellent. Again, I think you"re paying for location and the ability to walk to a lot of good stuff. But I think next time I"ll stay in Brookline, get more amenities, and use the subway a bit more.
This numerical ratings associated to this review are rO = 3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O),
Rooms(R), Service(S), Cleanliness(C) and Value(V) respectively. The ratings for the features Food(F), Location(L) and Noise(N) are absent (i.e., rF = rL = rN = 0).
The weights wf are computed from the following lists of common terms: LR ∩ T ={room}; wR = 0.066 LS ∩ T ={3 * Staff, amenities}; wS = 0.267 LC ∩ T = ∅; wC = 0 LV ∩ T ={$, rate}; wV = 0.133 LF ∩ T ={restaurant}; wF = 0.067 LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467 LN ∩ T = ∅; wN = 0 The root words "Staff" and "Center" were tripled and doubled respectively. The overall weight of the textual review is wO = 0.197. These values account reasonably well for the weights of different features in the discussion of the reviewer.
One point to note is that some terms in the lists Lf possess an inherent semantic orientation. For example the word "grime" (belonging to the list LC ) would be used most often to assert the presence, and not the absence of grime. This is unavoidable, but care was taken to ensure words from both sides of the spectrum were used. For this reason, some lists such as LR contain only nouns of objects that one would typically describe in a room (see Appendix A).
The goal of this section is to analyse the influence of the weights wi f on the numerical ratings ri f . Intuitively, users who spent a lot of their time discussing a feature f (i.e., wi f is high) had something to say about their experience with regard to this feature. Obviously, feature f is important for user i. Since people tend to be more knowledgeable in the aspects they consider important, our hypothesis is that the ratings ri f (corresponding to high weights wi f ) constitute a subset of expert ratings for feature f.
Figure 2 plots the distribution of the rates r i(h) C with respect to the weights w i(h) C for the cleanliness of a Las Vegas hotel, h. Here, the high ratings are restricted to the reviews that discuss little the cleanliness. Whenever cleanliness appears in the discussion, the ratings are low. Many hotels exhibit similar rating patterns for various features. Ratings corresponding to low weights span the whole spectrum from 1 to 5, while the ratings corresponding to high weights are more grouped together (either around good or bad ratings).
We therefore make the following hypothesis: Hypothesis 1. The ratings ri f corresponding to the reviews where wi f is high, are more similar to each other than to the overall collection of ratings.
To test the hypothesis, we take the entire set of reviews, and feature by feature, we compute the standard deviation of the ratings with high weights, and the standard deviation of the entire set of ratings. High weights were defined as those belonging to the upper 20% of the weight range for the corresponding feature. If Hypothesis 1 were true, the standard deviation of all ratings should be higher than the standard deviation of the ratings with high weights. 0 1 2 3 4 5 6 0 0.1 0.2 0.3 0.4 0.5 0.6 Rating Weight Figure 2: The distribution of ratings against the weight of the cleanliness feature.
We use a standard T-test to measure the significance of the results. City by city and feature by feature, Table 2 presents the average standard deviation of all ratings, and the average standard deviation of ratings with high weights.
Indeed, the ratings with high weights have lower standard deviation, and the results are significant at the standard 0.05 significance threshold (although for certain cities taken independently there doesn"t seem to be a significant difference, the results are significant for the entire data set). Please note that only the features O,R,S,C and V were considered, since for the others (F, L, and N) we didn"t have enough ratings.
Table 2: Average standard deviation for all ratings, and average standard deviation for ratings with high weights. In square brackets, the corresponding p-values for a positive difference between the two.
City O R S C V all 1.189 0.998 1.144 0.935 1.123 Boston high 0.948 0.778 0.954 0.767 0.891 p-val [0.000] [0.004] [0.045] [0.080] [0.009] all 1.040 0.832 1.101 0.847 0.963 Sydney high 0.801 0.618 0.691 0.690 0.798 p-val [0.012] [0.023] [0.000] [0.377] [0.037] all 1.272 1.142 1.184 1.119 1.242 Vegas high 1.072 0.752 1.169 0.907 1.003 p-val [0.0185] [0.001] [0.918] [0.120] [0.126] Hypothesis 1 not only provides some basic understanding regarding the rating behavior of online users, it also suggests some ways of computing better quality estimates. We can, for example, construct a feature-by-feature quality estimate with much lower variance: for each feature we take the subset of reviews that amply discuss that feature, and output as a quality estimate the average rating for this subset.
Initial experiments suggest that the average feature-by-feature ratings computed in this way are different from the average ratings computed on the whole data set. Given that, indeed, high weights are indicators of expert opinions, the estimates obtained in this way are more accurate than the current ones. Nevertheless, the validation of this underlying assumption requires further controlled experiments. 137
Two important assumptions are generally made about reviews submitted to online forums. The first is that ratings truthfully reflect the quality observed by the users; the second is that reviews are independent from one another. While anecdotal evidence [9, 22] challenges the first assumption3 , in this section, we address the second.
A perusal of online reviews shows that reviews are often part of discussion threads, where users make an effort to contradict, or vehemently agree with the remarks of previous users. Consider, for example, the following review: I don"t understand the negative reviews... the hotel was a little dark, but that was the style. It was very artsy. Yes it was close to the freeway, but in my opinion the sound of an occasional loud car is better than hearing the ding ding of slot machines all night! The staff on-hand is FABULOUS. The waitresses are great (and *** does not deserve the bad review she got, she was 100% attentive to us!), the bartenders are friendly and professional at the same time...
Here, the user was disturbed by previous negative reports, addressed these concerns, and set about trying to correct them. Not surprisingly, his ratings were considerably higher than the average ratings up to this point.
It seems that TripAdvisor users regularly read the reports submitted by previous users before booking a hotel, or before writing a review. Past reviews create some prior expectation regarding the quality of service, and this expectation has an influence on the submitted review. We believe this observation holds for most online forums. The subjective perception of quality is directly proportional to how well the actual experience meets the prior expectation, a fact confirmed by an important line of econometric and marketing research [17, 18, 16, 21].
The correlation between the reviews has also been confirmed by recent research on the dynamics of online review forums [6].
We define the prior expectation of user i regarding the feature f, as the average of the previously available ratings on the feature f4 : ef (i) = j<i,r j f =0 rj f j<i,r j f =0 1 As a first hypothesis, we assert that the rating ri f is a function of the prior expectation ef (i): Hypothesis 2. For a given hotel and feature, given the reviews i and j such that ef (i) is high and ef (j) is low, the rating rj f exceeds the rating ri f .
We define high and low expectations as those that are above, respectively below a certain cutoff value θ. The set of reviews preceded by high, respectively low expectations 3 part of Amazon reviews were recognized as strategic posts by book authors or competitors 4 if no previous ratings were assigned for feature f, ef (i) is assigned a default value of 4.
Table 3: Average ratings for reviews preceded by low (first value in the cell) and high (second value in the cell) expectations. The P-values for a positive difference are given square brackets.
City O R S C V
Boston 3.364 3.590 3.485 3.641 3.242 [0.011] [0.028] [0.0086] [0.0168] [0.0034]
Sydney 3.756 3.537 3.436 3.918 3.495 [0.000] [0.000] [0.035] [0.009] [0.000]
Las Vegas 3.140 3.530 2.952 3.530 3.351 [0.190] [0.529] [0.007] [0.529] [0.253] are defined as follows: Rhigh f = {ri f |ef (i) > θ} Rlow f = {ri f |ef (i) < θ} These sets are specific for each (hotel, feature) pair, and in our experiments we took θ = 4. This rather high value is close to the average rating across all features across all hotels, and is justified by the fact that our data set contains mostly high quality hotels.
For each city, we take all hotels and compute the average ratings in the sets Rhigh f and Rlow f (see Table 3). The average rating amongst reviews following low prior expectations is significantly higher than the average rating following high expectations.
As further evidence, we consider all hotels for which the function eV (i) (the expectation for the feature Value) has a high value (greater than 4) for some i, and a low value (less than 4) for some other i. Intuitively, these are the hotels for which there is a minimal degree of variation in the timely sequence of reviews: i.e., the cumulative average of ratings was at some point high and afterwards became low, or vice-versa. Such variations are observed for about half of all hotels in each city. Figure 3 plots the median (across considered hotels) rating, rV , when ef (i) is not more than x but greater than x − 0.5.
3
4
5
Medianofrating expectation Boston Sydney Vegas Figure 3: The ratings tend to decrease as the expectation increases. 138 There are two ways to interpret the function ef (i): • The expected value for feature f obtained by user i before his experience with the service, acquired by reading reports submitted by past users. In this case, an overly high value for ef (i) would drive the user to submit a negative report (or vice versa), stemming from the difference between the actual value of the service, and the inflated expectation of this value acquired before his experience. • The expected value of feature f for all subsequent visitors of the site, if user i were not to submit a report. In this case, the motivation for a negative report following an overly high value of ef is different: user i seeks to correct the expectation of future visitors to the site.
Unlike the interpretation above, this does not require the user to derive an a priori expectation for the value of f.
Note that neither interpretation implies that the average up to report i is inversely related to the rating at report i.
There might exist a measure of influence exerted by past reports that pushes the user behind report i to submit ratings which to some extent conforms with past reports: a low value for ef (i) can influence user i to submit a low rating for feature f because, for example, he fears that submitting a high rating will make him out to be a person with low standards5 . This, at first, appears to contradict Hypothesis
indefinitely: once the set of reports project a sufficiently deflated estimate for vf , future reviewers with comparatively positive impressions will seek to correct this misconception.
expectation Further insight into the rating behavior of TripAdvisor users can be obtained by analyzing the relationship between the weights wf and the values ef (i). In particular, we examine the following hypothesis: Hypothesis 3. When a large proportion of the text of a review discusses a certain feature, the difference between the rating for that feature and the average rating up to that point tends to be large.
The intuition behind this claim is that when the user is adamant about voicing his opinion regarding a certain feature, his opinion differs from the collective opinion of previous postings. This relies on the characteristic of reputation systems as feedback forums where a user is interested in projecting his opinion, with particular strength if this opinion differs from what he perceives to be the general opinion.
To test Hypothesis 3 we measure the average absolute difference between the expectation ef (i) and the rating ri f when the weight wi f is high, respectively low. Weights are classified high or low by comparing them with certain cutoff values: wi f is low if smaller than 0.1, while wi f is high if greater than θf . Different cutoff values were used for different features: θR = 0.4, θS = 0.4, θC = 0.2, and θV = 0.7.
Cleanliness has a lower cutoff since it is a feature rarely discussed; Value has a high cutoff for the opposite reason.
Results are presented in Table 4. 5 The idea that negative reports can encourage further negative reporting has been suggested before [14] Table 4: Average of |ri f −ef (i)| when weights are high (first value in the cell) and low (second value in the cell) with P-values for the difference in sq. brackets.
City R S C V
Boston 0.701 0.838 0.760 0.917 [0.022] [0.063] [0.000] [0.218]
Sydney 0.752 0.759 0.767 0.908 [0.179] [0.009] [0.165] [0.495]
Las Vegas 0.772 0.834 0.808 1.043 [0.071] [0.020] [0.006] [0.076] This demonstrates that when weights are unusually high, users tend to express an opinion that does not conform to the net average of previous ratings. As we might expect, for a feature that rarely was a high weight in the discussion, (e.g., cleanliness) the difference is particularly large. Even though the difference in the feature Value is quite large for Sydney, the P-value is high. This is because only few reviews discussed value heavily. The reason could be cultural or because there was less of a reason to discuss this feature.
Previous models suggest that users who are not highly opinionated will not choose to voice their opinions [12]. In this section, we extend this model to account for the influence of expectations. The motivation for submitting feedback is not only due to extreme opinions, but also to the difference between the current reputation (i.e., the prior expectation of the user) and the actual experience.
Such a rating model produces ratings that most of the time deviate from the current average rating. The ratings that confirm the prior expectation will rarely be submitted.
We test on our data set the proportion of ratings that attempt to correct the current estimate. We define a deviant rating as one that deviates from the current expectation by at least some threshold θ, i.e., |ri f − ef (i)| ≥ θ. For each of the three considered cities, the following tables, show the proportion of deviant ratings for θ = 0.5 and θ = 1.
Table 5: Proportion of deviant ratings with θ = 0.5 City O R S C V Boston 0.696 0.619 0.676 0.604 0.684 Sydney 0.645 0.615 0.672 0.614 0.675 Las Vegas 0.721 0.641 0.694 0.662 0.724 Table 6: Proportion of deviant ratings with θ = 1 City O R S C V Boston 0.420 0.397 0.429 0.317 0.446 Sydney 0.360 0.367 0.442 0.336 0.489 Las Vegas 0.510 0.421 0.483 0.390 0.472 The above results suggest that a large proportion of users (close to one half, even for the high threshold value θ = 1) deviate from the prior average. This reinforces the idea that users are more likely to submit a report when they believe they have something distinctive to add to the current stream of opinions for some feature. Such conclusions are in total agreement with prior evidence that the distribution of reports often follows bi-modal, U-shaped distributions. 139
RATERS To account for the observations described in the previous sections, we propose a model for the behavior of the users when submitting online reviews. For a given hotel, we make the assumption that the quality experienced by the users is normally distributed around some value vf , which represents the objective quality offered by the hotel on the feature f. The rating submitted by user i on feature f is: ˆri f = δf vi f + (1 − δf ) · sign vi f − ef (i) c + d(vi f , ef (i)|wi f ) (2) where: • vi f is the (unknown) quality actually experienced by the user. vi f is assumed normally distributed around some value vf ; • δf ∈ [0, 1] can be seen as a measure of the bias when reporting feedback. High values reflect the fact that users rate objectively, without being influenced by prior expectations. The value of δf may depend on various factors; we fix one value for each feature f; • c is a constant between 1 and 5; • wi f is the weight of feature f in the textual comment of review i, computed according to Eq. (1); • d(vi f , ef (i)|wi f ) is a distance function between the expectation and the observation of user i. The distance function satisfies the following properties: - d(y, z|w) ≥ 0 for all y, z ∈ [0, 5], w ∈ [0, 1]; - |d(y, z|w)| < |d(z, x|w)| if |y − z| < |z − x|; - |d(y, z|w1)| < |d(y, z|w2)| if w1 < w2; - c + d(vf , ef (i)|wi f ) ∈ [1, 5]; The second term of Eq. (2) encodes the bias of the rating. The higher the distance between the true observation vi f and the function ef , the higher the bias.
We use the data set of TripAdvisor reviews to validate the behavior model presented above. We split for convenience the rating values in three ranges: bad (B = {1, 2}), indifferent (I = {3, 4}), and good (G = {5}), and perform the following two tests: • First, we will use our model to predict the ratings that have extremal values. For every hotel, we take the sequence of reports, and whenever we encounter a rating that is either good or bad (but not indifferent) we try to predict it using Eq. (2) • Second, instead of predicting the value of extremal ratings, we try to classify them as either good or bad.
For every hotel we take the sequence of reports, and for each report (regardless of it value) we classify it as being good or bad However, to perform these tests, we need to estimate the objective value, vf , that is the average of the true quality observations, vi f . The algorithm we are using is based on the intuition that the amount of conformity rating is minimized.
In other words, the value vf should be such that as often as possible, bad ratings follow expectations above vf and good ratings follow expectations below vf .
Formally, we define the sets: Γ1 = {i|ef (i) < vf and ri f ∈ B}; Γ2 = {i|ef (i) > vf and ri f ∈ G}; that correspond to irregularities where even though the expectation at point i is lower than the delivered value, the rating is poor, and vice versa. We define vf as the value that minimize these union of the two sets: vf = arg min vf |Γ1 ∪ Γ2| (3) In Eq. (2) we replace vi f by the value vf computed in Eq. (3), and use the following distance function: d(vf , ef (i)|wi f ) = |vf − ef (i)| vf − ef (i) |vf 2 − ef (i)2 | · (1 + 2wi f ); The constant c ∈ I was set to min{max{ef (i), 3}}, 4}. The values for δf were fixed at {0.7, 0.7, 0.8, 0.7, 0.6} for the features {Overall, Rooms, Service, Cleanliness, Value} respectively. The weights are computed as described in Section 3.
As a first experiment, we take the sets of extremal ratings {ri f |ri f /∈ I} for each hotel and feature. For every such rating, ri f , we try to estimate it by computing ˆri f using Eq. (2). We compare this estimator with the one obtained by simply averaging the ratings over all hotels and features: i.e., ¯rf = j,r j f =0 rj f j,r j f =0 1 ; Table 7 presents the ratio between the root mean square error (RMSE) when using ˆri f and ¯rf to estimate the actual ratings. In all cases the estimate produced by our model is better than the simple average.
Table 7: Average of RMSE(ˆrf ) RMSE(¯rf ) City O R S C V Boston 0.987 0.849 0.879 0.776 0.913 Sydney 0.927 0.817 0.826 0.720 0.681 Las Vegas 0.952 0.870 0.881 0.947 0.904 As a second experiment, we try to distinguish the sets Bf = {i|ri f ∈ B} and Gf = {i|ri f ∈ G} of bad, respectively good ratings on the feature f. For example, we compute the set Bf using the following classifier (called σ): ri f ∈ Bf (σf (i) = 1) ⇔ ˆri f ≤ 4; Tables 8, 9 and 10 present the Precision(p), Recall(r) and s = 2pr p+r for classifier σ, and compares it with a naive majority classifier, τ, τf (i) = 1 ⇔ |Bf | ≥ |Gf |: We see that recall is always higher for σ and precision is usually slightly worse. For the s metric σ tends to add a 140 Table 8: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Boston O R S C V p 0.678 0.670 0.573 0.545 0.610 σ r 0.626 0.659 0.619 0.612 0.694 s 0.651 0.665 0.595 0.577 0.609 p 0.684 0.706 0.647 0.611 0.633 τ r 0.597 0.541 0.410 0.383 0.562 s 0.638 0.613 0.502 0.471 0.595 Table 9: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Las Vegas O R S C V p 0.654 0.748 0.592 0.712 0.583 σ r 0.608 0.536 0.791 0.474 0.610 s 0.630 0.624 0.677 0.569 0.596 p 0.685 0.761 0.621 0.748 0.606 τ r 0.542 0.505 0.767 0.445 0.441 s 0.605 0.607 0.670 0.558 0.511 1-20% improvement over τ, much higher in some cases for hotels in Sydney. This is likely because Sydney reviews are more positive than those of the American cities and cases where the number of bad reviews exceeded the number of good ones are rare. Replacing the test algorithm with one that plays a 1 with probability equal to the proportion of bad reviews improves its results for this city, but it is still outperformed by around 80%.
CONCLUSION The goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the incentives that encouraged him to submit a report in the first place. For that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.
Using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the review, and the noise present in the numerical rating.
Specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating. This observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy. Nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.
Second, we emphasize the dependence of ratings on previous reports. Previous reports create an expectation of quality which affects the subjective perception of the user. We validate two facts about the hotel reviews we collected from TripAdvisor: First, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings Table 10: Precision(p), Recall(r), s= 2pr p+r while spotting poor ratings for Sydney O R S C V p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations. Intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.
Second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation). Intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.
We have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users. The final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.
The gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature. The proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.
One assumption that we make is about the existence of an objective quality value vf for the feature f. This is rarely true, especially over large spans of time. Other explanations might account for the correlation of ratings with past reports. For example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.
However, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.
Our approach to approximate this elusive "objective value" is by no means perfect, but conforms neatly to the idea behind the model.
A natural direction for future work is to examine concrete applications of our results. Significant improvements of quality estimates are likely to be obtained by incorporating all empirical evidence about rating behavior. Exactly how different factors affect the decisions of the users is not clear. The answer might depend on the particular application, context and culture.

In a range of settings where markets mediate the interactions of buyers and sellers, one observes several recurring properties: Individual buyers and sellers often trade through intermediaries, not all buyers and sellers have access to the same intermediaries, and not all buyers and sellers trade at the same price. One example of this setting is the trade of agricultural goods in developing countries.
Given inadequate transportation networks, and poor farmers" limited access to capital, many farmers have no alternative to trading with middlemen in inefficient local markets. A developing country may have many such partially overlapping markets existing alongside modern efficient markets [2].
Financial markets provide a different example of a setting with these general characteristics. In these markets much of the trade between buyers and sellers is intermediated by a variety of agents ranging from brokers to market makers to electronic trading systems. For many assets there is no one market; trade in a single asset may occur simultaneously on the floor of an exchange, on crossing networks, on electronic exchanges, and in markets in other countries. Some buyers and sellers have access to many or all of these trading venues; others have access to only one or a few of them.
The price at which the asset trades may differ across these trading venues. In fact, there is no price as different traders pay or receive different prices. In many settings there is also a gap between the price a buyer pays for an asset, the ask price, and the price a seller receives for the asset, the bid price. One of the most striking examples of this phenomenon occurs in the market for foreign exchange, where there is an interbank market with restricted access and a retail market with much more open access. Spreads, defined as the difference between bid and ask prices, differ significantly across these markets, even though the same asset is being traded in the two markets.
In this paper, we develop a framework in which such phenomena emerge from a game-theoretic model of trade, with buyers, sellers, and traders interacting on a network. The edges of the network connect traders to buyers and sellers, and thus represent the access that different market participants have to one another. The traders serve as intermediaries in a two-stage trading game: they strategically choose bid and ask prices to offer to the sellers and buyers they are connected to; the sellers and buyers then react to the prices they face. Thus, the network encodes the relative power in the structural positions of the market participants, including the implicit levels of competition among traders. We show that this game always has a 143 subgame perfect Nash equilibrium, and that all equilibria lead to an efficient (i.e. socially optimal) allocation of goods. We also analyze how trader profits depend on the network structure, essentially characterizing in graph-theoretic terms how a trader"s payoff is determined by the amount of competition it experiences with other traders.
Our work here is connected to several lines of research in economics, finance, and algorithmic game theory, and we discuss these connections in more detail later in the introduction. At a general level, our approach can be viewed as synthesizing two important strands of work: one that treats buyer-seller interaction using network structures, but without attempting to model the processses by which prices are actually formed [1, 4, 5, 6, 8, 9, 10, 13]; and another strand in the literature on market microstructure that incorporates price-setting intermediaries, but without network-type constraints on who can trade with whom [12]. By developing a network model that explicitly includes traders as price-setting agents, in a system together with buyers and sellers, we are able to capture price formation in a network setting as a strategic process carried out by intermediaries, rather than as the result of a centrally controlled or exogenous mechanism.
The Basic Model: Indistinguishable Goods. Our goal in formulating the model is to express the process of price-setting in markets such as those discussed above, where the participants do not all have uniform access to one another. We are given a set B of buyers, a set S of sellers, and a set T of traders. There is an undirected graph G that indicates who is able to trade with whom. All edges have one end in B ∪ S and the other in T; that is, each edge has the form (i, t) for i ∈ S and t ∈ T, or (j, t) for j ∈ B and t ∈ T. This reflects the constraints that all buyer-seller transactions go through traders as intermediaries.
In the most basic version of the model, we consider identical goods, one copy of which is initially held by each seller. Buyers and sellers each have a value for one copy of the good, and we assume that these values are common knowledge. We will subsequently generalize this to a setting in which goods are distinguishable, buyers can value different goods differently, and potentially sellers can value transactions with different buyers differently as well. Having different buyer valuations captures settings like house purchases; adding different seller valuations as well captures matching markets - for example, sellers as job applicants and buyers as employers, with both caring about who ends up with which good (and with traders acting as services that broker the job search).
Thus, to start with the basic model, there is a single type of good; the good comes in individisible units; and each seller initially holds one unit of the good. All three types of agents value money at the same rate; and each i ∈ B ∪ S additionally values one copy of the good at θi units of money. No agent wants more than one copy of the good, so additional copies are valued at 0. Each agent has an initial endowment of money that is larger than any individual valuation θi; the effect of this is to guarantee that any buyer who ends up without a copy of the good has been priced out of the market due to its valuation and network position, not a lack of funds.
We picture each good that is sold flowing along a sequence of two edges: from a seller to a trader, and then from the trader to a buyer. The particular way in which goods flow is determined by the following game. First, each trader offers a bid price to each seller it is connected to, and an ask price to each buyer it is connected to. Sellers and buyers then choose from among the offers presented to them by traders. If multiple traders propose the same price to a seller or buyer, then there is no strict best response for the seller or buyer. In this case a selection must be made, and, as is standard (see for example [10]), we (the modelers) choose among the best offers. Finally, each trader buys a copy of the good from each seller that accepts its offer, and it sells a copy of the good to each buyer that accepts its offer. If a particular trader t finds that more buyers than sellers accept its offers, then it has committed to provide more copies of the good than it has received, and we will say that this results in a large penalty to the trader for defaulting; the effect of this is that in equilibrium, no trader will choose bid and ask prices that result in a default.
More precisely, a strategy for each trader t is a specification of a bid price βti for each seller i to which t is connected, and an ask price αtj for each buyer j to which t is connected. (We can also handle a model in which a trader may choose not to make an offer to certain of its adjacent sellers or buyers.) Each seller or buyer then chooses at most one incident edge, indicating the trader with whom they will transact, at the indicated price. (The choice of a single edge reflects the facts that (a) sellers each initially have only one copy of the good, and (b) buyers each only want one copy of the good.) The payoffs are as follows: For each seller i, the payoff from selecting trader t is βti, while the payoff from selecting no trader is θi. (In the former case, the seller receives βti units of money, while in the latter it keeps its copy of the good, which it values at θi.) For each buyer j, the payoff from selecting trader t is θj −αtj, whle the payoff from selecting no trader is 0. (In the former case, the buyer receives the good but gives up αtj units of money.) For each trader t, with accepted offers from sellers i1, . . . , is and buyers j1, . . . , jb, the payoff is P r αtjr − P r βtir , minus a penalty π if b > s. The penalty is chosen to be large enough that a trader will never incur it in equilibrium, and hence we will generally not be concerned with the penalty.
This defines the basic elements of the game. The equilibrium concept we use is subgame perfect Nash equilibrium.
Some Examples. To help with thinking about the model, we now describe three illustrative examples, depicted in Figure 1. To keep the figures from getting too cluttered, we adopt the following conventions: sellers are drawn as circles in the leftmost column and will be named i1, i2, . . . from top to bottom; traders are drawn as squares in the middle column and will be named t1, t2, . . . from top to bottom; and buyers are drawn as circles in the rightmost column and will be named j1, j2, . . . from top to bottom. All sellers in the examples will have valuations for the good equal to 0; the valuation of each buyer is drawn inside its circle; and the bid or ask price on each edge is drawn on top of the edge.
In Figure 1(a), we show how a standard second-price auction arises naturally from our model. Suppose the buyer valuations from top to bottom are w > x > y > z. The bid and ask prices shown are consistent with an equilibrium in which i1 and j1 accept the offers of trader t1, and no other buyer accepts the offer of its adjacent trader: thus, trader t1 receives the good with a bid price of x, and makes w − x by selling the good to buyer j1 for w. In this way, we can consider this particular instance as an auction for a single good in which the traders act as proxies for their adjacent buyers. The buyer with the highest valuation for the good ends up with it, and the surplus is divided between the seller and the associated trader.
Note that one can construct a k-unit auction with > k buyers just as easily, by building a complete bipartite graph on k sellers and traders, and then attaching each trader to a single distinct buyer.
In Figure 1(b), we show how nodes with different positions in the network topology can achieve different payoffs, even when all 144 w x y z x w x x y y z z (a) Auction 1 1 1 0 x x 0 1 x x 1 (b) Heterogeneous outcomes 1 1 1 0 x x 0 1 x x 1 (c) Implicit perfect competition Figure 1: (a) An auction, mediated by traders, in which the buyer with the highest valuation for the good ends up with it. (b) A network in which the middle seller and buyer benefit from perfect competition between the traders, while the other sellers and buyers have no power due to their position in the network. (c) A form of implicit perfect competition: all bid/ask spreads will be zero in equilibrium, even though no trader directly competes with any other trader for the same buyer-seller pair. buyer valuations are the same numerically. Specifically, seller i2 and buyer j2 occupy powerful positions, because the two traders are competing for their business; on the other hand, the other sellers and buyers are in weak positions, because they each have only one option. And indeed, in every equilibrium, there is a real number x ∈ [0, 1] such that both traders offer bid and ask prices of x to i2 and j2 respectively, while they offer bids of 0 and asks of 1 to the other sellers and buyers. Thus, this example illustrates a few crucial ingredients that we will identify at a more general level shortly. Specifically, i2 and j2 experience the benefits of perfect competition, in that the two traders drive the bid-ask spreads to 0 in competing for their business. On the other hand, the other sellers and buyers experience the downsides of monopoly - they receive 0 payoff since they have only a single option for trade, and the corresponding trader makes all the profit. Note further how this natural behavior emerges from the fact that traders are able to offer different prices to different agents - capturing the fact that there is no one fixed price in the kinds of markets that motivate the model, but rather different prices reflecting the relative power of the different agents involved.
The previous example shows perhaps the most natural way in which a trader"s profit on a particular transaction can drop to 0: when there is another trader who can replicate its function precisely. (In that example, two traders each had the ability to move a copy of the good from i2 to j2.) But as our subsequent results will show, traders make zero profit more generally due to global, graph-theoretic reasons. The example in Figure 1(c) gives an initial indication of this: one can show that for every equilibrium, there is a y ∈ [0, 1] such that every bid and every ask price is equal to y. In other words, all traders make zero profit, whether or not a copy of the good passes through them - and yet, no two traders have any seller-buyer paths in common. The price spreads have been driven to zero by a global constraint imposed by the long cycle through all the agents; this is an example of implicit perfect competition determined by the network topology.
Extending the Model to Distinguishable Goods. We extend the basic model to a setting with distinguishable goods, as follows.
Instead of having each agent i ∈ B ∪ S have a single numerical valuation θi, we index valuations by pairs of buyers and sellers: if buyer j obtains the good initially held by seller i, it gets a utility of θji, and if seller i sells its good to buyer j, it experiences a loss of utility of θij . This generalizes the case of indistinguishable goods, since we can always have these pairwise valuations depend only on one of the indices. A strategy for a trader now consists of offering a bid to each seller that specifies both a price and a buyer, and offering an ask to each buyer that specifies both a price and a seller. (We can also handle a model in which a trader offers bids (respectively, asks) in the form of vectors, essentially specifying a menu with a price attached to each buyer (resp. seller).) Each buyer and seller selects an offer from an adjacent trader, and the payoffs to all agents are determined as before.
This general framework captures matching markets [10, 13]: for example, a job market that is mediated by agents or employment search services (as in hiring for corporate executives, or sports or entertainment figures). Here the sellers are job applicants, buyers are employers, and traders are the agents that mediate the job market. Of course, if one specifies pairwise valuations on buyers but just single valuations for sellers, we model a setting where buyers can distinguish among the goods, but sellers don"t care whom they sell to - this (roughly) captures settings like housing markets.
Our Results. Our results will identify general forms of some of the principles noted in the examples discussed above - including the question of which buyers end up with the good; the question of how payoffs are differently realized by sellers, traders, and buyers; and the question of what structural properties of the network determine whether the traders will make positive profits.
To make these precise, we introduce the following notation. Any outcome of the game determines a final allocation of goods to some of the agents; this can be specified by a collection M of triples (ie, te, je), where ie ∈ S, te ∈ T, and je ∈ B; moreover, each seller and each buyer appears in at most one triple. The meaning is for each e ∈ M, the good initially held by ie moves to je through te. (Sellers appearing in no triple keep their copy of the good.) We say that the value of the allocation is equal to P e∈M θjeie − θieje . Let θ∗ denote the maximum value of any allocation M that is feasible given the network.
We show that every instance of our game has an equilibrium, and that in every such equilibrium, the allocation has value θ∗  145 in other words, it achieves the best value possible. Thus, equilibria in this model are always efficient, in that the market enables the right set of people to get the good, subject to the network constraints. We establish the existence and efficiency of equilibria by constructing a linear program to capture the flow of goods through the network; the dual of this linear program contains enough information to extract equilibrium prices.
By the definition of the game, the value of the equilibrium allocation is divided up as payoffs to the agents, and it is interesting to ask how this value is distributed - in particular how much profit a trader is able to make based on its position in the network. We find that, although all equilibria have the same value, a given trader"s payoff can vary across different equilibria. However, we are able to characterize the maximum and minimum amounts that a given trader is able to make, where these maxima and minima are taken over all equilibria, and we give an efficient algorithm to compute this. In particular, our results here imply a clean combinatorial characterization of when a given trader t can achieve non-zero payoff: this occurs if and only there is some edge e incident to t that is essential, in the sense that deleting e reduces the value of the optimal allocation θ∗ . We also obtain results for the sum of all trader profits.
Related Work. The standard baseline approach for analyzing the interaction of buyers and sellers is the Walrasian model in which anonymous buyers and sellers trade a good at a single market clearing price. This reduced form of trade, built on the idealization of a market price, is a powerful model which has led to many insights.
But it is not a good model to use to examine where prices come from or exactly how buyers and sellers and trade with each other.
The difficulty is that in the Walrasian model there is no agent who sets the price, and agents don"t actually trade with each other. In fact there is no market, in the everyday sense of that word, in the Walrasian model. That is, there is no physical or virtual place where buyers and sellers interact to trade and set prices. Thus in this simple model, all buyers and sellers are uniform and trade at the same price, and there is also no role for intermediaries.
There are several literatures in economics and finance which examine how prices are set rather than just determining equilibrium prices. The literature on imperfect competition is perhaps the oldest of these. Here a monopolist, or a group of oliogopolists, choose prices in order to maximize their profits (see [14] for the standard textbook treatment of these markets). A monopolist uses its knowledge of market demand to choose a price, or a collection of prices if it discriminates. Oliogopolists play a game in which their payoffs depend on market demand and the actions of their competitors.
In this literature there are agents who set prices, but the fiction of a single market is maintained. In the equilibrium search literature, firms set prices and consumers search over them (see [3]).
Consumers do end up paying different prices, but all consumers have access to all firms and there are no intermediaries. In the general equilibrium literature there have been various attempts to introduce price determination. A standard proof technique for the existence of competitive equilibrium involves a price adjustment mechanism in which prices respond to excess demand. The Walrasian auctioneer is often introduced as a device to explain how this process works, but this is a fundamentally a metaphor for an iterative priceupdating algorithm, not for the internals of an actual market. More sophisticated processes have been introduced to study the stability of equilibrium prices or the information necessary to compute them. But again there are no price-setting agents here.
In the finance literature the work on market microstructure does have price-setting agents (specialists), parts of it do determine separate bid and ask prices, and different agents receive different prices for the same asset (see [12] for a treatment of microstructure theory). Work in information economics has identified similar phenomena (see e.g. [7]). But there is little research in these literatures examining the effect of restrictions on who can trade with whom.
There have been several approaches to studying how network structure determines prices. These have posited price determination through definitions based on competitive equilibrium or the core, or through the use of truthful mechanisms. In briefly reviewing this work, we will note the contrast with our approach, in that we model prices as arising from the strategic behavior of agents in the system.
In recent work, Kakade et al. [8] have studied the distribution of prices at competitive equilibrium in a bipartite graph on buyers and sellers, generated using a probabilistic model capable of producing heavy-tailed degree distributions [11]. Even-Dar et al. [6] build on this to consider the strategic aspects of network formation when prices arise from competitive equilibrium.
Leonard [10], Babaioff et al. [1], and Chu and Shen [4] consider an approach based on mechanism design: buyers and sellers reside at different nodes in a graph, and they incur a given transportation cost to trade with one another. Leonard studies VCG prices in this setting; Babaioff et al. and Chu and Shen additionally provide a a budget-balanced mechanism. Since the concern here is with truthful mechanisms that operate on private valuations, there is an inherent trade-off between the efficiency of the allocation and the budget-balance condition.
In contrast, our model has known valuations and prices arising from the strategic behavior of traders. Thus, the assumptions behind our model are in a sense not directly comparable to those underlying the mechanism design approach: while we assume known valuations, we do not require a centralized authority to impose a mechanism. Rather, price-setting is part of the strategic outcome, as in the real markets that motivate our work, and our equilibria are simultaneously budget-balanced and efficient - something not possible in the mechanism design frameworks that have been used.
Demange, Gale, and Sotomayor [5], and Kranton and Minehart [9], analyze the prices at which trade occurs in a network, working within the framework of mechanism design. Kranton and Minehart use a bipartite graph with direct links between buyers and sellers, and then use an ascending auction mechanism, rather than strategic intermediaries, to determine the prices. Their auction has desirable equilibrium properties but as Kranton and Minehart note it is an abstraction of how goods are allocated and prices are determined that is similar in spirit to the Walrasian auctioneer abstraction. In fact, we can show how the basic model of Kranton and Minehart can be encoded as an instance of our game, with traders producing prices at equilibrium matching the prices produced by their auction mechanism.1 Finally, the classic results of Shapley and Shubik [13] on the assignment game can be viewed as studying the result of trade on a bipartite graph in terms of the core. They study the dual of a linear program based on the matching problem, similar to what we use for a reduced version of our model in the next section, but their focus is different as they do not consider agents that seek to set prices.
For understanding the ideas behind the analysis of the general model, it is very useful to first consider a special case with a re1 Kranton and Minehart, however, can also analyze a more general setting in which buyers values are private and thus buyers and sellers play a game of incomplete information. We deal only with complete information. 146 stricted form of traders that we refer to as pair-traders. In this case, each trader is connected to just one buyer and one seller. (Thus, it essentially serves as a trade route between the two.) The techniques we develop to handle this case will form a useful basis for reasoning about the case of traders that may be connected arbitrarily to the sellers and buyers.
We will relate profits in a subgame perfect Nash equilibrium to optimal solutions of a certain linear program, use this relation to show that all equilibria result in efficient allocation of the goods, and show that a pure equilibrium always exists. First, we consider the simplest model where sellers have indistinguishable items, and each buyer is interested in getting one item. Then we extend the results to the more general case of a matching market, as discussed in the previous section, where valuations depend on the identity of the seller and buyer. We then characterize the minimum and maximum profits traders can make. In the next section, we extend the results to traders that may be connected to any subset of sellers and buyers.
Given that we are working with pair-traders in this section, we can represent the problem using a bipartite graph G whose node set is B ∪ S, and where each trader t, connecting seller i and buyer j, appears as an edge t = (i, j) in G. Note, however, that we allow multiple traders to connect the same pair of agents. For each buyer and seller i, we will use adj(i) to denote the set of traders who can trade with i.
The socially optimal trade for the case of indistinguishable goods is the solution of the transportation problem: sending goods along the edges representing the traders. The edges along which trade occurs correspond to a matching in this bipartite graph, and the optimal trade is described by the following linear program. max SV (x) = X t∈T :t=(i,j) xt(θj − θi) xt ≥ 0 ∀t ∈ T X t∈adj(i) xt ≤ 1 ∀i ∈ S X t∈adj(j) xt ≤ 1 ∀j ∈ B Next we consider an equilibrium. Each trader t = (i, j) must offer a bid βt and an ask αt. (We omit the subscript denoting the seller and buyer here since we are dealing with pair-traders.) Given the bid and ask price, the agents react to these prices, as described earlier. Instead of focusing on prices, we will focus on profits. If a seller i sells to a trader t ∈ adj(i) with bid βt then his profit is pi = βt − θi. Similarly, if a buyer j buys from a trader t ∈ adj(j) with ask αt, then his profit is pj = θj − αt. Finally, if a trader t trades with ask αt and bid βt then his profit is yt = αt − βt. All agents not involved in trade make 0 profit. We will show that the profits at equilibrium are an optimal solution to the following linear program. min sum(p, y) = X i∈B∪S pi + X t∈T yt yt ≥ 0 ∀t ∈ T : pi ≥ 0 ∀i ∈ S ∪ B : yt ≥ (θj − pj) − (θi + pi) ∀t = (i, j) ∈ T LEMMA 2.1. At equilibrium the profits must satisfy the above inequalities.
Proof. Clearly all profits are nonnegative, as trading is optional for all agents.
To see why the last set of inequalities holds, consider two cases separately. For a trader t who conducted trade, we get equality by definition. For other traders t = (i, j), the value pi +θi is the price that seller i sold for (or θi if seller i decided to keep the good).
Offering a bid βt > pi + θi would get the seller to sell to trader t.
Similarly, θj − pj is the price that buyer j bought for (or θj if he didn"t buy), and for any ask αt < θj − pj, the buyer will buy from trader t. So unless θj − pj ≤ θi + pi the trader has a profitable deviation.
Now we are ready to prove our first theorem: THEOREM 2.2. In any equilibrium the trade is efficient.
Proof. Let x be a flow of goods resulting in an equilibrium, and let variables p and y be the profits.
Consider the linear program describing the socially optimal trade.
We will also add a set of additional constraints xt ≤ 1 for all traders t ∈ T; this can be added to the description, as it is implied by the other constraints. Now we claim that the two linear programs are duals of each other. The variables pi for agents B ∪ S correspond to the equations P t∈adj(i) xt ≤ 1. The additional dual variable yt corresponds to an additional inequality xt ≤ 1.
The optimality of the social value of the trade will follow from the claim that the solution of these two linear programs derived from an equilibrium satisfy the complementary slackness conditions for this pair of linear programs, and hence both x and (p, y) are optimal solutions to the corresponding linear programs.
There are three different complementary slackness conditions we need to consider, corresponding to the three sets of variables x, y and p. Any agent can only make profit if he transacts, so pi > 0 implies P t∈adj(i) xt = 1, and similarly, yt > 0 implies that xt = 1 also. Finally, consider a trader t with xt > 0 that trades between seller i and buyer j, and recall that we have seen above that the inequality yt ≥ (θj − pj) − (θi + pi) is satisfied with equality for those who trade.
Next we argue that equilibria always exist.
THEOREM 2.3. For any efficient trade between buyers and sellers there is a pure equilibrium of bid-ask values that supports this trade.
Proof. Consider an efficient trade; let xt = 1 if t trades and 0 otherwise; and consider an optimal solution (p, y) to the dual linear program.
We would like to claim that all dual solutions correspond to equilibrium prices, but unfortunately this is not exactly true. Before we can convert a dual solution to equilibrium prices, we may need to modify the solution slightly as follows. Consider any agent i that is only connected to a single trader t. Because the agent is only connected to a single trader, the variables yt and pi are dual variables corresponding to the same primal inequality xt ≤ 1, and they always appear together as yt + pi in all inequalities, and also in the objective function. Thus there is an optimal solution in which pi = 0 for all agents i connected only to a single trader.
Assume (p, y) is a dual solution where agents connected only to one trader have pi = 0. For a seller i, let βt = θi + pi be the bid for all traders t adjacent to i. Similarly, for each buyer j, let αt = θj − pj be the ask for all traders t adjacent to j. We claim that this set of bids and asks, together with the trade x, are an equilibrium. To see why, note that all traders t adjacent to a seller or buyer i offer the same ask or bid, and so trading with any trader is equally good for agent i. Also, if i is not trading in the solution 147 x then by complementary slackness pi = 0, and hence not trading is also equally good for i. This shows that sellers and buyers don"t have an incentive to deviate.
We need to show that traders have no incentive to deviate either.
When a trader t is trading with seller i and buyer j, then profitable deviations would involve increasing αt or decreasing βt. But by our construction (and assumption about monopolized agents) all sellers and buyers have multiple identical ask/bid offers, or trade is occurring at valuation. In either case such a deviation cannot be successful.
Finally, consider a trader t = (i, j) who doesn"t trade. A deviation for t would involve offering a lower ask to seller i and a higher bid to seller j than their current trade. However, yt = 0 by complementary slackness, and hence pi + θi ≥ θj − pj, so i sells for a price at least as high as the price at which j buys, so trader t cannot create profitable trade.
Note that a seller or buyer i connected to a single trader t cannot have profit at equilibrium, so possible equilibrium profits are in one-to-one correspondence with dual solutions for which pi = 0 whenever i is monopolized by one trader.
A disappointing feature of the equilibrium created by this proof is that some agents t may have to create ask-bid pairs where βt > αt, offering to buy for more than the price at which they are willing to sell. Agents that make such crossing bid-ask pairs never actually perform a trade, so it does not result in negative profit for the agent, but such pairs are unnatural. Crossing bid-ask pairs are weakly dominated by the strategy of offering a low bid β = 0 and an extremely high ask to guarantee that neither is accepted.
To formulate a way of avoiding such crossing pairs, we say an equilibrium is cross-free if αt ≥ βt for all traders t. We now show there is always a cross-free equilibrium.
THEOREM 2.4. For any efficient trade between buyers and sellers there is a pure cross-free equilibrium.
Proof. Consider an optimal solution to the dual linear program.
To get an equilibrium without crossing bids, we need to do a more general modification than just assuming that pi = 0 for all sellers and buyers connected to only a single trader. Let the set E be the set of edges t = (i, j) that are tight, in the sense that we have the equality yt = (θj − pj) − (θi + pi). This set E contain all the edges where trade occurs, and some more edges. We want to make sure that pi = 0 for all sellers and buyers that have degree at most 1 in E. Consider a seller i that has pi > 0. We must have i involved in a trade, and the edge t = (i, j) along which the trade occurs must be tight. Suppose this is the only tight edge adjacent to agent i; then we can decrease pi and increase yt till one of the following happens: either pi = 0 or the constraint of some other agent t ∈ adj(i) becomes tight. This change only increases the set of tight edges E, keeps the solution feasible, and does not change the objective function value. So after doing this for all sellers, and analogously changing yt and pj for all buyers, we get an optimal solution where all sellers and buyers i either have pi = 0 or have at least two adjacent tight edges.
Now we can set asks and bids to form a cross-free equilibrium.
For all traders t = (i, j) associated with an edge t ∈ E we set αt and βt as before: we set the bid βt = pi + θi and the ask αt = θj −pj. For a trader t = (i, j) ∈ E we have that pi +θi > θj −pj and we set αt = βt to be any value in the range [θj − pj, pi + θi].
This guarantees that for each seller or buyer the best sell or buy offer is along the edge where trade occurs in the solution. The askbid values along the tight edges guarantee that traders who trade cannot increase their spread. Traders t = (i, j) who do not trade cannot make profit due to the constraint pi + θi ≥ θj − pj 1 1 1 0 0 1 0 1 1 0 0 0 1 (a) No trader profit 1 1 1 0 x x x x 1 x x 0 x (b) Trader profit Figure 2: Left: an equilibrium with crossing bids where traders make no money. Right: an equilibrium without crossing bids for any value x ∈ [0, 1]. Total trader profit ranges between 1 and 2.
We now consider the case of distinguishable goods. As in the previous section, we can write a transshipment linear program for the socially optimal trade, with the only change being in the objective function. max SV (x) = X t∈T :t=(i,j) xt(θji − θij) We can show that the dual of this linear program corresponds to trader profits. Recall that we needed to add the constraints xt ≤ 1 for all traders. The dual is then: min sum(p, y) = X i∈B∪S pi + X t∈T yt yt ≥ 0 ∀t ∈ T : pi ≥ 0 ∀i ∈ S ∪ B : yt ≥ (θji − pj) − (θij + pi) ∀t = (i, j) ∈ T It is not hard to extend the proofs of Theorems 2.2 - 2.4 to this case.
Profits in an equilibrium satisfy the dual constraints, and profits and trade satisfy complementary slackness. This shows that trade is socially optimal. Taking an optimal dual solution where pi = 0 for all agents that are monopolized, we can convert it to an equilibrium, and with a bit more care, we can also create an equilibrium with no crossing bid-ask pairs.
THEOREM 2.5. All equilibria for the case of pair-traders with distinguishable goods result in socially optimal trade. Pure noncrossing equilibria exist.
We have seen that all equilibria are efficient. However, it turns out that equilibria may differ in how the value of the allocation is spread between the sellers, buyers and traders. Figure 2 depicts a simple example of this phenomenon.
Our goal is to understand how a trader"s profit is affected by its position in the network; we will use the characterization we obtained to work out the range of profits a trader can make. To maximize the profit of a trader t (or a subset of traders T ) all we need to do is to find an optimal solution to the dual linear program maximizing the value of yt (or the sum P t∈T yt). Such dual solutions will then correspond to equilibria with non-crossing prices. 148 THEOREM 2.6. For any trader t or subset of traders T the maximum total profit they can make in any equilibrium can be computed in polynomial time. This maximum profit can be obtained by a non-crossing equilibrium.
One way to think about the profit of a trader t = (i, j) is as a subtraction from the value of the corresponding edge (i, j). The value of the edge is the social value θji − θij if the trader makes no profit, and decreases to θji − θij − yt if the trader t insists on making yt profit. Trader t gets yt profit in equilibrium, if after this decrease in the value of the edge, the edge is still included in the optimal transshipment.
THEOREM 2.7. A trader t can make profit in an equilibrium if and only if t is essential for the social welfare, that is, if deleting agent t decreases social welfare. The maximum profit he can make is exactly his value to society, that is, the increase his presence causes in the social welfare.
If we allow crossing equilibria, then we can also find the minimum possible profit. Recall that in the proof of Theorem 2.3, traders only made money off of sellers or buyers that they have a monopoly over. Allowing such equilibria with crossing bids we can find the minimum profit a trader or set of traders can make, by minimizing the value yt (or sum P t∈T yt) over all optimal solutions that satisfy pi = 0 whenever i is connected to only a single trader.
THEOREM 2.8. For any trader t or subset of traders T the minimum total profit they can make in any equilibrium can be computed in polynomial time.
Next we extend the results to a model where traders may be connected to an arbitrary number of sellers and buyers. For a trader t ∈ T we will use S(t) and B(t) to denote the set of buyers and sellers connected to trader t. In this section we focus on the general case when goods are distinguishable (i.e. both buyers and sellers have valuations that are sensitive to the identity of the agent they are paired with in the allocation). In the full version of the paper we also discuss the special case of indistinguishable goods in more detail.
To get the optimal trade, we consider the bipartite graph G = (S ∪ B, E) connecting sellers and buyers where an edge e = (i, j) connects a seller i and a buyer j if there is a trader adjacent to both: E = {(i, j) : adj(i) ∩ adj(j) = ∅}. On this graph, we then solve the instance of the assignment problem that was also used in Section 2.2, with the value of edge (i, j) equal to θji − θij (since the value of trading between i and j is independent of which trader conducted the trade). We will also use the dual of this linear program: min val(z) = X i∈B∪S zi zi ≥ 0 ∀i ∈ S ∪ B. zi + zj ≥ θji − θij ∀i ∈ S, j ∈ B : adj(i) ∩ adj(j) = ∅.
First we need to understand what bidding model we will use.
Even when goods are indistinguishable, a trader may want to pricediscriminate, and offer different bid and ask values to different sellers and buyers. In the case of distinguishable goods, we have to deal with a further complication: the trader has to name the good she is proposing to sell or buy, and can possibly offer multiple different products.
There are two variants of our model depending whether a trader makes a single bid or ask to a seller or buyer, or she offers a menu of options. (i) A trader t can offer a buyer j a menu of asks αtji, a vector of values for all the products that she is connected to, where αtji is the ask for the product of seller i. Symmetrically, a trader t can offer to each seller i a menu of bids βtij for selling to different buyers j. (ii) Alternatively, we can require that each trader t can make at most one ask to each seller and one bid for each buyer, and an ask has to include the product sold, and a bid has to offer a particular buyer to sell to.
Our results hold in either model. For notational simplicity we will use the menu option here.
Next we need to understand the optimization problem of a trader t. Suppose we have bid and ask values for all other traders t ∈ T, t = t. What are the best bid and ask offers trader t can make as a best response to the current set of bids and asks? For each seller i let pi be the maximum profit seller i can make using bids by other traders, and symmetrically assume pj is the maximum profit buyer j can make using asks by other traders (let pi = 0 for any seller or buyer i who cannot make profit). Now consider a seller-buyer pair (i, j) that trader t can connect. Trader t will have to make a bid of at least βtij = θij +pi to seller i and an ask of at most αtji = θji −pj to buyer j to get this trade, so the maximum profit she can make on this trade is vtij = αtji − βtij = θji − pj − (θij + pi). The optimal trade for trader t is obtained by solving a matching problem to find the matching between the sellers S(t) and buyers B(t) that maximizes the total value vtij for trader t.
We will need the dual of the linear program of finding the trade of maximum profit for the trader t. We will use qti as the dual variable associated with the constraint of seller or buyer i. The dual is then the following problem. min val(qt) = X i∈B(t)∪S(t) qti qti ≥ 0 ∀i ∈ S(t) ∪ B(t). qti + qtj ≥ vtij ∀i ∈ S(t), j ∈ B(t).
We view qti as the profit made by t from trading with seller or buyer i. Theorem 3.1 summarizes the above discussion.
THEOREM 3.1. For a trader t, given the lowest bids βtij and highest asks αtji that can be accepted for sellers i ∈ S(t) and buyers j ∈ B(t), the best trade t can make is the maximum value matching between S(t) and B(t) with value vtij = αtji − βtij for the edge (i, j). This maximum value is equal to the minimum of the dual linear program above.
Now we can prove trade at equilibrium is always efficient.
THEOREM 3.2. Every equilibrium results in an efficient allocation of the goods.
Proof. Consider an equilibrium, with xe = 1 if and only if trade occurs along edge e = (i, j). Trade is a solution to the transshipment linear program used in Section 2.2.
Let pi denote the profit of seller or buyer i. Each trader t currently has the best solution to his own optimization problem. A trader t finds his optimal trade (given bids and asks by all other 149 traders) by solving a matching problem. Let qti for i ∈ B(t)∪S(t) denote the optimal dual solution to this matching problem as described by Theorem 3.1.
When setting up the optimization problem for a trader t above, we used pi to denote the maximum profit i can make without the offer of trader t. Note that this pi is exactly the same pi we use here, the profit of agent i. This is clearly true for all traders t that are not trading with i in the equilibrium. To see why it is true for the trader t that i is trading with we use that the current set of bid-ask values is an equilibrium. If for any agent i the bid or ask of trader t were the unique best option, then t could extract more profit by offering a bit larger ask or a bit smaller bid, a contradiction.
We show the trade x is optimal by considering the dual solution zi = pi + P t qti for all agents i ∈ B ∪ S. We claim z is a dual solution, and it satisfies complementary slackness with trade x. To see this we need to show a few facts.
We need that zi > 0 implies that i trades. If zi > 0 then either pi > 0 or qti > 0 for some trader t. Agent i can only make profit pi > 0 if he is involved in a trade. If qti > 0 for some t, then trader t must trade with i, as his solution is optimal, and by complementary slackness for the dual solution, qti > 0 implies that t trades with i.
For an edge (i, j) associated with a trader t we need to show the dual solution is feasible, that is zi + zj ≥ θji − θij .
Recall vtij = θji −pj −(θij +pi), and the dual constraint of the trader"s optimization problem requires qti + qtj ≥ vtij.
Putting these together, we have zi + zj ≥ pi + qti + pj + qtj ≥ vtij + pi + pj = θji − θij .
Finally, we need to show that the trade variables x also satisfy the complementary slackness constraint: when xe > 0 for an edge e = (i, j) then the corresponding dual constraint is tight. Let t be the trader involved in the trade. By complementary slackness of t"s optimization problem we have qti + qtj = vtij. To see that z satisfies complementary slackness we need to argue that for all other traders t = t we have both qt i = 0 and qt j = 0. This is true as qt i > 0 implies by complementary slackness of t "s optimization problem that t must trade with i at optimum, and t = t is trading.
Next we want to show that a non-crossing equilibrium always exists. We call an equilibrium non-crossing if the bid-ask offers a trader t makes for a seller-buyer pair (i, j) never cross, that is βtij ≤ αtji for all t, i, j.
THEOREM 3.3. There exists a non-crossing equilibrium supporting any socially optimal trade.
Proof. Consider an optimal trade x and a dual solution z as before.
To find a non-crossing equilibrium we need to divide the profit zi between i and the trader t trading with i. We will use qti as the trader t"s profit associated with agent i for any i ∈ S(t) ∪ B(t).
We will need to guarantee the following properties: Trader t trades with agent i whenever qti > 0. This is one of the complementary slackness conditions to make sure the current trade is optimal for trader t.
For all seller-buyer pairs (i, j) that a trader t can trade with, we have pi + qti + pj + qtj ≥ θji − θij , (1) which will make sure that qt is a feasible dual solution for the optimization problem faced by trader t.
We need to have equality in (1) when trader t is trading between i and j. This is one of the complementary slackness conditions for trader t, and will ensure that the trade of t is optimal for the trader.
Finally, we want to arrange that each agent i with pi > 0 has multiple offers for making profit pi, and the trade occurs at one of his best offers. To guarantee this in the corresponding bids and asks we need to make sure that whenever pi > 0 there are multiple t ∈ adj(i) that have equation in the above constraint (1).
We start by setting pi = zi for all i ∈ S ∪ B and qti = 0 for all i ∈ S ∪ B and traders t ∈ adj(i). This guarantees all invariants except the last property about multiple t ∈ adj(t) having equality in (1). We will modify p and q to gradually enforce the last condition, while maintaining the others.
Consider a seller with pi > 0. By optimality of the trade and dual solution z, seller i must trade with some trader t, and that trader will have equality in (1) for the buyer j that he matches with i. If this is the only trader t that has a tight constraint in (1) involving seller i then we increase qti and decrease pi till either pi = 0 or another trader t = t will be achieve equality in (1) for some buyer edge adjacent to i (possibly a different buyer j ). This change maintains all invariants, and increases the set of sellers that also satisfy the last constraint. We can do a similar change for a buyer j that has pj > 0 and has only one trader t with a tight constraint (1) adjacent to j. After possibly repeating this for all sellers and buyers, we get profits satisfying all constraints.
Now we get equilibrium bid and ask values as follows. For a trader t that has equality for the seller-buyer pair (i, j) in (1) we offer αtji = θji − pj and βtij = θij + pi. For all other traders t and seller-buyer pairs (i, j) we have the invariant (1), and using this we know we can pick a value γ in the range θij +pi+qti ≥ γ ≥ θji − (pj + qtj ). We offer bid and ask values βtij = αtji = γ.
Neither the bid nor the ask will be the unique best offer for the buyer, and hence the trade x remains an equilibrium.
Finally we turn to the goal of understanding, in the case of general traders, how a trader"s profit is affected by its position in the network.
First, we show how to maximize the total profit of a set of traders.
The profit of trader t in an equilibrium is P i qti. To find the maximum possible profit for a trader t or a set of traders T , we need to do the following: Find profits pi ≥ 0 and qti > 0 so that zi = pi + P t∈adj(i) qti is an optimal dual solution, and also satisfies the constraints (1) for any seller i and buyer j connected through a trader t ∈ T. Now, subject to all these conditions, we maximize the sum P t∈T P i∈S(t)∪B(t) qti. Note that this maximization is a secondary objective function to the primary objective that z is an optimal dual solution. Then we use the proof of Theorem 3.3 shows how to turn this into an equilibrium.
THEOREM 3.4. The maximum value for P t∈T P i qti above is the maximum profit the set T of traders can make.
Proof. By the proof of Theorem 3.2 the profits of trader t can be written in this form, so the set of traders T cannot make more profit than claimed in this theorem.
To see that T can indeed make this much profit, we use the proof of Theorem 3.3. We modify that proof to start with profit vectors p and qt for t ∈ T , and set qt = 0 for all traders t ∈ T . We verify that this starting solution satisfies the first three of the four required properties, and then we can follow the proof to make the fourth property true. We omit the details of this in the present version.
In Section 2.3 we showed that in the case of pair traders, a trader t can make money if he is essential for efficient trade. This is not 150 1 1 Figure 3: The top trader is essential for social welfare. Yet the only equilibrium is to have bid and ask values equal to 0, and the trader makes no profit. true for the type of more general traders we consider here, as shown by the example in Figure 3.
However, we still get a characterization for when a trader t can make a positive profit.
THEOREM 3.5. A trader t can make profit in an equilibrium if and only if there is a seller or buyer i adjacent to t such that the connection of trader t to agent i is essential for social welfarethat is, if deleting agent t from adj(i) decreases the value of the optimal allocation.
Proof. First we show the direction that if a trader t can make money there must be an agent i so that t"s connection to i is essential to social welfare. Let p, q be the profits in an equilibrium where t makes money, as described by Theorem 3.2 with P i∈S(t)∪B(t) qti > 0.
So we have some agent i with qti > 0. We claim that the connection between agent i and trader t must be essential, in particular, we claim that social welfare must decrease by at least qti if we delete t from adj(t). To see why note that decreasing the value of all edges of the form (i, j) associated with trader t by qti keeps the same trade optimum, as we get a matching dual solution by simply resetting qti to zero.
To see the opposite, assume deleting t from adj(t) decreases social welfare by some value γ. Assume i is a seller (the case of buyers is symmetric), and decrease by γ the social value of each edge (i, j) for any buyer j such that t is the only agent connecting i and j. By assumption the trade is still optimal, and we let z be the dual solution for this matching. Now we use the same process as in the proof of Theorem 3.3 to create a non-crossing equilibrium starting with pi = zi for all i ∈ S ∪B, and qti = γ, and all other q values 0. This creates an equilibrium with non-crossing bids where t makes at least γ profit (due to trade with seller i).
Finally, if we allow crossing equilibria, then we can find the minimum possible profit by simply finding a dual solution minimizing the dual variables associated with agents monopolized by some trader.
THEOREM 3.6. For any trader t or subset of traders T , the minimum total profit they can make in any equilibrium can be computed in polynomial time.
[1] M. Babaioff, N. Nisan, E. Pavlov. Mechanisms for a Spatially Distributed Market. ACM EC Conference, 2005. [2] C. Barrett, E. Mutambatsere. Agricultural markets in developing countries. The New Palgrave Dictionary of Economics, 2nd edition, forthcoming. [3] Kenneth Burdett and Kenneth Judd. Equilibrium Price Disperison. Econometrica, 51/4, July 1983, 955-969. [4] L. Chu, Z.-J. Shen. Agent Competition Double Auction Mechanism. Management Science, 52/8, 2006. [5] G. Demange, D. Gale, M. Sotomayor. Multi-item auctions. J.
Political Econ. 94(1986). [6] E. Even-Dar, M. Kearns, S. Suri. A Network Formation Game for Bipartite Exchange Economies. ACM-SIAM Symp. on Discrete Algorithms (SODA), 2007. [7] J. Kephart, J. Hanson, A. Greenwald. Dynamic Pricing by Software Agents. Computer Networks, 2000. [8] S. Kakade, M. Kearns, L. Ortiz, R. Pemantle, S. Suri.
Economic Properties of Social Networks. NIPS 2004. [9] R. Kranton, D. Minehart. A Theory of Buyer-Seller Networks. American Economic Review 91(3), June 2001. [10] H. Leonard. Elicitation of Honest Preferences for the Assignment of Individuals to Positions. J. Pol. Econ, 1983. [11] M. E. J. Newman. The structure and function of complex networks. SIAM Review, 45:167-256, 2003. [12] M. O"Hara. Market Microstructure Theory. Blackwell Publishers, Cambridge, MA, 1995. [13] L. Shapley M. Shubik, The Assignment Game I: The Core.

Combinatorial auctions. Combinatorial auctions are well-known mechanisms for resource and task allocation where bidders are allowed to simultaneously bid on combinations of items. This is desirable when a bidder"s valuation of a bundle of items is not equal to the sum of her valuations of the individual items. This framework is currently used to regulate agents" interactions in several application domains (cf., e.g., [21]) such as, electricity markets [13], bandwidth auctions [14], and transportation exchanges [18].
Formally, a combinatorial auction is a pair I, B , where I = {I1, ..., Im} is the set of items the auctioneer has to sell, and B = {B1, ..., Bn} is the set of bids from the buyers interested in the items in I. Each bid Bi has the form item(Bi), pay(Bi) , where pay(Bi) is a rational number denoting the price a buyer offers for the items in item(Bi) ⊆ I. An outcome for I, B is a subset b of B such that item(Bi)∩item(Bj) = ∅, for each pair Bi and Bj of bids in b with i = j.
The winner determination problem. A crucial problem for combinatorial auctions is to determine the outcome b∗ that maximizes the sum of the accepted bid prices (i.e.,
Bi∈b∗ pay(Bi)) over all the possible outcomes. This problem, called winner determination problem (e.g., [11]), is known to be intractable, actually NP-hard [17], and even not approximable in polynomial time unless NP = ZPP [19].
Hence, it comes with no surprise that several efforts have been spent to design practically efficient algorithms for general auctions (e.g., [20, 5, 2, 8, 23]) and to identify classes of instances where solving the winner determination problem is feasible in polynomial time (e.g., [15, 22, 12, 21]). In fact, constraining bidder interaction was proven to be useful for identifying classes of tractable combinatorial auctions.
Item graphs. Currently, the most general class of tractable combinatorial auctions has been singled out by modelling interactions among bidders with the notion of item graph, which is a graph whose nodes are in one-to-one correspondence with items, and edges are such that for any 152 Figure 1: Example MaxWSP problem: (a) Hypergraph H I0,B0 , and a packing h for it; (b) Primal graph for H I0,B0 ; and, (c,d) Two item graphs for H I0,B0 . bid, the items occurring in it induce a connected subgraph.
Indeed, the winner determination problem was proven to be solvable in polynomial time if interactions among bidders can be represented by means of a structured item graph, i.e., a tree or, more generally, a graph having tree-like structure [3]-formally bounded treewidth [16].
To have some intuition on how item graphs can be built, we notice that bidder interaction in a combinatorial auction I, B can be represented by means of a hypergraph H I,B such that its set of nodes N(H I,B ) coincides with set of items I, and where its edges E(H I,B ) are precisely the bids of the buyers {item(Bi) | Bi ∈ B}. A special item graph for I, B is the primal graph of H I,B , denoted by G(H I,B ), which contains an edge between any pair of nodes in some hyperedge of H I,B . Then, any item graph for H I,B can be viewed as a simplification of G(H I,B ) obtained by deleting some edges, yet preserving the connectivity condition on the nodes included in each hyperedge.
Example 1. The hypergraph H I0,B0 reported in Figure 1.(a) is an encoding for a combinatorial auction I0, B0 , where I0 = {I1, ..., I5}, and item(Bi) = hi, for each 1 ≤ i ≤ 3. The primal graph for H I0,B0 is reported in Figure 1.(b), while two example item graphs are reported in Figure 1.(c) and (d), where edges required for maintaining the connectivity for h1 are depicted in bold. ¡ Open Problem: Computing structured item graphs efficiently. The above mentioned tractability result on structured item graphs turns out to be useful in practice only when a structured item graph either is given or can be efficiently determined. However, exponentially many item graphs might be associated with a combinatorial auction, and it is not clear how to determine whether a structured item graph of a certain (constant) treewidth exists, and if so, how to compute such a structured item graph efficiently.
Polynomial time algorithms to find the best simplification of the primal graph were so far only known for the cases where the item graph to be constructed is a line [10], a cycle [4], or a tree [3], but it was an important open problem (cf. [3]) whether it is tractable to check if for a combinatorial auction, an item graph of treewidth bounded by a fixed natural number k exists and can be constructed in polynomial time, if so.
Weighted Set Packing. Let us note that the hypergraph representation H I,B of a combinatorial auction I, B is also useful to make the analogy between the winner determination problem and the maximum weighted-set packing problem on hypergraphs clear (e.g., [17]).
Formally, a packing h for a hypergraph H is a set of hyperedges of H such that for each pair h, h ∈ h with h = h , it holds that h ∩ h = ∅. Letting w be a weighting function for H, i.e., a polynomially-time computable function from E(H) to rational numbers, the weight of a packing h is the rational number w(h) = h∈h w(h), where w({}) = 0. Then, the maximum-weighted set packing problem for H w.r.t. w, denoted by MaxWSP(H, w), is the problem of finding a packing for H having the maximum weight over all the packings for H. To see that MaxWSP is just a different formulation for the winner determination problem, given a combinatorial auction I, B , it is sufficient to define the weighting function w I,B (item(Bi)) = pay(Bi). Then, the set of the solutions for the weighted set packing problem for H I,B w.r.t. w I,B coincides with the set of the solutions for the winner determination problem on I, B .
Example 2. Consider again the hypergraph H I0,B0  reported in Figure 1.(a). An example packing for H I0,B0 is h = {h1}, which intuitively corresponds to an outcome for I0, B0 , where the auctioneer accepted the bid B1. By assuming that bids B1, B2, and B3 are such that pay(B1) = pay(B2) = pay(B3), the packing h is not a solution for the problem MaxWSP(H I0,B0 , w I0,B0 ). Indeed, the packing h∗ = {h2, h3} is such that w I0,B0 (h∗ ) > w I0,B0 (h). ¡ Contributions The primary aim of this paper is to identify large tractable classes for the winner determination problem, that are, moreover polynomially recognizable. Towards this aim, we first study structured item graphs and solve the open problem in [3]. The result is very bad news: It is NP complete to check whether a combinatorial auction has a structured item graph of treewidth 3. More formally, letting C(ig, k) denote the class of all the hypergraphs having an item tree of treewidth bounded by k, we prove that deciding whether a hypergraph (associated with a combinatorial auction problem) belongs to C(ig, 3) is NP-complete.
In the light of this result, it was crucial to assess whether there are some other kinds of structural requirement that can be checked in polynomial time and that can still be used to isolate tractable classes of the maximum weightedset packing problem or, equivalently, the winner determination problem. Our investigations, this time, led to very good news which are summarized below: For a hypergraph H, its dual ¯H = (V, E) is such that nodes in V are in one-to-one correspondence with hyperedges in H, and for each node x ∈ N(H), {h | x ∈ h ∧ h ∈ 153 E(H)} is in E. We show that MaxWSP is tractable on the class of those instances whose dual hypergraphs have hypertree width[7] bounded by k (short: class C(hw, k) of hypergraphs). Note that a key issue of the tractability is to consider the hypertree width of the dual hypergraph ¯H instead of the auction hypergraph H. In fact, we can show that MaxWSP remains NP-hard even when H is acyclic (i.e., when it has hypertree width 1), even when each node is contained in 3 hyperedges at most.
For some relevant special classes of hypergraphs in C(hw, k), we design a higly-parallelizeable algorithm for MaxWSP. Specifically, if the weighting functions can be computed in logarithmic space and weights are polynomial (e.g., when all the hyperegdes have unitary weights and one is interested in finding the packing with the maximum number of edges), we show that MaxWSP can be solved by a LOGCFL algorithm. Recall, in fact, that LOGCFL is the class of decision problems that are logspace reducible to context free languages, and that LOGCFL ⊆ NC2 ⊆ P (see, e.g., [9]).
Surprisingly, we show that nothing is lost in terms of generality when considering the hypertree decomposition of dual hypergraphs instead of the treewidth of item graphs. To the contrary, the proposed hypertree-based decomposition method is strictly more general than the method of structured item graphs. In fact, we show that strictly larger classes of instances are tractable according to our new approach than according to the structured item graphs approach. Intuitively, the NP-hardness of recognizing bounded-width structured item graphs is thus not due to its great generality, but rather to some peculiarities in its definition.
The proof of the above results give us some interesting insight into the notion of structured item graph. Indeed, we show that structured item graphs are in one-to-one correspondence with some special kinds of hypertree decomposition of the dual hypergraph, which we call strict hypertree decompositions. A game-characterization for the notion of strict hypertree width is also proposed, which specializes the Robber and Marshals game in [6] (proposed to characterize the hypertree width), and which makes it clear the further requirements on hypertree decompositions.
The rest of the paper is organized as follows. Section 2 discusses the intractability of structured item graphs. Section 3 presents the polynomial-time algorithm for solving MaxWSP on the class of those instances whose dual hypergraphs have bounded hypertree width, and discusses the cases where the algorithm is also highly parallelizable. The comparison between the classes C(ig, k) and C(hw, k) is discussed in Section 4. Finally, in Section 5 we draw our conclusions by also outlining directions for further research.
ITEM GRAPHS Let H be a hypergraph. A graph G = (V, E) is an item graph for H if V = N(H) and, for each h ∈ E(H), the subgraph of G induced over the nodes in h is connected.
An important class of item graphs is that of structured item graphs, i.e., of those item graphs having bounded treewidth as formalized below.
A tree decomposition [16] of a graph G = (V, E) is a pair T, χ , where T = (N, F) is a tree, and χ is a labelling function assigning to each vertex p ∈ N a set of vertices χ(p) ⊆ V , such that the following conditions are satisfied: (1) for each vertex b of G, there exists p ∈ N such that b ∈ χ(p); (2) for each edge {b, d} ∈ E, there exists p ∈ N such that {b, d} ⊆ χ(p); (3) for each vertex b of G, the set {p ∈ N | b ∈ χ(p)} induces a connected subtree of T.
The width of T, χ is the number maxp∈N |χ(p) − 1|. The treewidth of G, denoted by tw(G), is the minimum width over all its tree decompositions.
The winner determination problem can be solved in polynomial time on item graphs having bounded treewidth [3].
Theorem 1 (cf. [3]). Assume a k-width tree decomposition T, χ of an item graph for H is given. Then,
MaxWSP(H, w) can be solved in time O(|T|2 ×(|E(H)|+1)k+1 ).
Many item graphs can be associated with a hypergraph.
As an example, observe that the item graph in Figure 1.(c) has treewidth 1, while Figure 1.(d) reports an item graph whose treewidth is 2. Indeed, it was an open question whether for a given constant k it can be checked in polynomial time if an item graph of treewidth k exists, and if so, whether such an item graph can be efficiently computed.
Let C(ig, k) denote the class of all the hypergraphs having an item graph G such that tw(G) ≤ k. The main result of this section is to show that the class C(ig, k) is hard to recognize.
Theorem 2. Deciding whether a hypergraph H belongs to C(ig, 3) is NP-hard.
The proof of this result relies on an elaborate reduction from the Hamiltonian path problem HP(s, t) of deciding whether there is an Hamiltonian path from a node s to a node t in a directed graph G = (N, E). To help the intuition, we report here a high-level overview of the main ingredients exploited in the proof1 .
The general idea it to build a hypergraph HG such that there is an item graph G for HG with tw(G ) ≤ 3 if and only if HP(s, t) over G has a solution. First, we discuss the way HG is constructed. See Figure 2.(a) for an illustration, where the graph G consists of the nodes s, x, y, and t, and the set of its edges is {e1 = (s, x), e2 = (x, y), e3 = (x, t), e4 = (y, t)}.
From G to HG. Let G = (N, E) be a directed graph.
Then, the set of the nodes in HG is such that: for each x ∈ N, N(HG) contains the nodes bsx, btx, bx, bx, bdx; for each e = (x, y) ∈ E, N(HG) contains the nodes nsx, nsx, nty, nty , nse x and nte y. No other node is in N(HG).
Hyperedges in HG are of three kinds: 1) for each x ∈ N, E(HG) contains the hyperedges: • Sx = {bsx} ∪ {nse x | e = (x, y) ∈ E}; • Tx = {btx} ∪ {nte x | e = (z, x) ∈ E}; • A1 x = {bdx, bx}, A2 x = {bdx, bx}, and A3 x = {bx, bx} -notice that these hyperedges induce a clique on the nodes {bx, bx, bdx}; 1 Detailed proofs can be found in the Appendix, available at www.mat.unical.it/∼ggreco/papers/ca.pdf. 154 Figure 2: Proof of Theorem 2: (a) from G to HG - hyperedges in 1) and 2) are reported only; (b) a skeleton for a tree decomposition TD for HG. • SA1 x = {bsx, bx}, SA2 x = {bsx, bx}, SA3 x = {bsx, bdx} -notice that these hyperedges plus A1 x, A2 x, and A3 x induce a clique on the nodes {bsx, bx, bx, bdx}; • TA1 x = {btx, bx}, TA2 x = {btx, bx}, and TA3 x = {btx, bdx} -notice that these hyperedges plus A1 x, A2 x, and A3 x induce a clique on the nodes {btx, bx, bx, bdx}; 2) for each e = (x, y) ∈ E, E(HG) contains the hyperedges: • SHx = {nsx, nsx}; • THy = {nty, nty }; • SEe = {nsx, nse x} and SEe = {nsx, nse x} -notice that these two hyperedges plus SHx induce a clique on the nodes {nsx, nsx, nse x}; • TEe = {nty, nte y} and TEe = {nty , nte y} -notice that these two hyperedges plus THy induce a clique on the nodes {nty, nty , nte y}.
Notice that each of the above hyperedges but those of the form Sx and Tx contains exactly two nodes. As an example of the hyperedges of kind 1) and 2), the reader may refer to the example construction reported in Figure 2.(a), and notice, for instance, that Sx = {bsx, nse2 x , nse3 x } and that Tt = {btt, nte4 t , nte3 t }. 3) finally, we denote by DG the set containing the hyperedges in E(HG) of the third kind. In the reduction we are exploiting, DG can be an arbitrary set of hyperedges satisfying the four conditions that are discussed below. Let PG be the set of the following |PG| ≤ |N| + 3 × |E| pairs: PG = {(bx, bx) | x ∈ N} ∪ {(nsx, nsx), (nty, nty ), (nse x, nte y) | e = (x, y) ∈ E}.
Also, let I(v) denote the set {h ∈ E(H) | v ∈ h} of the hyperedges of H that are touched by v; and, for a set V ⊆ N(H), let I(V ) = v∈V I(v). Then, DG has to be a set such that: (c1) ∀(α, β) ∈ PG, I(α) ∩ I(β) ∩ DG = ∅; (c2) ∀(α, β) ∈ PG, I(α) ∪ I(β) ⊇ DG; (c3) ∀α ∈ N such that ∃β ∈ N with (α, β) ∈ PG or (β, α) ∈ PG, it holds: I(α) ∩ DG = ∅; and, (c4) ∀S ⊆ N such that |S| ≤ 3 and where ∃α, β ∈ S with (α, β) ∈ PG, it is the case that: I(S) ⊇ DG.
Intuitively, the set DG is such that each of its hyperedges is touched by exactly one of the two nodes in every pair 155 of PG - cf. (c1) and (c2). Moreover, hyperedges in DG touch only vertices included in at least a pair of PG - cf. (c3); and, any triple of nodes is not capable of touching all the elements of DG if none of the pairs that can be built from it belongs to PG - cf. (c4).
The reader may now ask whether a set DG exists at all satisfying (c1), (c2), (c3) and (c4). In the following lemma, we positively answer this question and refer the reader to its proof for an example construction.
Lemma 1. A set DG, with |DG| = 2 × |PG| + 2, satisfying conditions (c1), (c2), (c3), and (c4) can be built in time O(|PG|2 ).
Key Ingredients. We are now in the position of presenting an overview of the key ingredients of the proof. Let G be an arbitrary item graph for HG, and let TD = T, χ be a 3-width tree decomposition of G (note that, because of the cliques, e.g., on the nodes {bsx, bx, bx, bdx}, any item graph for HG has treewidth 3 at least).
There are three basic observations serving the purpose of proving the correctness of the reduction.
Blocks of TD: First, we observe that TD must contain some special kinds of vertex. Specifically, for each node x ∈ N, TD contains a vertex bs(x) such that χ(bs(x)) ⊇ {bsx, bx, bx, bdx}, and a vertex bt(x) such that χ(bt(x)) ⊇ {btx, bx, bx, bdx}. And, for each edge e = (x, y) ∈ E, TD contains a vertex ns(x,e) such that χ(ns(x,e)) ⊇ {nse x, nsx, nsx}, and a vertex nt(y,e) such that χ(nt(y,e)) ⊇ {nte y, nty, nty }.
Intuitively, these vertices are required to cover the cliques of HG associated with the hyperedges of kind 1) and 2). Each of these vertices plays a specific role in the reduction. Indeed, each directed edge e = (x, y) ∈ E is encoded in TD by means of the vertices: ns(x,e), representing precisely that e starts from x; and, nt(y,e), representing precisely that e terminates into y. Also, each node x ∈ N is encoded in TD be means of the vertices: bs(x), representing the starting point of edges originating from x; and, bt(x), representing the terminating point of edges ending into x. As an example, Figure 2.(b) reports the skeleton of a tree decomposition TD. The reader may notice in it the blocks defined above and how they are related with the hypergraph HG in Figure 2.(a) - other blocks in it (of the form w(x,y)) are defined next.
Connectedness between blocks, and uniqueness of the connections: The second crucial observation is that in the path connecting a vertex of the form bs(x) (resp., bt(y)) with a vertex of the form ns(x,e) (resp., nt(y,e)) there is one special vertex of the form w(x,y) such that: χ(w(x,y)) ⊇ {nse x , nte y }, for some edge e = (x, y) ∈ E.
Guaranteeing the existence of one such vertex is precisely the role played by the hyperedges in DG. The arguments for the proof are as follows. First, we observe that I(χ(bs(x))) ∩ I(χ(ns(x,e))) ⊇ DG ∪ {Sx} and I(χ(bt(y))) ∩ I(χ(nt(y,e))) ⊇ DG ∪ {Ty}. Then, we show a property stating that for a pair of consecutive vertices p and q in the path connecting bs(x) and ns(x,e) (resp., bt(y) and nt(y,e)), I(χ(p) ∩ χ(q)) ⊇ I(χ(bs(x))) ∩ I(χ(ns(x,e))) (resp., I(χ(p) ∩ χ(q)) ⊇ I(χ(bt(x))) ∩ I(χ(nt(y,e)))). Thus, we have: I(χ(p) ∩ χ(q)) ⊇ DG ∪{Sx} (resp., I(χ(p)∩χ(q)) ⊇ DG ∪{Ty}).
Based on this observation, and by exploiting the properties of the hyperedges in DG, it is not difficult to show that any pair of consecutive vertices p and q must share two nodes of HG forming a pair in PG, and must both touch Sx (resp., Ty). When the treewidth of G is 3, we can conclude that a vertex, say w(x,y), in this path is such that χ(w(x,y)) ⊇ {nse x , nte y }, for some edge e = (x, y) ∈ E - to this end, note that nse x ∈ Sx, nte t ∈ Ty, and I(χ(w(x,y))) ⊇ DG. In particular, w(x,y) is the only kind of vertex satisfying these conditions, i.e., in the path there is no further vertex of the form w(x,z), for z = y (resp., w(z,y), for z = x).
To help the intuition, we observe that having a vertex of the form w(x,y) in TD corresponds to the selection of an edge from node x to node y in the Hamiltonian path. In fact, given the uniqueness of these vertices selected for ensuring the connectivity, a one-to-one correspondence can be established between the existence of a Hamiltonian path for G and the vertices of the form w(x,y). As an example, in Figure 2.(b), the vertices of the form w(s,x), w(x,y), and w(y,t) are in TD, and GT D shows the corresponding Hamiltonian path.
Unused blocks: Finally, the third ingredient of the proof is the observation that if a vertex of the form w(x,y), for an edge e = (x, y) ∈ E is not in TD (i.e., if the edge (x, y) does not belong to the Hamiltonian path), then the corresponding block ns(x,e ) (resp., nt(y,e )) can be arbitrarily appended in the subtree rooted at the block ns(x,e) (resp., nt(y,e)), where e is the edge of the form e = (x, z) (resp., e = (z, y)) such that w(x,z) (resp., w(z,y)) is in TD.
E.g., Figure 2.(a) shows w(x,t), which is not used in TD, and Figure 2.(b) shows how the blocks ns(x,e3) and nt(t,e3) can be arranged in TD for ensuring the connectedness condition.
DECOMPOSITIONS Since constructing structured item graphs is intractable, it is relevant to assess whether other structural restrictions can be used to single out classes of tractable MaxWSP instances.
To this end, we focus on the notion of hypertree decomposition [7], which is a natural generalization of hypergraph acyclicity and which has been profitably used in other domains, e.g, constraint satisfaction and database query evaluation, to identify tractability islands for NP-hard problems.
A hypertree for a hypergraph H is a triple T, χ, λ , where T = (N, E) is a rooted tree, and χ and λ are labelling functions which associate each vertex p ∈ N with two sets χ(p) ⊆ N(H) and λ(p) ⊆ E(H). If T = (N , E ) is a subtree of T, we define χ(T ) = v∈N χ(v). We denote the set of vertices N of T by vertices(T). Moreover, for any p ∈ N,
Tp denotes the subtree of T rooted at p.
Definition 1. A hypertree decomposition of a hypergraph H is a hypertree HD = T, χ, λ for H which satisfies all the following conditions:
such that h ⊆ χ(p) (we say that p covers h); 156 Figure 3: Example MaxWSP problem: (a) Hypergraph H1; (b) Hypergraph ¯H1; (b) A 2-width hypertree decomposition of ¯H1.
Y ∈ χ(p)} induces a (connected) subtree of T;
The width of a hypertree decomposition T, χ, λ is maxp∈vertices(T )|λ(p)|. The HYPERTREE width hw(H) of H is the minimum width over all its hypertree decompositions.
A hypergraph H is acyclic if hw(H) = 1. P Example 3. The hypergraph H I0,B0 reported in Figure 1.(a) is an example acyclic hypergraph. Instead, both the hypergraphs H1 and ¯H1 shown in Figure 3.(a) and Figure 3.(b), respectively, are not acyclic since their hypertree width is 2. A 2-width hypertree decomposition for ¯H1 is reported in Figure 3.(c).
In particular, observe that H1 has been obtained by adding the two hyperedges h4 and h5 to H I0,B0 to model, for instance, that two new bids, B4 and B5, respectively, have been proposed to the auctioneer. ¡ In the following, rather than working on the hypergraph H associated with a MaxWSP problem, we shall deal with its dual ¯H, i.e., with the hypergraph such that its nodes are in one-to-one correspondence with the hyperedges of H, and where for each node x ∈ N(H), {h | x ∈ h ∧ h ∈ E(H)} is in E( ¯H). As an example, the reader may want to check again the hypergraph H1 in Figure 3.(a) and notice that the hypergraph in Figure 3.(b) is in fact its dual.
The rationale for this choice is that issuing restrictions on the original hypergraph is a guarantee for the tractability only in very simple scenarios.
Theorem 3. On the class of acyclic hypergraphs, MaxWSP is (1) in P if each node occurs into two hyperedges at most; and, (2) NP-hard, even if each node is contained into three hyperedges at most.
Hypergraph and Tractable Packing Problems For a fixed constant k, let C(hw, k) denote the class of all the hypergraphs whose dual hypergraphs have hypertree width bounded by k. The maximum weighted-set packing problem can be solved in polynomial time on the class C(hw, k) by means of the algorithm ComputeSetPackingk, shown in Figure 4.
The algorithm receives in input a hypergraph H, a weighting function w, and a k-width hypertree decomposition HD = T=(N, E), χ, λ of ¯H.
For each vertex v ∈ N, let Hv be the hypergraph whose set of nodes N(Hv) ⊆ N(H) coincides with λ(v), and whose set of edges E(Hv) ⊆ E(H) coincides with χ(v). In an initialization step, the algorithm equips each vertex v with all the possible packings for Hv, which are stored in the set Hv. Note that the size of Hv is bounded by (|E(H)| + 1)k , since each node in λ(v) is either left uncovered in a packing or is covered with precisely one of the hyperedges in χ(v) ⊆ E(H). Then, ComputeSetPackingk is designed to filter these packings by retaining only those that conform with some packing for Hc, for each children c of v in T, as formalized next. Let hv and hc be two packings for Hv and Hc, respectively. We say that hv conforms with hc, denoted by hv ≈ hc if: for each h ∈ hc ∩ E(Hv), h is in hv; and, for each h ∈ (E(Hc) − hc), h is not in hv.
Example 4. Consider again the hypertree decomposition of ¯H1 reported in Figure 3.(c). Then, the set of all the possible packings (which are build in the initialization step of ComputeSetPackingk), for each of its vertices, is reFigure 5: Example application of Algorithm ComputeSetPackingk. 157 Input: H, w, and a k-width hypertree decomposition HD = T =(N, E), χ, λ of ¯H; Output: A solution to MaxWSP(H, w); var Hv : set of packings for Hv, for each v ∈ N; h∗ : packing for H; v hv : rational number, for each partial packing hv for Hv; hhv,c : partial packing for Hc, for each partial packing hv for Hv, and for each (v, c) ∈ E;  -------------------------------------------Procedure BottomUp; begin Done := the set of all the leaves of T ; while ∃v ∈ T such that (i) v ∈ Done, and (ii) {c | c is child of v} ⊆ Done do for each c such that (v, c) ∈ E do Hv := Hv − {hv | ∃hc ∈ Hc s.t. hv ≈ hc}; for each hv ∈ Hv do v hv := w(hv); for each c such that (v, c) ∈ E do ¯hc := arg maxhc∈Hc|hv≈ hc c hc − w(hc ∩ hv) ; hhv,c := ¯hc; (* set best packing *) v hv := v hv + c ¯hc − w(¯hc ∩ hv); end for end for Done := Done ∪ {v}; end while end;  -------------------------------------------begin (* MAIN *) for each vertex v in T do Hv := {hv packing for Hv}; BottomUp; let r be the root of T ; ¯hr := arg maxhr∈Hr r hr ; h∗ := ¯hr; (* include packing *) T opDown(r, hr); return h∗ ; end.
Procedure T opDown(v : vertex of N, ¯hv ∈ Hv); begin for each c ∈ N s.t. (v, c) ∈ E do ¯hc := h¯hv,c; h∗ := h∗ ∪ ¯hc; (* include packing *) T opDown(c, ¯hc); end for end; Figure 4: Algorithm ComputeSetPackingk. ported in Figure 5.(a). For instance, the root v1 is such that Hv1 = { {}, {h1}, {h3}, {h5} }.
Moreover, an arrow from a packing hc to hv denotes that hv conforms with hc. For instance, the reader may check that the packing {h3} ∈ Hv1 conforms with the packing {h2, h3} ∈ Hv3 , but do not conform with {h1} ∈ Hv3 . ¡ ComputeSetPackingk builds a solution by traversing T in two phases. In the first phase, vertices of T are processed from the leaves to the root r, by means of the procedure BottomUp. For each node v being processed, the set Hv is preliminary updated by removing all the packings hv that do not conform with any packing for some of the children of v. After this filtering is performed, the weight hv is updated. Intuitively, v hv stores the weight of the best partial packing for H computed by using only the hyperedges occurring in χ(Tv). Indeed, if v is a leaf, then v hv = w(hv).
Otherwise, for each child c of v in T, v hv is updated with the maximum of c hc − w(hc ∩ hv) over all the packings hc that conforms with hv (resolving ties arbitrarily). The packing ¯hc for which this maximum is achieved is stored in the variable hhv,c.
In the second phase, the tree T is processed starting from the root. Firstly, the packing h∗ is selected that maximizes the weight equipped with the packings in Hr. Then, procedure TopDown is used to extend h∗ to all the other partial packings for vertices of T. In particular, at each vertex v, h∗ is extended with the packing hhv,c, for each child c of v.
Example 5. Assume that, in our running example, w(h1) = w(h2) = w(h3) = w(h4) = 1. Then, an execution of ComputeSetPackingk is graphically depicted in Figure 5.(b), where an arrow from a packing hc to a packing hv is used to denote that hc = hhv,c. Specifically, the choices made during the computation are such that the packing {h2, h3} is computed.
In particular, during the bottom-up phase, we have that: (1) v4 is processed, and we set v4 {h2} = v4 {h4} = 1 and v4 {} = 0; (2) v3 is processed, and we set v3 {h1} = v3 {h3} = 1 and v3 {} = 0; (3) v2 is processed, and we set v2 {h1} = v2 {h2} = v2 {h3} = v2 {h4} = 1, v2 {h2,h3} = 2 and v3 {} = 0; (4) v1 is processed and we set v1 {h1} = 1, v1 {h5} = v1 {h3} = 2 and v1 {} = 0. For instance, note that v1 {h5} = 2 since {h5} conforms with the packing {h4} of Hv2 such that v2 {h4} = 1.
Then, at the beginning of the top-down phase,
ComputeSetPackingk selects {h3} as a packing for Hv1 and propagates this choice in the tree. Equivalently, the algorithm may have chosen {h5}.
As a further example, the way the solution {h1} is obtained by the algorithm when w(h1) = 5 and w(h2) = w(h3) = w(h4) = 1 is reported in Figure 5.(c). Notice that, this time, in the top-down phase, ComputeSetPackingk starts selecting {h1} as the best packing for Hv1 . ¡ Theorem 4. Let H be a hypergraph and w be a weighting function for it. Let HD = T, χ, λ be a complete k-width hypertree decomposition of ¯H. Then, ComputeSetPackingk on input H, w, and HD correctly outputs a solution for MaxWSP(H, w) in time O(|T| × (|E(H)| + 1)2k ).
Proof. [Sketch] We observe that h∗ (computed by ComputeSetPackingk) is a packing for H. Indeed, consider a pair of hyperedges h1 and h2 in h∗ , and assume, for the sake of contradiction, that h1 ∩ h2 = ∅. Let v1 (resp., v2) be an arbitrary vertex of T, for which ComputeSetPackingk included h1 (resp., h2) in h∗ in the bottom-down computation. By construction, we have h1 ∈ χ(v1) and h2 ∈ χ(v2). 158 Let I be an element in h1 ∩ h2. In the dual hypergraph H, I is a hyperedge in E( ¯H) which covers both the nodes h1 and h2. Hence, by condition (1) in Definition 1, there is a vertex v ∈ vertices(T) such that {h1, h2} ⊆ χ(v). Note that, because of the connectedness condition in Definition 1, we can also assume, w.l.o.g., that v is in the path connecting v1 and v2 in T.
Let hv ∈ Hv denote the element added by ComputeSetPackingk into h∗ during the bottom-down phase. Since the elements in Hv are packings for Hv, it is the case that either h1 ∈ hv or h2 ∈ hv. Assume, w.l.o.g., that h1 ∈ hv, and notice that each vertex w in T in the path connecting v to v1 is such that h1 ∈ χ(w), because of the connectedness condition. Hence, because of definition of conformance, the packing hw selected by ComputeSetPackingk to be added at vertex w in h∗ must be such that h1 ∈ hw.
This holds in particular for w = v1. Contradiction with the definition of v1.
Therefore, h∗ is a packing for H. It remains then to show that it has the maximum weight over all the packings for H.
To this aim, we can use structural induction on T to prove that, in the bottom-up phase, the variable v hv is updated to contain the weight of the packing on the edges in χ(Tv), which contains hv and which has the maximum weight over all such packings for the edges in χ(Tv). Then, the result follows, since in the top-down phase, the packing hr giving the maximum weight over χ(Tr) = E(H) is first included in h∗ , and then extended at each node c with the packing hhv,c conformingly with hv and such that the maximum value of v hv is achieved.
As for the complexity, observe that the initialization step requires the construction of the set Hv, for each vertex v, and each set has size (|E(H)| + 1)k at most. Then, the function BottomUp checks for the conformance between strategies in Hv with strategies in Hc, for each pair (v, c) ∈ E, and updates the weight v hv . These tasks can be carried out in time O((|E(H)| + 1)2k ) and must be repeated for each edge in T, i.e., O(|T|) times. Finally, the function TopDown can be implemented in linear time in the size of T, since it just requires updating h∗ by accessing the variable hhv,c.
The above result shows that if a hypertree decomposition of width k is given, the MaxWSP problem can be efficiently solved. Moreover, differently from the case of structured item graphs, it is well known that deciding the existence of a k-bounded hypertree decomposition and computing one (if any) are problems which can be efficiently solved in polynomial time [7]. Therefore, Theorem 4 witnesses that the class C(hw, k) actually constitutes a tractable class for the winner determination problem.
As the following theorem shows, for large subclasses (that depend only on how the weight function is specified),
MaxWSP(H, w) is even highly parallelizeable. Let us call a weighting function smooth if it is logspace computable and if all weights are polynomial (and thus just require O(log n) bits for their representation). Recall that LOGCFL is a parallel complexity class contained in NC2, cf. [9]. The functional version of LOGCFL is LLOGCFL , which is obtained by equipping a logspace transducer with an oracle in LOGCFL.
Theorem 5. Let H be a hypergraph in C(hw, k), and let w be a smooth weighting function for it. Then, MaxWSP(H, w) is in LLOGCFL .
STRUCTURED ITEM GRAPHS Given that the class C(hw, k) has been shown to be an island of tractability for the winner determination problem, and given that the class C(ig, k) has been shown not to be efficiently recognizable, one may be inclined to think that there are instances having unbounded hypertree width, but admitting an item graph of bounded tree width (so that the intractability of structured item graphs would lie in their generality).
Surprisingly, we establish this is not the case. The line of the proof is to first show that structured item graphs are in one-to-one correspondence with a special kind of hypertree decompositions of the dual hypergraph, which we shall call strict. Then, the result will follow by proving that k-width strict hypertree decompositions are less powerful than kwith hypertree decompositions.
Let H be a hypergraph, and let V ⊆ N(H) be a set of nodes and X, Y ∈ N(H). X is [V ]-adjacent to Y if there exists an edge h ∈ E(H) such that {X, Y } ⊆ (h − V ). A [V ]-path π from X to Y is a sequence X = X0, . . . , X = Y of variables such that: Xi is [V ]-adjacent to Xi+1, for each i ∈ [0... -1]. A set W ⊆ N(H) of nodes is [V ]-connected if ∀X, Y ∈ W there is a [V ]-path from X to Y . A [V ]-component is a maximal [V ]-connected non-empty set of nodes W ⊆ (N(H) − V ). For any [V ]-component C, let E(C) = {h ∈ E(H) | h ∩ C = ∅}.
Definition 2. A hypertree decomposition HD = T, χ, λ of H is strict if the following conditions hold:
such that s is a child of r, and for each [χ(r)]-component Cr s.t. Cr ∩ χ(Ts) = ∅, Cr is a [χ(r) ∩ N(λ(r) ∩ λ(s))]-component;
h ∈ λ(p) and h ⊆ χ(p) (we say p strongly covers h);
λ(p)} induces a (connected) subtree of T.
The strict hypertree width shw(H) of H is the minimum width over all its strict hypertree decompositions. P The basic relationship between nice hypertree decompositions and structured item graphs is shown in the following theorem.
Theorem 6. Let H be a hypergraph such that for each node v ∈ N(H), {v} is in E(H). Then, a k-width tree decomposition of an item graph for H exists if and only if ¯H has a (k + 1)-width strict hypertree decomposition2 .
Note that, as far as the maximum weighted-set packing problem is concerned, given a hypergraph H, we can always assume that for each node v ∈ N(H), {v} is in E(H). In fact, if this hyperedge is not in the hypergraph, then it can be added without loss of generality, by setting w({v}) = 0.
Therefore, letting C(shw, k) denote the class of all the hypergraphs whose dual hypergraphs (associated with maximum 2 The term +1 only plays the technical role of taking care of the different definition of width for tree decompositions and hypertree decompositions. 159 weighted-set packing problems) have strict hypertree width bounded by k, we have that C(shw, k + 1) = C(ig, k).
By definition, strict hypertree decompositions are special hypertree decompositions. In fact, we are able to show that the additional conditions in Definition 2 induce an actual restriction on the decomposition power.
Theorem 7. C(ig, k) = C(shw, k + 1) ⊂ C(hw, k + 1).
A Game Theoretic View. We shed further lights on strict hypertree decompositions by discussing an interesting characterization based on the strict Robber and Marshals Game, defined by adapting the Robber and Marshals game defined in [6], which characterizes hypertree width.
The game is played on a hypergraph H by a robber against k marshals which act in coordination. Marshals move on the hyperedges of H, while the robber moves on nodes of H. The robber sees where the marshals intend to move, and reacts by moving to another node which is connected with its current position and through a path in G(H) which does not use any node contained in a hyperedge that is occupied by the marshals before and after their move-we say that these hyperedges are blocked. Note that in the basic game defined in [6], the robber is not allowed to move on vertices that are occupied by the marshals before and after their move, even if they do not belong to blocked hyperedges.
Importantly, marshals are required to play monotonically, i.e., they cannot occupy an edge that was previously occupied in the game, and which is currently not. The marshals win the game if they capture the robber, by occupying an edge covering a node where the robber is. Otherwise, the robber wins.
Theorem 8. Let H be a hypergraph such that for each node v ∈ N(H), {v} is in E(H). Then, ¯H has a k-width strict hypertree decomposition if and only if k marshals can win the strict Robber and Marshals Game on ¯H, no matter of the robber"s moves.
We have solved the open question of determining the complexity of computing a structured item graph associated with a combinatorial auction scenario. The result is bad news, since it turned out that it is NP-complete to check whether a combinatorial auction has a structured item graph, even for treewidth 3. Motivated by this result, we investigated the use of hypertree decomposition (on the dual hypergraph associated with the scenario) and we shown that the problem is tractable on the class of those instances whose dual hypergraphs have bounded hypertree width. For some special, yet relevant cases, a highly parallelizable algorithm is also discussed. Interestingly, it also emerged that the class of structured item graphs is properly contained in the class of instances having bounded hypertree width (hence, the reason of their intractability is not their generality).
In particular, the latter result is established by showing a precise relationship between structured item graphs and restricted forms of hypertree decompositions (on the dual hypergraph), called query decompositions (see, e.g., [7]). In the light of this observation, we note that proving some approximability results for structured item graphs requires a deep understanding of the approximability of query decompositions, which is currently missing in the literature.
As a further avenue of research, it would be relevant to enhance the algorithm ComputeSetPackingk, e.g., by using specialized data structures, in order to avoid the quadratic dependency from (|E(H)| + 1)k .
Finally, an other interesting question is to assess whether the structural decomposition techniques discussed in the paper can be used to efficiently deal with generalizations of the winner determination problem. For instance, it might be relevant in several application scenarios to design algorithms that can find a selling strategy when several copies of the same item are available for selling, and when moreover the auctioneer is satisfied when at least a given number of copies is actually sold.
Acknowledgement G. Gottlob"s work was supported by the EC3 - E-Commerce Competence Center (Vienna) and by a Royal Society Wolfson Research Merit Award. In particular, this Award allowed Gottlob to invite G. Greco for a research visit to Oxford. In addition, G. Greco is supported by ICAR-CNR, and by M.I.U.R. under project TOCAI.IT.
[1] I. Adler, G. Gottlob, and M. Grohe. Hypertree-Width and Related Hypergraph Invariants. In Proc. of EUROCOMB"05, pages 5-10, 2005. [2] C. Boutilier. Solving Concisely Expressed Combinatorial Auction Problems. In Proc. of AAAI"02, pages 359-366, 2002. [3] V. Conitzer, J. Derryberry, and T. Sandholm.
Combinatorial auctions with structured item graphs.
In Proc. of AAAI"04, pages 212-218, 2004. [4] E. M. Eschen and J. P. Sinrad. An o(n2 ) algorithm for circular-arc graph recognition. In Proc. of SODA"93, pages 128-137, 1993. [5] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.
Taming the computational complexity of combinatorial auctions: Optimal and approximate. In Proc. of IJCAI"99, pages 548-553, 1999. [6] G. Gottlob, N. Leone, and F. Scarcello. Robbers, marshals, and guards: game theoretic and logical characterizations of hypertree width. Journal of Computer and System Sciences, 66(4):775-808, 2003. [7] G. Gottlob, N. Leone, and S. Scarcello. Hypertree decompositions and tractable queries. Journal of Computer and System Sciences, 63(3):579-627, 2002. [8] H. H. Hoos and C. Boutilier. Solving combinatorial auctions using stochastic local search. In Proc. of AAAI"00, pages 22-29, 2000. [9] D. Johnson. A Catalog of Complexity Classes. In P. Cramton, Y. Shoham, and R. Steinberg, editors,
Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, pages 67-161. 1990. [10] N. Korte and R. H. Mohring. An incremental linear-time algorithm for recognizing interval graphs.
SIAM Journal on Computing, 18(1):68-81, 1989. [11] D. Lehmann, R. M¨uller, and T. Sandholm. The Winner Determination Problem. In P. Cramton,
Y. Shoham, and R. Steinberg, editors, Combinatorial Auctions. MIT Press, 2006. [12] D. Lehmann, L. I. O"Callaghan, and Y. Shoham.
Truth revelation in approximately efficient 160 combinatorial auctions. J. ACM, 49(5):577-602, 2002. [13] R. McAfee and J. McMillan. Analyzing the airwaves auction. Journal of Economic Perspectives, 10(1):159175, 1996. [14] J. McMillan. Selling spectrum rights. Journal of Economic Perspectives, 8(3):145-62, 1994. [15] N. Nisan. Bidding and allocation in combinatorial auctions. In Proc. of EC"00, pages 1-12, 2000. [16] N. Robertson and P. Seymour. Graph minors ii. algorithmic aspects of tree width. Journal of Algorithms, 7:309-322, 1986. [17] M. H. Rothkopf, A. Pekec, and R. M. Harstad.
Computationally manageable combinatorial auctions.
Management Science, 44:1131-1147, 1998. [18] T. Sandholm. An implementation of the contract net protocol based on marginal cost calculations. In Proc. of AAAI"93, pages 256-262, 1993. [19] T. Sandholm. Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 135(1-2):1-54, 2002. [20] T. Sandholm. Winner determination algorithms. In P. Cramton, Y. Shoham, and R. Steinberg, editors,

In a large community of agents, an agent"s behavior is not likely to have a direct effect on most other agents: rather, it is just the agents who are close enough to him that will be affected. However, as these agents respond by adapting their behavior, more agents will feel the consequences and eventually the choices made by a single agent will propagate throughout the entire community.
This is the intuition behind graphical games, which were introduced by Kearns, Littman and Singh in [13] as a compact representation scheme for games with many players. In an n-player graphical game, each player is associated with a vertex of an underlying graph G, and the payoffs of each player depend on his action as well as on the actions of his neighbors in the graph. If the maximum degree of G is Δ, and each player has two actions available to him, then the game can be represented using n2Δ+1 numbers.
In contrast, we need n2n numbers to represent a general n-player 2-action game, which is only practical for small values of n. For graphical games with constant Δ, the size of the game is linear in n.
One of the most natural problems for a graphical game is that of finding a Nash equilibrium, the existence of which follows from Nash"s celebrated theorem (as graphical games are just a special case of n-player games). The first attempt to tackle this problem was made in [13], where the authors consider graphical games with two actions per player in which the underlying graph is a boundeddegree tree. They propose a generic algorithm for finding Nash equilibria that can be specialized in two ways: an exponential-time algorithm for finding an (exact) Nash equilibrium, and a fully polynomial time approximation scheme (FPTAS) for finding an approximation to a Nash equilibrium. For any > 0 this algorithm outputs an -Nash equilibrium, which is a strategy profile in which no player can improve his payoff by more than by unilaterally changing his strategy.
While -Nash equilibria are often easier to compute than exact Nash equilibria, this solution concept has several drawbacks. First, the players may be sensitive to a small loss in payoffs, so the strategy profile that is an -Nash equilibrium will not be stable. This will be the case even if there is only a small subset of players who are extremely price-sensitive, and for a large population of players it may be difficult to choose a value of that will satisfy everyone.
Second, the strategy profiles that are close to being Nash equilibria may be much better with respect to the properties under consideration than exact Nash equilibria. Therefore, the (approximation to the) value of the best solution that corresponds to an -Nash equilibrium may not be indicative of what can be achieved under an exact Nash equilibrium. This is especially important if the purpose of the approximate solution is to provide a good benchmark for a system of selfish agents, as the benchmark implied by an -Nash equilibrium may be unrealistic. For these reasons, in this paper we focus on the problem of computing exact Nash equilibria.
Building on ideas of [14], Elkind et al. [9] showed how to find an (exact) Nash equilibrium in polynomial time when the underlying 162 graph has degree 2 (that is, when the graph is a collection of paths and cycles). By contrast, finding a Nash equilibrium in a general degree-bounded graph appears to be computationally intractable: it has been shown (see [5, 12, 7]) to be complete for the complexity class PPAD. [9] extends this hardness result to the case in which the underlying graph has bounded pathwidth.
A graphical game may not have a unique Nash equilibrium, indeed it may have exponentially many. Moreover, some Nash equilibria are more desirable than others. Rather than having an algorithm which merely finds some Nash equilibrium, we would like to have algorithms for finding Nash equilibria with various sociallydesirable properties, such as maximizing overall payoff or distributing profit fairly.
A useful property of the data structure of [13] is that it simultaneously represents the set of all Nash equilibria of the underlying game. If this representation has polynomial size (as is the case for paths, as shown in [9]), one may hope to extract from it a Nash equilibrium with the desired properties. In fact, in [13] the authors mention that this is indeed possible if one is interested in finding an (approximate) -Nash equilibrium. The goal of this paper is to extend this to exact Nash equilibria.
In this paper, we study n-player 2-action graphical games on bounded-degree trees for which the data structure of [13] has size poly(n). We focus on the problem of finding exact Nash equilibria with certain socially-desirable properties. In particular, we show how to find a Nash equilibrium that (nearly) maximizes the social welfare, i.e., the sum of the players" payoffs, and we show how to find a Nash equilibrium that (nearly) satisfies prescribed payoff bounds for all players.
Graphical games on bounded-degree trees have a simple algebraic structure. One attractive feature, which follows from [13], is that every such game has a Nash equilibrium in which the strategy of every player is a rational number. Section 3 studies the algebraic structure of those Nash equilibria that maximize social welfare. We show (Theorems 1 and 2) that, surprisingly, the set of Nash equilibria that maximize social welfare is more complex. In fact, for any algebraic number α ∈ [0, 1] with degree at most n, we exhibit a graphical game on a path of length O(n) such that, in the unique social welfare-maximizing Nash equilibrium of this game, one of the players plays the mixed strategy α.1 This result shows that it may be difficult to represent an optimal Nash equilibrium. It seems to be a novel feature of the setting we consider here, that an optimal Nash equilibrium is hard to represent, in a situation where it is easy to find and represent a Nash equilibrium.
As the social welfare-maximizing Nash equilibrium may be hard to represent efficiently, we have to settle for an approximation.
However, the crucial difference between our approach and that of previous papers [13, 16, 19] is that we require our algorithm to output an exact Nash equilibrium, though not necessarily the optimal one with respect to our criteria. In Section 4, we describe an algorithm that satisfies this requirement. Namely, we propose an algorithm that for any > 0 finds a Nash equilibrium whose total payoff is within of optimal. It runs in polynomial time (Theorem 3,4) for any graphical game on a bounded-degree tree for which the data structure proposed by [13] (the so-called best response policy, defined below) is of size poly(n) (note that, as shown in [9], this is always the case when the underlying graph is a path). More pre1 A related result in a different context was obtained by Datta [8], who shows that n-player 2-action games are universal in the sense that any real algebraic variety can be represented as the set of totally mixed Nash equilibria of such games. cisely, the running time of our algorithm is polynomial in n, Pmax, and 1/ , where Pmax is the maximum absolute value of an entry of a payoff matrix, i.e., it is a pseudopolynomial algorithm, though it is fully polynomial with respect to . We show (Section 4.1) that under some restrictions on the payoff matrices, the algorithm can be transformed into a (truly) polynomial-time algorithm that outputs a Nash equilibrium whose total payoff is within a 1 − factor from the optimal.
In Section 5, we consider the problem of finding a Nash equilibrium in which the expected payoff of each player Vi exceeds a prescribed threshold Ti. Using the idea from Section 4 we give (Theorem 5) a fully polynomial time approximation scheme for this problem. The running time of the algorithm is bounded by a polynomial in n, Pmax, and . If the instance has a Nash equilibrium satisfying the prescribed thresholds then the algorithm constructs a Nash equilibrium in which the expected payoff of each player Vi is at least Ti − .
In Section 6, we introduce other natural criteria for selecting a good Nash equilibrium and we show that the algorithms described in the two previous sections can be used as building blocks in finding Nash equilibria that satisfy these criteria. In particular, in Section 6.1 we show how to find a Nash equilibrium that approximates the maximum social welfare, while guaranteeing that each individual payoff is close to a prescribed threshold. In Section 6.2 we show how to find a Nash equilibrium that (nearly) maximizes the minimum individual payoff. Finally, in Section 6.3 we show how to find a Nash equilibrium in which the individual payoffs of the players are close to each other.
Our approximation scheme (Theorem 3 and Theorem 4) shows a contrast between the games that we study and two-player n-action games, for which the corresponding problems are usually intractable.
For two-player n-action games, the problem of finding Nash equilibria with special properties is typically NP-hard. In particular, this is the case for Nash equilibria that maximize the social welfare [11, 6]. Moreover, it is likely to be intractable even to approximate such equilibria. In particular, Chen, Deng and Teng [4] show that there exists some , inverse polynomial in n, for which computing an -Nash equilibrium in 2-player games with n actions per player is PPAD-complete.
Lipton and Markakis [15] study the algebraic properties of Nash equilibria, and point out that standard quantifier elimination algorithms can be used to solve them. Note that these algorithms are not polynomial-time in general. The games we study in this paper have polynomial-time computable Nash equilibria in which all mixed strategies are rational numbers, but an optimal Nash equilibrium may necessarily include mixed strategies with high algebraic degree.
A correlated equilibrium (CE) (introduced by Aumann [2]) is a distribution over vectors of players" actions with the property that if any player is told his own action (the value of his own component) from a vector generated by that distribution, then he cannot increase his expected payoff by changing his action. Any Nash equilibrium is a CE but the converse does not hold in general. In contrast with Nash equilibria, correlated equilibria can be found for low-degree graphical games (as well as other classes of conciselyrepresented multiplayer games) in polynomial time [17]. But, for graphical games it is NP-hard to find a correlated equilibrium that maximizes total payoff [18]. However, the NP-hardness results apply to more general games than the one we consider here, in particular the graphs are not trees. From [2] it is also known that there exist 2-player, 2-action games for which the expected total payoff 163 of the best correlated equilibrium is higher than the best Nash equilibrium, and we discuss this issue further in Section 7.
We consider graphical games in which the underlying graph G is an n-vertex tree, in which each vertex has at most Δ children. Each vertex has two actions, which are denoted by 0 and 1. A mixed strategy of a player V is represented as a single number v ∈ [0, 1], which denotes the probability that V selects action 1.
For the purposes of the algorithm, the tree is rooted arbitrarily.
For convenience, we assume without loss of generality that the root has a single child, and that its payoff is independent of the action chosen by the child. This can be achieved by first choosing an arbitrary root of the tree, and then adding a dummy parent of this root, giving the new parent a constant payoff function, e.g., 0.
Given an edge (V, W ) of the tree G, and a mixed strategy w for W , let G(V,W ),W =w be the instance obtained from G by (1) deleting all nodes Z which are separated from V by W (i.e., all nodes Z such that the path from Z to V passes through W ), and (2) restricting the instance so that W is required to play mixed strategy w.
Definition 1. Suppose that (V, W ) is an edge of the tree, that v is a mixed strategy for V and that w is a mixed strategy for W .
We say that v is a potential best response to w (denoted by v ∈ pbrV (w)) if there is an equilibrium in the instance G(V,W ),W =w in which V has mixed strategy v. We define the best response policy for V , given W , as B(W, V ) = {(w, v) | v ∈ pbrV (w), w ∈ [0, 1]}.
The upstream pass of the generic algorithm of [13] considers every node V (other than the root) and computes the best response policy for V given its parent. With the above assumptions about the root, the downstream pass is straightforward. The root selects a mixed strategy w for the root W and a mixed strategy v ∈ B(W, V ) for each child V of W . It instructs each child V to play v. The remainder of the downward pass is recursive. When a node V is instructed by its parent to adopt mixed strategy v, it does the following for each child U - It finds a pair (v, u) ∈ B(V, U) (with the same v value that it was given by its parent) and instructs U to play u.
The best response policy for a vertex U given its parent V can be represented as a union of rectangles, where a rectangle is defined by a pair of closed intervals (IV , IU ) and consists of all points in IV × IU ; it may be the case that one or both of the intervals IV and IU consists of a single point. In order to perform computations on B(V, U), and to bound the number of rectangles, [9] used the notion of an event point, which is defined as follows. For any set A ⊆ [0, 1]2 that is represented as a union of a finite number of rectangles, we say that a point u ∈ [0, 1] on the U-axis is a Uevent point of A if u = 0 or u = 1 or the representation of A contains a rectangle of the form IV × IU and u is an endpoint of IU ; V -event points are defined similarly.
For many games considered in this paper, the underlying graph is an n-vertex path, i.e., a graph G = (V, E) with V = {V1, . . . , Vn} and E = {(V1, V2), . . . , (Vn−1, Vn)}. In [9], it was shown that for such games, the best response policy has only polynomially-many rectangles. The proof that the number of rectangles in B(Vj+1, Vj) is polynomial proceeds by first showing that the number of event points in B(Vj+1, Vj ) cannot exceed the number of event points in B(Vj, Vj−1) by more than 2, and using this fact to bound the number of rectangles in B(Vj+1, Vj ).
Let P0 (V ) and P1 (V ) be the expected payoffs to V when it plays 0 and 1, respectively. Both P0 (V ) and P1 (V ) are multilinear functions of the strategies of V "s neighbors. In what follows, we will frequently use the following simple observation.
CLAIM 1. For a vertex V with a single child U and parent W , given any A, B, C, D ∈ Q, A , B , C , D ∈ Q, one can select the payoffs to V so that P0 (V ) = Auw + Bu + Cw + D, P1 (V ) = A uw + B u + C w + D . Moreover, if all A, B, C, D, A , B ,
C , D are integer, the payoffs to V are integer as well.
PROOF. We will give the proof for P0 (V ); the proof for P1 (V ) is similar. For i, j = 0, 1, let Pij be the payoff to V when U plays i, V plays 0 and W plays j. We have P0 (V ) = P00(1 − u)(1 − w) + P10u(1 − w) + P01(1 − u)w + P11uw. We have to select the values of Pij so that P00 − P10 − P01 + P11 = A, −P00 + P10 = B, −P00 + P01 = C, P00 = D. It is easy to see that the unique solution is given by P00 = D, P01 = C + D,
P10 = B + D, P11 = A + B + C + D.
The input to all algorithms considered in this paper includes the payoff matrices for each player. We assume that all elements of these matrices are integer. Let Pmax be the greatest absolute value of any element of any payoff matrix. Then the input consists of at most n2Δ+1 numbers, each of which can be represented using log Pmax bits.
THE SOCIAL WELFARE: SOLUTIONS IN R \ Q From the point of view of social welfare, the best Nash equilibrium is the one that maximizes the sum of the players" expected payoffs. Unfortunately, it turns out that computing such a strategy profile exactly is not possible: in this section, we show that even if all players" payoffs are integers, the strategy profile that maximizes the total payoff may have irrational coordinates; moreover, it may involve algebraic numbers of an arbitrary degree.
We start by providing an example of a graphical game on a path of length 3 with integer payoffs such that in the Nash equilibrium that maximizes the total payoff, one of the players has a strategy in R \ Q. In the next subsection, we will extend this example to algebraic numbers of arbitrary degree n; to do so, we have to consider paths of length O(n).
THEOREM 1. There exists an integer-payoff graphical game G on a 3-vertex path UV W such that, in any Nash equilibrium of G that maximizes social welfare, the strategy, u, of the player U and the total payoff, p, satisfy u, p ∈ R \ Q.
PROOF. The payoffs to the players in G are specified as follows.
The payoff to U is identically 0, i.e., P0 (U) = P1 (U) = 0. Using Claim 1, we select the payoffs to V so that P0 (V ) = −uw + 3w and P1 (V ) = P0 (V ) + w(u + 2) − (u + 1), where u and w are the (mixed) strategies of U and W , respectively. It follows that V is indifferent between playing 0 and 1 if and only if w = f(u) = u+1 u+2 . Observe that for any u ∈ [0, 1] we have f(u) ∈ [0, 1]. The payoff to W is 0 if it selects the same action as V and 1 otherwise.
CLAIM 2. All Nash equilibria of the game G are of the form (u, 1/2, f(u)). That is, in any Nash equilibrium, V plays v = 1/2 and W plays w = f(u). Moreover, for any value of u, the vector of strategies (u, 1/2, f(u)) constitutes a Nash equilibrium.
PROOF. It is easy to check that for any u ∈ [0, 1], the vector (u, 1/2, f(u)) is a Nash equilibrium. Indeed, U is content to play 164 any mixed strategy u no matter what V and W do. Furthermore,
V is indifferent between 0 and 1 as long as w = f(u), so it can play 1/2. Finally, if V plays 0 and 1 with equal probability, W is indifferent between 0 and 1, so it can play f(u).
Conversely, suppose that v > 1/2. Then W strictly prefers to play 0, i.e., w = 0. Then for V we have P1 (V ) = P0 (V ) − (u + 1), i.e., P1 (V ) < P0 (V ), which implies v = 0, a contradiction.
Similarly, if v < 1/2, player W prefers to play 1, so we have w = 1. Hence, P1 (V ) = P0 (V ) + (u + 2) − (u + 1), i.e.,
P1 (V ) > P0 (V ), which implies v = 1, a contradiction. Finally, if v = 1/2, but w = f(u), player V is not indifferent between 0 and 1, so he would deviate from playing 1/2. This completes the proof of Claim 2.
By Claim 2, the total payoff in any Nash equilibrium of this game is a function of u. More specifically, the payoff to U is 0, the payoff to V is −uf(u) + 3f(u), and the payoff to W is 1/2. Therefore, the Nash equilibrium with the maximum total payoff corresponds to the value of u that maximizes g(u) = −u (u + 1) u + 2 + 3 u + 1 u + 2 = − (u − 3)(u + 1) u + 2 .
To find extrema of g(u), we compute h(u) = − d du g(u). We have h(u) = (2u − 2)(u + 2) − (u − 3)(u + 1) (u + 2)2 = u2 + 4u − 1 (u + 2)2 .
Hence, h(u) = 0 if and only if u ∈ {−2 + √ 5, −2 − √ 5}. Note that −2 + √ 5 ∈ [0, 1].
The function g(u) changes sign at −2, −1, and 3. We have g(u) < 0 for g > 3, g(u) > 0 for u < −2, so the extremum of g(u) that lies between 1 and 3, i.e., u = −2 + √ 5, is a local maximum. We conclude that the social welfare-maximizing Nash equilibrium for this game is given by the vector of strategies (−2+√ 5, 1/2, (5 − √ 5)/5). The respective total payoff is 0 − ( √ 5 − 5)( √ 5 − 1) √ 5 + 1 2 = 13/2 − 2 √
This concludes the proof of Theorem 1.
We have shown that in the social welfare-maximizing Nash equilibrium, some players" strategies can be quadratic irrationalities, and so can the total payoff. In this subsection, we will extend this result to show that we can construct an integer-payoff graphical game on a path whose social welfare-maximizing Nash equilibrium involves arbitrary algebraic numbers in [0, 1].
THEOREM 2. For any degree-n algebraic number α ∈ [0, 1], there exists an integer payoff graphical game on a path of length O(n) such that, in all social welfare-maximizing Nash equilibria of this game, one of the players plays α.
PROOF. Our proof consists of two steps. First, we construct a rational expression R(x) and a segment [x , x ] such that x , x ∈ Q and α is the only maximum of R(x) on [x , x ]. Second, we construct a graphical game whose Nash equilibria can be parameterized by u ∈ [x , x ], so that at the equilibrium that corresponds to u the total payoff is R(u) and, moreover, some player"s strategy is u. It follows that to achieve the payoff-maximizing Nash equilibrium, this player has to play α. The details follow.
LEMMA 1. Given an algebraic number α ∈ [0, 1], deg(α) = n, there exist K2, . . . , K2n+2 ∈ Q and x , x ∈ (0, 1) ∩ Q such that α is the only maximum of R(x) = K2 x + 2 + · · · + K2n+2 x + 2n + 2 on [x , x ].
PROOF. Let P(x) be the minimal polynomial of α, i.e., a polynomial of degree n with rational coefficients whose leading coefficient is 1 such that P(α) = 0. Let A = {α1, . . . , αn} be the set of all roots of P(x). Consider the polynomial Q1(x) = −P2 (x). It has the same roots as P(x), and moreover, for any x ∈ A we have Q1(x) < 0. Hence, A is the set of all maxima of Q1(x). Now, set R(x) = Q1(x) (x+2)...(x+2n+1)(x+2n+2) . Observe that R(x) ≤ 0 for all x ∈ [0, 1] and R(x) = 0 if and only if Q1(x) = 0. Hence, the set A is also the set of all maxima of R(x) on [0, 1].
Let d = min{|αi − α| | αi ∈ A, αi = α}, and set α = max{α − d/2, 0}, α = min{α + d/2, 1}. Clearly, α is the only zero (and hence, the only maximum) of R(x) on [α , α ].
Let x and x be some rational numbers in (α , α) and (α, α ), respectively; note that by excluding the endpoints of the intervals we ensure that x , x = 0, 1. As [x , x ] ⊂ [α , α ], we have that α is the only maximum of R(x) on [x , x ].
As R(x) is a proper rational expression and all roots of its denominator are simple, by partial fraction decomposition theorem,
R(x) can be represented as R(x) = K2 x + 2 + · · · + K2n+2 x + 2n + 2 , where K2, . . . , K2n+2 are rational numbers.
Consider a graphical game on the path U−1V−1U0V0U1V1 . . . Uk−1Vk−1Uk, where k = 2n + 2. Intuitively, we want each triple (Ui−1, Vi−1,
Ui) to behave similarly to the players U, V , and W from the game described in the previous subsection. More precisely, we define the payoffs to the players in the following way. • The payoff to U−1 is 0 no matter what everyone else does. • The expected payoff to V−1 is 0 if it plays 0 and u0 − (x − x )u−1 −x if it plays 1, where u0 and u−1 are the strategies of U0 and U−1, respectively. • The expected payoff to V0 is 0 if it plays 0 and u1(u0 + 1)− u0 if it plays 1, where u0 and u1 are the strategies of U0 and U1, respectively. • For each i = 1, . . . , k − 1, the expected payoff to Vi when it plays 0 is P0 (Vi) = Aiuiui+1 − Aiui+1, and the expected payoff to Vi when it plays 1 is P1 (Vi) = P0 (Vi) + ui+1(2 − ui) − 1, where Ai = −Ki+1 and ui+1 and ui are the strategies of Ui+1 and Ui, respectively. • For each i = 0, . . . , k, the payoff to Ui does not depend on Vi and is 1 if Ui and Vi−1 select different actions and 0 otherwise.
We will now characterize the Nash equilibria of this game using a sequence of claims.
CLAIM 3. In all Nash equilibria of this game V−1 plays 1/2, and the strategies of u−1 and u0 satisfy u0 = (x − x )u−1 + x .
Consequently, in all Nash equilibria we have u0 ∈ [x , x ]. 165 PROOF. The proof is similar to that of Claim 2. Let f(u−1) = (x − x )u−1 + x . Clearly, the player V−1 is indifferent between playing 0 and 1 if and only if u0 = f(u−1). Suppose that v−1 < 1/2. Then U0 strictly prefers to play 1, i.e., u0 = 1, so we have P1 (V−1) = P0 (V−1) + 1 − (x − x )u−1 − x .
As 1 − x ≤ 1 − (x − x )u−1 − x ≤ 1 − x for u−1 ∈ [0, 1] and x < 1, we have P1 (V−1) > P0 (V−1), so V−1 prefers to play 1, a contradiction. Similarly, if v−1 > 1/2, the player U0 strictly prefers to play 0, i.e., u0 = 0, so we have P1 (V−1) = P0 (V−1) − (x − x )u−1 − x .
As x < x , x > 0, we have P1 (V−1) < P0 (V−1), so V−1 prefers to play 0, a contradiction. Finally, if V−1 plays 1/2, but u0 = f(u−1), player V−1 is not indifferent between 0 and 1, so he would deviate from playing 1/2.
Also, note that f(0) = x , f(1) = x , and, moreover, f(u−1) ∈ [x , x ] if and only if u−1 ∈ [0, 1]. Hence, in all Nash equilibria of this game we have u0 ∈ [x , x ].
CLAIM 4. In all Nash equilibria of this game for each i = 0, . . . , k − 1, we have vi = 1/2, and the strategies of the players Ui and Ui+1 satisfy ui+1 = fi(ui), where f0(u) = u/(u + 1) and fi(u) = 1/(2 − u) for i > 0.
PROOF. The proof of this claim is also similar to that of Claim 2.
We use induction on i to prove that the statement of the claim is true and, additionally, ui = 1 for i > 0.
For the base case i = 0, note that u0 = 0 by the previous claim (recall that x , x are selected so that x , x = 0, 1) and consider the triple (U0, V0, U1). Let v0 be the strategy of V0. First, suppose that v0 > 1/2. Then U1 strictly prefers to play 0, i.e., u1 = 0.
Then for V0 we have P1 (V0) = P0 (V0) − u0. As u0 = 0, we have P1 (V0) < P0 (V0), which implies v1 = 0, a contradiction.
Similarly, if v0 < 1/2, player U1 prefers to play 1, so we have u1 = 1. Hence, P1 (V0) = P0 (V0) + 1. It follows that P1 (V0) > P0 (V0), which implies v0 = 1, a contradiction. Finally, if v0 = 1/2, but u1 = u0/(u0 + 1), player V0 is not indifferent between 0 and 1, so he would deviate from playing 1/2. Moreover, as u1 = u0/(u0 + 1) and u0 ∈ [0, 1], we have u1 = 1.
The argument for the inductive step is similar. Namely, suppose that the statement is proved for all i < i and consider the triple (Ui, Vi, Ui+1).
Let vi be the strategy of Vi. First, suppose that vi > 1/2. Then Ui+1 strictly prefers to play 0, i.e., ui+1 = 0. Then for Vi we have P1 (Vi) = P0 (Vi)−1, i.e., P1 (Vi) < P0 (Vi), which implies vi = 0, a contradiction. Similarly, if vi < 1/2, player Ui+1 prefers to play 1, so we have ui+1 = 1. Hence, P1 (Vi) = P0 (Vi) + 1 − ui. By inductive hypothesis, we have ui < 1. Consequently,
P1 (Vi) > P0 (Vi), which implies vi = 1, a contradiction. Finally, if vi = 1/2, but ui+1 = 1/(2 − ui), player Vi is not indifferent between 0 and 1, so he would deviate from playing 1/2. Moreover, as ui+1 = 1/(2 − ui) and ui < 1, we have ui+1 < 1.
CLAIM 5. Any strategy profile of the form (u−1, 1/2, u0, 1/2, u1, 1/2, . . . , uk−1, 1/2, uk), where u−1 ∈ [0, 1], u0 = (x − x )u−1 + x , u1 = u0/(u0 + 1), and ui+1 = 1/(2 − ui) for i ≥ 1 constitutes a Nash equilibrium.
PROOF. First, the player U−1"s payoffs do not depend on other players" actions, so he is free to play any strategy in [0, 1]. As long as u0 = (x −x )u−1 +x , player V−1 is indifferent between 0 and 1, so he is content to play 1/2; a similar argument applies to players V0, . . . , Vk−1. Finally, for each i = 0, . . . , k, the payoffs of player Ui only depend on the strategy of player Vi−1. In particular, as long as vi−1 = 1/2, player Ui is indifferent between playing 0 and 1, so he can play any mixed strategy ui ∈ [0, 1]. To complete the proof, note that (x − x )u−1 + x ∈ [0, 1] for all u−1 ∈ [0, 1], u0/(u0 + 1) ∈ [0, 1] for all u0 ∈ [0, 1], and 1/(2 − ui) ∈ [0, 1] for all ui ∈ [0, 1], so we have ui ∈ [0, 1] for all i = 0, . . . , k.
Now, let us compute the total payoff under a strategy profile of the form given in Claim 5. The payoff to U−1 is 0, and the expected payoff to each of the Ui, i = 0, . . . , k, is 1/2. The expected payoffs to V−1 and V0 are 0. Finally, for any i = 1, . . . , k − 1, the expected payoff to Vi is Ti = Aiuiui+1 − Aiui+1. It follows that to find a Nash equilibrium with the highest total payoff, we have to maximize Pk−1 i=1 Ti subject to conditions u−1 ∈ [0, 1], u0 = (x −x )u−1+x , u1 = u0/(u0+1), and ui+1 = 1/(2−ui) for i = 1, . . . , k − 1.
We would like to express Pk−1 i=1 Ti as a function of u0. To simplify notation, set u = u0.
LEMMA 2. For i = 1, . . . , k, we have ui = u+i−1 u+i .
PROOF. The proof is by induction on i. For i = 1, we have u1 = u/(u + 1). Now, for i ≥ 2 suppose that ui−1 = (u + i − 2)/(u + i − 1). We have ui = 1/(2 − ui−1) = (u + i − 1)/(2u + 2i − 2 − u − i + 2) = (u + i − 1)/(u + i).
It follows that for i = 1, . . . , k − 1 we have Ti = Ai u + i − 1 u + i u + i u + i + 1 − Ai u + i u + i + 1 = −Ai 1 u + i + 1 = Ki+1 u + i + 1 .
Observe that as u−1 varies from 0 to 1, u varies from x to x .
Therefore, to maximize the total payoff, we have to choose u ∈ [x , x ] so as to maximize K2 u + 2 + · · · + Kk u + k = R(u).
By construction, the only maximum of R(u) on [x , x ] is α. It follows that in the payoff-maximizing Nash equilibrium of our game U0 plays α.
Finally, note that the payoffs in our game are rational rather than integer. However, it is easy to see that we can multiply all payoffs to a player by their greatest common denominator without affecting his strategy. In the resulting game, all payoffs are integer. This concludes the proof of Theorem 2.
OPTIMAL NASH EQUILIBRIUM We have seen that the Nash equilibrium that maximizes the social welfare may involve strategies that are not in Q. Hence, in this section we focus on finding a Nash equilibrium that is almost optimal from the social welfare perspective. We propose an algorithm that for any > 0 finds a Nash equilibrium whose total payoff is within from optimal. The running time of this algorithm is polynomial in 1/ , n and |Pmax| (recall that Pmax is the maximum absolute value of an entry of a payoff matrix).
While the negative result of the previous section is for graphical games on paths, our algorithm applies to a wider range of scenarios. Namely, it runs in polynomial time on bounded-degree trees 166 as long as the best response policy of each vertex, given its parent, can be represented as a union of a polynomial number of rectangles. Note that path graphs always satisfy this condition: in [9] we showed how to compute such a representation, given a graph with maximum degree 2. Consequently, for path graphs the running time of our algorithm is guaranteed to be polynomial. (Note that [9] exhibits a family of graphical games on bounded-degree trees for which the best response policies of some of the vertices, given their parents, have exponential size, when represented as unions of rectangles.) Due to space restrictions, in this version of the paper we present the algorithm for the case where the graph underlying the graphical game is a path. We then state our result for the general case; the proof can be found in the full version of this paper [10].
Suppose that s is a strategy profile for a graphical game G. That is, s assigns a mixed strategy to each vertex of G. let EPV (s) be the expected payoff of player V under s and let EP(s) =P V EPV (s). Let M(G) = max{EP(s) | s is a Nash equilibrium for G}.
THEOREM 3. Suppose that G is a graphical game on an nvertex path. Then for any > 0 there is an algorithm that constructs a Nash equilibrium s for G that satisfies EP(s ) ≥ M(G)− . The running time of the algorithm is O(n4 P3 max/ 3 ) PROOF. Let {V1, . . . , Vn} be the set of all players. We start by constructing the best response policies for all Vi, i = 1, . . . , n − 1.
As shown in [9], this can be done in time O(n3 ).
Let N > 5n be a parameter to be selected later, set δ = 1/N, and define X = {jδ | j = 0, . . . , N}. We say that vj is an event point for a player Vi if it is a Vi-event point for B(Vi, Vi−1) or B(Vi+1, Vi). For each player Vi, consider a finite set of strategies Xi given by Xi = X ∪ {vj |vj is an event point for Vi}.
It has been shown in [9] that for any i = 2, . . . , n, the best response policy B(Vi, Vi−1) has at most 2n + 4 Vi-event points. As we require N > 5n, we have |Xi| ≤ 2N; assume without loss of generality that |Xi| = 2N. Order the elements of Xi in increasing order as x1 i = 0 < x2 i < · · · < x2N i . We will refer to the strategies in Xi as discrete strategies of player Vi; a strategy profile in which each player has a discrete strategy will be referred to as a discrete strategy profile.
We will now show that even we restrict each player Vi to strategies from Xi, the players can still achieve a Nash equilibrium, and moreover, the best such Nash equilibrium (with respect to the social welfare) has total payoff at least M(G) − as long as N is large enough.
Let s be a strategy profile that maximizes social welfare. That is, let s = (s1, . . . , sn) where si is the mixed strategy of player Vi and EP(s) = M(G). For i = 1, . . . , n, let ti = max{xj i | xj i ≤ si}.
First, we will show that the strategy profile t = (t1, . . . , tn) is a Nash equilibrium for G.
Fix any i, 1 < i ≤ n, and let R = [v1, v2]×[u1, u2] be the rectangle in B(Vi, Vi−1) that contains (si, si−1). As v1 is a Vi-event point of B(Vi, Vi−1), we have v1 ≤ ti, so the point (ti, si−1) is inside R. Similarly, the point u1 is a Vi−1-event point of B(Vi, Vi−1), so we have u1 ≤ ti−1, and therefore the point (ti, ti−1) is inside R.
This means that for any i, 1 < i ≤ n, we have ti−1 ∈ pbrVi−1 (ti), which implies that t = (t1, . . . , tn) is a Nash equilibrium for G.
Now, let us estimate the expected loss in social welfare caused by playing t instead of s.
LEMMA 3. For any pair of strategy profiles t, s such that |ti − si| ≤ δ we have |EPVi (s) − EPVi (t)| ≤ 24Pmaxδ for any i = 1, . . . , n.
PROOF. Let Pi klm be the payoff of the player Vi, when he plays k, Vi−1 plays l, and Vi+1 plays m. Fix i = 1, . . . , n and for k, l, m ∈ {0, 1}, set tklm = tk i−1(1 − ti−1)1−k tl i(1 − ti)1−l tm i+1(1 − ti+1)1−m sklm = sk i−1(1 − si−1)1−k sl i(1 − si)1−l sm i+1(1 − si+1)1−m .
We have |EPVi (s) − EPVi (t)| ≤ X k,l,m=0,1 |Pi klm(tklm − sklm )| ≤ 8Pmax max klm |tklm − sklm | We will now show that for any k, l, m ∈ {0, 1} we have |tklm − sklm | ≤ 3δ; clearly, this implies the lemma.
Indeed, fix k, l, m ∈ {0, 1}. Set x = tk i−1(1 − ti−1)1−k , x = sk i−1(1 − si−1)1−k , y = tl i(1 − ti)1−l , y = sl i(1 − si)1−l , z = tm i+1(1 − ti+1)1−m , z = sm i+1(1 − si+1)1−m .
Observe that if k = 0 then x − x = (1 − ti−1) − (1 − si−1), and if k = 1 then x − x = ti−1 − si−1, so |x − x | ≤ δ. A similar argument shows |y − y | ≤ δ, |z − z | ≤ δ. Also, we have x, x , y, y , z, z ∈ [0, 1]. Hence, |tklm −sklm | = |xyz−x y z | = |xyz − x yz + x yz − x y z + x y z − x y z | ≤ |x − x |yz + |y − y |x z + |z − z |x y ≤ 3δ.
Lemma 3 implies Pn i=1 |EPVi (s) − EPVi (t)| ≤ 24nPmaxδ, so by choosing δ < /(24nPmax), or, equivalently, setting N > 24nPmax/ , we can ensure that the total expected payoff for the strategy profile t is within from optimal.
We will now show that we can find the best discrete Nash equilibrium (with respect to the social welfare) using dynamic programming. As t is a discrete strategy profile, this means that the strategy profile found by our algorithm will be at least as good as t.
Define ml,k i to be the maximum total payoff that V1, . . . , Vi−1 can achieve if each Vj , j ≤ i, chooses a strategy from Xj , for each j < i the strategy of Vj is a potential best response to the strategy of Vj+1, and, moreover, Vi−1 plays xl i−1, Vi plays xk i . If there is no way to choose the strategies for V1, . . . , Vi−1 to satisfy these conditions, we set ml,k i = −∞. The values ml,k i , i = 1, . . . , n; k, l = 1, . . . , N, can be computed inductively, as follows.
We have ml,k 1 = 0 for k, l = 1, . . . , N. Now, suppose that we have already computed ml,k j for all j < i; k, l = 1, . . . , N. To compute mk,l i , we first check if (xk i , xl i−1) ∈ B(Vi, Vi−1). If this is not the case, we have ml,k i = −∞. Otherwise, consider the set Y = Xi−2 ∩ pbrVi−2 (xl i−1), i.e., the set of all discrete strategies of Vi−2 that are potential best responses to xl i−1. The proof of Theorem 1 in [9] implies that the set pbrVi−2 (xl i−1) is non-empty: the player Vi−2 has a potential best response to any strategy of Vi−1, in particular, xl i−1. By construction of the set Xi−2, this implies that Y is not empty. For each xj i−2 ∈ Y , let pjlk be the payoff that Vi−1 receives when Vi−2 plays xj i−2, Vi−1 plays xl i−1, and Vi plays xk i . Clearly, pjlk can be computed in constant time.
Then we have ml,k i = max{mj,l i−1 + pjlk | xj i−2 ∈ Y }.
Finally, suppose that we have computed ml,k n for l, k = 1, . . . , N.
We still need to take into account the payoff of player Vn. Hence, 167 we consider all pairs (xk n, xl n−1) that satisfy xl n−1 ∈ pbrVn−1 (xk n), and pick the one that maximizes the sum of mk,l n and the payoff of Vn when he plays xk n and Vn−1 plays xl n−1. This results in the maximum total payoff the players can achieve in a Nash equilibrium using discrete strategies; the actual strategy profile that produces this payoff can be reconstructed using standard dynamic programming techniques.
It is easy to see that each ml,k i can be computed in time O(N), i.e., all of them can be computed in time O(nN3 ). Recall that we have to select N ≥ (24nPmax)/ to ensure that the strategy profile we output has total payoff that is within from optimal. We conclude that we can compute an -approximation to the best Nash equilibrium in time O(n4 P3 max/ 3 ). This completes the proof of Theorem 3.
To state our result for the general case (i.e., when the underlying graph is a bounded-degree tree rather than a path), we need additional notation. If G has n players, let q(n) be an upper bound on the number of event points in the representation of any best response policy. That is, we assume that for any vertex U with parent V , B(V, U) has at most q(n) event points. We will be interested in the situation in which q(n) is polynomial in n.
THEOREM 4. Let G be an n-player graphical game on a tree in which each node has at most Δ children. Suppose we are given a set of best-response policies for G in which each best-response policy B(V, U) is represented by a set of rectangles with at most q(n) event points. For any > 0, there is an algorithm that constructs a Nash equilibrium s for G that satisfies EP(s ) ≥ M(G) − . The running time of the algorithm is polynomial in n, Pmax and −1 provided that the tree has bounded degree (that is, Δ = O(1)) and q(n) is a polynomial in n. In particular, if N = max((Δ + 1)q(n) + 1, n2Δ+2 (Δ + 2)Pmax −1 ) and Δ > 1 then the running time is O(nΔ(2N)Δ .
For the proof of this theorem, see [10].
multiplicative approximation The running time of our algorithm is pseudopolynomial rather than polynomial, because it includes a factor which is polynomial in Pmax, the maximum (in absolute value) entry in any payoff matrix. If we are interested in multiplicative approximation rather than additive one, this can be improved to polynomial.
First, note that we cannot expect a multiplicative approximation for all inputs. That is, we cannot hope to have an algorithm that computes a Nash equilibrium with total payoff at least (1 − )M(G). If we had such an algorithm, then for graphical games G with M(G) = 0, the algorithm would be required to output the optimal solution. To show that this is infeasible, observe that we can use the techniques of Section 3.2 to construct two integercoefficient graphical games on paths of length O(n) such that for some X ∈ R the maximal total payoff in the first game is X, the maximal total payoff in the second game is −X, and for both games, the strategy profiles that achieve the maximal total payoffs involve algebraic numbers of degree n. By combining the two games so that the first vertex of the second game becomes connected to the last vertex of the first game, but the payoffs of all players do not change, we obtain a graphical game in which the best Nash equilibrium has total payoff 0, yet the strategies that lead to this payoff have high algebraic complexity.
However, we can achieve a multiplicative approximation when all entries of the payoff matrices are positive and the ratio between any two entries is polynomially bounded. Recall that we assume that all payoffs are integer, and let Pmin > 0 be the smallest entry of any payoff matrix. In this case, for any strategy profile the payoff to player i is at least Pmin, so the total payoff in the social-welfare maximizing Nash equilibrium s satisfies M(G) ≥ nPmin.
Moreover, Lemma 3 implies that by choosing δ < /(24Pmax/Pmin), we can ensure that the Nash equilibrium t produced by our algorithm satisfies nX i=1 EPVi (s) − nX i=1 EPVi (t) ≤ 24Pmaxδn ≤ nPmin ≤ M(G), i.e., for this value of δ we have Pn i=1 EPVi (t) ≥ (1 − )M(G).
Recall that the running time of our algorithm is O(nN3 ), where N has to be selected to satisfy N > 5n, N = 1/δ. It follows that if Pmin > 0, Pmax/Pmin = poly(n), we can choose N so that our algorithm provides a multiplicative approximation guarantee and runs in time polynomial in n and 1/ .
Another natural way to define what is a good Nash equilibrium is to require that each player"s expected payoff exceeds a certain threshold. These thresholds do not have to be the same for all players. In this case, in addition to the payoff matrices of the n players, we are given n numbers T1, . . . , Tn, and our goal is to find a Nash equilibrium in which the payoff of player i is at least Ti, or report that no such Nash equilibrium exists. It turns out that we can design an FPTAS for this problem using the same techniques as in the previous section.
THEOREM 5. Given a graphical game G on an n-vertex path and n rational numbers T1, . . . , Tn, suppose that there exists a strategy profile s such that s is a Nash equilibrium for G and EPVi (s) ≥ Ti for i = 1, . . . , n. Then for any > 0 we can find in time O(max{nP3 max/ 3 , n4 / 3 }) a strategy profile s such that s is a Nash equilibrium for G and EPVi (s ) ≥ Ti − for i = 1, . . . , n.
PROOF. The proof is similar to that of Theorem 3. First, we construct the best response policies for all players, choose N > 5n, and construct the sets Xi, i = 1, . . . , n, as described in the proof of Theorem 3.
Consider a strategy profile s such that s is a Nash equilibrium for G and EPVi (s) ≥ Ti for i = 1, . . . , n. We construct a strategy profile ti = max{xj i | xj i ≤ si} and use the same argument as in the proof of Theorem 3 to show that t is a Nash equilibrium for G. By Lemma 3, we have |EPVi (s) − EPVi (t)| ≤ 24Pmaxδ, so choosing δ < /(24Pmax), or, equivalently, N > max{5n, 24Pmax/ }, we can ensure EPVi (t) ≥ Ti − for i = 1, . . . , n.
Now, we will use dynamic programming to find a discrete Nash equilibrium that satisfies EPVi (t) ≥ Ti − for i = 1, . . . , n. As t is a discrete strategy profile, our algorithm will succeed whenever there is a strategy profile s with EPVi (s) ≥ Ti− for i = 1, . . . , n.
Let zl,k i = 1 if there is a discrete strategy profile such that for any j < i the strategy of the player Vj is a potential best response to the strategy of Vj+1, the expected payoff of Vj is at least Tj − , and, moreover, Vi−1 plays xl i−1, Vi plays xk i . Otherwise, let zl,k i = 0.
We can compute zl,k i , i = 1, . . . , n; k, l = 1, . . . , N inductively, as follows.
We have zl,k 1 = 1 for k, l = 1, . . . , N. Now, suppose that we have already computed zl,k j for all j < i; k, l = 1, . . . , N. To compute zk,l i , we first check if (xk i , xl i−1) ∈ B(Vi, Vi−1). If this 168 is not the case, clearly, zk,l i = 0. Otherwise, consider the set Y = Xi−2 ∩pbrVi−2 (xl i−1), i.e., the set of all discrete strategies of Vi−2 that are potential best responses to xl i−1. It has been shown in the proof of Theorem 3 that Y = ∅. For each xj i−2 ∈ Y , let pjlk be the payoff that Vi−1 receives when Vi−2 plays xj i−2, Vi−1 plays xl i−1, and Vi plays xk i . Clearly, pjlk can be computed in constant time. If there exists an xj i−2 ∈ Y such that zj,l i−1 = 1 and pjlk ≥ Ti−2 − , set zl,k i = 1. Otherwise, set zl,k i = 0.
Having computed zl,k n , l, k = 1, . . . , N, we check if zl,k n = 1 for some pair (l, k). if such a pair of indices exists, we instruct Vn to play xk n and use dynamic programming techniques (or, equivalently, the downstream pass of the algorithm of [13]) to find a Nash equilibrium s that satisfies EPVi (s ) ≥ Ti − for i = 1, . . . , n (recall that Vn is a dummy player, i.e., we assume Tn = 0, EPn(s ) = 0 for any choice of s ). If zl,k n = 0 for all l, k = 1, . . . , N, there is no discrete Nash equilibrium s that satisfies EPVi (s ) ≥ Ti − for i = 1, . . . , n and hence no Nash equilibrium s (not necessarily discrete) such that EPVi (s) ≥ Ti for i = 1, . . . , n.
The running time analysis is similar to that for Theorem 3; we conclude that the running time of our algorithm is O(nN3 ) = O(max{nP3 max/ 3 , n4 / 3 }).
REMARK 1. Theorem 5 can be extended to trees of bounded degree in the same way as Theorem 4.
Another approach to finding Nash equilibria with bounded payoffs is based on inductively computing the subsets of the best response policies of all players so as to exclude the points that do not provide sufficient payoffs to some of the players. Formally, we say that a strategy v of the player V is a potential best response to a strategy w of its parent W with respect to a threshold vector T = (T1, . . . , Tn), (denoted by v ∈ pbrV (w, T)) if there is an equilibrium in the instance G(V,W ),W =w in which V plays mixed strategy v and the payoff to any player Vi downstream of V (including V ) is at least Ti. The best response policy for V with respect to a threshold vector T is defined as B(W, V, T) = {(w, v) | v ∈ pbrV (w, T), w ∈ [0, 1]}.
It is easy to see that if any of the sets B(Vj, Vj−1, T), j = 1, . . . , n, is empty, it means that it is impossible to provide all players with expected payoffs prescribed by T. Otherwise, one can apply the downstream pass of the original algorithm of [13] to find a Nash equilibrium. As we assume that Vn is a dummy vertex whose payoff is identically 0, the Nash equilibrium with these payoffs exists as long as Tn ≤ 0 and B(Vn, Vn−1, T) is not empty.
Using the techniques developed in [9], it is not hard to show that for any j = 1, . . . , n, the set B(Vj , Vj−1, T) consists of a finite number of rectangles, and one can compute B(Vj+1, Vj , T) given B(Vj , Vj−1, T). The advantage of this approach is that it allows us to represent all Nash equilibria that provide required payoffs to the players. However, it is not likely to be practical, since it turns out that the rectangles that appear in the representation of B(Vj , Vj−1, T) may have irrational coordinates.
CLAIM 6. There exists a graphical game G on a 3-vertex path UV W and a vector T = (T1, T2, T3) such that B(V, W, T) cannot be represented as a union of a finite number of rectangles with rational coordinates.
PROOF. We define the payoffs to the players in G as follows.
The payoff to U is identically 0, i.e., P0 (U) = P1 (U) = 0.
Using Claim 1, we select the payoffs to V so that P0 (V ) = uw,
P1 (V ) = P0 (V ) + w − .8u − .1, where u and w are the (mixed) strategies of U and W , respectively. It follows that V is indifferent between playing 0 and 1 if and only if w = f(u) = .8u + .1; observe that for any u ∈ [0, 1] we have f(u) ∈ [0, 1]. It is not hard to see that we have B(W, V ) = [0, .1]×{0} ∪ [.1, .9]×[0, 1] ∪ [.9, 1]×{1}.
The payoffs to W are not important for our construction; for example, set P0(W ) = P0(W ) = 0.
Now, set T = (0, 1/8, 0), i.e., we are interested in Nash equilibria in which V "s expected payoff is at least 1/8. Suppose w ∈ [0, 1]. The player V can play a mixed strategy v when W is playing w as long as U plays u = f−1 (w) = 5w/4 − 1/8 (to ensure that V is indifferent between 0 and 1) and P0 (V ) = P1 (V ) = uw = w(5w/4 − 1/8) ≥ 1/8. The latter condition is satisfied if w ≤ (1 − √ 41)/20 < 0 or w ≥ (1 + √ 41)/20. Note that we have .1 < (1 + √ 41)/20 < .9. For any other value of w, any strategy of U either makes V prefer one of the pure strategies or does not provide it with a sufficient expected payoff. There are also some values of w for which V can play a pure strategy (0 or 1) as a potential best response to W and guarantee itself an expected payoff of at least 1/8; it can be shown that these values of w form a finite number of segments in [0, 1]. We conclude that any representation of B(W, V, T) as a union of a finite number of rectangles must contain a rectangle of the form [(1 + √ 41)/20, w ]×[v , v ] for some w , v , v ∈ [0, 1].
On the other hand, it can be shown that for any integer payoff matrices and threshold vectors and any j = 1, . . . , n − 1, the sets B(Vj+1, Vj, T) contain no rectangles of the form [u , u ]×{v} or {v}×[w , w ], where v ∈ R\Q. This means that if B(Vn, Vn−1, T) is non-empty, i.e., there is a Nash equilibrium with payoffs prescribed by T, then the downstream pass of the algorithm of [13] can always pick a strategy profile that forms a Nash equilibrium, provides a payoff of at least Ti to the player Vi, and has no irrational coordinates. Hence, unlike in the case of the Nash equilibrium that maximizes the social welfare, working with irrational numbers is not necessary, and the fact that the algorithm discussed in this section has to do so can be seen as an argument against using this approach.
NASH EQUILIBRIUM In this section, we consider several other criteria that can be useful in selecting a Nash equilibrium.
bounds on payoffs In many real life scenarios, we want to maximize the social welfare subject to certain restrictions on the payoffs to individual players. For example, we may want to ensure that no player gets a negative expected payoff, or that the expected payoff to player i is at least Pi max − ξ, where Pi max is the maximum entry of i"s payoff matrix and ξ is a fixed parameter. Formally, given a graphical game G and a vector T1, . . . , Tn, let S be the set of all Nash equilibria s of G that satisfy Ti ≤ EPVi (s) for i = 1, . . . , n, and let ˆs = argmaxs∈S EP(s).
If the set S is non-empty, we can find a Nash equilibrium ˆs that is -close to satisfying the payoff bounds and is within from ˆs with respect to the total payoff by combining the algorithms of Section 4 and Section 5.
Namely, for a given > 0, choose δ as in the proof of Theorem 3, and let Xi be the set of all discrete strategies of player Vi (for a 169 formal definition, see the proof of Theorem 3). Combining the proofs of Theorem 3 and Theorem 5, we can see that the strategy profile ˆt given by ˆti = max{xj i | xj i ≤ ˆsi} satisfies EPVi (ˆt) ≥ Ti − , |EP(ˆs) − EP(ˆt)| ≤ .
Define ˆml,k i to be the maximum total payoff that V1, . . . , Vi−1 can achieve if each Vj, j ≤ i, chooses a strategy from Xj , for each j < i the strategy of Vj is a potential best response to the strategy of Vj+1 and the payoff to player Vj is at least Tj − , and, moreover, Vi−1 plays xl i−1, Vi plays xk i . If there is no way to choose the strategies for V1, . . . , Vi−1 to satisfy these conditions, we set ml,k i = −∞. The ˆml,k i can be computed by dynamic programming similarly to the ml,k i and zl,k i in the proofs of Theorems 3 and 5. Finally, as in the proof of Theorem 3, we use ml,k n to select the best discrete Nash equilibrium subject to the payoff constraints.
Even more generally, we may want to maximize the total payoff to a subset of players (who are assumed to be able to redistribute the profits fairly among themselves) while guaranteeing certain expected payoffs to (a subset of) the other players. This problem can be handled similarly.
A more egalitarian measure of the quality of a Nash equilibrium is the minimal expected payoff to a player. The optimal solution with respect to this measure is a Nash equilibrium in which the minimal expected payoff to a player is maximal. To find an approximation to such a Nash equilibrium, we can combine the algorithm of Section 5 with binary search on the space of potential lower bounds. Note that the expected payoff to any player Vi given a strategy s always satisfies −Pmax ≤ EPVi (s) ≤ Pmax.
For a fixed > 0, we start by setting T = −Pmax, T = Pmax,
T∗ = (T + T )/2. We then run the algorithm of Section 5 with T1 = · · · = Tn = T∗ . If the algorithm succeeds in finding a Nash equilibrium s that satisfies EPVi (s ) ≥ T∗ − for all i = 1, . . . , n, we set T = T∗ , T∗ = (T + T )/2; otherwise, we set T = T∗ , T∗ = (T + T )/2 and loop. We repeat this process until |T − T | ≤ . It is not hard to check that for any p ∈ R, if there is a Nash equilibrium s such that mini=1,...,n EPVi (s) ≥ p, then our algorithm outputs a Nash equilibrium s that satisfies mini=1,...,n EPVi (s) ≥ p−2 . The running time of our algorithm is O(max{nP3 max log −1 / 3 , n4 log −1 / 3 }).
When the players" payoff matrices are not very different, it is reasonable to demand that the expected payoffs to the players do not differ by much either. We will now show that Nash equilibria in this category can be approximated in polynomial time as well.
Indeed, observe that the algorithm of Section 5 can be easily modified to deal with upper bounds on individual payoffs rather than lower bounds. Moreover, we can efficiently compute an approximation to a Nash equilibrium that satisfies both the upper bound and the lower bound for each player. More precisely, suppose that we are given a graphical game G, 2n rational numbers T1, . . . , Tn, T1, . . . , Tn and > 0. Then if there exists a strategy profile s such that s is a Nash equilibrium for G and Ti ≤ EPVi (s) ≤ Ti for i = 1, . . . , n, we can find a strategy profile s such that s is a Nash equilibrium for G and Ti − ≤ EPVi (s ) ≤ Ti + for i = 1, . . . , n. The modified algorithm also runs in time O(max{nP3 max/ 3 , [4]n4 / 3 }).
This observation allows us to approximate Nash equilibria in which all players" expected payoffs differ by at most ξ for any fixed ξ > 0. Given an > 0, we set T1 = · · · = Tn = −Pmax,
T1 = · · · = Tn = −Pmax + ξ + , and run the modified version of the algorithm of Section 5. If it fails to find a solution, we increment all Ti, Ti by and loop. We continue until the algorithm finds a solution, or Ti ≥ Pmax.
Suppose that there exists a Nash equilibrium s that satisfies |EPVi (s) − EPVj (s)| ≤ ξ for all i, j = 1, . . . , n. Set r = mini=1,...,n EPVi (s); we have r ≤ EPVi (s) ≤ r + ξ for all i = 1, . . . , n. There exists a k ≥ 0 such that −Pmax + (k − 1) ≤ r ≤ −Pmax + k . During the kth step of the algorithm, we set T1 = · · · = Tn = −Pmax +(k−1) , i.e., we have r− ≤ Ti ≤ r, r + ξ ≤ Ti ≤ r + ξ + . That is, the Nash equilibrium s satisfies Ti ≤ r ≤ EPVi (s) ≤ r + ξ ≤ Ti , which means that when Ti is set to −Pmax + (k − 1) , our algorithm is guaranteed to output a Nash equilibrium t that satisfies r − 2 ≤ Ti − ≤ EPVi (t) ≤ Ti + ≤ r +ξ +2 . We conclude that whenever such a Nash equilibrium s exists, our algorithm outputs a Nash equilibrium t that satisfies |EPVi (t) − EPVj (t)| ≤ ξ + 4 for all i, j = 1, . . . , n.
The running time of this algorithm is O(max{nP3 max/ 4 , n4 / 4 }).
Note also that we can find the smallest ξ for which such a Nash equilibrium exists by combining this algorithm with binary search over the space ξ = [0, 2Pmax]. This identifies an approximation to the fairest Nash equilibrium, i.e., one in which the players" expected payoffs differ by the smallest possible amount.
Finally, note that all results in this section can be extended to bounded-degree trees.
We have studied the problem of equilibrium selection in graphical games on bounded-degree trees. We considered several criteria for selecting a Nash equilibrium, such as maximizing the social welfare, ensuring a lower bound on the expected payoff of each player, etc. First, we focused on the algebraic complexity of a social welfare-maximizing Nash equilibrium, and proved strong negative results for that problem. Namely, we showed that even for graphical games on paths, any algebraic number α ∈ [0, 1] may be the only strategy available to some player in all social welfaremaximizing Nash equilibria. This is in sharp contrast with the fact that graphical games on trees always possess a Nash equilibrium in which all players" strategies are rational numbers.
We then provided approximation algorithms for selecting Nash equilibria with special properties. While the problem of finding approximate Nash equilibria for various classes of games has received a lot of attention in recent years, most of the existing work aims to find -Nash equilibria that satisfy (or are -close to satisfying) certain properties. Our approach is different in that we insist on outputting an exact Nash equilibrium, which is -close to satisfying a given requirement. As argued in the introduction, there are several reasons to prefer a solution that constitutes an exact Nash equilibrium.
Our algorithms are fully polynomial time approximation schemes, i.e., their running time is polynomial in the inverse of the approximation parameter , though they may be pseudopolynomial with respect to the input size. Under mild restrictions on the inputs, they can be modified to be truly polynomial. This is the strongest positive result one can derive for a problem whose exact solutions may be hard to represent, as is the case for many of the problems considered here. While we prove our results for games on a path, they can be generalized to any tree for which the best response policies have compact representations as unions of rectangles. In the full version of the paper we describe our algorithms for the general case.
Further work in this vein could include extensions to the kinds of guarantees sought for Nash equilibria, such as guaranteeing total payoffs for subsets of players, selecting equilibria in which some players are receiving significantly higher payoffs than their peers, etc. At the moment however, it is perhaps more important to inves170 tigate whether Nash equilibria of graphical games can be computed in a decentralized manner, in contrast to the algorithms we have introduced here.
It is natural to ask if our results or those of [9] can be generalized to games with three or more actions. However, it seems that this will make the analysis significantly more difficult. In particular, note that one can view the bounded payoff games as a very limited special case of games with three actions per player. Namely, given a two-action game with payoff bounds, consider a game in which each player Vi has a third action that guarantees him a payoff of Ti no matter what everyone else does. Then checking if there is a Nash equilibrium in which none of the players assigns a nonzero probability to his third action is equivalent to checking if there exists a Nash equilibrium that satisfies the payoff bounds in the original game, and Section 5.1 shows that finding an exact solution to this problem requires new ideas.
Alternatively it may be interesting to look for similar results in the context of correlated equilibria (CE), especially since the best CE may have higher value (total expected payoff) than the best NE.
The ratio between these values is called the mediation value in [1].
It is known from [1] that the mediation value of 2-player, 2-action games with non-negative payoffs is at most 4 3 , and they exhibit a 3-player game for which it is infinite. Furthermore, a 2-player, 3action example from [1] also has infinite mediation value.
[1] I. Ashlagi, D. Monderer and M. Tenneholtz, On the Value of Correlation, Proceedings of Dagstuhl seminar 05011 (2005) [2] R. Aumann, Subjectivity and Correlation in Randomized Strategies, Journal of Mathematical Economics 1 pp. 67-96 (1974) [3] B. Blum, C. R. Shelton, and D. Koller, A Continuation Method for Nash Equilibria in Structured Games, Proceedings of IJCAI"03 [4] X. Chen, X. Deng and S. Teng, Computing Nash Equilibria: Approximation and Smoothed Complexity, Proceedings of FOCS"06 [5] X. Chen, X. Deng, Settling the Complexity of 2-Player Nash-Equilibrium, Proceedings of FOCS"06 [6] V. Conitzer and T. Sandholm, Complexity Results about Nash Equilibria, Proceedings of IJCAI"03 [7] C. Daskalakis, P. W. Goldberg and C. H. Papadimitriou, The Complexity of Computing a Nash Equilibrium, Proceedings of STOC"06 [8] R. S. Datta, Universality of Nash Equilibria, Mathematics of Operations Research 28:3, 2003 [9] E. Elkind, L. A. Goldberg, and P. W. Goldberg, Nash Equilibria in Graphical games on Trees Revisited,
Proceedings of ACM EC"06 [10] E. Elkind, L. A. Goldberg, and P. W. Goldberg, Computing Good Nash Equilibria in Graphical Games, http://arxiv.org/abs/cs.GT/0703133 [11] I. Gilboa and E. Zemel, Nash and Correlated Equilibria: Some Complexity Considerations, Games and Economic Behavior, 1 pp. 80-93 (1989) [12] P. W. Goldberg and C. H. Papadimitriou, Reducibility Among Equilibrium Problems, Proceedings of STOC"06 [13] M. Kearns, M. Littman, and S. Singh, Graphical Models for Game Theory, Proceedings of UAI"01 [14] M. Littman, M. Kearns, and S. Singh, An Efficient Exact Algorithm for Singly Connected Graphical Games,

Multiattribute trading mechanisms extend traditional, price-only mechanisms by facilitating the negotiation over a set of predefined attributes representing various non-price aspects of the deal. Rather than negotiating over a fully defined good or service, a multiattribute mechanism delays commitment to specific configurations until the most promising candidates are identified. For example, a procurement department of a company may use a multiattribute auction to select a supplier of hard drives. Supplier offers may be evaluated not only over the price they offer, but also over various qualitative attributes such as volume, RPM, access time, latency, transfer rate, and so on. In addition, suppliers may offer different contract conditions such as warranty, delivery time, and service.
In order to account for traders" preferences, the auction mechanism must extract evaluative information over a complex domain of multidimensional configurations. Constructing and communicating a complete preference specification can be a severe burden for even a moderate number of attributes, therefore practical multiattribute auctions must either accommodate partial specifications, or support compact expression of preferences assuming some simplified form. By far the most popular multiattribute form to adopt is the simplest: an additive representation where overall value is a linear combination of values associated with each attribute. For example, several recent proposals for iterative multiattribute auctions [2, 3, 8, 19] require additive preference representations.
Such additivity reduces the complexity of preference specification exponentially (compared to the general discrete case), but precludes expression of any interdependencies among the attributes. In practice, however, interdependencies among natural attributes are quite common. For example, the buyer may exhibit complementary preferences for size and access time (since the performance effect is more salient if much data is involved), or may view a strong warranty as a good substitute for high reliability ratings. Similarly, the seller"s production characteristics (such as increasing access time is harder for larger hard drives) can easily violate additivity. In such cases an additive value function may not be able to provide even a reasonable approximation of real preferences.
On the other hand, fully general models are intractable, and it is reasonable to expect multiattribute preferences to exhibit some structure. Our goal, therefore, is to identify the subtler yet more widely applicable structured representations, and exploit these properties of preferences in trading mechanisms.
We propose an iterative auction mechanism based on just such a flexible preference structure. Our approach is inspired by the design of an iterative multiattribute procurement auction for additive preferences, due to Parkes and Kalagnanam (PK) [19]. PK propose two types of iterative auctions: the first (NLD) makes no assumptions about traders" preferences, and lets sellers bid on the full multidimensional attribute space. Because NLD maintains an exponential price structure, it is suitable only for small domains. The other auction (AD) assumes additive buyer valuation and seller cost functions. It collects sell bids per attribute level and for a single discount term. The price of a configuration is defined as the sum of the prices of the chosen attribute levels minus the discount.
The auction we propose also supports compact price spaces, albeit for levels of clusters of attributes rather than singletons. We employ a preference decomposition based on generalized additive independence (GAI), a model flexible enough to accommodate interdependencies to the exact degree of accuracy desired, yet providing a compact functional form to the extent that interdependence can be limited. Given its roots in multiattribute utility theory [13], 227 the GAI condition is defined with respect to the expected utility function. To apply it for modeling values for certain outcomes, therefore, requires a reinterpretation for preference under certainty.
To this end, we exploit the fact that auction outcomes are associated with continuous prices, which provide a natural scale for assessing magnitude of preference.
We first lay out a representation framework for preferences that captures, in addition to simple orderings among attribute configuration values, the difference in the willingness to pay (wtp) for each.
That is, we should be able not only to compare outcomes but also decide whether the difference in quality is worth a given difference in price. Next, we build a direct, formally justified link from preference statements over priced outcomes to a generalized additive decomposition of the wtp function. After laying out this infrastructure, we employ this representation tool for the development of a multiattribute iterative auction mechanism that allows traders to express their complex preferences in GAI format. We then study the auction"s allocational, computational, and practical properties.
In Section 2 we present essential background on our representation framework, the measurable value function (MVF). Section 3 develops new multiattribute structures for MVF, supporting generalized additive decompositions. Next, we show the applicability of the theoretical framework to preferences in trading. The rest of the paper is devoted to the proposed auction mechanism.
As mentioned, most tools facilitating expression of multiattribute value for trading applications assume that agents" preferences can be represented in an additive form. By way of background, we start by introducing the formal prerequisites justifying the additive representation, as provided by multiattribute utility theory. We then present the generalized additive form, and develop the formal underpinnings for measurable value needed to extend this model to the case of choice under certainty.
Let Θ denote the space of possible outcomes, with a preference relation (weak total order) over Θ. Let A = {a0, . . . , am} be a set of attributes describing Θ. Capital letters denote subsets of variables, small letters (with or without numeric subscripts) denote specific variables, and ¯X denotes the complement of X with respect to A. We indicate specific variable assignments with prime signs or superscripts. To represent an instantiation of subsets X, Y at the same time we use a sequence of instantiation symbols, as in X Y .
DEFINITION 1. A set of attributes Y ⊂ A is preferentially independent (PI) of its complement Z = A \ Y if the conditional preference order over Y given a fixed level Z0 of Z is the same regardless of the choice of Z0 .
In other words, the preference order over the projection of A on the attributes in Y is the same for any instantiation of the attributes in Z.
DEFINITION 2. A = {a1, . . . , am} is mutually preferentially independent (MPI) if any subset of A is preferentially independent of its complement.
The preference relation when no uncertainty is modeled is usually represented by a value function v [17]. The following fundamental result greatly simplifies the value function representation.
THEOREM 1 ([9]). A preference order over set of attributes A has an additive value function representation v(a1, . . . , am) = mX i=1 vi(ai) iff A is mutually preferential independent.
Essentially, the additive forms used in trading mechanisms assume mutual preferential independence over the full set of attributes, including the money attribute. Intuitively that means that willingness to pay for value of an attribute or attributes cannot be affected by the value of other attributes.
A cardinal value function representing an ordering over certain outcomes need not in general coincide with the cardinal utility function that represents preference over lotteries or expected utility (EU). Nevertheless, EU functions may possess structural properties analogous to that for value functions, such as additive decomposition. Since the present work does not involve decisions under uncertainty, we do not provide a full exposition of the EU concept.
However we do make frequent reference to the following additive independence relations.
DEFINITION 3. Let X, Y, Z be a partition of the set of attributes A. X and Y are conditionally additive independent given Z, denoted as CAI(X, Y | Z), if preferences over lotteries on A depend only on their marginal conditional probability distributions over X and Y .
DEFINITION 4. Let I1, . . . , Ig ⊆ A such that Sg i=1 Ii = A.
I1, . . . , Ig are called generalized additive independent (GAI) if preferences over lotteries on A depend only on their marginal distributions over I1, . . . , Ig.
An (expected) utility function u(·) can be decomposed additively according to its (possibly overlapping) GAI sub-configurations.
THEOREM 2 ([13]). Let I1, . . . , Ig be GAI. Then there exist functions f1, . . . , fg such that u(a1, . . . , am) = g X r=1 fr(Ir). (1) What is now known as the GAI condition was originally introduced by Fishburn [13] for EU, and was named GAI and brought to the attention of AI researchers by Bacchus and Grove [1].
Graphical models and elicitation procedures for GAI decomposable utility were developed for EU [4, 14, 6], for a cardinal representation of the ordinal value function [15], and for an ordinal preference relations corresponding to a TCP-net structure by Brafman et al. [5].
Apart from the work on GAI in the context of preference handling that were discussed above, GAI have been recently used in the context of mechanism design by Hyafil and Boutilier [16], as an aid in direct revelation mechanisms.
As shown by Bacchus and Grove [1], GAI structure can be identified based on a set of CAI conditions, which are much easier to detect and verify. In general, utility functions may exhibit GAI structure not based on CAI. However, to date all proposals for reasoning and eliciting utility in GAI form take advantage of the GAI structure primarily to the extent that it represents a collection of CAI conditions. For example, GAI trees [14] employ triangulation of the CAI map, and Braziunas and Boutilier"s [6] conditional set Cj of a set Ij corresponds to the CAI separating set of Ij.
Since the CAI condition is also defined based on preferences over lotteries, we cannot apply Bacchus and Grove"s result without first establishing an alternative framework based on priced outcomes. We develop such a framework using the theory of measurable value functions, ultimately producing a GAI decomposition 228 (Eq. 1) of the wtp function. Readers interested primarily in the multiattribute auction and willing to grant the well-foundedness of the preference structure may skip down to Section 5.
Trading decisions represent a special case of decisions under certainty, where choices involve multiattribute outcomes and corresponding monetary payments. In such problems, the key decision often hinges on relative valuations of price differences compared to differences in alternative configurations of goods and services.
Theoretically, price can be treated as just another attribute, however, such an approach fails to exploit the special character of the money dimension, and can significantly add to complexity due to the inherent continuity and typical wide range of possible monetary outcome values.
We build on the fundamental work of Dyer and Sarin [10, 11] on measurable value functions (MVFs). As we show below, wtp functions in a quasi-linear setting can be interpreted as MVFs. However we first present the MVF framework in a more generic way, where the measurement is not necessarily monetary. We present the essential definitions and refer to Dyer and Sarin for more detailed background and axiomatic treatment. The key concept is that of preference difference. Let θ1 , θ2 , ϑ1 , ϑ2 ∈ Θ such that θ1 θ2 and ϑ1 ϑ2 . [θ2 , θ1 ] denotes the preference difference between θ2 and θ1 , interpreted as the strength, or degree, to which θ2 is preferred over θ1 . Let ∗ denote a preference order over Θ × Θ. We interpret the statement [θ2 , θ1 ] ∗ [ϑ2 , ϑ1 ] as the preference of ϑ2 over ϑ1 is at least as strong as the preference of θ2 over θ1 . We use the symbol ∼∗ to represent equality of preference differences.
DEFINITION 5. u : D → is a measurable value function (MVF) wrt ∗ if for any θ1 , θ2 , ϑ1 , ϑ2 ∈ D, [θ2 , θ1 ] ∗ [ϑ2 , ϑ1 ] ⇔ u(θ2 ) − u(θ1 ) ≤ u(ϑ2 ) − u(ϑ1 ).
Note that an MVF can also be used as a value function representing , since [θ , θ] ∗ [θ , θ] iff θ θ .
DEFINITION 6 ([11]). Attribute set X ⊂ A is called difference independent of ¯X if for any two assignments X1 ¯X X2 ¯X , [X1 ¯X , X2 ¯X ] ∼∗ [X1 ¯X , X2 ¯X ] for any assignment ¯X .
Or, in words, the preference differences on assignments to X given a fixed level of ¯X do not depend on the particular level chosen for ¯X.
As with additive independence for EU, this condition is stronger than preferential independence of X. Also analogously to EU, mutual preferential independence combined with other conditions leads to additive decomposition of the MVF. Moreover, Dyer and Sarin [11] have defined analogs of utility independence [17] for MVF, and worked out a parallel set of decomposition results.
Our first step is to generalize Definition 6 to a conditional version.
DEFINITION 7. Let X, Y, Z be a partition of the set of attributes A. X is conditionally difference independent of Y given Z, denoted as CDI(X, Y | Z), if ∀ instantiations ˆZ, X1 , X2 , Y 1 , Y 2 [X1 Y 1 ˆZ, X2 Y 1 ˆZ] ∼ [X1 Y 2 ˆZ, X2 Y 2 ˆZ].
Since the conditional set is always the complement, we sometimes leave it implicit, using the abbreviated notation CDI(X, Y ).
CDI leads to a decomposition similar to that obtained from CAI [17].
LEMMA 3. Let u(A) be an MVF representing preference differences. Then CDI(X, Y | Z) iff u(A) = u(X0 , Y, Z) + u(X, Y 0 , Z) − u(X0 , Y 0 , Z).
To complete the analogy with CAI, we generalize Lemma 3 as follows.
PROPOSITION 4. CDI(X, Y | Z) iff there exist functions ψ1(X, Z) and ψ2(Y, Z), such that u(X, Y, Z) = ψ1(X, Z) + ψ2(Y, Z). (2) An immediate result of Proposition 4 is that CDI is a symmetric relation.
The conditional independence condition is much more applicable than the unconditional one. For example, if attributes a ∈ X and b /∈ X are complements or substitutes, X cannot be difference independent of ¯X. However, X \ {a} may still be CDI of ¯X given a.
A single CDI condition decomposes the value function into two parts. We seek a finer-grain global decomposition of the utility function, similar to that obtained from mutual preferential independence. For this purpose we are now ready to employ the results of Bacchus and Grove [1], who establish that the CAI condition has a perfect map [20]; that is, there exists a graph whose nodes correspond to the set A, and its node separation reflects exactly the complete set of CAI conditions on A. Moreover, they show that the utility function decomposes over the set of maximal cliques of the perfect map. Their proofs can be easily adapted to CDI, since they only rely on the decomposition property of CAI that is also implied by CDI according to Proposition 4.
THEOREM 5. Let G = (A, E) be a perfect map for the CDI conditions on A. Then u(A) = g X r=1 fr(Ir), (3) where I1, . . . , Ig are (overlapping) subsets of A, each corresponding to a maximal clique of G.
Given Theorem 5, we can now identify an MVF GAI structure from a collection of CDI conditions. The CDI conditions, in turn, are particularly intuitive to detect when the preference differences carry a direct interpretation, as in the case with monetary differences discussed below. Moreover, the assumption or detection of CDI conditions can be performed incrementally, until the MVF is decomposed to a reasonable dimension. This is in contrast with the fully additive decomposition of MVF that requires mutual preferential independence [11].
Theorem 5 defines a decomposition structure, but to represent the actual MVF we need to specify the functions over the cliques. 229 The next theorem establishes that the functional constituents of MVF are the same as those for GAI decompositions as defined by Fishburn [13] for EU. We adopt the following conventional notation. Let (a0 1, . . . , a0 m) be a predefined vector called the reference outcome. For any I ⊆ A, the function u([I]) stands for the projection of u(A) to I where the rest of the attributes are fixed at their reference levels.
THEOREM 6. Let G = (A, E) be a perfect map for the CDI condition on A, and {I1, . . . , Ig} a set of maximal cliques as defined in Theorem 5. Then the functional decomposition from that theorem can be defined as f1 = u([I1]), and for r = 2, . . . , g (4) fr = u([Ir]) + r−1X k=1 (−1)k X 1≤i1<···<ik<r u([ k\ s=1 Iis ∩ Ir]) The proof directly shows that if graph G = (A, E) is a perfect map of CDI, u(A) decomposes to a sum over the functions defined in (4).1 Thus this proof does not rely on the decomposition result of Theorem 5, only on the existence of the perfect map.
To summarize, the results of this section generalize additive MVF theory. In particular it justifies the application of methods recently developed under the EU framework [1, 4, 14, 6] to representation of value under certainty.
In this section we apply measurable value to represent differences of willingness to pay for outcomes. We assume that the agent has a preference order over outcome space, represented by a set of attributes A, and an attribute p representing monetary consequence.
Note that in evaluating a purchase decision, p would correspond to the agent"s money holdings net of the transaction (i.e., wealth after purchase), not the purchase price. An outcome in this space is represented for example by (θ , p ), where θ is an instantiation of A and p is a value of p. We further assume that preferences are quasi-linear in p, that is there exists a value function of the form v(A, p) = u(A) + L(p), where L is a positive linear function.2 The quasi-linear form immediately qualifies money as a measure of preference differences, and establishes a monetary scale for u(A).
DEFINITION 8. Let v(A, p) = u(A)+L(p) represent , where p is the attribute representing money. We call u(A) a willingnessto-pay (wtp) function.
Note that wtp may also refer to the seller"s willingness to accept function. The wtp u(A) is a cardinal function, unique up to a positive linear transformation. Since (θ1, p ) (θ2, p ) ⇔ u(θ1) − u(θ2) ≤ L(p − p ), (where θ1, θ1 ∈ Θ, the domain of A) the wtp function can be used to choose among priced outcomes. 1 This proof and most other proofs in this paper are omitted for space consideration, and are available in an online appendix. 2 In many procurement applications, the deals in question are small relative to the enterprises involved, so the quasi-linearity assumption is warranted. This assumption can be relaxed to a condition called corresponding tradeoffs [17], which does not require the value over money to be linear. To simplify the presentation, however, we maintain the stronger assumption.
Naturally, elicitation of wtp function is most intuitive when using direct monetary values. In other words, we elicit a function in which L(p) = p, so v(A, p) = u(A) + p. We define a reference outcome (θ0 , p0 ), and assuming continuity of p, for any assignment ˆθ there exists a ˆp such that (ˆθ, ˆp) ∼ (θ0 , p0 ). As v is normalized such that v(θ0 , p0 ) = 0, ˆp is interpreted as the wtp for ˆθ, or the reserve price of ˆθ.
PROPOSITION 7. The wtp function is an MVF over differences in the reserve prices.
We note that the wtp function is used extensively in economics, and that all the development in Section 3 could be performed directly in terms of wtp, relying on quasi-linearity for preference measurement, and without formalization using MVFs. This formalization however aligns this work with the fundamental difference independence theory by Dyer and Sarin.
In addition to facilitating the detection of GAI structure, the CDI condition supports elicitation using local queries, similar to how CAI is used by Braziunas and Boutilier [6]. We adopt their definition of conditional set of Ir, noted here Sr, as the set of neighbors of attributes in Ir not including the attributes of Ir. Clearly, Sr is the separating set of Ir in the CDI map, hence CDI(Ir, Vr), where Vr = A \ (Ir ∪ Sr). From the definition of CDI, for any V 1 r , V 2 r we have: u(I1 r S0 r V 1 r ) − u(I2 r S0 r V 1 r ) = u(I1 r S0 r V 2 r ) − u(I2 r S0 r V 2 r ).
Eliciting the wtp function therefore amounts to eliciting the utility (wtp) of one full outcome (the reference outcome θ0 ), and then obtaining the function over each maximal clique using monetary differences between its possible assignments (technique known as pricing out [17]), keeping the variables in the conditional set fixed.
These ceteris paribus elicitation queries are local in the sense that the agent does not need to consider the values of the rest of the attributes. Furthermore, in eliciting MVFs we can avoid the global scaling step that is required for EU functions. Since the preference differences are extracted with respect to specific amounts of the attribute p, the utility is already scaled according to that external measure. Hence, once the conditional utility functions u([Ij]) are obtained, we can calculate u(A) according to (4).
This last step may require (in the worst case) computation of a number of terms that is exponential in the number of max cliques.
In practice however we do not expect the intersection of the cliques to go that deep; intersection of more than just a few max cliques would normally be empty. To take advantage of that we can use the search algorithm suggested by Braziunas and Boutilier [6], which efficiently finds all the nonempty intersections for each clique.
As shown, the wtp function can be used directly for pairwise comparisons of priced outcomes. Another preference query often treated in the literature is optimization, or choice of best outcome, possibly under constraints.
Typical decisions about exchange of a good or service exhibit what we call first-order preferential independence (FOPI), under which most or all single attributes have a natural ordering of quality, independent of the values of the rest.3 For example, when choosing a PC we always prefer more memory, faster CPU, longer warranty, and so on. Under FOPI, the unconstrained optimization of 3 This should not be mistaken with the highly demanding condition of mutual preferential independence, that requires all tradeoffs between attributes to be independent. 230 unpriced outcomes is trivial, hence we consider choice among attribute points with prices. Since any outcome can be best given enough monetary compensation, this problem is not well-defined unless the combinations are constrained somehow.
A particularly interesting optimization problem arises in the context of negotiation, where we consider the utility of both buyers and sellers. The multiattribute matching problem (MMP) [12] is concerned with finding an attribute point that maximizes the surplus of a trade, or the difference between the utilities of the buyer and the seller, ub(A) − us(A). GAI, as an additive decomposition, has the property that if ub and us are in GAI form then ub(A)−us(A) is in GAI form as well. We can therefore use combinatorial optimization procedures for GAI decomposition, based on the well studied variable elimination schemes (e.g., [15]) to find the best trading point.
Similarly, this optimization can be done to maximize surplus between a trader"s utility function and a pricing system that assigns a price to each level of each GAI element, and this way guide traders to their optimal bidding points. In the rest of the paper we develop a multiattribute procurement auction that builds on this idea.
In the procurement setting a single buyer wishes to procure a single good, in some configuration θ ∈ Θ from one of the candidate sellers s1, . . . , sn. The buyer has some private valuation function (wtp) ub : Θ → R, and similarly each seller si has a private valuation function (willingness-to-accept). For compliance with the procurement literature we refer to seller si"s valuation as a cost function, denoted by ci. The multiattribute allocation problem (MAP) [19] is the welfare optimization problem in procurement over a discrete domain, and it is defined as: i∗ , θ∗ = arg max i,θ (ub(θ) − ci(θ)). (5) To illustrate the need for a GAI price space we consider the case of traders with non-additive preferences bidding in an additive price space such as in PK"s auction AD. If the buyer"s preferences are not additive, choosing preferred levels per attribute (as in auction AD) admits undesired combinations and fails to guide the sellers to the efficient configurations. Non-additive sellers face an exposure problem, somewhat analogous to traders with complementary preferences that participate in simultaneous auctions. A value a1 for attribute a may be optimal given that the value of another attribute b is b1 , and arbitrarily suboptimal given other values of b.
Therefore bidding a1 and b1 may result in a poor allocation if the seller is outbid on b1 but left holding a1 .4 Instead of assuming full additivity, the auction designer can come up with a GAI preference structure that captures the set of common interdependencies between attributes. If traders could bid on clusters of interdependent attributes, it would solve the problems discussed above. For example, if a and b are interdependent (meaning CDI(a, b) does not hold), we should be able to bid on the cluster ab. If b in turn depends on c, we need another cluster bc. This is still better than a general pricing structure that solicits bids for the cluster abc. We stress that each trader may have a different set of interdependencies, and therefore to be completely general the 4 If only the sellers are non-additive, the auction design could potentially alleviate this problem by collecting a new set of bids each round and forgetting bids from previous rounds, and also guiding non-additive sellers to bid on only one level per attribute in order to avoid undesired combinations. ya yb    yc Z Z Z (i)   ¨ ©a, b   ¨ ©b, c (ii) Figure 1: (i) CDI map for {a, b, c}, reflecting the single condition CDI(a, c). (ii) The corresponding GAI network.
GAI structure needs to account for all.5 However, in practice many domains have natural dependencies that are mutual to traders.
Assume that preferences of all traders are reflected in a GAI structure I1, . . . , Ig. We call each Ir a GAI element, and any assignment to Ir a sub-configuration. We use θr to denote the subconfiguration formed by projecting configuration θ to element Ir.
DEFINITION 9. Let α be an assignment to Ir and β an assignment to Ir . The sub-configurations α and β are consistent if for any attribute aj ∈ Ir ∩ Ir , α and β agree on the value of aj. A collection ν of sub-configurations is consistent if all pairs α, β ∈ ν are consistent. The collection is called a cover if it contains exactly one sub-configuration αr corresponding to each element Ir, r ∈ {1, . . . , g}.
Note that a consistent cover {α1, . . . , αg} represents a full configuration, which we denote by (α1, . . . , αg).
A GAI network is a graph G whose nodes correspond to the GAI elements I1, . . . , Ig, with an edge between Ir, Ir iff Ir ∩ Ir = ∅.
Equivalently, a GAI network is the clique graph of a CDI-map.
In order to justify the compact pricing structure we require that for any set of optimal configurations (wrt a given utility function), with a corresponding collection of sub-configurations γ, all consistent covers in γ must be optimal configurations as well. To ensure this (see Lemmas 8 and 10), we assume a GAI decomposition in the form of a tree or a forest (the GAI tree). A tree structure can be achieved for any set of CDI conditions by triangulation of the CDI-map prior to construction of the clique graph (GAI networks and GAI trees are defined by Gonzales and Perny [14], who also provide a triangulation algorithm). Under GAI, the buyer"s value function ub and sellers" cost functions ci can be decomposed as in (1). We use fb,r and fi,r to denote the local functions of buyer and sellers (respectively), according to (4).
For example, consider the procurement of a good with three attributes, a, b, c. Each attribute"s domain has two values (e.g., {a1 , a2 } is the domain of A). Let the GAI structure be I1 = {a, b}, I2 = {b, c}. Figure 1 shows the simple CDI map and the corresponding GAI network, which is a GAI tree. Here, subconfigurations are assignments of the form a1 b1 , a1 b2 , b1 c1 , and so on. The set of sub-configurations {a1 b1 , b1 c1 } is a consistent cover, corresponding to the configuration a1 b1 c1 . In contrast, the set {a1 b1 , b2 c1 } is inconsistent.
We define an iterative multiattribute auction that maintains a GAI pricing structure: that is, a price pt (·) corresponding to each subconfiguration of each GAI-tree element. The price of a configuration θ at time t is defined as pt (θ) = g X r=1 pt (θr) − Δ. 5 We relax this requirement in Section 6. 231 Bidders submit sub-bids on sub-configurations and on an additional global discount term Δ.6 Sub-bids are always submitted for current prices, and need to be resubmitted at each round, therefore they do not need to explicitly carry the price. The set of full bids of a seller contains all consistent covers that can be generated from that seller"s current set of sub-bids. The existence of a full bid over a configuration θ represents the seller"s willingness to accept the price pt (θ) for supplying θ.
At the start of the auction, the buyer reports (to the auction, not to sellers) her complete valuation in GAI form. The initial prices of sub-configurations are set at some level above the buyer"s valuations, that is, p1 (θr) > fb,r(θr) for all θr. The discount Δ is initialized to zero. The auction has the dynamics of a descending clock auction: at each round t, bids are collected for current prices and then prices are reduced according to price rules. A seller is considered active in a round if she submits at least one full bid. In round t > 1, only sellers who where active in round t − 1 are allowed to participate, and the auction terminates when no more than a single seller is active. We denote the set of sub-bids submitted by si by Bt i , and the corresponding set of full bids is Bt i = {θ = (θ1, . . . , θg) ∈ Θ | ∀r.θr ∈ Bt i }.
In our example, a seller could submit sub-bids on a set of subconfigurations such as a1 b1 and b1 c1 , and that combines to a full bid on a1 b1 c1 .
The auction proceeds in two phases. In the first phase (A), at each round t the auction computes a set of preferred sub-configurations Mt . Section 5.4 shows how to define Mt to ensure convergence, and Section 5.5 shows how to efficiently compute it.
In phase A, the auction adjusts prices after each round, reducing the price of every sub-configuration that has received a bid but is not in the preferred set. Let be the prespecified price increment parameter. Specifically, the phase A price change rule is applied to all θr ∈ Sn i=1 Bt i \ Mt : pt+1 (θr) ← max(pt (θr) − g , fb,r(θr)). [A] The RHS maximum ensures that prices do not get reduced below the buyer"s valuation in phase A.
Let Mt denote the set of configurations that are consistent covers in Mt : Mt = {θ = (θ1, . . . , θg) ∈ Θ | ∀r.θr ∈ Mt } The auction switches to phase B when all active sellers have at least one full bid in the buyer"s preferred set: ∀i. Bt i = ∅ ∨ Bt i ∩ Mt = ∅. [SWITCH] Let T be the round at which [SWITCH] becomes true. At this point, the auction selects the buyer-optimal full bid ηi for each seller si. ηi = arg max θ∈BT i (ub(θ) − pT (θ)). (6) In phase B, si may bid only on ηi. The prices of sub-configurations are fixed at pT (·) during this phase. The only adjustment in phase B is to Δ, which is increased in every round by . The auction terminates when at most one seller (if exactly one, designate it sˆi) is active. There are four distinct cases:
holds). The auction returns with no allocation. 6 The discount term could be replaced with a uniform price reduction across all sub-configurations.
auction selects the best seller (sˆi) from the preceding round, and applies the applicable case below.
buyer"s valuation, pT (ηˆi) − Δ > ub(ηˆi). The auction offers the winner sˆi an opportunity to supply ηˆi at price ub(ηˆi).
(ηˆi)− Δ ≤ ub(ηˆi). This is the ideal situation, where the auction allocates the chosen configuration and seller at this resulting price.
The overall auction is described by high-level pseudocode in Algorithm 1. As explained in Section 5.4, the role of phase A is to guide the traders to their efficient configurations. Phase B is a one-dimensional competition over the surplus that remaining seller candidates can provide to the buyer. In Section 5.5 we discuss the computational tasks associated with the auction, and Section 5.6 provides a detailed example.
Algorithm 1 GAI-based multiattribute auction collect a reported valuation, ˆv from the buyer set high initial prices, p1 (θr) on each level θr, and set Δ = 0 while not [SWITCH] do collect sub-bids from sellers compute Mt apply price change by [A] end while compute ηi while more than one active seller do increase Δ by collect bids on (ηi, Δ) from sellers end while implement allocation and payment to winning seller
When the optimal solution to MAP (5) provides negative welfare and sellers do not bid below their cost, the auction terminates in phase A, no trade occurs and the auction is trivially efficient. We therefore assume throughout the analysis that the optimal (seller,configuration) pair provides non-negative welfare.
The buyer profit from a configuration θ is defined as7 πb(θ) = ub(θ) − p(θ) and similarly πi(θ) = p(θ) − ci(θ) is the profit of si. In addition, for μ ⊆ {1, . . . , g} we denote the corresponding set of subconfigurations by θμ, and define the profit from a configuration θ over the subset μ as πb(θμ) = X r∈μ (fb,r(θr) − p(θr)). πi(θμ) is defined similarly for si. Crucially, for any μ and its complement ¯μ and for any trader τ, πτ (θ) = πτ (θμ) + πτ (θ¯μ).
The function σi : Θ → R represents the welfare, or surplus function ub(·) − ci(·). For any price system p, σi(θ) = πb(θ) + πi(θ). 7 We drop the t superscript in generic statements involving price and profit functions, understanding that all usage is with respect to the (currently) applicable prices. 232 Since we do not assume anything about the buyer"s strategy, the analysis refers to profit and surplus with respect to the face value of the buyer"s report. The functions πi and σi refer to the true cost functions of si.
DEFINITION 10. A seller is called Straightforward Bidder (SB) if at each round t she bids on Bt i as follows: if maxθ∈Θ πt i (θ) < 0, then Bt i = ∅. Otherwise let Ωt i ⊆ arg max θ∈Θ πt i (θ) Bt i = {θr | θ ∈ Ωt i, r ∈ {1, . . . , g}}.
Intuitively, an SB seller follows a myopic best response strategy (MBR), meaning they bid myopically rather than strategically by optimizing their profit with respect to current prices. To calculate Bt i sellers need to optimize their current profit function, as discussed in Section 4.2.
The following lemma bridges the apparent gap between the compact pricing and bid structure and the global optimization performed by the traders.
LEMMA 8. Let Ψ be a set of configurations, all maximizing profit for a trader τ (seller or buyer) at the relevant prices. Let Φ = {θr | θ ∈ Ψ, r ∈ {1, . . . , g}. Then any consistent cover in Φ is also a profit-maximizing configuration for τ.
Proof sketch (full proof in the online appendix): A source of an element θr is a configuration ˜θ ∈ Ψ from which it originated (meaning, ˜θr = θr). Starting from the supposedly suboptimal cover θ1 , we build a series of covers θ1 , . . . , θL . At each θj we flip the value of a set of sub-configurations μj corresponding to a subtree, with the sub-configurations of the configuration ˆθj ∈ Ψ which is the source of the parent γj of μj . That ensures that all elements in μj ∪ {γj} have a mutual source ˆθj . We show that all θj are consistent and that they must all be suboptimal as well, and since all elements of θL have a mutual source, meaning θL = ˆθL ∈ Ψ, it contradicts optimality of Ψ.
COROLLARY 9. For SB seller si, ∀t, ∀θ ∈ Bt i , πt i (θ ) = max θ∈Θ πt i (θ).
Next we consider combinations of configurations that are only within some δ of optimality.
LEMMA 10. Let Ψ be a set of configurations, all are within δ of maximizing profit for a trader τ at the prices, and Φ defined as in Lemma 8. Then any consistent cover in Φ is within δg of maximizing utility for τ.
This bound is tight, that is for any GAI tree and a non-trivial domain we can construct a set Ψ as above in which there exists a consistent cover whose utility is exactly δg below the maximal.
Next we formally define Mt . For connected GAI trees, Mt is the set of sub-configurations that are part of a configuration within of optimal. When the GAI tree is in fact a forest, we apportion the error proportionally across the disconnected trees. Let G be comprised of trees G1, . . . , Gh. We use θj to denote the projection of a configuration θ on the tree Gj , and gj denotes the number of GAI elements in Gj .
Mt j = {θr | πt b(θj) ≥ max θj ∈Θj πt b(θj ) − gj g , r ∈ Gj } Then define Mt = Sh j=1 Mt j.
Let ej = gj −1 denote the number of edges in Gj . We define the connectivity parameter, e = maxj=1,...,h ej . As shown below, this connectivity parameter is an important factor in the performance of the auction.
COROLLARY 11. ∀θ ∈ Mt , πt b(θ ) ≥ max θ∈Θ πt b(θ) − (e + 1) In the fully additive case this loss of efficiency reduces to . On the other extreme, if the GAI network is connected then e+1 = g. We also note that without assuming any preference structure, meaning that the CDI map is fully connected, g = 1 and the efficiency loss is again .
Lemmas 12 through 15 show that through the price system, the choice of buyer preferred configurations, and price change rules,
Phase A leads the buyer and each of the sellers to their mutually efficient configuration.
LEMMA 12. maxθ∈Θ πt b(θ) does not change in any round t of phase A.
PROOF. We prove the lemma per each tree Gj. The optimal values for disconnected components are independent of each other hence if the maximal profit for each component does not change the combined maximal profit does not change as well. If the price of θj was reduced during phase A, that is pt+1 (θj) = pt (θj ) − δ, it must be the case that some w ≤ gj sub-configurations of θj are not in Mt j, and δ = w g . The definition of Mt j ensures πt b(θj ) < max θ∈Θ πt b(θj) − gj g .
Therefore, πt+1 b (θ ) = πt (θ ) + δ = πt (θ ) + w g ≤ max θ∈Θ πt b(θj).
This is true for any configuration whose profit improves, therefore the maximal buyer profit does not change during phase A.
LEMMA 13. The price of at least one sub-configuration must be reduced at every round in phase A.
PROOF. In each round t < T of phase A there exists an active seller i for whom Bt i ∩ Mt = ∅. However to be active in round t,
Bt i = ∅. Let ˆθ ∈ Bt i . If ∀r.ˆθr ∈ Mt , then ˆθ ∈ Mt by definition of Mt . Therefore there must be ˆθr ∈ Mt . We need to prove that for at least one of these sub-configurations, πt b(ˆθr) < 0 to ensure activation of rule [A].
Assume for contradiction that for any ˆθr ∈ ¯Mt , πt b(ˆθr) ≥ 0.
For simplicity we assume that for any θr, π1 b (θr) is some product of g (that can be easily done), and that ensures that πt b(ˆθr) = 0 because once profit hits 0 it cannot increase by rule [A].
If ˆθr ∈ ¯Mt , ∀r = 1, . . . , g then πt b(ˆθ) = 0. This contradicts Lemma 12 since we set high initial prices. Therefore some of the sub-configurations of ˆθ are in Mt , and WLOG we assume it is ˆθ1, . . . , ˆθk. To be in Mt these k sub-configurations must have been in some preferred full configuration, meaning there exists θ ∈ Mt such that θ = (ˆθ1, . . . , ˆθk, θk+1, . . . , θg) Since ˆθ /∈ Mt It must be that case that πt b(ˆθ) < πt b(θ ). Therefore πt b(θk+1, . . . , θg) > πt b(ˆθk+1, . . . , ˆθg) = 0 Hence for at least one r ∈ {k + 1, . . . , g}, πt b(θr) > 0 contradicting rule [A]. 233 LEMMA 14. When the solution to MAP provides positive surplus, and at least the best seller is SB, the auction must reach phase B.
PROOF. By Lemma 13 prices must go down in every round of phase A. Rule [A] sets a lower bound on all prices therefore the auction either terminates in phase A or must reach condition [SWITCH].
We set the initial prices are high such that maxθ∈Θ π1 b (θ) < 0, and by Lemma 12 maxθ∈Θ πt b(θ) < 0 during phase A. We assume that the efficient allocation (θ∗ , i∗ ) provides positive welfare, that is σi∗ (θ∗ ) = πt b(θ∗ ) + πt i∗ (θ∗ ) > 0. si∗ is SB therefore she will leave the auction only when πt i∗ (θ∗ ) < 0. This can happen only when πt b(θ∗ ) > 0, therefore si∗ does not drop in phase A hence the auction cannot terminate before reaching condition [SWITCH].
LEMMA 15. For SB seller si, ηi is (e + 1) -efficient.
PROOF. ηi is chosen to maximize the buyer"s surplus out of Bt i at the end of phase A. Since Bt i ∩ Mt = ∅, clearly ηi ∈ Mt . From Corollary 11 and Corollary 9, for any ˜θ, πT b (ηi) ≥ πT b (˜θ) − (e + 1) πT i (ηi) ≥ πT i (˜θ) ⇒ σi(ηi) ≥ σi(˜θ) − (e + 1) This establishes the approximate bilateral efficiency of the results of Phase A (at this point under the assumption of SB). Based on Phase B"s simple role as a single-dimensional bidding competition over the discount, we next assert that the overall result is efficient under SB, which in turn proves to be an approximately ex-post equilibrium strategy in the two phases.
LEMMA 16. If sellers si and sj are SB, and si is active at least as long as sj is active in phase B, then σi(ηi) ≥ max θ∈Θ σj(θ) − (e + 2) .
THEOREM 17. Given a truthful buyer and SB sellers, the auction is (e+2) -efficient: the surplus of the final allocation is within (e + 2) of the maximal surplus.
Following PK, we rely on an equivalence to the one-sided VCG auction to establish incentive properties for the sellers. In the onesided multiattribute VCG auction, buyer and sellers report valuation and cost functions ˆub, ˆci, and the buyer pays the sell-side VCG payment to the winning seller.
DEFINITION 11. Let (θ∗ , i∗ ) be the optimal solution to MAP.
Let (˜θ,˜i) be the best solution to MAP when i∗ does not participate.
The sell-side VCG payment is V CG(ˆub, ˆci) = ˆub(θ∗ ) − (ˆub(˜θ) − ˆc˜i(˜θ)).
It is well-known that truthful bidding is a dominant strategy for sellers in the one-sided VCG auction. It is also shown by PK that the maximal regret for buyers from bidding truthfully in this mechanism is ub(θ∗ ) − ci∗ (θ∗ ) − (ub(˜θ) − ˆc˜i(˜θ)), that is, the marginal product of the efficient seller.
Usually in iterative auctions the VCG outcome is only nearly achieved, but the deviation is bounded by the minimal price change.
We show a similar result, and therefore define δ-VCG payments.
DEFINITION 12. Sell-side δ-VCG payment for MAP is a payment p such that V CG(ˆub, ˆci) − δ ≤ p ≤ V CG(ˆub, ˆci) + δ.
When payment is guaranteed to be δ-VCG sellers can only affect their payment within that range, therefore their gain by falsely reporting their cost is bounded by 2δ.
LEMMA 18. When sellers are SB, the payment in the end of GAI auction is sell-side (e + 2) -VCG.
THEOREM 19. SB is an (3e + 5) ex-post Nash Equilibrium for sellers in GAI auction. That is, sellers cannot gain more than (3e + 5) by deviating.
In practice, however, sellers are unlikely to have the information that would let them exploit that potential gain. They are much more likely to lose from bidding on their less attractive configurations.
The size of the price space maintained in the auction is equal to the total number of sub-configurations, meaning it is exponential in maxr |Ir|. This is also equivalent to the tree-width (plus one) of the original CDI-map. For the purpose of the computational analysis let dj denote the domain of attribute aj, and I = Sg r=1 Q j∈Ir dj, the collection of all sub-configurations. The first purpose of this sub-section is to show that the complexity of all the computations required for the auction depends only on |I|, i.e., no computation depends on the size of the full exponential domain.
We are first concerned with the computation of Mt . Since Mt grows monotonically with t, a naive application of optimization algorithm to generate the best outcomes sequentially might end up enumerating significant portions of the fully exponential domain.
However as shown below this plain enumeration can be avoided.
PROPOSITION 20. The computation of Mt can be done in time O(|I|2 ). Moreover, the total time spent on this task throughout the auction is O(|I|(|I| + T)).
The bounds are in practice significantly lower, based on results on similar problems from the probabilistic reasoning literature [18].
One of the benefits of the compact pricing structure is the compact representation it lends for bids: sellers submit only sub-bids, and therefore the number of them submitted and stored per seller is bounded by |I|. Since the computation tasks: Bt i = ∅, rule [SWITCH] and choice of ηi are all involving the set Bt i , it is important to note that their performance only depend on the size of the set Bt i , since they are all subsumed by the combinatorial optimization task over Bt i or Bt i ∩ Mt .
Next, we analyze the number of rounds it takes for the auction to terminate. Phase B requires maxi=1,...n πT i (ηi)1 . Since this is equivalent to price-only auctions, the concern is only with the time complexity of phase A. Since prices cannot go below fb,r(θr), an upper bound on the number of rounds required is T ≤ X θr∈I (p1 (θr) − fb,r(θr)) g However phase A may converge faster. Let the initial negative profit chosen by the auctioneer be m = maxθ∈Θ π1 b (θ). In the worst case phase A needs to run until ∀θ ∈ Θ.πb(θ) = m. This happens for example when ∀θr ∈ I.pt (θr) = fb,r(θr) + m g . In general, the closer the initial prices reflect buyer valuation, the faster phase A converges. One extreme is to choose p1 (θr) = 234 I1 I2 a1 b1 a2 b1 a1 b2 a2 b2 b1 c1 b2 c1 b1 c2 b2 c2 fb 65 50 55 70 50 85 60 75 f1 35 20 30 70 65 65 70 61 f2 35 20 25 25 55 110 70 95 Table 1: GAI utility functions for the example domain. fb represents the buyer"s valuation, and f1 and f2 costs of the sellers s1 and s2. fb,r(θr) + m g . That would make phase A redundant, at the cost of full initial revelation of buyer"s valuation as done in other mechanisms discussed below. Between this option and the other extreme, which is ∀α, ˆα ∈ I, p1 (α) = p1 (ˆα) the auctioneer has a range of choices to determine the right tradeoff between convergence time and information revelation. In the example below the choice of a lower initial price for the domain of I1 provides some speedup by revealing a harmless amount of information.
Another potential concern is the communication cost associated with the Japanese auction style. The sellers need to send their bids over and over again at each round. A simple change can be made to avoid much of the redundant communication: the auction can retain sub-bids from previous rounds on sub-configurations whose price did not change. Since combinations of sub-bids from different rounds can yield sub-optimal configurations, each sub-bid should be tagged with the number of the latest round in which it was submitted, and only consistent combinations from the same round are considered to be full bids. With this implementation sellers need not resubmit their bid until a price of at least one sub-configuration has changed.
We use the example settings introduced in Section 5.2. Recall that the GAI structure is I1 = {a, b}, I2 = {b, c} (note that e = 1).
Table 1 shows the GAI utilities for the buyer and the two sellers s1, s2. The efficient allocation is (s1, a1 b2 c1 ) with a surplus of 45.
The maximal surplus of the second best seller, s2, is 25, achieved by a1 b1 c1 , a2 b1 c1 , and a2 b2 c2 . We set all initial prices over I1 to 75, and all initial prices over I2 to 90. We set = 8, meaning that price reduction for sub-configurations is 4. Though with these numbers it is not guaranteed by Theorem 17, we expect s1 to win on either the efficient allocation or on a1 b2 c2 which provides a surplus of 39. The reason is that these are the only two configurations which are within (e + 1) = 16 of being efficient for s1 (therefore one of them must be chosen by Phase A), and both provide more than surplus over s2"s most efficient configuration (and this is sufficient in order to win in Phase B).
Table 2 shows the progress of phase A. Initially all configuration have the same cost (165), so sellers bid on their lowest cost configuration which is a2 b1 c1 for both (with profit 80 to s1 and 90 to s2), and that translates to sub-bids on a2 b1 and b1 c1 . M1 contains the sub-configurations a2 b2 and b2 c1 of the highest value configuration a2 b2 c1 . Price is therefore decreased on a2 b1 and b1 c1 . After the price change, s1 has higher profit (74) on a1 b2 c2 and she therefore bids on a1 b2 and b2 c2 . Now (round 2) their prices go down, reducing the profit on a1 b2 c2 to 66 and therefore in round 3 s1 prefers a2 b1 c2 (profit 67). After the next price change the configurations a1 b2 c1 and a1 b2 c2 both become optimal (profit 66), and the subbids a1 b2 , b2 c1 and b2 c2 capture the two. These configurations stay optimal for another round (5), with profit 62. At this point s1 has a full bid (in fact two full bids: a1 b2 c2 and a1 b2 c1 ) in M5 , and I1 I2 t a1b1 a2b1 a1b2 a2b2 b1c1 b2c1 b1c2 b2c2 1 75 75 75 75 90 90 90 90 s1, s2 ∗ s1, s2 ∗ 2 75 71 75 75 86 90 90 90 s2 s1 ∗ s2 ∗ s1 3 75 67 71 75 82 90 90 86 s1, s2 ∗ s2 ∗ s1 ∗ 4 75 63 71 75 78 90 86 86 s2 s1 ∗ s2 ∗, s1 ∗, s1 5 75 59 67 75 74 90 86 86 s2 ∗, s1 ∗ s2 ∗, s1 ∗, s1 6 71 59 67 75 70 90 86 86 s2 ∗, s1 ∗ ∗, s1 s2 ∗, s1 7 71 55 67 75 70 90 82 86 s2 ∗, s1 ∗ s2 ∗, s1 ∗, s1 8 67 55 67 75 66 90 82 86 ∗ s2 ∗, s1 ∗ ∗ ∗, s1 s2 ∗, s1 9 67 51 67 75 66 90 78 86 ∗, s2 ∗, s1 ∗ ∗, s2 ∗, s1 ∗, s1 Table 2: Auction progression in phase A. Sell bids and designation of Mt (using ∗) are shown below the price of each subconfiguration. therefore she no longer changes her bids since the price of her optimal configurations does not decrease. s2 sticks to a2 b1 c1 during the first four rounds, switching to a1 b1 c1 in round 5. It takes four more rounds for s2 and Mt to converge (M10 ∩B10 2 = {a1 b1 c1 }).
After round 9 the auction sets η1 = a1 b2 c1 (which yields more buyer profit than a1 b2 c2 ) and η2 = a1 b1 c1 . For the next round (10) Δ = 8, increased by 8 for each subsequent round. Note that p9 (a1 b1 c1 ) = 133, and c2(a1 b1 c1 ) = 90, therefore πT 2 (η2) =
(a1 b1 c1 ) = 85 and that causes s2 to drop out, setting the final allocation to (s1, a1 b2 c1 ) and p15 (a1 b2 c1 ) = 157 − 48 = 109. That leaves the buyer with a profit of 31 and s1 with a profit of 14, less than below the VCG profit 20.
The welfare achieved in this case is optimal. To illustrate how some efficiency loss could occur consider the case that c1(b2 c2 ) =
b2 c2 provides the same profit (67) as a2 b1 c2 , and s1 bids on both. While a2 b1 c2 is no longer optimal after the price change, a1 b2 c2 remains optimal on subsequent rounds because b2 c2 ∈ Mt , and the price change of a1 b2 affects both a1 b2 c2 and the efficient configuration a1 b2 c1 .
When phase A ends B10 1 ∩ M10 = {a1 b2 c2 } so the auction terminates with the slightly suboptimal configuration and surplus 40.
A key aspect in implementing GAI based auctions is the choice of the preference structure, that is, the elements {I1, . . . , Ig}. In some domains the structure can be more or less robust over time and over different decision makers. When this is not the case, extracting reliable structure from sellers (in the form of CDI conditions) is a serious challenge. This could have been a deal breaker for such domains, but in fact it can be overcome. It turns out that we can run this auction without any assumptions on sellers" preference structure. The only place where this assumption is used in our analysis is for Lemma 8. If sellers whose preference structure does not agree with the one used by the auction are guided to submit only one full bid at each round, or a set of bids that does not yield undesired consistent combinations, all the properties of the auction 235 still hold. Locally, the sellers can optimize their profit functions using the union of their GAI structure with the auction"s structure.
It is therefore essential only that the buyer"s preference structure is accurately modeled. Of course, capturing sellers" structures as well is still preferred since it can speed up the execution and let sellers take advantage of the compact bid representation.
In both cases the choice of clusters may significantly affect the complexity of the price structure and the runtime of the auction.
It is sometimes better to ignore some weaker interdependencies in order to reduce dimensionality. The complexity of the structure also affects the efficiency of the auction through the value of e.
In considering information properties of this mechanism we compare to the standard approach for iterative multiattribute auctions, which is based on the theoretical foundations of Che [7]. In most of these mechanisms the buyer reveals a scoring function and then the mechanism solicits bids from the sellers [3, 22, 8, 21] (the mechanisms suggested by Beil and Wein [2] is different since buyers can modify their scoring function each round, but the goal there is to maximize the buyer"s profit). Whereas these iterative procurement mechanisms tend to relieve the burden of information revelation from the sellers, a major drawback is that the buyer"s utility function must be revealed to the sellers before receiving any commitment. In the mechanisms suggested by PK and in our GAI auction above, buyer information is revealed only in exchange for sell commitments. In particular, sellers learn nothing (beyond the initial price upper bound, which can be arbitrarily loose) about the utility of configurations for which no bid was submitted. When bids are submitted for a configuration θ, sellers would be able to infer its utility relative to the current preferred configurations only after the price of θ is driven down sufficiently to make it a preferred configuration as well.
We propose a novel exploitation of preference structure in multiattribute auctions. Rather than assuming full additivity, or no structure at all, we model preferences using the GAI decomposition.
We developed an iterative auction mechanism directly relying on the decomposition, and also provided direct means of constructing the representation from relatively simple statements of willingnessto-pay. Our auction mechanism generalizes PK"s preference modeling, while in essence retaining their information revelation properties. It allows for a range of tradeoffs between accuracy of preference representation and both the complexity of the pricing structure and efficiency of the auction, as well as tradeoffs between buyer"s information revelation and the time required for convergence.
This work was supported in part by NSF grants IIS-0205435 and IIS-0414710, and the STIET program under NSF IGERT grant
[1] F. Bacchus and A. Grove. Graphical models for preference and utility. In Eleventh Conference on Uncertainty in Artificial Intelligence, pages 3-10, Montreal, 1995. [2] D. R. Beil and L. M. Wein. An inverse-optimization-based auction for multiattribute RFQs. Management Science, 49:1529-1545, 2003. [3] M. Bichler. The Future of e-Markets: Multi-Dimensional Market Mechanisms. Cambridge University Press, 2001. [4] C. Boutilier, F. Bacchus, and R. I. Brafman. UCP-networks: A directed graphical representation of conditional utilities. In Seventeenth Conference on Uncertainty in Artificial Intelligence, pages 56-64, Seattle, 2001. [5] R. I. Brafman, C. Domshlak, and T. Kogan. Compact value-function representations for qualitative preferences. In Twentieth Conference on Uncertainty in Artificial Intelligence, pages 51-59, Banff, 2004. [6] D. Braziunas and C. Boutilier. Local utility elicitation in GAI models. In Twenty-first Conference on Uncertainty in Artificial Intelligence, pages 42-49, Edinburgh, 2005. [7] Y.-K. Che. Design competition through multidimensional auctions. RAND Journal of Economics, 24(4):668-680,
[8] E. David, R. Azoulay-Schwartz, and S. Kraus. An English auction protocol for multi-attribute items. In Agent Mediated Electronic Commerce IV: Designing Mechanisms and Systems, volume 2531 of Lecture Notes in Artificial Intelligence, pages 52-68. Springer, 2002. [9] G. Debreu. Topological methods in cardinal utility theory. In K. Arrow, S. Karlin, and P. Suppes, editors, Mathematical Methods in the Social Sciences. Stanford Univ. Press, 1959. [10] J. S. Dyer and R. K. Sarin. An axiomatization of cardinal additive conjoint measurement theory. Working Paper 265,
WMSI, UCLA, February 1977. [11] J. S. Dyer and R. K. Sarin. Measurable multiattribute value functions. Operations Research, 27:810-822, 1979. [12] Y. Engel, M. P. Wellman, and K. M. Lochner. Bid expressiveness and clearing algorithms in multiattribute double auctions. In Seventh ACM Conference on Electronic Commerce, pages 110-119, Ann Arbor, MI, 2006. [13] P. C. Fishburn. Interdependence and additivity in multivariate, unidimensional expected utility theory. Intl.
Economic Review, 8:335-342, 1967. [14] C. Gonzales and P. Perny. GAI networks for utility elicitation. In Ninth Intl. Conf. on the Principles of Knowledge Representation and Reasoning, pages 224-234,
Whistler, BC, 2004. [15] C. Gonzales and P. Perny. GAI networks for decision making under certainty. In IJCAI-05 Workshop on Advances in Preference Handling, Edinburgh, 2005. [16] N. Hyafil and C. Boutilier. Regret-based incremental partial revelation mechanisms. In Twenty-first National Conference on Artificial Intelligence, pages 672-678, Boston, MA, 2006. [17] R. L. Keeney and H. Raiffa. Decisions with Multiple Objectives: Preferences and Value Tradeoffs. Wiley, 1976. [18] D. Nilsson. An efficient algorithm for finding the M most probable configurations in probabilistic expert systems.
Statistics and Computinge, 8(2):159-173, 1998. [19] D. C. Parkes and J. Kalagnanam. Models for iterative multiattribute procurement auctions. Management Science, 51:435-451, 2005. [20] J. Pearl and A. Paz. Graphoids: A graph based logic for reasoning about relevance relations. In B. Du Boulay, editor,

Mechanism design studies algorithmic constructions under the presence of strategic players who hold the inputs to the algorithm. Algorithmic mechanism design has focused mainly on settings were the social planner or designer wishes to maximize the social welfare (or equivalently, minimize social cost), or on auction settings where revenuemaximization is the main goal. Alternative optimization goals, such as those that incorporate fairness criteria (which have been investigated algorithmically and in social choice theory), have received very little or no attention.
In this paper, we consider such an alternative goal in the context of machine scheduling, namely, makespan minimization. There are n jobs or tasks that need to be assigned to m machines, where each job has to be assigned to exactly one machine. Assigning a job j to a machine i incurs a load (cost) of pij ≥ 0 on machine i, and the load of a machine is the sum of the loads incurred due to the jobs assigned to it; the goal is to schedule the jobs so as to minimize the maximum load of a machine, which is termed the makespan of the schedule. Makespan minimization is a common objective in scheduling environments, and has been well studied algorithmically in both the Computer Science and Operations Research communities (see, e.g., the survey [12]). Following the work of Nisan and Ronen [22], we consider each machine to be a strategic player or agent who privately knows its own processing time for each job, and may misrepresent these values in order to decrease its load (which is its incurred cost). Hence, we approach the problem via mechanism design: the social designer, who holds the set of jobs to be assigned, needs to specify, in addition to a schedule, suitable payments to the players in order to incentivize them to reveal their true processing times. Such a mechanism is called a truthful mechanism. The makespan-minimization objective is quite different from the classic goal of social-welfare maximization, where one wants to maximize the total welfare (or minimize the total cost) of all players. Instead, it 252 corresponds to maximizing the minimum welfare and the notion of max-min fairness, and appears to be a much harder problem from the viewpoint of mechanism design. In particular, the celebrated VCG [26, 9, 10] family of mechanisms does not apply here, and we need to devise new techniques.
The possibility of constructing a truthful mechanism for makespan minimization is strongly related to assumptions on the players" processing times, in particular, the dimensionality of the domain. Nisan and Ronen considered the setting of unrelated machines where the pij values may be arbitrary. This is a multidimensional domain, since a player"s private value is its entire vector of processing times (pij)j.
Very few positive results are known for multidimensional domains in general, and the only positive results known for multidimensional scheduling are O(m)-approximation truthful mechanisms [22, 20]. We emphasize that regardless of computational considerations, even the existence of a truthful mechanism with a significantly better (than m) approximation ratio is not known for any such scheduling domain.
On the negative side, [22] showed that no truthful deterministic mechanism can achieve approximation ratio better than 2, and strengthened this lower bound to m for two specific classes of deterministic mechanisms. Recently, [20] extended this lower bound to randomized mechanisms, and [8] improved the deterministic lower bound.
In stark contrast with the above state of affairs, much stronger (and many more) positive results are known for a special case of the unrelated machines problem, namely, the setting of related machines. Here, we have pij = pj/si for every i, j, where pj is public knowledge, and the speed si is the only private parameter of machine i. This assumption makes the domain of players" types single-dimensional.
Truthfulness in such domains is equivalent to a convenient value-monotonicity condition [21, 3], which appears to make it significantly easier to design truthful mechanisms in such domains. Archer and Tardos [3] first considered the related machines setting and gave a randomized 3-approximation truthful-in-expectation mechanism. The gap between the single-dimensional and multidimensional domains is perhaps best exemplified by the fact that [3] showed that there exists a truthful mechanism that always outputs an optimal schedule. (Recall that in the multidimensional unrelated machines setting, it is impossible to obtain a truthful mechanism with approximation ratio better than 2.) Various follow-up results [2, 4, 1, 13] have strengthened the notion of truthfulness and/or improved the approximation ratio.
Such difficulties in moving from the single-dimensional to the multidimensional setting also arise in other mechanism design settings (e.g., combinatorial auctions). Thus, in addition to the specific importance of scheduling in strategic environments, ideas from multidimensional scheduling may also have a bearing in the more general context of truthful mechanism design for multidimensional domains.
In this paper, we consider the makespan-minimization problem for a special case of unrelated machines, where the processing time of a job is either low or high on each machine. More precisely, in our setting, pij ∈ {Lj, Hj} for every i, j, where the Lj, Hj values are publicly known (Lj ≡low, Hj ≡high). We call this model the jobdependent two-values case. This model generalizes the classic restricted machines setting, where pij ∈ {Lj, ∞} which has been well-studied algorithmically. A special case of our model is when Lj = L and Hj = H for all jobs j, which we denote simply as the two-values scheduling model. Both of our domains are multidimensional, since the machines are unrelated: one job may be low on one machine and high on the other, while another job may follow the opposite pattern. Thus, the private information of each machine is a vector specifying which jobs are low and high on it. Thus, they retain the core property underlying the hardness of truthful mechanism design for unrelated machines, and by studying these special settings we hope to gain some insights that will be useful for tackling the general problem.
Our Results and Techniques We present various positive results for our multidimensional scheduling domains.
Our first result is a general method to convert any capproximation algorithm for the job-dependent two values setting into a 3c-approximation truthful-in-expectation mechanism. This is one of the very few known results that use an approximation algorithm in a black-box fashion to obtain a truthful mechanism for a multidimensional problem. Our result implies that there exists a 3-approximation truthfulin-expectation mechanism for the Lj-Hj setting.
Interestingly, the proof of truthfulness is not based on supplying explicit prices, and our construction does not necessarily yield efficiently-computable prices (but the allocation rule is efficiently computable). Our second result applies to the twovalues setting (Lj = L, Hj = H), for which we improve both the approximation ratio and strengthen the notion of truthfulness. We obtain a deterministic 2-approximation truthful mechanism (along with prices) for this problem. These are the first truthful mechanisms with non-trivial performance guarantees for a multidimensional scheduling domain.
Complementing this, we observe that even this seemingly simple setting does not admit truthful mechanisms that return an optimal schedule (unlike in the case of related machines).
By exploiting the multidimensionality of the domain, we prove that no truthful deterministic mechanism can obtain an approximation ratio better than 1.14 to the makespan (irrespective of computational considerations).
The main technique, and one of the novelties, underlying our constructions and proofs, is that we do not rely on explicit price specifications in order to prove the truthfulness of our mechanisms. Instead we exploit certain algorithmic monotonicity conditions that characterize truthfulness to first design an implementable algorithm, i.e., an algorithm for which prices ensuring truthfulness exist, and then find these prices (by further delving into the proof of implementability). This kind of analysis has been the method of choice in the design of truthful mechanisms for singledimensional domains, where value-monotonicity yields a convenient characterization enabling one to concentrate on the algorithmic side of the problem (see, e.g., [3, 7, 4, 1, 13]).
But for multidimensional domains, almost all positive results have relied on explicit price specifications in order to prove truthfulness (an exception is the work on unknown single-minded players in combinatorial auctions [17, 7]), a fact that yet again shows the gap in our understanding of multidimensional vs. single-dimensional domains.
Our work is the first to leverage monotonicity conditions for truthful mechanism design in arbitrary domains. The monotonicity condition we use, which is sometimes called cycle monotonicity, was first proposed by Rochet [23] (see also [11]). It is a generalization of value-monotonicity and completely characterizes truthfulness in every domain. Our methods and analyses demonstrate the potential benefits 253 of this characterization, and show that cycle monotonicity can be effectively utilized to devise truthful mechanisms for multidimensional domains. Consider, for example, our first result showing that any c-approximation algorithm can be exported to a 3c-approximation truthful-in-expectation mechanism. At the level of generality of an arbitrary approximation algorithm, it seems unlikely that one would be able to come up with prices to prove truthfulness of the constructed mechanism. But, cycle monotonicity does allow us to prove such a statement. In fact, some such condition based only on the underlying algorithm (and not on the prices) seems necessary to prove such a general statement.
The method for converting approximation algorithms into truthful mechanisms involves another novel idea. Our randomized mechanism is obtained by first constructing a truthful mechanism that returns a fractional schedule. Moving to a fractional domain allows us to plug-in truthfulness into the approximation algorithm in a rather simple fashion, while losing a factor of 2 in the approximation ratio.
We then use a suitable randomized rounding procedure to convert the fractional assignment into a random integral assignment. For this, we use a recent rounding procedure of Kumar et al. [14] that is tailored for unrelated-machine scheduling. This preserves truthfulness, but we lose another additive factor equal to the approximation ratio. Our construction uses and extends some observations of Lavi and Swamy [16], and further demonstrates the benefits of fractional mechanisms in truthful mechanism design.
Related Work Nisan and Ronen [22] first considered the makespan-minimization problem for unrelated machines. They gave an m-approximation positive result and proved various lower bounds. Recently, Mu"alem and Schapira [20] proved a lower bound of 2 on the approximation ratio achievable by truthful-in-expectation mechanisms, and Christodoulou,
Koutsoupias, and Vidali [8] proved a (1 + √ 2)-lower bound for deterministic truthful mechanisms.Archer and Tardos [3] first considered the related-machines problem and gave a 3-approximation truthful-in-expectation mechanism. This been improved in [2, 4, 1, 13] to: a 2-approximation randomized mechanism [2]; an FPTAS for any fixed number of machines given by Andelman, Azar and Sorani [1], and a 3-approximation deterministic mechanism by Kov´acs [13].
The algorithmic problem (i.e., without requiring truthfulness) of makespan-minimization on unrelated machines is well understood and various 2-approximation algorithms are known. Lenstra, Shmoys and Tardos [18] gave the first such algorithm. Shmoys and Tardos [25] later gave a 2approximation algorithm for the generalized assignment problem, a generalization where there is a cost cij for assigning a job j to a machine i, and the goal is to minimize the cost subject to a bound on the makespan. Recently, Kumar,
Marathe, Parthasarathy, and Srinivasan [14] gave a randomized rounding algorithm that yields the same bounds. We use their procedure in our randomized mechanism.
The characterization of truthfulness for arbitrary domains in terms of cycle monotonicity seems to have been first observed by Rochet [23] (see also Gui et al. [11]). This generalizes the value-monotonicity condition for single-dimensional domains which was given by Myerson [21] and rediscovered by [3]. As mentioned earlier, this condition has been exploited numerous times to obtain truthful mechanisms for single-dimensional domains [3, 7, 4, 1, 13]. For convex domains (i.e., each players" set of private values is convex), it is known that cycle monotonicity is implied by a simpler condition, called weak monotonicity [15, 6, 24]. But even this simpler condition has not found much application in truthful mechanism design for multidimensional problems.
Objectives other than social-welfare maximization and revenue maximization have received very little attention in mechanism design. In the context of combinatorial auctions, the problems of maximizing the minimum value received by a player, and computing an envy-minimizing allocation have been studied briefly. Lavi, Mu"alem, and Nisan [15] showed that the former objective cannot be implemented truthfully; Bezakova and Dani [5] gave a 0.5-approximation mechanism for two players with additive valuations. Lipton et al. [19] showed that the latter objective cannot be implemented truthfully. These lower bounds were strengthened in [20].
In our scheduling problem, we are given n jobs and m machines, and each job must be assigned to exactly one machine. In the unrelated-machines setting, each machine i is characterized by a vector of processing times (pij)j, where pij ∈ R≥0 ∪ {∞} denotes i"s processing time for job j with the value ∞ specifying that i cannot process j. We consider two special cases of this problem:
{Lj, Hj} for every i, j, with Lj ≤ Hj, and the values Lj, Hj are known. This generalizes the classic scheduling model of restricted machines, where Hj = ∞.
where Lj = L and Hj = H for all jobs j, i.e., pij ∈ {L, H} for every i, j.
We say that a job j is low on machine i if pij = Lj, and high if pij = Hj. We will use the terms schedule and assignment interchangeably. We represent a deterministic schedule by a vector x = (xij)i,j, where xij is 1 if job j is assigned to machine i, thus we have xij ∈ {0, 1} for every i, j,
P i xij = 1 for every job j. We will also consider randomized algorithms and algorithms that return a fractional assignment. In both these settings, we will again specify an assignment by a vector x = (xij)i,j with P j xij = 1, but now xij ∈ [0, 1] for every i, j. For a randomized algorithm, xij is simply the probability that j is assigned to i (thus, x is a convex combination of integer assignments).
We denote the load of machine i (under a given assignment) by li = P j xijpij, and the makespan of a schedule is defined as the maximum load on any machine, i.e., maxi li.
The goal in the makespan-minimization problem is to assign the jobs to the machines so as to minimize the makespan of the schedule.
We consider the makespan-minimization problem in the above scheduling domains in the context of mechanism design. Mechanism design studies strategic settings where the social designer needs to ensure the cooperation of the different entities involved in the algorithmic procedure. Following the work of Nisan and Ronen [22], we consider the machines to be the strategic players or agents. The social designer holds the set of jobs that need to be assigned, but does 254 not know the (true) processing times of these jobs on the different machines. Each machine is a selfish entity, that privately knows its own processing time for each job. on a machine incurs a cost to the machine equal to the true processing time of the job on the machine, and a machine may choose to misrepresent its vector of processing times, which are private, in order to decrease its cost.
We consider direct-revelation mechanisms: each machine reports its (possibly false) vector of processing times, the mechanism then computes a schedule and hands out payments to the players (i.e., machines) to compensate them for the cost they incur in processing their assigned jobs. A (direct-revelation) mechanism thus consists of a tuple (x, P): x specifies the schedule, and P = {Pi} specifies the payments handed out to the machines, where both x and the Pis are functions of the reported processing times p = (pij)i,j.
The mechanism"s goal is to compute a schedule that has near-optimal makespan with respect to the true processing times; a machine i is however only interested in maximizing its own utility, Pi − li, where li is its load under the output assignment, and may declare false processing times if this could increase its utility. The mechanism must therefore incentivize the machines/players to truthfully reveal their processing times via the payments. This is made precise using the notion of dominant-strategy truthfulness.
Definition 2.1 (Truthfulness) A scheduling mechanism is truthful if, for every machine i, every vector of processing times of the other machines, p−i, every true processing-time vector p1 i and any other vector p2 i of machine i, we have: P1 i − X j x1 ijp1 ij ≥ P2 i − X j x2 ijp1 ij, (1) where (x1 , P1 ) and (x2 , P2 ) are respectively the schedule and payments when the other machines declare p−i and machine i declares p1 i and p2 i , i.e., x1 = x(p1 i , p−i), P1 i = Pi(p1 i , p−i) and x2 = x(p2 i , p−i), P2 i = Pi(p2 i , p−i).
To put it in words, in a truthful mechanism, no machine can improve its utility by declaring a false processing time, no matter what the other machines declare.
We will also consider fractional mechanisms that return a fractional assignment, and randomized mechanisms that are allowed to toss coins and where the assignment and the payments may be random variables. The notion of truthfulness for a fractional mechanism is the same as in Definition 2.1, where x1 , x2 are now fractional assignments.
For a randomized mechanism, we will consider the notion of truthfulness in expectation [3], which means that a machine (player) maximizes her expected utility by declaring her true processing-time vector. Inequality (1) also defines truthfulness-in-expectation for a randomized mechanism, where P1 i , P2 i now denote the expected payments made to player i, x1 , x2 are the fractional assignments denoting the randomized algorithm"s schedule (i.e., xk ij is the probability that j is assigned to i in the schedule output for (pk i , p−i)).
For our two scheduling domains, the informational assumption is that the values Lj, Hj are publicly known. The private information of a machine is which jobs have value Lj (or L) and which ones have value Hj (or H) on it. We emphasize that both of our domains are multidimensional, since each machine i needs to specify a vector saying which jobs are low and high on it.
Although truthfulness is defined in terms of payments, it turns out that truthfulness actually boils down to a certain algorithmic condition of monotonicity. This seems to have been first observed for multidimensional domains by Rochet [23] in 1987, and has been used successfully in algorithmic mechanism design several times, but for singledimensional domains. However for multidimensional domains, the monotonicity condition is more involved and there has been no success in employing it in the design of truthful mechanisms. Most positive results for multidimensional domains have relied on explicit price specifications in order to prove truthfulness. One of the main contributions of this paper is to demonstrate that the monotonicity condition for multidimensional settings, which is sometimes called cycle monotonicity, can indeed be effectively utilized to devise truthful mechanisms. We include a brief exposition on it for completeness. The exposition here is largely based on [11].
Cycle monotonicity is best described in the abstract social choice setting: there is a finite set A of alternatives, there are m players, and each player has a private type (valuation function) v : A → R, where vi(a) should be interpreted as i"s value for alternative a. In the scheduling domain, A represents all the possible assignments of jobs to machines, and vi(a) is the negative of i"s load in the schedule a. Let Vi denote the set of all possible types of player i. A mechanism is a tuple (f, {Pi}) where f : V1 × · · · × Vm → A is the algorithm for choosing the alternative, and Pi : V1 × · · · × Vm → A is the price charged to player i (in the scheduling setting, the mechanism pays the players, which corresponds to negative prices). The mechanism is truthful if for every i, every v−i ∈ V−i = Q i =i Vi , and any vi, vi ∈ Vi we have vi(a) − Pi(vi, v−i) ≥ vi(b) − Pi(vi, v−i), where a = f(vi, v−i) and b = f(vi, v−i). A basic question that arises is given an algorithm f : V1 × · · · × Vm → A, do there exist prices that will make the resulting mechanism truthful? It is well known (see e.g. [15]) that the price Pi can only depend on the alternative chosen and the others" declarations, that is, we may write Pi : V−i × A → R. Thus, truthfulness implies that for every i, every v−i ∈ V−i, and any vi, vi ∈ Vi with f(vi, v−i) = a and f(vi, v−i) = b, we have vi(a) − Pi(a, v−i) ≥ vi(b) − Pi(b, v−i).
Now fix a player i, and fix the declarations v−i of the others. We seek an assignment to the variables {Pa}a∈A such that vi(a) − vi(b) ≥ Pa − Pb for every a, b ∈ A and vi ∈ Vi with f(vi, v−i) = a. (Strictly speaking, we should use A = f(Vi, v−i) instead of A here.) Define δa,b := inf{vi(a)− vi(b) : vi ∈ Vi, f(vi, v−i) = a}. We can now rephrase the above price-assignment problem: we seek an assignment to the variables {Pa}a∈A such that Pa − Pb ≤ δa,b ∀a, b ∈ A (2) This is easily solved by looking at the allocation graph and applying a standard basic result of graph theory.
Definition 3.1 (Gui et al. [11]) The allocation graph of f is a directed weighted graph G = (A, E) where E = A × A and the weight of an edge b → a (for any a, b ∈ A) is δa,b.
Theorem 3.2 There exists a feasible assignment to (2) iff the allocation graph has no negative-length cycles.
Furthermore, if all cycles are non-negative, a feasible assignment is 255 obtained as follows: fix an arbitrary node a∗ ∈ A and set Pa to be the length of the shortest path from a∗ to a.
This leads to the following definition, which is another way of phrasing the condition that the allocation graph have no negative cycles.
Definition 3.3 (Cycle monotonicity) A social choice function f satisfies cycle monotonicity if for every player i, every v−i ∈ V−i, every integer K, and every v1 i , . . . , vK i ∈ Vi,
KX k=1 h vk i (ak) − vk i (ak+1) i ≥ 0 where ak = f(vk i , v−i) for 1 ≤ k ≤ K, and aK+1 = a1.
Corollary 3.4 There exist prices P such that the mechanism (f, P) is truthful iff f satisfies cycle monotonicity.1 We now consider our specific scheduling domain. Fix a player i, p−i, and any p1 i , . . . , pK i . Let x(pk i , p−i) = xk for 1 ≤ k ≤ K, and let xK+1 = x1 , pK+1 = p1 . xk could be a {0, 1}-assignment or a fractional assignment. We have vk i (xk ) = − P j xk ijpk ij, so cycle monotonicity translates to PK k=1 ˆ − P j xk ijpk ij + P j xk+1 ij pk ij ˜ ≥ 0. Rearranging, we get KX k=1 X j xk+1 ij ` pk ij − pk+1 ij ´ ≥ 0. (3) Thus (3) reduces our mechanism design problem to a concrete algorithmic problem. For most of this paper, we will consequently ignore any strategic considerations and focus on designing an approximation algorithm for minimizing makespan that satisfies (3).
RANDOMIZED MECHANISMS In this section, we consider the case of job-dependent Lj,
Hj values (with Lj ≤ Hj), which generalizes the classical restricted-machines model (where Hj = ∞). We show the power of randomization, by providing a general technique that converts any c-approximation algorithm into a 3c-approximation, truthful-in-expectation mechanism. This is one of the few results that shows how to export approximation algorithms for a multidimensional problem into truthful mechanisms when the algorithm is given as a black box.
Our construction and proof are simple, and based on two ideas. First, as outlined above, we prove truthfulness using cycle monotonicity. It seems unlikely that for an arbitrary approximation algorithm given only as a black box, one would be able to come up with payments in order to prove truthfulness; but cycle-monotonicity allows us to prove precisely this. Second, we obtain our randomized mechanism by (a) first moving to a fractional domain, and constructing a fractional truthful mechanism that is allowed to return fractional assignments; then (b) using a rounding procedure to express the fractional schedule as a convex combination of integer schedules. This builds upon a theme introduced by Lavi and Swamy [16], namely that of using fractional mechanisms to obtain truthful-in-expectation mechanisms. 1 It is not clear if Theorem 3.2, and hence, this statement, hold if A is not finite.
We should point out however that one cannot simply plug in the results of [16]. Their results hold for social-welfaremaximization problems and rely on using VCG to obtain a fractional truthful mechanism. VCG however does not apply to makespan minimization, and in our case even the existence of a near-optimal fractional truthful mechanism is not known. We use the following result adapted from [16].
Lemma 4.1 (Lavi and Swamy [16]) Let M = (x, P) be a fractional truthful mechanism. Let A be a randomized rounding algorithm that given a fractional assignment x, outputs a random assignment X such that E ˆ Xij ˜ = xij for all i, j. Then there exist payments P such that the mechanism M = (A, P ) is truthful in expectation. Furthermore, if M is individually rational then M is individually rational for every realization of coin tosses.
Let OPT(p) denote the optimal makespan (over integer schedules) for instance p. As our first step, we take a capproximation algorithm and convert it to a 2c-approximation fractional truthful mechanism. This conversion works even when the approximation algorithm returns only a fractional schedule (satisfying certain properties) of makespan at most c · OPT(p) for every instance p. We prove truthfulness by showing that the fractional algorithm satisfies cycle monotonicity (3). Notice that the alternative-set of our fractional mechanism is finite (although the set of all fractional assignments is infinite): its cardinality is at most that of the inputdomain, which is at most 2mn in the two-value case. Thus, we can apply Corollary 3.4 here. To convert this fractional truthful mechanism into a randomized truthful mechanism we need a randomized rounding procedure satisfying the requirements of Lemma 4.1. Fortunately, such a procedure is already provided by Kumar, Marathe, Parthasarathy, and Srinivasan [14].
Lemma 4.2 (Kumar et al. [14]) Given a fractional assignment x and a processing time vector p, there exists a randomized rounding procedure that yields a (random) assignment X such that,
ˆ Xij ˜ = xij.
P j Xijpij < P j xijpij + max{j:xij ∈(0,1)} pij with probability 1.
Property 1 will be used to obtain truthfulness in expectation, and property 2 will allow us to prove an approximation guarantee. We first show that any algorithm that returns a fractional assignment having certain properties satisfies cycle monotonicity.
Lemma 4.3 Let A be an algorithm that for any input p, outputs a (fractional) assignment x such that, if pij = Hj then xij ≤ 1/m, and if pij = Lj then xij ≥ 1/m. Then A satisfies cycle-monotonicity.
Proof. Fix a player i, and the vector of processing times of the other players p−i. We need to prove (3), that is,
PK k=1 P j xk+1 ij ` pk ij − pk+1 ij ´ ≥ 0 for every p1 i , . . . , pK i , where index k = K + 1 is taken to be k = 1. We will show that for every job j,
PK k=1 xk+1 ij ` pk ij − pk+1 ij ´ ≥ 0.
If pk ij is the same for all k (either always Lj or always Hj), then the above inequality clearly holds. Otherwise we can 256 divide the indices 1, . . . , K, into maximal segments, where a maximal segment is a maximal set of consecutive indices k , k + 1, . . . , k − 1, k (where K + 1 ≡ 1) such that pk ij = Hj ≥ pk +1 ij ≥ · · · ≥ pk ij = Lj. This follows because there must be some k such that pk ij = Hj > pk−1 ij = Lj. We take k = k and then keep including indices in this segment till we reach a k such that pk ij = Lj and pk+1 ij = Hj. We set k = k, and then start a new maximal segment with index k + 1. Note that k = k and k + 1 = k − 1. We now have a subset of indices and we can continue recursively. So all indices are included in some maximal segment. We will show that for every such maximal segment k , k +1, . . . , k ,P k −1≤k<k xk+1 ij ` pk ij − pk+1 ij ´ ≥ 0. Adding this for each segment yields the desired inequality.
So now focus on a maximal segment k , k + 1, . . . , k − 1, k . Thus, there is some k∗ such that for k ≤ k < k∗ , we have pk ij = Hj, and for k∗ ≤ k ≤ k , we have pk ij = Lj. Now the left hand side of the above inequality for this segment is simply xk ij (Lj −Hj)+xk∗ ij (Hj −Lj) ≥ 0, since xk ij ≤ 1 m ≤ xk∗ ij as pk ij = Hj and pk∗ ij = Lj.
We now describe how to use a c-approximation algorithm to obtain an algorithm satisfying the property in Lemma 4.3.
For simplicity, first suppose that the approximation algorithm returns an integral schedule. The idea is to simply spread this schedule. We take each job j assigned to a high machine and assign it to an extent 1/m on all machines; for each job j assigned to a low machine, say i, we assign 1/m-fraction of it to the other machines where it is low, and assign its remaining fraction (which is at least 1/m) to i.
The resulting assignment clearly satisfies the desired properties. Also observe that the load on any machine has at most increased by 1 m · (load on other machines) ≤ makespan, and hence the makespan has at most doubled. This spreading out can also be done if the initial schedule is fractional. We now describe the algorithm precisely.
Algorithm 1 Let A be any algorithm that on any input p outputs a possibly fractional assignment x such that xij > 0 implies that pij ≤ T, where T is the makespan of x. (In particular, note that any algorithm that returns an integral assignment has these properties.) Our algorithm, which we call A , returns the following assignment xF . Initialize xF ij = 0 for all i, j. For every i, j,
ij = P i :pi j =Hj xi j/m;
ij = xij + P i =i:pi j =Lj (xi j −xij)/m+ P i :pi j =Hj xi j/m.
Theorem 4.4 Suppose algorithm A satisfies the conditions in Algorithm 1 and returns a makespan of at most c·OPT(p) for every p. Then, the algorithm A constructed above is a 2c-approximation, cycle-monotone fractional algorithm.
Moreover, if xF ij > 0 on input p, then pij ≤ c · OPT(p).
Proof. First, note that xF is a valid assignment: for every job j,
P i xF ij = P i xij + P i,i =i:pij =pi j =Lj (xi j − xij)/m = P i xij = 1. We also have that if pij = Hj then xF ij = P i :pi j =Hj xi j/m ≤ 1/m. If pij = Lj, then xF ij = xij(1 − /m) + P i =i xi j/m where = |{i = i : pi j = Lj}| ≤ m − 1; so xF ij ≥ P i xi j/m ≥ 1/m. Thus, by Lemma 4.3, A satisfies cycle monotonicity.
The total load on any machine i under xF is at mostP j:pij =Hj P i :pi j =Hj Hj· xi j m + P j:pij =Lj Lj ` xij+ P i =i xi j m ´ , which is at most P j pijxij + P i =i P j pi jxi j/m ≤ 2c · OPT(p). Finally, if xF ij > 0 and pij = Lj, then pij ≤ OPT(p). If pij = Hj, then for some i (possibly i) with pi j = Hj we have xi j > 0, so by assumption, pi j = Hj = pij ≤ c · OPT(p).
Theorem 4.4 combined with Lemmas 4.1 and 4.2, gives a 3c-approximation, truthful-in-expectation mechanism. The computation of payments will depend on the actual approximation algorithm used. Section 3 does however give an explicit procedure to compute payments ensuring truthfulness, though perhaps not in polynomial-time.
Theorem 4.5 The procedure in Algorithm 1 converts any c-approximation fractional algorithm into a 3c-approximation, truthful-in-expectation mechanism.
Taking A in Algorithm 1 to be the algorithm that returns an LP-optimum assignment satisfying the required conditions (see [18, 25]), we obtain a 3-approximation mechanism.
Corollary 4.6 There is a truthful-in-expectation mechanism with approximation ratio 3 for the Lj-Hj setting.
THE TWO-VALUES CASE We now present a deterministic 2-approximation truthful mechanism for the case where pij ∈ {L, H} for all i, j. In the sequel, we will often say that j is assigned to a lowmachine to denote that j is assigned to a machine i where pij = L. We will call a job j a low job of machine i if pij = L; the low-load of i is the load on i due to its low jobs, i.e.,
P j:pij =L xijpij.
As in Section 4, our goal is to obtain an approximation algorithm that satisfies cycle monotonicity. We first obtain a simplification of condition (3) for our two-values {L, H} scheduling domain (Proposition 5.1) that will be convenient to work with. We describe our algorithm in Section 5.1.
In Section 5.2, we bound its approximation guarantee and prove that it satisfies cycle-monotonicity. In Section 5.3, we compute explicit payments giving a truthful mechanism.
Finally, in Section 5.4 we show that no deterministic mechanism can achieve the optimum makespan. Define nk,
H = ˛ ˛{j : xk ij = 1, pk ij = L, pij = H} ˛ ˛ (4) nk,
L = ˛ ˛{j : xk ij = 1, pk ij = H, pij = L} ˛ ˛. (5) Then,
P j xk+1 ij (pk ij − pk+1 ij ) = (nk+1,k H − nk+1,k L )(H − L).
Plugging this into (3) and dividing by (H − L), we get the following.
Proposition 5.1 Cycle monotonicity in the two-values scheduling domain is equivalent to the condition that, for every player i, every p−i, every integer K, and every p1 i , . . . , pK i ,
KX k=1 ` nk+1,k H − nk+1,k L ´ ≥ 0. (6) 257
We now describe an algorithm that satisfies condition (6) and achieves a 2-approximation. We will assume that L, H are integers, which is without loss of generality.
A core component of our algorithm will be a procedure that takes an integer load threshold T and computes an integer partial assignment x of jobs to machines such that (a) a job is only assigned to a low machine; (b) the load on any machine is at most T; and (c) the number of jobs assigned is maximized. Such an assignment can be computed by solving a max-flow problem: we construct a directed bipartite graph with a node for every job j and every machine i, and an edge (j, i) of infinite capacity if pij = L. We also add a source node s with edges (s, j) having capacity 1, and sink node t with edges (i, t) having capacity T/L . Clearly any integer flow in this network corresponds to a valid integer partial assignment x of makespan at most T, where xij = 1 iff there is a flow of 1 on the edge from j to i. We will therefore use the terms assignment and flow interchangeably. Moreover, there is always an integral max-flow (since all capacities are integers). We will often refer to such a max-flow as the max-flow for (p, T).
We need one additional concept before describing the algorithm. There could potentially be many max-flows and we will be interested in the most balanced ones, which we formally define as follows. Fix some max-flow. Let ni p,T be the amount of flow on edge (i, t) (or equivalently the number of jobs assigned to i in the corresponding schedule), and let np,T be the total size of the max-flow, i.e., np,T = P i ni p,T .
For any T ≤ T, define ni p,T |T = min(ni p,T , T ), that is, we truncate the flow/assignment on i so that the total load on i is at most T . Define np,T |T = P i ni p,T |T . We define a prefix-maximal flow or assignment for T as follows.
Definition 5.2 (Prefix-maximal flow) A flow for the above network with threshold T is prefix-maximal if for every integer T ≤ T, we have np,T |T = np,T .
That is, in a prefix-maximal flow for (p, T), if we truncate the flow at some T ≤ T, we are left with a max-flow for (p, T ). An elementary fact about flows is that if an assignment/flow x is not a maximum flow for (p, T) then there must be an augmenting path P = (s, j1, i1, . . . , jK , iK , t) in the residual graph that allows us to increase the size of the flow. The interpretation is that in the current assignment, j1 is unassigned, xi j = 0, which is denoted by the forward edges (j , i ), and xi j +1 = 1, which is denoted by the reverse edges (i , j +1). Augmenting x using P changes the assignment so that each j is assigned to i in the new assignment, which increases the value of the flow by 1. A simple augmenting path does not decrease the load of any machine; thus, one can argue that a prefix-maximal flow for a threshold T always exists. We first compute a max-flow for threshold 1, use simple augmenting paths to augment it to a max-flow for threshold 2, and repeat, each time augmenting the max-flow for the previous threshold t to a max-flow for threshold t + 1 using simple augmenting paths.
Algorithm 2 Given a vector of processing times p, construct an assignment of jobs to machines as follows.
(p) = min ˘ T ≥ H, T multiple of L : np,T · L + (n − np,T ) · H ≤ m · T ¯ .
Note that np,T ·L+(n−np,T )·H −m·T is a decreasing function of T, so T∗ (p) can be computed in polynomial time via binary search.
(p) and the corresponding partial assignment (i.e., j is assigned to i iff there is 1 unit of flow on edge (j, i)).
the flow-phase, in a greedy manner as follows.
Consider these jobs in an arbitrary order and assign each job to the machine with the current lowest load (where the load includes the jobs assigned in the flow-phase).
Our algorithm needs to compute a prefix-maximal assignment for the threshold T∗ (p). The proof showing the existence of a prefix-maximal flow only yields a pseudopolynomial time algorithm for computing it. But notice that the max-flow remains the same for any T ≥ T = n · L. So a prefix-maximal flow for T is also prefix-maximal for any T ≥ T . Thus, we only need to compute a prefix-maximal flow for T = min{T∗ (p), T }. This can be be done in polynomial time by using the iterative-augmenting-paths algorithm in the existence proof to compute iteratively the maxflow for the polynomially many multiples of L up to (and including) T .
Theorem 5.3 One can efficiently compute payments that when combined with Algorithm 2 yield a deterministic 2approximation truthful mechanism for the two-values scheduling domain.
Let OPT(p) denote the optimal makespan for p. We now prove that Algorithm 2 is a 2-approximation algorithm that satisfies cycle monotonicity. This will then allow us to compute payments in Section 5.3 and prove Theorem 5.3.
Claim 5.4 If OPT(p) < H, the makespan is at most OPT(p).
Proof. If OPT(p) < H, it must be that the optimal schedule assigns all jobs to low machines, so np,OPT(p) = n.
Thus, we have T∗ (p) = L · H L . Furthermore, since we compute a prefix-maximal flow for threshold T∗ (p) we have np,T ∗(p)|OPT(p) = np,OPT(p) = n, which implies that the load on each machine is at most OPT(p). So in this case the makespan is at most (and hence exactly) OPT(p).
Claim 5.5 If OPT(p) ≥ H, then T∗ (p) ≤ L · OPT(p) L ≤ OPT(p) + L.
Proof. Let nOPT(p) be the number of jobs assigned to low machines in an optimum schedule. The total load on all machines is exactly nOPT(p) · L + (n − nOPT(p)) · H, and is at most m · OPT(p), since every machine has load at most OPT(p). So taking T = L · OPT(p) L ≥ H, since np,T ≥ nOPT(p) we have that np,T ·L+(n−np,T )·H ≤ m·T. Hence,
T∗ (p), the smallest such T, is at most L · OPT(p) L .
Claim 5.6 Each job assigned in step 3 of the algorithm is assigned to a high machine. 258 Proof. Suppose j is assigned to machine i in step 3. If pij = L, then we must have ni p,T ∗(p) = T∗ (p), otherwise we could have assigned j to i in step 2 to obtain a flow of larger value. So at the point just before j is assigned in step 3, the load of each machine must be at least T∗ (p). Hence, the total load after j is assigned is at least m · T∗ (p) + L > m · T∗ (p). But the total load is also at most np,T ∗(p) · L + (n − np,T ∗(p)) · H ≤ m · T∗ (p), yielding a contradiction.
Lemma 5.7 The above algorithm returns a schedule with makespan at most OPT(p)+max ˘ L, H(1− 1 m ) ¯ ≤ 2·OPT(p).
Proof. If OPT(p) < H, then by Claim 5.4, we are done.
So suppose OPT(p) ≥ H. By Claim 5.5, we know that T∗ (p) ≤ OPT(p) + L. If there are no unassigned jobs after step 2 of the algorithm, then the makespan is at most T∗ (p) and we are done. So assume that there are some unassigned jobs after step 2. We will show that the makespan after step 3 is at most T +H ` 1− 1 m ´ where T = min ˘ T∗ (p), OPT(p) ¯ .
Suppose the claim is false. Let i be the machine with the maximum load, so li > T + H ` 1 − 1 m ´ . Let j be the last job assigned to i in step 3, and consider the point just before it is assigned to i. So li > T − H/m at this point. Also since j is assigned to i, by our greedy rule, the load on all the other machines must be at least li. So the total load after j is assigned, is at least H + m · li > m · T (since pij = H by Claim 5.6). Also, for any assignment of jobs to machines in step 3, the total load is at most np,T ∗(p) · L + (n − np,T ∗(p)) · H since there are np,T ∗(p) jobs assigned to low machines. Therefore, we must have m · T < np,T ∗(p) · L + (n − np,T ∗(p)) · H. But we will argue that m · T ≥ np,T ∗(p) ·L+(n−np,T ∗(p))·H, which yields a contradiction.
If T = T∗ (p), this follows from the definition of T∗ (p).
If T = OPT(p), then letting nOPT(p) denote the number of jobs assigned to low machines in an optimum schedule, we have np,T ∗(p) ≥ nOPT(p). So np,T ∗(p) ·L+(n−np,T ∗(p))·H ≤ nOPT(p) ·L+(n−nOPT(p))·H. This is exactly the total load in an optimum schedule, which is at most m · OPT(p).
Lemma 5.8 Consider any two instances p = (pi, p−i) and p = (pi, p−i) where pi ≥ pi, i.e., pij ≥ pij ∀j. If T is a threshold such that np,T > np ,T , then every maximum flow x for (p , T) must assign all jobs j such that pij = L.
Proof. Let Gp denote the residual graph for (p , T) and flow x . Suppose by contradiction that there exists a job j∗ with pij∗ = L that is unassigned by x . Since pi ≥ pi, all edges (j, i) that are present in the network for (p , T) are also present in the network for (p, T). Thus, x is a valid flow for (p, T). But it is not a max-flow, since np,T > np ,T .
So there exists an augmenting path P in the residual graph for (p, T) and flow x . Observe that node i must be included in P, otherwise P would also be an augmenting path in the residual graph Gp contradicting the fact that x is a maxflow. In particular, this implies that there is a path P ⊂ P from i to the sink t. Let P = (i, j1, i1, . . . , jK , iK , t). All the edges of P are also present as edges in Gp - all reverse edges (i , j +1) are present since such an edge implies that xi j +1 = 1; all forward edges (j , i ) are present since i = i so pi j = pi j = L, and xi j +1 = 0. But then there is an augmenting path (j∗ , i, j1, i1, . . . , jK , iK , t) in Gp which contradicts the maximality of x .
Let L denote the all-low processing time vector. Define TL i (p−i) = T∗ (L, p−i). Since we are focusing on machine i, and p−i is fixed throughout, we abbreviate TL i (p−i) to TL .
Also, let pL = (L, p−i). Note that T∗ (p) ≥ TL for every instance p = (pi, p−i).
Corollary 5.9 Let p = (pi, p−i) be any instance and let x be any prefix-maximal flow for (p, T∗ (p)). Then, the low-load on machine i is at most TL .
Proof. Let T∗ = T∗ (p). If T∗ = TL , then this is clearly true. Otherwise, consider the assignment x truncated at TL .
Since x is prefix-maximal, we know that this constitutes a max-flow for (p, TL ). Also, np,T L < npL,T L because T∗ > TL . So by Lemma 5.8, this truncated flow must assign all the low jobs of i. Hence, there cannot be a job j with pij = L that is assigned to i after the TL -threshold since then j would not be assigned by this truncated flow. Thus, the low-load of i is at most TL .
Using these properties, we will prove the following key inequality: for any p1 = (p−i, p1 i ) and p2 = (p−i, p2 i ), np1,T L ≥ np2,T L − n2,1 H + n2,1 L (7) where n2,1 H and n2,1 L are as defined in (4) and (5), respectively. Notice that this immediately implies cycle monotonicity, since if we take p1 = pk and p2 = pk+1 , then (7) implies that npk,T L ≥ npk+1,T L − nk+1,k H + nk+1,k L ; summing this over all k = 1, . . . , K gives (6).
Lemma 5.10 If T∗ (p1 ) > TL , then (7) holds.
Proof. Let T1 = T∗ (p1 ) and T2 = T∗ (p2 ). Take the prefix-maximal flow x2 for (p2 , T2 ), truncate it at TL , and remove all the jobs from this assignment that are counted in n2,1 H , that is, all jobs j such that x2 ij = 1, p2 ij = L, p1 ij = H.
Denote this flow by x. Observe that x is a valid flow for (p1 , TL ), and the size of this flow is exactly np2,T 2 |T L −n2,1 H = np2,T L −n2,1 H . Also none of the jobs that are counted in n2,1 L are assigned by x since each such job j is high on i in p2 .
Since T1 > TL , we must have np1,T L < npL,T L . So if we augment x to a max-flow for (p1 , TL ), then by Lemma 5.8 (with p = pL and p = p1 ), all the jobs corresponding to n2,1 L must be assigned in this max-flow. Thus, the size of this max-flow is at least (size of x) + n2,1 L , that is, np1,T L ≥ np2,T L − n2,1 H + n2,1 L , as claimed.
Lemma 5.11 Suppose T∗ (p1 ) = TL . Then (7) holds.
Proof. Again let T1 = T∗ (p1 ) = TL and T2 = T∗ (p2 ).
Let x1 , x2 be the complete assignment, i.e., the assignment after both steps 2 and 3, computed by our algorithm for p1 , p2 respectively. Let S = {j : x2 ij = 1 and p2 ij = L} and S = {j : x2 ij = 1 and p1 ij = L}. Therefore, |S | = |S| − n2,1 H + n2,1 L and |S| = ni p2,T 2 = ni p2,T 2 |T L (by Corollary 5.9).
Let T = |S | · L. We consider two cases.
Suppose first that T ≤ TL . Consider the following flow for (p1 , TL ): assign to every machine other than i the lowassignment of x2 truncated at TL , and assign the jobs in S to machine i. This is a valid flow for (p1 , TL ) since the load on i is T ≤ TL . Its size is equal to P i =i ni p2,T 2 |T L +|S | = np2,T 2 |T L −n2,1 H +n2,1 L = np2,T L −n2,1 H +n2,1 L . The size of the max-flow for (p1 , TL ) is no smaller, and the claim follows. 259 Now suppose T > TL . Since |S| · L ≤ TL (by Corollary 5.9), it follows that n2,1 L > n2,1 H ≥ 0. Let ˆT = T − L ≥ TL since T , TL are both multiples of L. Let M = np2,T 2 − n2,1 H + n2,1 L = |S | + P i =i ni p2,T 2 . We first show that m · ˆT < M · L + (n − M) · H. (8) Let N be the number of jobs assigned to machine i in x2 .
The load on machine i is |S|·L+(N −|S|)·H ≥ |S |·L−n2,1 L · L+(N−|S|)·H which is at least |S |·L > ˆT since n2,1 L ≤ N− |S|. Thus we get the inequality |S |·L+(N −|S |)·H > ˆT.
Now consider the point in the execution of the algorithm on instance p2 just before the last high job is assigned to i in Step 3 (there must be such a job since n2,1 L > 0). The load on i at this point is |S| · L + (N − |S| − 1) · H which is least |S | · L − L = ˆT by a similar argument as above. By the greedy property, every i = i also has at least this load at this point, so P j p2 i jx2 i j ≥ ˆT. Adding these inequalities for all i = i, and the earlier inequality for i, we get that |S | · L + (N − |S |) · H + P i =i P j p2 i jx2 i j > m ˆT. But the left-hand-side is exactly M · L + (n − M) · H.
On the other hand, since T1 = TL , we have m · ˆT ≥ m · TL ≥ np1,T L · L + (n − np1,T L ) · H. (9) Combining (8) and (9), we get that np1,T L > M = np2,T 2 − n2,1 H + n2,1 L ≥ np2,T L − n2,1 H + n2,1 L .
Lemma 5.12 Algorithm 2 satisfies cycle monotonicity.
Proof. Taking p1 = pk and p2 = pk+1 in (7), we get that npk,T L ≥ npk+1,T L −nk+1,k H +nk+1,k L . Summing this over all k = 1, . . . , K (where K + 1 ≡ 1) yields (6).
Lemmas 5.7 and 5.12 show that our algorithm is a 2approximation algorithm that satisfies cycle monotonicity.
Thus, by the discussion in Section 3, there exist prices that yield a truthful mechanism. To obtain a polynomial-time mechanism, we also need to show how to compute these prices (or payments) in polynomial-time. It is not clear, if the procedure outlined in Section 3 based on computing shortest paths in the allocation graph yields a polynomial time algorithm, since the allocation graph has an exponential number of nodes (one for each output assignment).
Instead of analyzing the allocation graph, we will leverage our proof of cycle monotonicity, in particular, inequality (7), and simply spell out the payments.
Recall that the utility of a player is ui = Pi − li, where Pi is the payment made to player i. For convenience, we will first specify negative payments (i.e., the Pis will actually be prices charged to the players) and then show that these can be modified so that players have non-negative utilities (if they act truthfully). Let Hi denote the number of jobs assigned to machine i in step 3. By Corollary 5.6, we know that all these jobs are assigned to high machines (according to the declared pis). Let H−i = P i =i Hi and n−i p,T = P i =i ni p,T . The payment Pi to player i is defined as: Pi(p) = −L · n−i p,T ∗(p) − H · H−i (p) − (H − L) ` np,T ∗(p) − np,T L i (p−i) ´ (10) We can interpret our payments as equating the player"s cost to a careful modification of the total load (in the spirit of VCG prices). The first and second terms in (10), when subtracted from i"s load li equate i"s cost to the total load.
The term np,T ∗(p) − np,T L i (p−i) is in fact equal to n−i p,T ∗(p) − n−i p,T ∗(p)|T L i (p−i) since the low-load on i is at most TL i (p−i) (by Claim 5.9). Thus the last term in equation (10) implies that we treat the low jobs that were assigned beyond the TL i (p−i) threshold (to machines other than i) effectively as high jobs for the total utility calculation from i"s point of view. It is not clear how one could have conjured up these payments a priori in order to prove the truthfulness of our algorithm. However, by relying on cycle monotonicity, we were not only able to argue the existence of payments, but also our proof paved the way for actually inferring these payments. The following lemma explicitly verifies that the payments defined above do indeed give a truthful mechanism.
Lemma 5.13 Fix a player i and the other players" declarations p−i. Let i"s true type be p1 i . Then, under the payments defined in (10), i"s utility when she declares her true type p1 i is at least her utility when she declares any other type p2 i .
Proof. Let c1 i , c2 i denote i"s total cost, defined as the negative of her utility, when she declares p1 , and p2 , respectively (and the others declare p−i). Since p−i is fixed, we omit p−i from the expressions below for notational clarity.
The true load of i when she declares her true type p1 i is L · ni p1,T ∗(p1) + H · Hi (p1 ), and therefore c1 i = L · np1,T ∗(p1) + H · (n − np1,T ∗(p1)) + (H − L) ` np1,T ∗(p1) − np1,T L i ´ = n · H − (H − L)np1,T L i (11) On the other hand, i"s true load when she declares p2 i is L · (ni p2,T ∗(p2) − n2,1 H + n2,1 L ) + H · (Hi + n2,1 H − n2,1 L ) (since i"s true processing time vector is p1 i ), and thus c2 i = n · H − (H − L)np2,T L i + (H − L)n2,1 H − (H − L)n2,1 L .
Thus, (7) implies that c1 i ≤ c2 i .
Price specifications are commonly required to satisfy, in addition to truthfulness, individual rationality, i.e., a player"s utility should be non-negative if she reveals her true value.
The payments given by (10) are not individually rational as they actually charge a player a certain amount. However, it is well-known that this problem can be easily solved by adding a large-enough constant to the price definition. In our case, for example, letting H denote the vector of all H"s, we can add the term n·H −(H −L)n(H,p−i),T L i (p−i) to (10).
Note that this is a constant for player i. Thus, the new payments are Pi (p) = n · H − L · n−i p,T ∗(p) − H · H−i (p) − (H −L) ` np,T ∗(p) −np,T L i (p−i) +n(H,p−i),T L i (p−i) ´ . As shown by (11), this will indeed result in a non-negative utility for i (since n(H,p−i),T L i (p−i) ≤ n(pi,p−i),T L i (p−i) for any type pi of player i). This modification also ensures the additionally desired normalization property that if a player receives no jobs then she receives zero payment: if player i receives the empty set for some type pi then she will also receive the empty set for the type H (this is easy to verify for our specific algorithm), and for the type H, her utility equals zero; thus, by truthfulness this must also be the utility of every other declaration that results in i receiving the empty set.
This completes the proof of Theorem 5.3. 260
We now show that irrespective of computational considerations, there does not exist a cycle-monotone algorithm for the L-H case with an approximation ratio better than 1.14.
Let H = α·L for some 2 < α < 2.5 that we will choose later.
There are two machines I, II and seven jobs. Consider the following two scenarios: Scenario 1. Every job has the same processing time on both machines: jobs 1-5, are L, and jobs 6, 7 are H. Any optimal schedule assigns jobs 1-5 to one machine and jobs 6, 7 to the other, and has makespan OPT1 = 5L. The secondbest schedule has makespan at least Second1 = 2H + L.
Scenario 2. If the algorithm chooses an optimal schedule for scenario 1, assume without loss of generality that jobs 6, 7 are assigned to machine II. In scenario 2, machine I has the same processing-time vector. Machine II lowers jobs 6, 7 to L and increases 1-5 to H. An optimal schedule has makespan 2L + H, where machine II gets jobs 6, 7 and one of the jobs 1-5. The second-best schedule for this scenario has makespan at least Second2 = 5L.
Theorem 5.14 No deterministic truthful mechanism for the two-value scheduling problem can obtain an approximation ratio better than 1.14.
Proof. We first argue that a cycle-monotone algorithm cannot choose the optimal schedule in both scenarios. This follows because otherwise cycle monotonicity is violated for machine II. Taking p1 II , p2 II to be machine II"s processingtime vectors for scenarios 1, 2 respectively, we get P j(p1 II ,j − p2 II ,j)(x2 II ,j −x1 II ,j) = (L−H)(1−0) < 0. Thus, any truthful mechanism must return a sub-optimal makespan in at least one scenario, and therefore its approximation ratio is at least min ˘Second1 OPT1 , Second2 OPT2 ¯ ≥ 1.14 for α = 2.364.
We remark that for the {Lj, Hj}-case where there is a common ratio r = Hj Lj for all jobs (this generalizes the restricted-machines setting) one can obtain a fractional truthful mechanism (with efficiently computable prices) that returns a schedule of makespan at most OPT(p) for every p. One can view each job j as consisting of Lj sub-jobs of size 1 on a machine i if pij = Lj, and size r if pij = Hj.
For this new instance ˜p, note that ˜pij ∈ {1, r} for every i, j. Notice also that any assignment ˜x for the instance ˜p translates to a fractional assignment x for p, where pijxij =P j : sub-job of j ˜pij ˜xij . Thus, if we use Algorithm 2 to obtain a schedule for the instance ˜p, equation (6) translates precisely to (3) for the assignment x; moreover, the prices for ˜p translate to prices for the instance p. The number of sub-jobs assigned to low-machines in the flow-phase is simply the total work assigned to low-machines. Thus, we can implement the above reduction by setting up a max-flow problem that seems to maximize the total work assigned to low machines. Moreover, since we have a fractional domain, we can use a more efficient greedy rule for packing the unassigned portions of jobs and argue that the fractional assignment has makespan at most OPT(p). The assignment x need not however satisfy the condition that xij > 0 implies pij ≤ OPT(p) for arbitrary r, therefore, the rounding procedure of Lemma 4.2 does not yield a 2-approximation truthful-in-expectation mechanism. But if r > OPT(p) (as in the restricted-machines setting), this condition does hold, so we get a 2-approximation truthful mechanism.
Acknowledgments We thank Elias Koutsoupias for his help in refining the analysis of the lower bound in Section 5.4, and the reviewers for their helpful comments.
[1] N. Andelman, Y. Azar, and M. Sorani. Truthful approximation mechanisms for scheduling selfish related machines. In Proc. 22nd STACS, 69-82, 2005. [2] A. Archer. Mechanisms for discrete optimization with rational agents. PhD thesis, Cornell University, 2004. [3] A. Archer and ´E. Tardos. Truthful mechanisms for one-parameter agents. In Proc. 42nd FOCS, pages 482-491,
[4] V. Auletta, R. De-Prisco, P. Penna, and G. Persiano.
Deterministic truthful approximation mechanisms for scheduling related machines. In Proc. 21st STACS, pages 608-619, 2004. [5] I. Bez´akov´a and V. Dani. Allocating indivisible goods. In ACM SIGecom Exchanges, 2005. [6] S. Bikhchandani, S. Chatterjee, R. Lavi, A. Mu"alem,
N. Nisan, and A. Sen. Weak monotonicity characterizes deterministic dominant-strategy implementation.
Econometrica, 74:1109-1132, 2006. [7] P. Briest, P. Krysta, and B. Vocking. Approximation techniques for utilitarian mechanism design. In Proc. 37th STOC, pages 39-48, 2005. [8] G. Christodoulou, E. Koutsoupias, and A. Vidali. A lower bound for scheduling mechanisms. In Proc. 18th SODA, pages 1163-1170, 2007. [9] E. Clarke. Multipart pricing of public goods. Public Choice, 8:17-33, 1971. [10] T. Groves. Incentives in teams. Econometrica, 41:617-631,
[11] H. Gui, R. Muller, and R. V. Vohra. Characterizing dominant strategy mechanisms with multi-dimensional types, 2004.
Working paper. [12] L. A. Hall. Approximation algorithms for scheduling. In D. Hochbaum, editor, Approximation Algorithms for NP-Hard Problems. PWS Publishing, MA, 1996. [13] A. Kov´acs. Fast monotone 3-approximation algorithm for scheduling related machines. In Proc. 13th ESA, pages 616-627, 2005. [14] V. S. A. Kumar, M. V. Marathe, S. Parthasarathy, and A. Srinivasan. Approximation algorithms for scheduling on multiple machines. In Proc. 46th FOCS, pages 254-263, 2005. [15] R. Lavi, A. Mu"alem, and N. Nisan. Towards a characterization of truthful combinatorial auctions. In Proc. 44th FOCS, pages 574-583, 2003. [16] R. Lavi and C. Swamy. Truthful and near-optimal mechanism design via linear programming. In Proc. 46th FOCS, pages 595-604, 2005. [17] D. Lehmann, L. O"Callaghan, and Y. Shoham. Truth revelation in approximately efficient combinatorial auctions.
Journal of the ACM, 49:577-602, 2002. [18] J. K. Lenstra, D. B. Shmoys, and ´E. Tardos. Approximation algorithms for scheduling unrelated parallel machines. Math.
Prog., 46:259-271, 1990. [19] R. J. Lipton, E. Markakis, E. Mossel, and A. Saberi. On approximately fair allocations of indivisible goods. In Proc. 5th EC, pages 125-131, 2004. [20] A. Mu"alem and M. Schapira. Setting lower bounds on truthfulness. In Proc. 18th SODA, 1143-1152, 2007. [21] R. Myerson. Optimal auction design. Mathematics of Operations Research, 6:58-73, 1981. [22] N. Nisan and A. Ronen. Algorithmic mechanism design.

Consider an interaction in a multi-agent system, in which every player holds some private information, which is called the player"s type. For example, in an auction interaction the type of a player is its valuation, or, in more complex auctions, its valuation function. Every player has a set of actions, and a strategy of a player is a function that maps each of its possible types to an action. This interaction is modeled as a game with incomplete information. This game is called a Bayesian game, when a commonly known probability measure on the profiles of types is added to the system. Otherwise it is called a pre-Bayesian game. In this paper we deal only with pre-Bayesian games. The leading solution concept for pre-Bayesian games is the ex post equilibrium: A profile of strategies, one for each player, such that no player has a profitable deviation independently of the types of the other players. Consider the following simple example of a pre-Bayesian game, which possesses an ex post equilibrium. The game is denoted by H. a b a 5, 2 3, 0 b 0, 0 4, 2 A a b a 2, 2 0, 0 b 3, 3 5, 2 B At the game H there are two players. Both players can choose among two actions: a and b. The column player, player 2, has a private type, A or B (player 1 has only one possible type). A strategy of player 1 is g1,where g1 = a or g1 = b. A strategy of player 2 is a function g2 : {A, B} → {a, b}.That is, player 2 has 4 strategies. In this game the strategy profile (g1, g2) is an ex post equilibrium, where g1 = b and g2(A) = b, g2(B) = a.
Unfortunately, pre-Bayesian games do not, in general, possess ex post equilibria, even if we allow mixed strategies. In order to address this problem we suggest in this paper the use of mediators. A mediator is a reliable entity that can interact with the players and perform on their behalf actions in a given game. However, a mediator can not enforce behavior. Indeed, an agent is free to participate in the game without the help of the mediator. The mediator"s behavior on behalf of the agents that give it the right of play is pre-specified, and is conditioned on information the agents would provide to the mediator. This notion is highly natural; in many systems there is some form of reliable party or 279 administrator that can be used as a mediator. The simplest form of a mediator discussed in the game theory literature is captured by the notion of correlated equilibrium [1]. This notion was generalized to communication equilibrium by [5, 15]. Another type of mediators is discussed in [13]. However, in all these settings the mediator can not perform actions on behalf of the agents that allow it to do so. Mediators that can obtain the right of play but can not enforce the use of their services have been already defined and discussed for games with complete information in [14].1 The topic of mediators for games with complete information has been further generalized and analyzed in [16]. In this paper we introduce another use of mediators, in establishing behaviors which are stable against unilateral deviations in games with incomplete information. Notice that we assume that the multi-agent interaction (formalized as a game) is given, and all the mediator can do is to perform actions on behalf of the agents that explicitly allow it to do so.2 In order to illustrate the power of mediators for games with incomplete information consider the following pre-Bayesian game G that does not possess an ex post equilibrium. In G, the column player has two possible types: A and B. a b a 5, 2 3, 0 b 0, 0 2, 2 A a b a 2, 2 0, 0 b 3, 0 5, 2 B A mediator for G should specify the actions it will choose on behalf of the players that give it the right to play. If player 2 wants to give the mediator the right to play it should also report a type. Consider the following mediator: If both players give the mediator the right of play, then the mediator will play on their behalf (a, a) if player 2 reports A and (b, b) if player 2 reports B. If only player 1 gives the mediator the right of play then the mediator will choose a on his behalf. If only player 2 gives the mediator the right of play, the mediator will choose action a (resp. b) on its behalf, if B (resp. A) has been reported.
The mediator generates a new pre-Bayesian game,which is called the mediated game. In the mediated game player 1 has three actions: Give the mediator the right of play, denoted by m, or play directly a or b. Player 2 has four actions: m − A, m − B,a,b, where m − A (m − B) means reporting A (B) to the mediator and give it the right of play.
The mediated game is described in the following figure: m − A m − B a b m 5, 2 2, 2 5, 2 3, 0 a 3, 0 5, 2 5, 2 3, 0 b 2, 2 0, 0 0, 0 2, 2 A 1 For games with complete information the main interest is in leading agents to behaviors, which are stable against deviations by coalitions. A special case of mediators was already discussed in [8]. In this paper the authors discussed mediators for a two-person game, which is known to the players but not to the mediators, and they looked for Nash equilibrium in the new game generated by the mediator. 2 This natural setting is different from the one discussed in the classical theories of implementation and mechanism design, where a designer designs a new game from scratch in order to yield some desired behavior. m − A m − B a b m 2, 2 5, 2 2, 2 0, 0 a 0, 0 2, 2 2, 2 0, 0 b 5, 2 3, 0 3, 0 5, 2 B It is now easy to verify that giving the mediator the right of play, and reporting truthfully, is an ex-post equilibrium at the mediated game. That is, (f1, f2) is an ex post equilibrium, where f1 = m, and f2(A) = m−A, f2(B) = m−B.
The aim of this paper is twofold. We introduce mediators for games with incomplete information, and apply them in the context of position auctions. Our choice of positions auctions as the domain of application is not an accident; indeed, positions auctions have become a central issue in advertisement and the selection of appropriate position auctions for that task is a subject of considerable amount of study [17, 3, 9, 4].3 Current position auctions however do not possess ex-post equilibrium, i.e. solutions which are stable against unilateral deviations regardless of the agents" private information, nor guarantee optimal social surplus. In contrast, in the VCG position auction, which is currently not used in practice, there is a truth-revealing ex post equilibrium, which yields optimal surplus. We therefore suggest the use of mediators in order to attempt and implement the output of the VCG position auction, by transforming other (and in particular current) position auctions into a VCG position auction.4 More specifically, the mediated game will have an ex post equilibrium, which generates the outcome of the VCG position auction. One such mediator has already been discussed for other purposes in the literature: An English auction type of algorithm was constructed in [3] that takes as an input the valuations of the players and outputs bids for the next-price position auction. It was proved there that reporting the true type to this algorithm by each player forms an ex post equilibrium, which generates the VCG outcome.
In our language this algorithm can be almost considered as a mediator for the next-price position auction that implement the VCG outcome function. What is missing, is a component that punishes players who send their bids directly to the auctioneer, and a proof that using the mediator services and reporting the true type by each player is an ex post equilibrium in the mediated game defined by the algorithm and by the additional component. A mediator may generate a desired outcome by punishing the players who do not use its services with very high bids by the players that use its services. We believe that such mediators are not realistic, and therefore we concentrate on the search for valid mediators that generate an ex post equilibrium and satisfy the additional rationality condition: an agent"s payoff can not be negative regardless of the actions taken by the agents who did not choose the mediator"s services, or agents who report false types to the mediator. We prove the existence of such desired mediators for the next-price (Google-like) position auctions5 , as well as for a richer class of position auctions, including all k-price position auctions, k > 1. For k=1, the self-price position auction, we show that the ex3 See also [12], where position auctions are titled ad auctions. 4 In general, except for the VCG position auction we do not expect position auctions to possess an ex post equilibrium (see Footnote 7). 5 Our proof uses an algorithm, which is different from the algorithm in [3] discussed earlier. 280 istence of such mediator depends on the tie breaking rule used in the auction.
Mediators in one-item auctions (in particular first price and second price auctions) have been already discussed in [6, 11, 2]; however they all used a Bayesian model.
Position auctions are a restricted type of general pre-Bayesian games. In this conference version we make the formal definition of mediators and implementing by mediation for the special case of position auctions, and only in the full version we present the general theory of mediators for pre-Bayesian games. Most of the proofs are omitted from this conference version.
In a position auction there is a seller who sells a finite number of positions j ∈ K = {1, ..., m}. There is a finite number of (potential) bidders i ∈ N = {1, ..., n}. We assume that there are more bidders than positions, i.e. n > m.
The positions are sold for a fixed period of time. For each position j there is a commonly-known number αj > 0, which is interpreted as the expected number of visitors at that position. αj is called the click-through rate of position j.
We assume that α1 > α2 > αm > 0. If i holds a position then every visitor to this position gives i a revenue of vi > 0, where vi is called the valuation of i. The set of possible valuations of i is Vi = (0, ∞).
We assume that the players" utility functions are quasilinear. That is, if player i is assigned to position j and pays pi per click then his utility is αj(vi − pi).
Every player is required to submit a bid, bi ∈ Bi = [0, ∞).
We assume that bidding 0 is a symbol for non-participation.
Therefore, a player with a bid 0 is not assigned to any position, and it pays 0.
In all position auctions we consider, the player with the highest positive bid receives the first position, the player with the second highest positive bid receives the second position, and so on. It is useful to define for every position auction two dummy positions m + 1 and −1, which more than one player may be assigned to. All players, who participate in the auction but do not get a position in K are assigned to position m + 1 and all players who choose not to participate are assigned to position −1. We also define αm+1 = α−1 = 0.
An assignment of players to positions is called an allocation. Hence, an allocation is a vector s = (s1, s2, · · · , sn) with si ∈ K ∪ {−1, m + 1} such that if si ∈ K then si = sl for every l = i; si is the position of player i. Given the above, a position auction is defined by its tie breaking rule, which determines the allocation in case of ties, and by its payment scheme. These are discussed below.
In practice, the most commonly used tie breaking rule is the First-Arrival rule: if a set of players submit the same bid, their priority in receiving the positions is determined by the times their bids were recorded; An earlier bid receives a higher priority. In auction theory this tie breaking rule is typically modelled by assuming that the auctioneer is using a random priority rule. More specifically, let Γ be the set of all permutations, γ = (γ1, ..., γn) of N. Every such γ defines a priority rule as follows: i has a higher priority than k if and only if γi < γk. Every vector of bids b and a permutation γ uniquely determine an allocation. An auctioneer who is using the random priority rule chooses a fixed priority rule γ by randomizing uniformly over Γ. However, the resulting priority rule is not told to the players before they make their bids. When the priority rule γ is told to the players before they make their bids, the tie breaking rule is called a fixed priority rule. Dealing with a fixed priority rule simplifies notations and proofs, and in most cases, and in particular in this paper, results that are obtained with this tie breaking rule are identical to the results obtained with the random priority rule. Therefore we will assume this tie breaking rule. In contrast, in Section 7 we discuss a non-standard approach for analyzing directly the first-arrival tie breaking rule.
Unless we say specifically otherwise we assume in this paper a fixed priority rule.
Without loss of generality we assume that the fixed priority rule is defined by the natural order, ˜γ = (1, 2, ..., n).
That is, bidder i has a higher priority than bidder k if and only if i < k. Given this fixed priority rule we can make the following definitions, which apply to all position auctions: We denote by s(b, i) the position player i is assigned to when the bid profile is b. The allocation determined by b is denoted by s(b) = (s(b, 1), s(b, 2), · · · , s(b, n)).
For every j ∈ K ∪ {−1, m + 1} we denote by δ(b, j) the set of players assigned to position j. Note that for j ∈ K, δ(b, j) contains at most one player.
Let α be a click-trough rate vector. Each position j ∈ K ∪ {−1, m + 1} is associated with a payment function, pα j : B → R+, where pα j (b) is the payment for position j when the bid profile is b. Naturally we assume that pα −1 is identically zero. However, we also assume that pα m+1 is identically zero. Hence, a participant who is not assigned a real position pays nothing.
We call the vector of payment functions pα = (pα j )j∈K the position payment scheme.
Remark: Whenever α is fixed or its value is clear from the context we will allow ourselves to omit the superscript α from the payment and other functions.
We deal with anonymous position payment schemes, i.e. the players" payments to the auctioneer are not influenced by their identities. This is modeled as follows: Let b ∈ B = B1 × B2 × · · · × Bn be a bid profile. We denote by b(j) the jth highest bid in b. For j > n we let b(j) = 0. For example if b = (3, 7, 3, 0, 2) then b(1) = 7, b(2) = 3, b(3) = 3, b(4) = 2, b(5) = 0. We denote b∗ = (b(1), · · · , b(n)). Anonymity is modeled by the requirement that for every two bid profiles b, d ∈ B, p(b) = p(d) whenever b∗ = d∗ . That is, for every position j there exists a real-valued function ˜pj defined over all ordered vectors of bids such that for every b ∈ B pj(b) = ˜pj(b∗ ).
We further assume that a player never pays more than his bid. That is pj(b) ≤ b(j) for every b ∈ B and for every j ∈ K.
It is convenient in certain cases to describe the payment functions indexed by the players. Let G be a position auction with a position payment scheme p.
For every player i we denote qi(b) = ps(b,i)(b), 281 and q(b) = (q1(b), q2(b), · · · qn(b)).
Note that the correspondence p → q is one-to-one. We call q the player payment scheme. All our assumptions about the position payment schemes can be transformed to analogous assumptions about the player payment schemes. For convenience, a position auction will be described either by its position payment scheme or by its player payment scheme.
The utility function for player i, wi : Vi × B → R+ is defined as follows: wi(vi, b) = αs(b,i)(vi − qi(b)) = αs(b,i)(vi − ps(b,i)(b)).
We next describe the payment schemes of three central position auctions.
Self-price position auctions: Each player who is assigned to a position with a positive click-through rate pays his own bid. That is, for every j ∈ K and every b ∈ B pj(b) = b(j) (1) Next-price position auctions: In this auction (run with a slight variation by Google), every player who is assigned to a position with a positive click-through rate pays the bid of the player assigned to the position right after him if there is such a player, and zero otherwise. That is for every j ∈ K and for every b ∈ B pj(b) = b(j+1) (2) VCG position auctions: In a Vickrey-Clarke-Groves (VCG) position auction the payment function for position j ∈ K is defined as follows.6 For every b ∈ B pvcg j (b) = Pm+1 k=j+1 b(k)(αk−1 − αk) αj (3) Note that the VCG position auction is not the next-price position auction unless there is only one position and α1 = 1.
We denote by G = G(α, p) the position auction with the click-through rate vector α and the payment scheme p. Recall that the set of types of i is Vi = (0, ∞). Let V = V1 × V2 × · · · × Vn be the set of profile of types, and for every S ⊆ N let VS = ×i∈SVi.
A mediator for G is a vector of functions m = (mS)S⊆N , where mS : VS → BS. The mediator m generates a preBayesian game Gm, which is called the mediated game. In this game every player i receives his type vi and can either send a type, ˆvi (not necessarily the true type) to the mediator, or send a bid directly to the auction. If S is the set of players that send a type to the mediator, the mediator bids on their behalf mS(ˆvS). Hence, the action set of player i in the mediated game is Bi ∪Vi, where conveniently Vi denotes both, (0, ∞) , and a copy of (0, ∞), which is disjoint from 6 We use the standard payment function of the VCG mechanism. A general VCG mechanism may be obtained from the standard one by adding an additional payment function to each player, which depends only on the types of the other players. Some authors (see e.g., [7]) call the standard VCG mechanism, the VC mechanism. According to this terminology we actually deal with VC position auctions. However, we decided to use the more common terminology.
Bi. We introduce the following terminology: The T-strategy for a player in the mediated game is the strategy, in which this player uses the mediator"s services and reports his true value to the mediator. The T-strategy profile is the profile of strategies in which every player is using the T-strategy. The T-strategy profile is an ex post equilibrium in the mediated game if for every player i and type vi, and for every vector of types of the other players, v−i, the following two conditions hold: E1: i is not better off when he gives the mediator the right of play and report a false type. That is, for every ˆvi ∈ Vi wi(vi, mN (vi, v−i)) ≥ wi(vi, mN (ˆvi, v−i)).
E2: i is not better off when he bids directly. That is for every bi ∈ Bi, wi(vi, mN (vi, v−i)) ≥ wi(vi, (bi, mN\i(v−i))).
Whenever the T-strategy profile is an ex post equilibrium in Gm, the mediator m implements an outcome function in G. This outcome function is denoted by ϕm , and it is defined as follows: ϕm (v) = (s(mN (v)), q(mN (v)).
Hence, the range of the function ϕm is the Cartesian product of the set of allocations with Rn +.
FUNCTION BY MEDIATION In general, except for the VCG position auction we do not expect position auctions to possess an ex post equilibrium.7 Therefore, the behavior of the participants in most position auctions cannot be analytically predicted, and in practice it can form a non-efficient allocation: an allocation that does not maximize social surplus. In contrast, in the VCG position auction the truth-reporting strategy is a dominant strategy for every player, and the resulting allocation is efficient. Given a position auction G our goal is to construct a mediator that would implement the outcome function of the VCG position auction. This outcome function is defined as follows: ϕvcg (v) = (s(v), qvcg (v)).
Definition: Let G be a position auction . Let m be a mediator for G. We say the m implements the VCG outcome function in G, or that it implements ϕvcg in G if the Tstrategy profile is an ex post equilibrium in Gm, and ϕm = ϕvcg .
We demonstrate our definitions so far by a simple example: Example 1. Consider a self-price auction G = G(α, p) with 2 players and one position, with α1 = 1. That is, G is a standard two-person first-price auction. The corresponding VCG position auction is a standard second-price auction. We define a family of mediators mc , c ≥ 1, each of them implements the VCG position auction. Assume both 7 Actually, it can be shown that if a strategy profile b in a position auction is an ex post equilibrium then for every player i bi is a dominant strategy. It is commonly conjectured that except for some extremely artificial combinatorial auctions , the VCG combinatorial auctions are the only ones with dominant strategies (see [10]). 282 players use the mediator"s services and send him the types ˆv = (ˆv1, ˆv2), then all mediators act similarly and as follows: If ˆv1 ≥ ˆv2 the mediator makes the following bids on behalf of the players: b1 = ˆv2, and b2 = 0. If ˆv2 > ˆv1, the mediator makes the bids b1 = 0, b2 = ˆv1. If only one player uses the mediator services, say player i, then mediator mc bids bi = cˆvi on behalf of i. We claim that for every c > 1, the T-strategy profile is an ex post equilibrium in the mediated game generated by mc . Indeed, assume player 2 reports his type v2 to the mediator, and consider player 1.
If v1 ≥ v2 then by using the T-strategy player 1 receives the position and pays v2. Hence, 1"s utility is v1 −v2. If player 1 deviates by using the mediator"s services and reporting ˆv1 ≥ v2 his utility is still v1 − v2. If he reports ˆv1 < v2 his utility will be 0. If player 1 does not use the mediator, he should bid at least cv2 in order to get the positions, and therefore his utility cannot exceed v1 − v2.
If v1 < v2, then the T-strategy yields 0 to player 1, and any other strategy yields a non-positive utility.
Obviously each of the mediators mc implements the VCG outcome function. Note however, that the T-strategy is not a dominant strategy when c > 1; e.g. if v1 > v2 and player 2 bids directly v2 (without using the mediator services), then bidding directly v1 is better for player 1 than using the Tstrategy: in the former case player 1"s utility is 0 and in the latter case her utility is negative.
It is interesting to note that this simple example can not be extended to general self-price position auctions, as will be discussed in section 4.
While each of the mediators mc in Example 1 implements the VCG outcome function, the mediator with c = 1 has a distinct characteristic: a player who uses the T-strategy cannot get a negative utility. In contrast, for every c > 1, if say player 2 does not use the mediator services, participates directly and bids less than cv1, then the T-strategy yields a negative utility of (1 − c)v1 to player 1. This motivates our definition of valid mediators: Let G be a position auction. A mediator for G is valid, if for every player, using the T-strategy guarantees a nonnegative level of utility.
Formally, a mediator m for G, is valid if for every subset S ⊆ N and every player i ∈ S wi(vi, mS(vS), b−S) ≥ 0 for every b−S ∈ B−S and every vs ∈ VS.
AUCTIONS We now show that there exists a valid mediator, which implements the VCG outcome function in next-price position auctions. Although in the following section we prove a more general result, we present this result first, given the importance of next-price position auctions in the literature and in practice. Our proof makes use of the following technical lemma.
Lemma 1. Let pvcg be the VCG payment scheme.
j (b) ≤ b(j+1) for every j ∈ K.
j (b) ≥ pvcg j+1(b) for every j = 1, ..., m − 1 and for every b ∈ B, where for every j, equality holds if and only if b(j+1) = b(j+2) = · · · = b(m+1).
The proof of Lemma 1 is given in the full version. We can now show: Theorem 2. Let G be a next-price position auction. There exists a valid mediator that implements ϕvcg in G.
Theorem 2 follows from a more general theorem given in the next section. However, we still provide a proof since a more simple and intuitive mediator is constructed for this case.
Proof of Theorem 2. We define a mediator m, which will implement the VCG outcome function in G: For every v ∈ V let mN (v) = b(v), where b(v) is defined as follows: For every player i such that 2 ≤ s(v, i) ≤ m let bi(v) = pvcg s(v,i)−1(v).8 For every i ∈ δ(v, m + 1), bi(v) = pvcg m (v).
Let bδ(v,1)(v) = 1 + max{i:s(v,i)≥2}bi(v).
For every S ⊆ N such that S = N and for every vS ∈ VS let mS(v) = vS. This completes the description of the mediator m.
We show that ϕm (v) = ϕvcg (v) for every v ∈ V . Let v ∈ V be an arbitrary valuation vector.
We have to show that s(b(v)) = s(v) and that q(b(v)) = qvcg (v): We begin by showing that s(b(v)) = s(v). It is sufficient to show that whenever 1 ≤ s(v, i) < s(v, l) ≤ m + 1 for some i = l, then s(b(v), i) < s(b(v), l).
We first show it for s(v, i) = 1, that is δ(v, 1) = i. In this case bδ(v,1)(v) > bj(v) for every j = i, s(b(v), i) = 1.
Therefore s(b(v), i) < s(b(v), l). If s(v, i) > 1, we distinguish between two cases.
implies that i < l. By the second part of Lemma 1, pvcg s(v,i)−1(v) ≥ pvcg s(v,l)−1(v). Therefore bi(v) ≥ bl(v), which yields s(b(v), i) < s(b(v), l).
therefore by the second part of Lemma 1, pvcg s(v,i)−1(v) > pvcg s(v,i)(v). Since s(v, i) ≤ s(v, l) − 1, by the second part of Lemma 1, pvcg s(v,i)(v) ≥ pvcg s(v,l)−1(v).
Therefore pvcg s(v,i)−1(v) > pvcg s(v,l)−1(v), which yields bi(v) > bl(v). Therefore s(b(v), i) < s(b(v), l).
This completes the proof that s(b(v)) = s(v) for all v ∈ V .
Observe that for every player i such that s(b(v), i) ∈ K ps(b(v),i)(b(v)) = pvcg s(v,i)(v).
Therefore qi(b(v)) = qvcg i (v) for every i ∈ N. This shows that q(b(v)) = qvcg (v) for all v ∈ V . Hence, ϕm = ϕvcg .
We proceed to prove that the T-strategy is an ex-post equilibrium. Note that by the truthfulness of VCG, it is not beneficial for any player i to miss report her value to the mediator, given that all other players use the T-strategy.
Next we show that it is not beneficial for a single player i ∈ N to participate in the auction directly if all other players use the T-strategy. Fix some v ∈ V . Assume that player i is the only player that participates directly in the auction.
Hence, v−i is the vector of bids submitted by the mediator.
Let bi be player i"s bid. Let k = s(v, i). Therefore, since ϕm = ϕvcg , s(b(v), i) = k. Let j be player i"s position in the deviation. Hence j = s((v−i, bi), i). If j /∈ K then player i"s 8 Recall that s(b, i) denotes the position of player i under the bid profile b, and δ(b, j) denotes the set of players assigned to position j. Whenever j ∈ K, we slightly abuse notations, and also refer to δ(b, j) as the player that is assigned to position j. 283 utility is zero and therefore deviating is not worthwhile for i. Suppose j ∈ K. Then αk(vi − pk(b(v))) = αk(vi − pvcg k (v)) ≥ αj(vi − pvcg j (v−i, bi)) ≥ αj(vi − v(j+1)), where the first equality follows from ϕm = ϕvcg , the first inequality follows since VCG is truthful, and the second inequality follows from the first part of Lemma 1. Since pj is position j"s payment function in the next-price position auction, αj(vi − v(j+1)) = αj(vi − pj(v−i, bi)). Therefore αk(vi − pk(b(v))) ≥ αj(vi − pj(v−i, bi)).
Hence, player i does not gain from participating directly in the auction.
Finally we show that m is valid. If all players choose the mediator then by the first part of Lemma 1 each player which uses the T-strategy will not pay more than his value.
Consider the situation in which a subset of players, S, participate directly in the auction. Since the mediator submits the reported values on behalf of the other players, these other players will not pay more than their reported values.
Hence a player which used the T-strategy will not pay more than his value. 2
NEXTPRICE POSITION AUCTIONS In the previous section we discussed the implementation of the VCG outcome function in the next price position auction. In this section we deal with a more general family of position auctions, in which the payment of each player who has been assigned a position, is a function of the bids of players assigned to lower positions than his own. The payment scheme p of such a position auction satisfies the following condition: N1: For every j ∈ K and every b1 , b2 ∈ B such that b1 (l) = b2 (l) for every l > j, we have that pj(b1 ) = pj(b2 ).
We next provide sufficient conditions for implementing the VCG outcome function by a valid mediator in position auctions whose payment schemes satisfy N1.
We need the following notation and definition. For every position auction G and every b ∈ B let ϕG (b) = (s(b), q(b)).
We say that G is a V CG cover if for every v ∈ V there exists b ∈ B such that ϕG (b) = ϕvcg (v).
We say that G is monotone if pj(b) ≥ pj(b ) for every j ∈ K and for every b ≥ b , where b ≥ b if and only if bi ≥ bi for every i ∈ N.
We are now able to show: Theorem 3. Let G = G(α, p) be a position auction such that p satisfies N1. If the following conditions hold then there exists a valid mediator that implements ϕvcg in G:
The proof of Theorem 3 is given in the full version. We next provide the construction of the valid mediator, which will implement the VCG outcome function in a position auction G, which satisfies the conditions of Theorem 3: Algorithm for building m for G: • For every v ∈ V let mN (v) = b(v), where b(v) is some bid profile such that ϕG (b(v)) = ϕvcg (v) • For every i and for every v−i ∈ V−i, let vi = (v−i, M(v−i)), where M(v−i) = 1 + maxj=ivj. • For every i ∈ N and every v−i ∈ V−i, let mN\{i}(v−i) = b−i(vi ), where b(vi ) is some bid profile such that ϕG (b(vi )) = ϕvcg (vi ). • For every S ⊆ N, such that 1 ≤ |S| ≤ n − 2, let mS(vS) = vS.
Remark: As we wrote, Theorem 3 applies in particular to next-price position auctions discussed in Section 4.
However, this Theorem applies to many other interesting position auctions as will be shown later. Moreover, the mediator constructed for this general case is different from the one in the proof of Theorem 2.
We now show that condition N1 as well as the requirement that G is a V CG cover, and the requirement that G is monotone are all necessary for establishing our result. It is easy to see that if G is not a V CG cover then Theorem 3 does not hold. The following example shows the necessity of the monotonicity condition.
Example 4. Let G = G(α, p) be the following position auction. Let n = 4, m = 3, α = (100, 10, 1), p1(b) = b(2) − b(3) and p2(b) = b(3)+b(4) 2 , and p3(b) = b(4). Notice that G is not monotone. Observe that condition N1 is satisfied. In the full version we show that G is a V CG cover, and it is not possible to implement the VCG outcome function in G with a valid mediator.
The next example shows that Theorem 3 does not hold, when condition N1 is not satisfied.
Example 5. Let G = G(α, p) be the following position auction. Let N = {1, 2, 3}, K = {1, 2} and α = (2, 1). Let p1(b) = b(1) 4 and p2(b) = b(2). It is immediate to see that the monotonicity condition is satisfied. We next show that G is a V CG cover. Let v ∈ V be an arbitrary valuation vector.
We need to find a bid profile b(v) such that ϕG (b(v)) = ϕvcg (v). Note that pvcg 1 (v) = v(2)+v(3) 2 and pvcg 2 (v) = v(3).
We define the bid profile b(v) as follows.
Let bδ(v,3)(v) = v(3) 2 , bδ(v,2)(v) = v(3) and bδ(v,1)(v) = 2v2 + 2v(3).
By the construction of b(v), s(b(v), i) = s(v, i) for i = 1, 2, 3 . In addition observe that pj(b(v)) = pvcg j (v) for j = 1, 2, 3, 4. Therefore ϕG (b(v)) = ϕvcg(v). Since v is arbitrary, G is a V CG cover.
Naturally N1 is not satisfied. Suppose in negation that there exists a valid mediator m, which implements the VCG outcome function in G. Consider the following vector of valuations v = (12, 10, 8). If all players use the mediator then player 2 (with valuation 10) gets position 2, pays 8, and therefore her utility is 1(10−8) = 2. Player 2 can always bid more than the other players, and by that cause some other player to be positioned second; Since the mediator is required to be valid it must be that the mediator submits not more than 12 on behalf of both players 1 and 3. But then player 2 can bid 13, and win the first position; therefore, player 2"s utility will be 2(10 − 13 4 ) > 8. This contradicts that m is a valid mediator that implements the VCG outcome function in G.
To summarize, we have shown sufficient conditions for transforming a large class of position auctions to the V CG 284 position auction by mediation. Moreover by dropping any of our conditions we get that such transformation might not be feasible.
In the next subsections we provide classes of interesting position auctions which can be transformed to the VCG position auction by mediation. These auctions satisfy the conditions of Theorem 3. However, in order to use Theorem 3 one has to check that a certain position auction, G is a VCG cover. In the full version paper, before we apply this theorem we present another useful theorem that gives sufficient conditions guaranteeing that G is a VCG cover.
In a generalized next-price auction the payment scheme is of the following form. For every j ∈ K and for every b ∈ B pj(b) = b(l(j)) where l(j) is an integer such that l(j) > j.9 .
We show: Proposition 1. Let G be a generalized next-price position auction. There exists a valid mediator that implements ϕvcg in G if and only if the following two conditions hold: (i) l(j + 1) > l(j) for j = 1, ..., m − 1, and (ii) l(m) ≤ n.
In k-next-price position auctions the payment scheme is defined as follows: For every j ∈ K and for every b pj(b) = b(j+k). K-next-price position auctions are, in particular generalized next-price position auctions. Therefore Proposition 1 yields as a corollary: Proposition 2. Let k ≥ 1. Let G be a k-next-price position auction. There exists a valid mediator that implements ϕvcg in G if and only if n ≥ m + k − 1.
In weighted next-price position auctions the payment schemes are of the following form. For every j ∈ K and for every b ∈ B, pj(b) = b(j+1) cj , where cj ≥ 1.
Proposition 3. Let G be a weighted next-price position auction with the weights c1, c2, ..., cm. There exists a valid mediator that implements ϕvcg in G if and only if c1 ≥ · · · ≥ cm.
Google-like ad auctions are slightly different from nextprice auction. In these auctions the click-trough rate of an ad i in position j is the product of the quality of ad i, βi > 0, and the position click-trough rate αj > 0.10 Players are ranked in the positions by biβi.
Let b ∈ B. Let ˜δ(b, j) be defined as follows. For every j ∈ K, let ˜δ(b, j) be the player i that obtains position j, and for j = m let ˜δ(b, j) = i, where i obtained position m in case there is more than one player i such that bi = b(m), then i is chosen between them via the breaking rule ˜γ.
If player i obtains position j ∈ K then she pays pj(b) = β˜δ(b,j+1) b˜δ(b,j+1) βi . Therefore player i"s utility will be αjβi(vi − b˜δ(b,j+1) βi β˜δ(b,j+1)) = αj(viβi − b˜δ(b,j+1)β˜δ(b,j+1)). 9 Recall that b(j) = 0 for every j > n 10 See e.g. [17].
Hence by denoting ˜vi = viβi for every i ∈ N, and by applying Theorem 2 we obtain: Proposition 4. There exists a valid mediator which implements the VCG outcome function in the Google-like position auction.
Let G be a self-price position auction as described at section 2. At example 1 we showed that when there is one position and two players, the VCG outcome function is implemented by a valid mediator in this auction. The proof in this example can be easily generalized to show that the VCG outcome function can be implemented by a valid mediator in a self-price position auction, in which there is one position and an arbitrary number of players, n ≥ 2.
Next we show that it is impossible to implement the VCG outcome function, even by a non-valid mediator, in a selfprice position auction which has more than one position (m > 1).
Theorem 6. Let G be a self-price position auction with more than one position. There is no mediator that implements the VCG outcome function in G.
Proof. Let v ∈ V be the following valuation profile. vn = 10 and v1 = v2 = · · · = vn−1 = 5. The VCG outcome function assigns to this v an allocation, in which player n receives position 1 and player 1 receives position 2. The payments of players n and 1 are both equal to 5. In order to implement such an outcome, a mediator must bid 5 on behalf of player n (so that this player pays 5), and it must bid less than 5 on behalf of any other player, because otherwise another player receives position 1. Note that the bid of any other player cannot equal 5 because every other player has an higher priority than n. In particular, even if player 1 gets indeed position 2 he will pay less than 5. Hence, no mediator can implement the VCG outcome function in G.2 The proof of Theorem 6 heavily uses the fixed priority rule assumption. However, as we have already said, all our results including this theorem hold also for the tie breaking rule defined by the random priority rule. The proof of the impossibility theorem for the random priority rule uses the fact that the particular bad priority rule used in the proof of Theorem 6, has a positive probability.
As we previously discussed, the fixed and random priority rules are just convenient ways to model the first-arrival rule, which is common in practice. When one attempts to directly model position auctions that use the first-arrival rule without these modeling choices he tackles a lot of modeling problems. In particular, it is not clear how to model a position auction with the first-arrival rule as a game with incomplete information. To do this, one has to allow a player not only to submit a bid but also to decide about the time of the bid. This raises a lot of additional modeling problems, such as determining the relationship between the time a player decides to submit a bid and the time in which this bid is actually recorded. Hence, efficient modeling as a game may be untractable. Nevertheless, in the next section we will analyze mediators in position auctions, which use the first-arrival rule. We will define ex post equilibrium and the notion of implementation by mediation without explicitly modeling well-defined games. We will show that in this case 285 there is a way to implement the VCG outcome function in a self-price position auction. Moreover, we will find a valid mediator that does the job.
ARRIVAL RULE Let G be a position auction with the first-arrival rule.
Every mediator for G has the ability to determine the order in which the bids he submits on behalf of the players are recorded; He can just submit the bids sequentially, waiting for a confirmation before submitting the next bid. We need the following notations.
Every order of bidding can be described by some γ ∈ Γ; i bids before k if and only if γi < γk. Hence, an order of bids can serve as a priority rule. For every order of bids γ and a vector of bids b we define s(b, γ, i) as the position assigned to i. We denote the payment of i when the vector of bids is b and the order of bidding is γ by qi(b, γ) = ps(b,γ,i)(b), and we denote wi(vi, b, γ) the utility of i.
A mediator for G should determine the bids of the players who use its services and also the order of bids as a function of the reported types. However, all mediators discussed in this paper will use the same rule to determine the order of bids: If all players report the vector of types ˆv, the mediator uses the order of bids γˆv , which is defined as follows: γˆv i < γˆv k if and only if ˆvi > ˆvk, or ˆvi = ˆvk and i < k. For example, if n = 3 and the reported types are ˆv = (6, 7, 6), then γˆv = (2, 1, 3). If only a strict subset of the players use the mediator"s services, the mediator applies the same order of bids rule to this subset. A mediator for a position auction with the first arrival rule is therefore defined by a vector m = (mS)S⊆N . However, such a mediator is called a directed mediator in order to stress the fact that it determines not only the bids but also the order of bids. To summarize: If all players use the directed mediator m, and the reported bids are ˆv, then the directed mediator bids mN (ˆv)i on behalf of i, i receives the position s(ˆv, γˆv , i), and pays qi(mN (ˆv), γˆv ). If only the subset S uses the mediator"s services, the reported types are ˆvS, and the other players bid directly b−S then the actual order of bids is not uniquely determined. If this order is γ then the position of i ∈ N is s(b, γ, i), and its payment is qi(b, γ), where b = (mS(ˆvS), b−S). In particular, if every player is using the T-strategy and the players" profile of types is v, then the outcome generated by the directed mediator is ψm (v) = (s(v, γv ), q(mN (v), γv ).
But why should the players use the T strategy? Assume all players but i use the T strategy. If player i deviates from the T strategy by reporting a false type to the directed mediator, the resulting outcome is well-defined. On the other hand, when this player sends a bid directly to the auctioneer, the resulting outcome is not clear, because the order of bids is not clear.11 A good desired directed mediator would be one that no player would want to deviate from the T strategy independently of the order in which the bids are recorded because of his deviation. More specifically: Definition: Let G be a position auction with the firstarrival rule, and let m be a directed mediator for G. The 11 It is clear however, that the resulting order γ is consistent with the well-defined order of bids of N \ i.
T-strategy profile is an ex post equilibrium with respect to m if for every player i and type vi, and for every vector of types of the other players, v−i, the following two conditions hold: F1: i is not better off when he gives the directed mediator the right of play and reports a false type. That is, for every ˆvi ∈ Vi wi(vi, mN (vi, v−i), γ(vi,v−i) ) ≥ wi(vi, mN (ˆvi, v−i), γ(ˆvi,v−i) ).
F2: i is not better off when he bids directly independently of the resulting order of recorded bids. That is for every bi ∈ Bi, and for every γ ∈ Γ, which is consistent with the order of bids of members of N \ i resulting from the vector of types v−i, wi(vi, mN (vi, v−i), γ(vi,v−i) ) ≥ wi(vi, (bi, mN\i(v−i)), γ).
The notion of valid directed mediators is analogously defined: Definition: Let G be a position auction with the firstarrival rule. A directed mediator for G is valid, if for every player, using the T-strategy guarantees a non-negative level of utility.
Formally, a directed mediator m for G is valid, if for every player i with type vi, for every subset S ⊆ N such that i ∈ S, for every vS\i, and for every b−S, wi(vi, mS(vS), b−S, γ) ≥ 0 for every γ ∈ Γ, which is consistent with the standard order of bids of S determined by the mediator when the reported types are vS.
The notion of implementation by mediation remains as before: The directed mediator m implements the VCG outcome function in G if ψm = ϕvcg .
Our previous results remain true for directed mediators for position auctions with the first arrival rule. Next we show that in contrast to Theorem 6, it is possible to implement the VCG outcome function in every self-price position auction with the first-arrival rule.
Theorem 7. Let G = G(α, p) be the self-price position auction with the first arrival rule. There exists a valid directed mediator that implements the VCG outcome function in G.
In the following theorem we provide sufficient conditions for implementing that the VCG outcome function in a position auction with the first-arrival rule. A special characteristic of auctions satisfying these sufficient conditions is that players" payments may depend also on their own bid, in contrast to the auctions discussed in Theorem 3. The long proof of this theorem is in the spirit of all previous proofs, and therefore it is omitted.
Theorem 8. Let G = G(α, p) be a position auction with the first-arrival rule. If the following conditions hold then there exists a valid directed mediator for G that implements the VCG outcome function in G.
v(j) for every j ∈ K.
, b2 ∈ B such that b1 (l) = b2 (l) for every l ≥ j, pj(b1 ) = pj(b2 ). 286
[1] R.J. Aumann. Subjectivity and correlation in randomized strategies. Journal of Mathematical Economics, 1:67-96, 1974. [2] N.A.R. Bhat, K. Leyton-Brown, Y. Shoham, and M. Tennenholtz. Bidding Rings Revisited. Working Paper, 2005. [3] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. NBER working paper 11765, Novenmber 2005. [4] J. Feng, H.K. Bhargava, and D.M. Pennock.
Implementing sponsored search in web search engines: Computational evaluation of alternative mechanisms.
INFORMS Journal on Computing, 2006. [5] F. M. Forges. An approach to communication equilibria. Econometrica, 54(6):1375-85, 1986. [6] D. Graham and R. Marshall. Collusive Bidder Behavior at Single-Object Second-Price and English Auctions. Journal of Political Economy, 95:1217-1239,
[7] R. Holzman, N. Kfir-Dahav, D. Monderer, and M. Tennenholtz. Bundling equilibrium in combinatorial auctions. Games and Economic Behavior, 47:104-123, 2004. [8] E. Kalai and R.W. Rosenthal. Arbitration of Two-Party Disputes under Ignorance. International Journal of Game Theory, 7:65-72, 1976. [9] S. Lahaie. An analysis of alternative slot auction designs for sponsored search. In Proceedings of the 7th ACM conference on Electronic commerce, pages 218-227, 2006. [10] R. Lavi, A. Mu"alem, and N. Nisan. Towards a characterization of truthful combinatorial auctions. In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS)., 2003. [11] R. McAfee and J. McMillan. Bidding Rings. American Economic Review, 82:579-599, 1992. [12] A. Mehta, A. Saberi, , V. Vazirani, and U. Vazirani.
Adwords and Generalized Online Matching. In Twentieth International joint conference on Artificial Intelligence (FOCS 05) , 2005. [13] D. Monderer and M. Tennenholtz. K-Implementation.
Journal of Artificial Intelligence Research (JAIR), 21:37-62, 2004. [14] D. Monderer and M. Tennenholtz. Strong mediated equilibrium. In Proceedings of the AAAI, 2006. [15] R. B. Myerson. Multistage games with communication. Econometrica, 54(2):323-58, 1986. [16] O. Rozenfeld and M. Tennenholtz. Routing mediators.

Many important problems in computer science and electronic commerce can be modeled as resource allocation problems. In such problems, we want to allocate the resources (or items) to the agents that value them the most.
Unfortunately, agents" valuations are private knowledge, and self-interested agents will lie about their valuations if this is to their benefit. One solution is to auction off the items, possibly in a combinatorial auction where agents can bid on bundles of items. There exist ways of determining the payments that the agents make in such an auction that incentivizes the agents to report their true valuations-that is, the payments make the auction strategy-proof. One very general way of doing so is to use the VCG mechanism [23, 4, 12]. (The VCG mechanism is also known as the Clarke mechanism or, in the specific context of auctions, the Generalized Vickrey Auction.) Besides strategy-proofness, the VCG mechanism has several other nice properties in the context of resource allocation problems. It is efficient: the chosen allocation always maximizes the sum of the agents" valuations. It is also (expost) individually rational: participating in the mechanism never makes an agent worse off than not participating.
Finally, it has a no-deficit property: the sum of the agents" payments is always nonnegative.
In many settings, another property that would be desirable is (strong) budget balance, meaning that the payments sum to exactly 0. Suppose the agents are trying to distribute some resources among themselves that do not have a previous owner. For example, the agents may be trying to allocate the right to use a shared good on a given day.
Or, the agents may be trying to allocate a resource that they have collectively constructed, discovered, or otherwise obtained. If the agents use an auction to allocate these resources, and the sum of the agents" payments in the auction is positive, then this surplus payment must leave the system 30 of the agents (for example, the agents must give the money to an outside party, or burn it). Na¨ıve redistribution of the surplus payment (e.g. each of the n agents receives 1/n of the surplus) will generally result in a mechanism that is not strategy-proof (e.g. in a Vickrey auction, the second-highest bidder would want to increase her bid to obtain a larger redistribution payment). Unfortunately, the VCG mechanism is not budget balanced: typically, there is surplus payment.
Unfortunately, in general settings, it is in fact impossible to design mechanisms that satisfy budget balance in addition to the other desirable properties [16, 11, 21].
In light of this impossibility result, several authors have obtained budget balance by sacrificing some of the other desirable properties [2, 6, 22, 5]. Another approach that is perhaps preferable is to use a mechanism that is more budget balanced than the VCG mechanism, and maintains all the other desirable properties. One way of trying to design such a mechanism is to redistribute some of the VCG payment back to the agents in a way that will not affect the agents" incentives (so that strategy-proofness is maintained), and that will maintain the other properties. In 2006,
Cavallo [3] pursued exactly this idea, and designed a mechanism that redistributes a large amount of the total VCG payment while maintaining all of the other desirable properties of the VCG mechanism. For example, in a single-item auction (where the VCG mechanism coincides with the second-price sealed-bid auction), the amount redistributed to bidder i by Cavallo"s mechanism is 1/n times the second-highest bid among bids other than i"s bid. The total redistributed is at most the second-highest bid overall, and the redistribution to agent i does not affect i"s incentives because it does not depend on i"s own bid.
In this paper, we restrict our attention to a limited setting, and in this setting we extend Cavallo"s result. We study allocation settings where there are multiple indistinguishable units of a single good, and all agents have unit demand, i.e. they want only a single unit. For this specific setting, Cavallo"s mechanism coincides with a mechanism proposed by Bailey in 1997 [2]. Here we propose the family of linear VCG redistribution mechanisms. All mechanisms in this family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case (with the caveat that we only study allocation settings with multiple indistinguishable units of a single good and unit demand, while Bailey"s and Cavallo"s mechanisms can be applied outside these settings as well). We then provide an optimization model for finding the optimal mechanism inside the family, based on worst-case analysis. Both numerical and analytical solutions of this model are provided, and the resulting mechanism shows significant improvement over the BaileyCavallo mechanism (in the worst case). For example, for the problem of allocating a single unit, when the number of agents is 10, our mechanism always redistributes more than 98% of the total VCG payment back to the agents (whereas the Bailey-Cavallo mechanism redistributes only 80% in the worst case). Finally, we prove that our mechanism is in fact optimal among all anonymous deterministic mechanisms (even nonlinear ones) that satisfy the desirable properties.
Around the same time, the same mechanism has been independently derived by Moulin [19].1 Moulin actually pursues a different objective (also based on worst-case analysis): whereas our objective is to maximize the percentage of VCG payments that are redistributed, Moulin tries to minimize the overall payments from agents as a percentage of efficiency. It turns out that the resulting mechanisms are the same. Towards the end of this paper, we consider dropping the individual rationality requirement, and show that this does not change the optimal mechanism for our objective.
For Moulin"s objective, dropping individual rationality does change the optimal mechanism (but only if there are multiple units).
Let n denote the number of agents, and let m denote the number of units. We only consider the case where m < n (otherwise the problem becomes trivial). We also assume that m and n are always known. (This assumption is not harmful: in environments where anyone can join the auction, running a redistribution mechanism is typically not a good idea anyway, because everyone would want to join to collect part of the redistribution.) Let the set of agents be {a1, a2, . . . , an}, where ai is the agent with ith highest report value ˆvi-that is, we have ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn ≥ 0. Let vi denote the true value of ai.
Given that the mechanism is strategy-proof, we can assume vi = ˆvi.
Under the VCG mechanism, each agent among a1, . . . , am wins a unit, and pays ˆvm+1 for this unit. Thus, the total VCG payment equals mˆvm+1. When m = 1, this is the second-price or Vickrey auction.
We modify the mechanism as follows. After running the original VCG mechanism, the center returns to each agent ai some amount zi, agent ai"s redistribution payment. We do not allow zi to depend on ˆvi; because of this, ai"s incentives are unaffected by this redistribution payment, and the mechanism remains strategy-proof.
MECHANISMS We are now ready to introduce the family of linear VCG redistribution mechanisms. Such a mechanism is defined by a vector of constants c0, c1, . . . , cn−1. The amount that the mechanism returns to agent ai is zi = c0 + c1ˆv1 + c2ˆv2 + . . . + ci−1ˆvi−1 + ciˆvi+1 + . . . + cn−1ˆvn. That is, an agent receives c0, plus c1 times the highest bid other than the agent"s own bid, plus c2 times the second-highest other bid, etc. The mechanism is strategy-proof, because for all i, zi is independent of ˆvi. Also, the mechanism is anonymous.
It is helpful to see the entire list of redistribution payments: z1 = c0 + c1ˆv2 + c2ˆv3 + c3ˆv4 + . . . + cn−2ˆvn−1 + cn−1ˆvn z2 = c0 + c1ˆv1 + c2ˆv3 + c3ˆv4 + . . . + cn−2ˆvn−1 + cn−1ˆvn z3 = c0 + c1ˆv1 + c2ˆv2 + c3ˆv4 + . . . + cn−2ˆvn−1 + cn−1ˆvn z4 = c0 + c1ˆv1 + c2ˆv2 + c3ˆv3 + . . . + cn−2ˆvn−1 + cn−1ˆvn . . . zi = c0 + c1ˆv1 + c2ˆv2 + . . . + ci−1ˆvi−1 + ciˆvi+1 + . . . + cn−1ˆvn . . . zn−2 = c0 + c1ˆv1 + c2ˆv2 + c3ˆv3 + . . . + cn−2ˆvn−1 + cn−1ˆvn zn−1 = c0 + c1ˆv1 + c2ˆv2 + c3ˆv3 + . . . + cn−2ˆvn−2 + cn−1ˆvn zn = c0 + c1ˆv1 + c2ˆv2 + c3ˆv3 + . . . + cn−2ˆvn−2 + cn−1ˆvn−1 1 We thank Rakesh Vohra for pointing us to Moulin"s working paper. 31 Not all choices of the constants c0, . . . , cn−1 produce a mechanism that is individually rational, and not all choices of the constants produce a mechanism that never incurs a deficit. Hence, to obtain these properties, we need to place some constraints on the constants.
To satisfy the individual rationality criterion, each agent"s utility should always be non-negative. An agent that does not win a unit obtains a utility that is equal to the agent"s redistribution payment. An agent that wins a unit obtains a utility that is equal to the agent"s valuation for the unit, minus the VCG payment ˆvm+1, plus the agent"s redistribution payment.
Consider agent an, the agent with the lowest bid. Since this agent does not win an item (m < n), her utility is just her redistribution payment zn. Hence, for the mechanism to be individually rational, the ci must be such that zn is always nonnegative. If the ci have this property, then it actually follows that zi is nonnegative for every i, for the following reason. Suppose there exists some i < n and some vector of bids ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn ≥ 0 such that zi < 0.
Then, consider the bid vector that results from replacing ˆvj by ˆvj+1 for all j ≥ i, and letting ˆvn = 0. If we omit ˆvn from this vector, the same vector results that results from omitting ˆvi from the original vector. Therefore, an"s redistribution payment under the new vector should be the same as ai"s redistribution payment under the old vector-but this payment is negative.
If all redistribution payments are always nonnegative, then the mechanism must be individually rational (because the VCG mechanism is individually rational, and the redistribution payment only increases an agent"s utility). Therefore, the mechanism is individually rational if and only if for any bid vector, zn ≥ 0.
To satisfy the non-deficit criterion, the sum of the redistribution payments should be less than or equal to the total VCG payment. So for any bid vector ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn ≥ 0, the constants ci should make z1 + z2 + . . . + zn ≤ mˆvm+1.
We define the family of linear VCG redistribution mechanisms to be the set of all redistribution mechanisms corresponding to constants ci that satisfy the above constraints (so that the mechanisms will be individually rational and have the no-deficit property). We now give two examples of mechanisms in this family.
Example 1 (Bailey-Cavallo mechanism): Consider the mechanism corresponding to cm+1 = m n and ci = 0 for all other i. Under this mechanism, each agent receives a redistribution payment of m n times the (m+1)th highest bid from another agent. Hence, a1, . . . , am+1 receive a redistribution payment of m n ˆvm+2, and the others receive m n ˆvm+1. Thus, the total redistribution payment is (m+1)m n ˆvm+2 +(n−m− 1)m n ˆvm+1. This redistribution mechanism is individually rational, because all the redistribution payments are nonnegative, and never incurs a deficit, because (m + 1) m n ˆvm+2 + (n−m−1)m n ˆvm+1 ≤ nm n ˆvm+1 = mˆvm+1. (We note that for this mechanism to make sense, we need n ≥ m + 2.) Example 2: Consider the mechanism corresponding to cm+1 = m n−m−1 , cm+2 = − m(m+1) (n−m−1)(n−m−2) , and ci = 0 for all other i. In this mechanism, each agent receives a redistribution payment of m n−m−1 times the (m + 1)th highest reported value from other agents, minus m(m+1) (n−m−1)(n−m−2) times the (m+2)th highest reported value from other agents.
Thus, the total redistribution payment is mˆvm+1 − m(m+1)(m+2) (n−m−1)(n−m−2) ˆvm+3. If n ≥ 2m+3 (which is equivalent to m n−m−1 ≥ m(m+1) (n−m−1)(n−m−2) ), then each agent always receives a nonnegative redistribution payment, thus the mechanism is individually rational. Also, the mechanism never incurs a deficit, because the total VCG payment is mˆvm+1, which is greater than the amount mˆvm+1 − m(m+1)(m+2) (n−m−1)(n−m−2) ˆvm+3 that is redistributed.
Which of these two mechanisms is better? Is there another mechanism that is even better? This is what we study in the next section.
MECHANISMS Among all linear VCG redistribution mechanisms, we would like to be able to identify the one that redistributes the greatest percentage of the total VCG payment.2 This is not a well-defined notion: it may be that one mechanism redistributes more on some bid vectors, and another more on other bid vectors. We emphasize that we do not assume that a prior distribution over bidders" valuations is available, so we cannot compare them based on expected redistribution.
Below, we study three well-defined ways of comparing redistribution mechanisms: best-case performance, dominance, and worst-case performance.
Best-case performance. One way of evaluating a mechanism is by considering the highest redistribution percentage that it achieves. Consider the previous two examples.
For the first example, the total redistribution payment is (m + 1)m n ˆvm+2 + (n − m − 1)m n ˆvm+1. When ˆvm+2 = ˆvm+1, this is equal to the total VCG payment mˆvm+1. Thus, this mechanism redistributes 100% of the total VCG payment in the best case. For the second example, the total redistribution payment is mˆvm+1 − m(m+1)(m+2) (n−m−1)(n−m−2) ˆvm+3. When ˆvm+3 = 0, this is equal to the total VCG payment mˆvm+1.
Thus, this mechanism also redistributes 100% of the total VCG payment in the best case.
Moreover, there are actually infinitely many mechanisms that redistribute 100% of the total VCG payment in the best case-for example, any convex combination of the above two will redistribute 100% if both ˆvm+2 = ˆvm+1 and ˆvm+3 = 0.
Dominance. Inside the family of linear VCG redistribution mechanisms, we say one mechanism dominates another mechanism if the first one redistributes at least as much as the other for any bid vector. For the previous two examples, neither dominates the other, because they each redistribute 100% in different cases. It turns out that there is no mechanism in the family that dominates all other mechanisms in the family. For suppose such a mechanism exists. Then, it should dominate both examples above. Consider the remaining VCG payment (the VCG payment failed to be redistributed). The remaining VCG payment of the dominant mechanism should be 0 whenever ˆvm+2 = ˆvm+1 or ˆvm+3 = 0.
Now, the remaining VCG payment is a linear function of the ˆvi (linear redistribution), and therefore also a polynomial function. The above implies that this function can be written as (ˆvm+2 − ˆvm+1)(ˆvm+3)P(ˆv1, ˆv2, . . . , ˆvn), where P is a 2 The percentage redistributed seems the natural criterion to use, among other things because it is scale-invariant: if we multiply all bids by the same positive constant (for example, if we change the units by re-expressing the bids in euros instead of dollars), we would not want the behavior of our mechanism to change. 32 polynomial function. But since the function must be linear (has degree at most 1), it follows that P = 0. Thus, a dominant mechanism would always redistribute all of the VCG payment, which is not possible. (If it were possible, then our worst-case optimal redistribution mechanism would also always redistribute all of the VCG payment, and we will see later that it does not.) Worst-case performance. Finally, we can evaluate a mechanism by considering the lowest redistribution percentage that it guarantees. For the first example, the total redistribution payment is (m+1)m n ˆvm+2 +(n−m−1)m n ˆvm+1, which is greater than or equal to (n−m−1) m n ˆvm+1. So in the worst case, which is when ˆvm+2 = 0, the percentage redistributed is n−m−1 n . For the second example, the total redistribution payment is mˆvm+1 − m(m+1)(m+2) (n−m−1)(n−m−2) ˆvm+3, which is greater than or equal to mˆvm+1(1− (m+1)(m+2) (n−m−1)(n−m−2) ). So in the worst case, which is when ˆvm+3 = ˆvm+1, the percentage redistributed is 1 − (m+1)(m+2) (n−m−1)(n−m−2) . Since we assume that the number of agents n and the number of units m are known, we can determine which example mechanism has better worst-case performance by comparing the two quantities. When n = 6 and m = 1, for the first example (Bailey-Cavallo mechanism), the percentage redistributed in the worst case is 2 3 , and for the second example, this percentage is 1 2 , which implies that for this pair of n and m, the first mechanism has better worst-case performance. On the other hand, when n = 12 and m = 1, for the first example, the percentage redistributed in the worst case is 5 6 , and for the second example, this percentage is 14 15 , which implies that this time the second mechanism has better worst-case performance.
Thus, it seems most natural to compare mechanisms by the percentage of total VCG payment that they redistribute in the worst case. This percentage is undefined when the total VCG payment is 0. To deal with this, technically, we define the worst-case redistribution percentage as the largest k so that the total amount redistributed is at least k times the total VCG payment, for all bid vectors. (Hence, as long as the total amount redistributed is at least 0 when the total VCG payment is 0, these cases do not affect the worst-case percentage.) This corresponds to the following optimization problem: Maximize k (the percentage redistributed in the worst case) Subject to: For every bid vector ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn ≥ 0 zn ≥ 0 (individual rationality) z1 + z2 + . . . + zn ≤ mˆvm+1 (non-deficit) z1 + z2 + . . . + zn ≥ kmˆvm+1 (worst-case constraint) We recall that zi = c0 + c1ˆv1 + c2ˆv2 + . . . + ci−1ˆvi−1 + ciˆvi+1 + . . . + cn−1ˆvn.
PROGRAMMING The optimization problem given in the previous section can be rewritten as a linear program, based on the following observations.
Claim 1. If c0, c1, . . . , cn−1 satisfy both the individual rationality and the non-deficit constraints, then ci = 0 for i = 0, . . . , m.
Proof. First, let us prove that c0 = 0. Consider the bid vector in which ˆvi = 0 for all i. To obtain individual rationality, we must have c0 ≥ 0. To satisfy the non-deficit constraint, we must have c0 ≤ 0. Thus we know c0 = 0.
Now, if ci = 0 for all i, there is nothing to prove. Otherwise, let j = min{i|ci = 0}. Assume that j ≤ m. We recall that we can write the individual rationality constraint as follows: zn = c0 +c1ˆv1 +c2ˆv2 +c3ˆv3 +. . .+cn−2ˆvn−2 +cn−1ˆvn−1 ≥ 0 for any bid vector. Let us consider the bid vector in which ˆvi = 1 for i ≤ j and ˆvi = 0 for the rest. In this case zn = cj, so we must have cj ≥ 0. The non-deficit constraint can be written as follows: z1 + z2 + . . . + zn ≤ mˆvm+1 for any bid vector. Consider the same bid vector as above. We have zi = 0 for i ≤ j, because for these bids, the jth highest other bid has value 0, so all the ci that are nonzero are multiplied by 0. For i > j, we have zi = cj, because the jth highest other bid has value 1, and all lower bids have value 0. So the non-deficit constraint tells us that cj(n − j) ≤ mˆvm+1.
Because j ≤ m, ˆvm+1 = 0, so the right hand side is 0. We also have n − j > 0 because j ≤ m < n. So cj ≤ 0. Because we have already established that cj ≥ 0, it follows that cj = 0; but this is contrary to assumption. So j > m.
Incidentally, this claim also shows that if m = n − 1, then ci = 0 for all i. Thus, we are stuck with the VCG mechanism. From here on, we only consider the case where m < n − 1.
Claim 2. The individual rationality constraint can be written as follows: Pj i=m+1 ci ≥ 0 for j = m + 1, . . . , n − 1.
Before proving this claim, we introduce the following lemma.
Lemma 1. Given a positive integer k and a set of real constants s1, s2, . . . , sk, (s1t1 + s2t2 + . . . + sktk ≥ 0 for any t1 ≥ t2 ≥ . . . ≥ tk ≥ 0) if and only if ( Pj i=1 si ≥ 0 for j = 1, 2, . . . , k).
Proof. Let di = ti −ti+1 for i = 1, 2, . . . , k−1, and dk = tk. Then (s1t1 +s2t2 +. . .+sktk ≥ 0 for any t1 ≥ t2 ≥ . . . ≥ tk ≥ 0) is equivalent to (( P1 i=1 si)d1 + ( P2 i=1 si)d2 + . . . + ( Pk i=1 si)dk ≥ 0 for any set of arbitrary non-negative dj).
When Pj i=1 si ≥ 0 for j = 1, 2, . . . , k, the above inequality is obviously true. If for some j,
Pj i=1 si < 0, if we set dj > 0 and di = 0 for all i = j, then the above inequality becomes false. So Pj i=1 si ≥ 0 for j = 1, 2, . . . , k is both necessary and sufficient.
We are now ready to present the proof of Claim 2.
Proof. The individual rationality constraint can be written as zn = c0 + c1ˆv1 + c2ˆv2 + c3ˆv3 + . . . + cn−2ˆvn−2 + cn−1ˆvn−1 ≥ 0 for any bid vector ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn−1 ≥ ˆvn ≥ 0. We have already shown that ci = 0 for i ≤ m.
Thus, the above can be simplified to zn = cm+1ˆvm+1 + cm+2ˆvm+2+. . .+cn−2ˆvn−2+cn−1ˆvn−1 ≥ 0 for any bid vector.
By the above lemma, this is equivalent to Pj i=m+1 ci ≥ 0 for j = m + 1, . . . , n − 1.
Claim 3. The non-deficit constraint and the worst-case constraint can also be written as linear inequalities involving only the ci and k.
Proof. The non-deficit constraint requires that for any bid vector, z1 +z2 +. . .+zn ≤ mˆvm+1, where zi = c0 +c1ˆv1 + 33 c2ˆv2 +. . .+ci−1ˆvi−1 +ciˆvi+1 +. . .+cn−1ˆvn for i = 1, 2, . . . , n.
Because ci = 0 for i ≤ m, we can simplify this inequality to qm+1ˆvm+1 + qm+2ˆvm+2 + . . . + qnˆvn ≥ 0 qm+1 = m − (n − m − 1)cm+1 qi = −(i−1)ci−1 −(n−i)ci, for i = m+2, . . . , n−1 (when m + 2 > n − 1, this set of equalities is empty) qn = −(n − 1)cn−1 By the above lemma, this is equivalent to Pj i=m+1 qi ≥ 0 for j = m + 1, . . . , n. So, we can simplify further as follows: qm+1 ≥ 0 ⇐⇒ (n − m − 1)cm+1 ≤ m qm+1 + . . . + qm+i ≥ 0 ⇐⇒ n Pj=m+i−1 j=m+1 cj + (n − m − i)cm+i ≤ m for i = 2, . . . , n − m − 1 qm+1 + . . . + qn ≥ 0 ⇐⇒ n Pj=n−1 j=m+1 cj ≤ m So, the non-deficit constraint can be written as a set of linear inequalities involving only the ci.
The worst-case constraint can be also written as a set of linear inequalities, by the following reasoning. The worstcase constraint requires that for any bid input z1 +z2 +. . .+ zn ≥ kmˆvm+1, where zi = c0 +c1ˆv1 +c2ˆv2 +. . .+ci−1ˆvi−1 + ciˆvi+1 + . . . + cn−1ˆvn for i = 1, 2, . . . , n. Because ci = 0 for i ≤ m, we can simplify this inequality to Qm+1ˆvm+1 + Qm+2ˆvm+2 + . . . + Qnˆvn ≥ 0 Qm+1 = (n − m − 1)cm+1 − km Qi = (i − 1)ci−1 + (n − i)ci, for i = m + 2, . . . , n − 1 Qn = (n − 1)cn−1 By the above lemma, this is equivalent to Pj i=m+1 Qi ≥ 0 for j = m + 1, . . . , n. So, we can simplify further as follows: Qm+1 ≥ 0 ⇐⇒ (n − m − 1)cm+1 ≥ km Qm+1 + . . . + Qm+i ≥ 0 ⇐⇒ n Pj=m+i−1 j=m+1 cj + (n − m − i)cm+i ≥ km for i = 2, . . . , n − m − 1 Qm+1 + . . . + Qn ≥ 0 ⇐⇒ n Pj=n−1 j=m+1 cj ≥ km So, the worst-case constraint can also be written as a set of linear inequalities involving only the ci and k.
Combining all the claims, we see that the original optimization problem can be transformed into the following linear program.
Variables: cm+1, cm+2, . . . , cn−1, k Maximize k (the percentage redistributed in the worst case) Subject to:Pj i=m+1 ci ≥ 0 for j = m + 1, . . . , n − 1 km ≤ (n − m − 1)cm+1 ≤ m km ≤ n Pj=m+i−1 j=m+1 cj + (n − m − i)cm+i ≤ m for i = 2, . . . , n − m − 1 km ≤ n Pj=n−1 j=m+1 cj ≤ m
For selected values of n and m, we solved the linear program using Glpk (GNU Linear Programming Kit). In the table below, we present the results for a single unit (m = 1).
We present 1−k (the percentage of the total VCG payment that is not redistributed by the worst-case optimal mechanism in the worst case) instead of k in the second column because writing k would require too many significant digits.
Correspondingly, the third column displays the percentage 5 10 15 20 25 30
1 Number of AgentsWorst−caseRedistributionPercentage 1 Unit WO 1 Unit BC 2 Units WO 2 Units BC 3 Units WO 3 Units BC 4 Units WO 4 Units BC Figure 1: A comparison of the worst-case optimal mechanism (WO) and the Bailey-Cavallo mechanism (BC). of the total VCG payment that is not redistributed by the Bailey-Cavallo mechanism in the worst case (which is equal to 2 n ). n 1 − k Bailey − Cavallo Mechanism 3 66.7% 66.7% 4 42.9% 50.0% 5 26.7% 40.0% 6 16.1% 33.3% 7 9.52% 28.6% 8 5.51% 25.0% 9 3.14% 22.2% 10 1.76% 20.0% 20 3.62e − 5 10.0% 30 5.40e − 8 6.67e − 2 40 7.09e − 11 5.00e − 2 The worst-case optimal mechanism significantly outperforms the Bailey-Cavallo mechanism in the worst case.
Perhaps more surprisingly, the worst-case optimal mechanism sometimes does better in the worst case than the BaileyCavallo mechanism does on average, as the following example shows.
Recall that the total redistribution payment of the BaileyCavallo mechanism is (m + 1)m n ˆvm+2 + (n − m − 1)m n ˆvm+1.
For the single-unit case, this simplifies to 2 n ˆv3 + n−2 n ˆv2.
Hence the percentage of the total VCG payment that is not redistributed is ˆv2− 2 n ˆv3− n−2 n ˆv2 ˆv2 = 2 n − 2 n ˆv3 ˆv2 , which has an expected value of E( 2 n − 2 n ˆv3 ˆv2 ) = 2 n − 2 n E ˆv3 ˆv2 .
Suppose the bid values are drawn from a uniform distribution over [0, 1]. The theory of order statistics tells us that the 34 joint probability density function of ˆv2 and ˆv3 is f(ˆv3, ˆv2) = n(n − 1)(n − 2)ˆvn−3 3 (1 − ˆv2) for ˆv2 ≥ ˆv3. Now, E ˆv3 ˆv2 = R 1 0 R ˆv2 0 ˆv3 ˆv2 f(ˆv3, ˆv2)dˆv3dˆv2 = n−2 n−1 . So, the expected value of the remaining percentage is 2 n − 2 n n−2 n−1 = 2 n(n−1) . For n = 20, this is 5.26e − 3, whereas the remaining percentage for the worst-case optimal mechanism is 3.62e−5 in the worst case.
Let us present the optimal solution for the case n = 5 in detail. By solving the above linear program, we find that the optimal values for the ci are c2 = 11 45 , c3 = −1 9 , and c4 = 1 15 .
That is, the redistribution payment received by each agent is: 11 45 times the second highest bid among the other agents, minus 1 9 times the third highest bid among the other agents, plus 1 15 times the fourth highest bid among the other agents.
The total amount redistributed is 11 15 ˆv2 + 4 15 ˆv3 − 4 15 ˆv4 + 4 15 ˆv5; in the worst case, 11 15 ˆv2 is redistributed. Hence, the percentage of the total VCG payment that is not redistributed is never more than 4 15 = 26.7%.
Finally, we compare the worst-case optimal mechanism to the Bailey-Cavallo mechanism for m = 1, 2, 3, 4, n = m + 2, . . . , 30. These results are in Figure 1.
We see that for any m, when n = m + 2, the worst-case optimal mechanism has the same worst-case performance as the Bailey-Cavallo mechanism (actually, in this case, the worst-case optimal mechanism is identical to the BaileyCavallo mechanism). When n > m + 2, the worst-case optimal mechanism outperforms the Bailey-Cavallo mechanism (in the worst case).
OF THE WORST-CASE OPTIMAL MECHANISM We recall that our linear program has the following form: Variables: cm+1, cm+2, . . . , cn−1, k Maximize k (the percentage redistributed in the worst case) Subject to:Pj i=m+1 ci ≥ 0 for j = m + 1, . . . , n − 1 km ≤ (n − m − 1)cm+1 ≤ m km ≤ n Pj=m+i−1 j=m+1 cj + (n − m − i)cm+i ≤ m for i = 2, . . . , n − m − 1 km ≤ n Pj=n−1 j=m+1 cj ≤ m A linear program has no solution if and only if either the objective is unbounded, or the constraints are contradictory (there is no feasible solution). It is easy to see that k is bounded above by 1 (redistributing more than 100% violates the non-deficit constraint). Also, a feasible solution always exists, for example, k = 0 and ci = 0 for all i. So an optimal solution always exists. Observe that the linear program model depends only on the number of agents n and the number of units m. Hence the optimal solution is a function of n and m. It turns out that this optimal solution can be analytically characterized as follows.
Theorem 1. For any m and n with n ≥ m+2, the worstcase optimal mechanism (among linear VCG redistribution mechanisms) is unique. For this mechanism, the percentage redistributed in the worst case is k∗ = 1 − `n−1 m ´ Pn−1 j=m `n−1 j ´ The worst-case optimal mechanism is characterized by the following values for the ci: c∗ i = (−1)i+m−1 (n − m) `n−1 m−1 ´ i Pn−1 j=m `n−1 j ´ 1 `n−1 i ´ n−1X j=i n − 1 j ! for i = m + 1, . . . , n − 1.
It should be noted that we have proved ci = 0 for i ≤ m in Claim 1.
Proof. We first rewrite the linear program as follows.
We introduce new variables xm+1, xm+2, . . . , xn−1, defined by xj = Pj i=m+1 ci for j = m + 1, . . . , n − 1. The linear program then becomes: Variables: xm+1, xm+2, . . . , xn−1, k Maximize k Subject to: km ≤ (n − m − 1)xm+1 ≤ m km ≤ (m + i)xm+i−1 + (n − m − i)xm+i ≤ m for i = 2, . . . , n − m − 1 km ≤ nxn−1 ≤ m xi ≥ 0 for i = m + 1, m + 2, . . . , n − 1 We will prove that for any optimal solution to this linear program, k = k∗ . Moreover, we will prove that when k = k∗ , xj = Pj i=m+1 c∗ i for j = m + 1, . . . , n − 1. This will prove the theorem.
We first make the following observations: (n − m − 1)c∗ m+1 = (n − m − 1) (n−m)(n−1 m−1) (m+1) Pn−1 j=m (n−1 j ) 1 (n−1 m+1) Pn−1 j=m+1 `n−1 j ´ = (n − m − 1) (n−m)(n−1 m−1) (m+1) Pn−1 j=m (n−1 j ) 1 (n−1 m+1) ( Pn−1 j=m `n−1 j ´ − `n−1 m ´ ) = (n − m − 1) m n−m−1 − (n − m − 1) m(n−1 m ) (n−m−1) Pn−1 j=m (n−1 j ) = m − (1 − k∗ )m = k∗ m For i = m + 1, . . . , n − 2, ic∗ i + (n − i − 1)c∗ i+1 = i (−1)i+m−1 (n−m)(n−1 m−1) i Pn−1 j=m (n−1 j ) 1 (n−1 i ) Pn−1 j=i `n−1 j ´ + (n − i − 1) (−1)i+m (n−m)(n−1 m−1) (i+1) Pn−1 j=m (n−1 j ) 1 (n−1 i+1 ) Pn−1 j=i+1 `n−1 j ´ = (−1)i+m−1 (n−m)(n−1 m−1) Pn−1 j=m (n−1 j ) 1 (n−1 i ) Pn−1 j=i `n−1 j ´ − (n − i − 1) (−1)i+m−1 (n−m)(n−1 m−1) (i+1) Pn−1 j=m (n−1 j ) i+1 (n−1 i )(n−i−1) Pn−1 j=i+1 `n−1 j ´ = (−1)i+m−1 (n−m)(n−1 m−1) Pn−1 j=m (n−1 j ) = (−1)i+m−1 m(1 − k∗ ) Finally, (n − 1)c∗ n−1 = (n − 1) (−1)n+m (n−m)(n−1 m−1) (n−1) Pn−1 j=m (n−1 j ) 1 (n−1 n−1) Pn−1 j=n−1 `n−1 j ´ = (−1)m+n m(1 − k∗ ) Summarizing the above, we have: (n − m − 1)c∗ m+1 = k∗ m (m + 1)c∗ m+1 + (n − m − 2)c∗ m+2 = m(1 − k∗ ) (m + 2)c∗ m+2 + (n − m − 3)c∗ m+3 = −m(1 − k∗ ) (m + 3)c∗ m+3 + (n − m − 4)c∗ m+4 = m(1 − k∗ ) ... 35 (n − 3)c∗ n−3 + 2c∗ n−2 = (−1)m+n−2 m(1 − k∗ ) (n − 2)c∗ n−2 + c∗ n−1 = (−1)m+n−1 m(1 − k∗ ) (n − 1)c∗ n−1 = (−1)m+n m(1 − k∗ ) Let x∗ j = Pj i=m+1 c∗ i for j = m + 1, m + 2, . . . , n − 1, the first equation in the above tells us that (n − m − 1)x∗ m+1 = k∗ m.
By adding the first two equations, we get (m + 2)x∗ m+1 + (n − m − 2)x∗ m+2 = m By adding the first three equations, we get (m + 3)x∗ m+2 + (n − m − 3)x∗ m+3 = k∗ m By adding the first i equations, where i = 2, . . . , n−m−1, we get (m + i)x∗ m+i−1 + (n − m − i)x∗ m+i = m if i is even (m + i)x∗ m+i−1 + (n − m − i)x∗ m+i = k∗ m if i is odd Finally by adding all the equations, we get nx∗ n−1 = m if n − m is even; nx∗ n−1 = k∗ m if n − m is odd.
Thus, for all of the constraints other than the nonnegativity constraints, we have shown that they are satisfied by setting xj = x∗ j = Pj i=m+1 c∗ i and k = k∗ . We next show that the nonnegativity constraints are satisfied by these settings as well.
For m + 1 ≤ i, i + 1 ≤ n − 1, we have 1 i Pn−1 j=i (n−1 j ) (n−1 i ) = 1 i Pn−1 j=i i!(n−1−i)! j!(n−1−j)! ≥ 1 i+1 Pn−2 j=i i!(n−1−i)! j!(n−1−j)! ≥ 1 i+1 Pn−2 j=i (i+1)!(n−1−i−1)! (j+1)!(n−1−j−1)! = 1 i+1 Pn−1 j=i+1 (n−1 j ) (n−1 i+1 ) This implies that the absolute value of c∗ i is decreasing as i increases (if the c∗ contains more than one number).
We further observe that the sign of c∗ i alternates, with the first element c∗ m+1 positive. So x∗ j = Pj i=m+1 c∗ i ≥ 0 for all j. Thus, we have shown that these xi = x∗ i together with k = k∗ form a feasible solution of the linear program.
We proceed to show that it is in fact the unique optimal solution.
First we prove the following claim: Claim 4. If ˆk, ˆxi, i = m + 1, m + 2, . . . , n − 1 satisfy the following inequalities: ˆkm ≤ (n − m − 1)ˆxm+1 ≤ m ˆkm ≤ (m + i)ˆxm+i−1 + (n − m − i)ˆxm+i ≤ m for i = 2, . . . , n − m − 1 ˆkm ≤ nˆxn−1 ≤ m ˆk ≥ k∗ Then we must have that ˆxi = ˆx∗ i and ˆk = k∗ .
Proof of claim. Consider the first inequality. We know that (n − m − 1)x∗ m+1 = k∗ m, so (n − m − 1)ˆxm+1 ≥ ˆkm ≥ k∗ m = (n − m − 1)x∗ m+1. It follows that ˆxm+1 ≥ x∗ m+1 (n − m − 1 = 0).
Now, consider the next inequality for i = 2. We know that (m + 2)x∗ m+1 + (n − m − 2)x∗ m+2 = m. It follows that (n−m−2)ˆxm+2 ≤ m−(m+2)ˆxm+1 ≤ m−(m+2)x∗ m+1 = (n − m − 2)x∗ m+2, so ˆxm+2 ≤ x∗ m+2 (i = 2 ≤ n − m − 1 ⇒ n − m − 2 = 0).
Now consider the next inequality for i = 3. We know that (m + 3)x∗ m+2 + (n − m − 3)x∗ m+3 = m. It follows that (n−m−3)ˆxm+3 ≥ ˆkm−(m+3)ˆxm+2 ≥ k∗ m−(m+3)x∗ m+2 = (n − m − 3)x∗ m+3, so ˆxm+3 ≥ x∗ m+3 (i = 3 ≤ n − m − 1 ⇒ n − m − 3 = 0).
Proceeding like this all the way up to i = n−m−1, we get that ˆxm+i ≥ x∗ m+i if i is odd and ˆxm+i ≤ x∗ m+i if i is even.
Moreover, if one inequality is strict, then all subsequent inequalities are strict. Now, if we can prove ˆxn−1 = x∗ n−1, it would follow that the x∗ i are equal to the ˆxi (which also implies that ˆk = k∗ ). We consider two cases: Case 1: n − m is even. We have: n − m even ⇒ n − m − 1 odd ⇒ ˆxn−1 ≥ x∗ n−1. We also have: n−m even ⇒ nx∗ n−1 = m. Combining these two, we get m = nx∗ n−1 ≤ nˆxn−1 ≤ m ⇒ ˆxn−1 = x∗ n−1.
Case 2: n − m is odd. In this case, we have ˆxn−1 ≤ x∗ n−1, and nx∗ n−1 = k∗ m. Then, we have: k∗ m ≤ ˆkm ≤ nˆxn−1 ≤ nx∗ n−1 = k∗ m ⇒ ˆxn−1 = x∗ n−1.
This completes the proof of the claim.
It follows that if ˆk, ˆxi, i = m + 1, m + 2, . . . , n − 1 is a feasible solution and ˆk ≥ k∗ , then since all the inequalities in Claim 4 are satisfied, we must have ˆxi = x∗ i and ˆk = k∗ . Hence no other feasible solution is as good as the one described in the theorem.
Knowing the analytical characterization of the worst-case optimal mechanism provides us with at least two major benefits. First, using these formulas is computationally more efficient than solving the linear program using a generalpurpose solver. Second, we can derive the following corollary.
Corollary 1. If the number of units m is fixed, then as the number of agents n increases, the worst-case percentage redistributed linearly converges to 1, with a rate of convergence 1 2 . (That is, limn→∞ 1−k∗ n+1 1−k∗ n = 1 2 . That is, in the limit, the percentage that is not redistributed halves for every additional agent.) We note that this is consistent with the experimental data for the single-unit case, where the worst-case remaining percentage roughly halves each time we add another agent.
The worst-case percentage that is redistributed under the Bailey-Cavallo mechanism also converges to 1 as the number of agents goes to infinity, but the convergence is much slower-it does not converge linearly (that is, letting kC n be the percentage redistributed by the Bailey-Cavallo mechanism in the worst case for n agents, limn→∞ 1−kC n+1 1−kC n = limn→∞ n n+1 = 1). We now present the proof of the corollary.
Proof. When the number of agents is n, the worst-case percentage redistributed is k∗ n = 1 − (n−1 m ) Pn−1 j=m (n−1 j ) . When the number of agents is n + 1, the percentage becomes k∗ n+1 = 1 − (n m) Pn j=m (n j ) . For n sufficiently large, we will have 2n − mnm−1 > 0, and hence 1−k∗ n+1 1−k∗ n = (n m) Pn−1 j=m (n−1 j ) (n−1 m ) Pn j=m (n j ) = n n−m 2n−1 − Pm−1 j=0 (n−1 j ) 2n− Pm−1 j=0 (n j ) , and n n−m 2n−1 −m(n−1)m−1 2n ≤ 1−k∗ n+1 1−k∗ n ≤ n n−m 2n−1 2n−mnm−1 (because `n j ´ ≤ ni if j ≤ i).
Since we have limn→∞ n n−m 2n−1 −m(n−1)m−1 2n = 1 2 , and limn→∞ n n−m 2n−1 2n−mnm−1 = 1 2 , it follows that limn→∞ 1−k∗ n+1 1−k∗ n = 1 2 . 36
THE FAMILY In this section, we prove that the worst-case optimal redistribution mechanism among linear VCG redistribution mechanisms is in fact optimal (in the worst case) among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint. Thus, restricting our attention to linear VCG redistribution mechanisms did not come at a loss.
To prove this theorem, we need the following lemma. This lemma is not new: it was informally stated by Cavallo [3].
For completeness, we present it here with a detailed proof.
Lemma 2. A VCG redistribution mechanism is deterministic, anonymous and strategy-proof if and only if there exists a function f : Rn−1 → R, so that the redistribution payment zi received by ai satisfies zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) for all i and all bid vectors.
Proof. First, let us prove the only if direction, that is, if a VCG redistribution mechanism is deterministic, anonymous and strategy-proof then there exists a deterministic function f : Rn−1 → R, which makes zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) for all i and all bid vectors.
If a VCG redistribution mechanism is deterministic and anonymous, then for any bid vector ˆv1 ≥ ˆv2 ≥ . . . ≥ ˆvn, the mechanism outputs a unique redistribution payment list: z1, z2, . . . , zn. Let G : Rn → Rn be the function that maps ˆv1, ˆv2, . . . , ˆvn to z1, z2, . . . , zn for all bid vectors. Let H(i, x1, x2, . . . , xn) be the ith element of G(x1, x2, . . . , xn), so that zi = H(i, ˆv1, ˆv2, . . . , ˆvn) for all bid vectors and all 1 ≤ i ≤ n. Because the mechanism is anonymous, two agents should receive the same redistribution payment if their bids are the same. So, if ˆvi = ˆvj, H(i, ˆv1, ˆv2, . . . , ˆvn) = H(j, ˆv1, ˆv2, . . . , ˆvn). Hence, if we let j = min{t|ˆvt = ˆvi}, then H(i, ˆv1, ˆv2, . . . , ˆvn) = H(j, ˆv1, ˆv2, . . . , ˆvn).
Let us define K : Rn → N × Rn as follows: K(y, x1, x2, . . . , xn−1) = [j, w1, w2, . . . , wn], where w1, w2, . . . , wn are y, x1, x2, . . . , xn−1 sorted in descending order, and j = min{t|wt = y}. ({t|wt = y} = ∅ because y ∈ {w1, w2, . . . , wn}). Also let us define F : Rn → R by F(ˆvi, ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) = H ◦ K(ˆvi, ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) = H(min{t|ˆvt = ˆvi}, ˆv1, ˆv2, . . . , ˆvn) = H(i, ˆv1, ˆv2, . . . , ˆvn) = zi.
That is, F is the redistribution payment to an agent that bids ˆvi when the other bids are ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn.
Since our mechanism is required to be strategy-proof, and the space of valuations is unrestricted, zi should be independent of ˆvi by Lemma 1 in Cavallo [3]. Hence, we can simply ignore the first variable input to F; let f(x1, x2, . . . , xn−1) = F(0, x1, x2, . . . , xn−1). So, we have for all bid vectors and i, zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn). This completes the proof for the only if direction.
For the if direction, if the redistribution payment received by ai satisfies zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) for all bid vectors and i, then this is clearly a deterministic and anonymous mechanism. To prove strategy-proofness, we observe that because an agent"s redistribution payment is not affected by her own bid, her incentives are the same as in the VCG mechanism, which is strategy-proof.
Now we are ready to introduce the next theorem: Theorem 2. For any m and n with n ≥ m+2, the worstcase optimal mechanism among the family of linear VCG redistribution mechanisms is worst-case optimal among all mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint.
While we needed individual rationality earlier in the paper, this theorem does not mention it, that is, we can not find a mechanism with better worst-case performance even if we sacrifice individual rationality. (The worst-case optimal linear VCG redistribution mechanism is of course individually rational.) Proof. Suppose there is a redistribution mechanism (when the number of units is m and the number of agents is n) that satisfies all of the above properties and has a better worstcase performance than the worst-case optimal linear VCG redistribution mechanism, that is, its worst-case redistribution percentage ˆk is strictly greater than k∗ .
By Lemma 2, for this mechanism, there is a function f : Rn−1 → R so that zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) for all i and all bid vectors. We first prove that f has the following properties.
Claim 5. f(1, 1, . . . , 1, 0, 0, . . . , 0) = 0 if the number of 1s is less than or equal to m.
Proof of claim. We assumed that for this mechanism, the worst-case redistribution percentage satisfies ˆk > k∗ ≥
payment should be in [ˆkx, x] (non-deficit criterion).
Consider the case where all agents bid 0, so that the total VCG payment is also 0. Hence, the total redistribution payment should be in [ˆk · 0, 0]-that is, it should be 0. Hence every agent"s redistribution payment f(0, 0, . . . , 0) must be 0.
Now, let ti = f(1, 1, . . . , 1, 0, 0, . . . , 0) where the number of 1s equals i. We proved t0 = 0. If tn−1 = 0, consider the bid vector where everyone bids 1. The total VCG payment is m and the total redistribution payment is nf(1, 1, . . . , 1) = ntn−1 = 0. This corresponds to 0% redistribution, which is contrary to our assumption that ˆk > k∗ ≥ 0. Now, consider j = min{i|ti = 0} (which is well-defined because tn−1 = 0).
If j > m, the property is satisfied. If j ≤ m, consider the bid vector where ˆvi = 1 for i ≤ j and ˆvi = 0 for all other i. Under this bid vector, the first j agents each get redistribution payment tj−1 = 0, and the remaining n − j agents each get tj. Thus, the total redistribution payment is (n − j)tj. Because the total VCG payment for this bid vector is 0, we must have (n − j)tj = 0. So tj = 0 (j ≤ m < n). But this is contrary to the definition of j. Hence f(1, 1, . . . , 1, 0, 0, . . . , 0) = 0 if the number of 1s is less than or equal to m.
Claim 6. f satisfies the following inequalities: ˆkm ≤ (n − m − 1)tm+1 ≤ m ˆkm ≤ (m + i)tm+i−1 + (n − m − i)tm+i ≤ m for i = 2, 3, . . . , n − m − 1 ˆkm ≤ ntn−1 ≤ m Here ti is defined as in the proof of Claim 5. 37 Proof of claim. For j = m + 1, . . . , n, consider the bid vectors where ˆvi = 1 for i ≤ j and ˆvi = 0 for all other i.
These bid vectors together with the non-deficit constraint and worst-case constraint produce the above set of inequalities: for example, when j = m + 1, we consider the bid vector ˆvi = 1 for i ≤ m + 1 and ˆvi = 0 for all other i.
The first m+1 agents each receive a redistribution payment of tm = 0, and all other agents each receive tm+1. Thus, the total VCG redistribution is (n − m − 1)tm+1. The nondeficit constraint gives (n − m − 1)tm+1 ≤ m (because the total VCG payment is m). The worst-case constraint gives (n − m − 1)tm+1 ≥ ˆkm. Combining these two, we get the first inequality. The other inequalities can be obtained in the same way.
We now observe that the inequalities in Claim 6, together with ˆk ≥ k∗ , are the same as those in Claim 4 (where the ti are replaced by the ˆxi). Thus, we can conclude that ˆk = k∗ , which is contrary to our assumption ˆk > k∗ . Hence no mechanism satisfying all the listed properties has a redistribution percentage greater than k∗ in the worst case.
So far we have only talked about the case where n ≥ m+2.
For the purpose of completeness, we provide the following claim for the n = m + 1 case.
Claim 7. For any m and n with n = m + 1, the original VCG mechanism (that is, redistributing nothing) is (uniquely) worst-case optimal among all redistribution mechanisms that are deterministic, anonymous, strategy-proof, efficient and satisfy the non-deficit constraint.
We recall that when n = m+1, Claim 1 tells us that the only mechanism inside the family of linear redistribution mechanisms is the original VCG mechanism, so that this mechanism is automatically worst-case optimal inside this family.
However, to prove the above claim, we need to show that it is worst-case optimal among all redistribution mechanisms that have the desired properties.
Proof. Suppose a redistribution mechanism exists that satisfies all of the above properties and has a worst-case performance as good as the original VCG mechanism, that is, its worst-case redistribution percentage is greater than or equal to 0. This implies that the total redistribution payment of this mechanism is always nonnegative.
By Lemma 2, for this mechanism, there is a function f : Rn−1 → R so that zi = f(ˆv1, ˆv2, . . . , ˆvi−1, ˆvi+1, . . . , ˆvn) for all i and all bid vectors. We will prove that f(x1, x2, . . . , xn−1) = 0 for all x1 ≥ x2 ≥ . . . ≥ xn−1 ≥ 0.
First, consider the bid vector where ˆvi = 0 for all i. Here, each agent receives a redistribution payment f(0, 0, . . . , 0).
The total redistribution payment is then nf(0, 0, . . . , 0), which should be both greater than or equal to 0 (by the above observation) as well less than or equal to 0 (using the nondeficit criterion and the fact that the total VCG payment is 0). It follows that f(0, 0, . . . , 0) = 0. Now, let us consider the bid vector where ˆv1 = x1 ≥ 0 and ˆvi = 0 for all other i.
For this bid vector, the agent with the highest bid receives a redistribution payment of f(0, 0, . . . , 0) = 0, and the other n − 1 agents each receive f(x1, 0, . . . , 0). By the same reasoning as above, the total redistribution payment should be both greater than or equal to 0 and less than or equal to 0, hence f(x1, 0, . . . , 0) = 0 for all x1 ≥ 0.
Proceeding by induction, let us assume f(x1, x2, . . . , xk, 0, . . . , 0) = 0 for all x1 ≥ x2 ≥ . . . ≥ xk ≥ 0, for some k < n − 1. Consider the bid vector where ˆvi = xi for i ≤ k + 1, and ˆvi = 0 for all other i, where the xi are arbitrary numbers satisfying x1 ≥ x2 ≥ . . . ≥ xk ≥ xk+1 ≥ 0.
For the agents with the highest k + 1 bids, their redistribution payment is specified by f acting on an input with only k non-zero variables. Hence they all receive 0 by induction assumption. The other n − k − 1 agents each receive f(x1, x2, . . . , xk, xk+1, 0, . . . , 0). The total redistribution payment is then (n−k−1)f(x1, x2, . . . , xk, xk+1, 0, . . . , 0), which should be both greater than or equal to 0, and less than or equal to the total VCG payment. Now, in this bid vector, the lowest bid is 0 because k + 1 < n. But since n = m + 1, the total VCG payment is mˆvn = 0. So we have f(x1, x2, . . . , xk, xk+1, 0, . . . , 0) = 0 for all x1 ≥ x2 ≥ . . . ≥ xk ≥ xk+1 ≥ 0. By induction, this statement holds for all k < n − 1; when k + 1 = n − 1, we have f(x1, x2, . . . , xn−2, xn−1) = 0 for all x1 ≥ x2 ≥ . . . ≥ xn−2 ≥ xn−1 ≥ 0. Hence, in this mechanism, the redistribution payment is always 0; that is, the mechanism is just the original VCG mechanism.
Incidentally, we obtain the following corollary: Corollary 2. No VCG redistribution mechanism satisfies all of the following: determinism, anonymity, strategyproofness, efficiency, and (strong) budget balance. This holds for any n ≥ m + 1.
Proof. For the case n ≥ m + 2: If such a mechanism exists, its worst-case performance would be better than that of the worst-case optimal linear VCG redistribution mechanism, which by Theorem 1 obtains a redistribution percentage strictly less than 1. But Theorem 2 shows that it is impossible to outperform this mechanism in the worst case.
For the case n = m + 1: If such a mechanism exists, it would perform as well as the original VCG mechanism in the worst case, which implies that it is identical to the VCG mechanism by Claim 7. But the VCG mechanism is not (strongly) budget balanced.
For allocation problems with one or more items, the wellknown Vickrey-Clarke-Groves (VCG) mechanism is efficient, strategy-proof, individually rational, and does not incur a deficit. However, the VCG mechanism is not (strongly) budget balanced: generally, the agents" payments will sum to more than 0. If there is an auctioneer who is selling the items, this may be desirable, because the surplus payment corresponds to revenue for the auctioneer. However, if the items do not have an owner and the agents are merely interested in allocating the items efficiently among themselves, any surplus payment is undesirable, because it will have to flow out of the system of agents. In 2006,
Cavallo [3] proposed a mechanism that redistributes some of the VCG payment back to the agents, while maintaining efficiency, strategy-proofness, individual rationality, and the non-deficit property. In this paper, we extended this result in a restricted setting. We studied allocation settings where there are multiple indistinguishable units of a single good, and agents have unit demand. (For this specific setting, Cavallo"s mechanism coincides with a mechanism proposed by Bailey in 1997 [2].) Here we proposed a family of mechanisms that redistribute some of the VCG payment 38 back to the agents. All mechanisms in the family are efficient, strategy-proof, individually rational, and never incur a deficit. The family includes the Bailey-Cavallo mechanism as a special case. We then provided an optimization model for finding the optimal mechanism-that is, the mechanism that maximizes redistribution in the worst case-inside the family, and showed how to cast this model as a linear program. We gave both numerical and analytical solutions of this linear program, and the (unique) resulting mechanism shows significant improvement over the Bailey-Cavallo mechanism (in the worst case). Finally, we proved that the obtained mechanism is optimal among all anonymous deterministic mechanisms that satisfy the above properties.
One important direction for future research is to try to extend these results beyond multi-unit auctions with unit demand. However, it turns out that in sufficiently general settings, the worst-case optimal redistribution percentage is 0. In such settings, the worst-case criterion provides no guidance in determining a good redistribution mechanism (even redistributing nothing achieves the optimal worst-case percentage), so it becomes necessary to pursue other criteria.
Alternatively, one can try to identify other special settings in which positive redistribution in the worst case is possible.
Another direction for future research is to consider whether this mechanism has applications to collusion. For example, in a typical collusive scheme, there is a bidding ring consisting of a number of colluders, who submit only a single bid [10, 17]. If this bid wins, the colluders must allocate the item amongst themselves, perhaps using payments-but of course they do not want payments to flow out of the ring.
This work is part of a growing literature on designing mechanisms that obtain good results in the worst case.
Traditionally, economists have mostly focused either on designing mechanisms that always obtain certain properties (such as the VCG mechanism), or on designing mechanisms that are optimal with respect to some prior distribution over the agents" preferences (such as the Myerson auction [20] and the Maskin-Riley auction [18] for maximizing expected revenue).
Some more recent papers have focused on designing mechanisms for profit maximization using worst-case competitive analysis (e.g. [9, 1, 15, 8]). There has also been growing interest in the design of online mechanisms [7] where the agents arrive over time and decisions must be taken before all the agents have arrived. Such work often also takes a worst-case competitive analysis approach [14, 13]. It does not appear that there are direct connections between our work and these other works that focus on designing mechanisms that perform well in the worst case. Nevertheless, it seems likely that future research will continue to investigate mechanism design for the worst case, and hopefully a coherent framework will emerge.
[1] G. Aggarwal, A. Fiat, A. Goldberg, J. Hartline,
N. Immorlica, and M. Sudan. Derandomization of auctions. STOC, 619-625, 2005. [2] M. J. Bailey. The demand revealing process: to distribute the surplus. Public Choice, 91:107-126,
[3] R. Cavallo. Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments.
AAMAS, 882-889, 2006. [4] E. H. Clarke. Multipart pricing of public goods. Public Choice, 11:17-33, 1971. [5] B. Faltings. A budget-balanced, incentive-compatible scheme for social choice. AMEC, 30-43, 2005. [6] J. Feigenbaum, C. Papadimitriou, and S. Shenker.
Sharing the cost of muliticast transmissions. JCSS, 63:21-41, 2001. [7] E. Friedman and D. Parkes. Pricing WiFi at Starbucks - Issues in online mechanism design. EC, 240-241, 2003. [8] A. Goldberg, J. Hartline, A. Karlin, M. Saks, and A. Wright. Competitive auctions. Games and Economic Behavior, 2006. [9] A. Goldberg, J. Hartline, and A. Wright. Competitive auctions and digital goods. SODA, 735-744, 2001. [10] D. A. Graham and R. C. Marshall. Collusive bidder behavior at single-object second-price and English auctions. Journal of Political Economy, 95(6):1217-1239, 1987. [11] J. Green and J.-J. Laffont. Characterization of satisfactory mechanisms for the revelation of preferences for public goods. Econometrica, 45:427-438, 1977. [12] T. Groves. Incentives in teams. Econometrica, 41:617-631, 1973. [13] M. T. Hajiaghayi, R. Kleinberg, M. Mahdian, and D. C. Parkes. Online auctions with re-usable goods.
EC, 165-174, 2005. [14] M. T. Hajiaghayi, R. Kleinberg, and D. C. Parkes.
Adaptive limited-supply online auctions. EC, 71-80,
[15] J. Hartline and R. McGrew. From optimal limited to unlimited supply auctions. EC, 175-182, 2005. [16] L. Hurwicz. On the existence of allocation systems whose manipulative Nash equilibria are Pareto optimal, 1975. Presented at the 3rd World Congress of the Econometric Society. [17] K. Leyton-Brown, Y. Shoham, and M. Tennenholtz.
Bidding clubs in first-price auctions. AAAI, 373-378,
[18] E. Maskin and J. Riley. Optimal multi-unit auctions.
In F. Hahn, editor, The Economics of Missing Markets, Information, and Games, chapter 14, 312-335. Clarendon Press, Oxford, 1989. [19] H. Moulin. Efficient and strategy-proof assignment with a cheap residual claimant. Working paper, March

The role of kidneys is to filter waste from blood. Kidney failure results in accumulation of this waste, which leads to death in months. One treatment option is dialysis, in which the patient goes to a hospital to have his/her blood filtered by an external machine. Several visits are required per week, and each takes several hours. The quality of life on dialysis can be extremely low, and in fact many patients opt to withdraw from dialysis, leading to a natural death.
Only 12% of dialysis patients survive 10 years [23].
Instead, the preferred treatment is a kidney transplant.
Kidney transplants are by far the most common transplant.
Unfortunately, the demand for kidneys far outstrips supply.
In the United States in 2005, 4,052 people died waiting for a life-saving kidney transplant. During this time, almost 30,000 people were added to the national waiting list, while only 9,913 people left the list after receiving a deceaseddonor kidney. The waiting list currently has over 70,000 people, and the median waiting time ranges from 2 to 5 years, depending on blood type.1 For many patients with kidney disease, the best option is to find a living donor, that is, a healthy person willing to donate one of his/her two kidneys. Although there are marketplaces for buying and selling living-donor kidneys, the commercialization of human organs is almost universally regarded as unethical, and the practice is often explicitly illegal, such as in the US. However, in most countries, live donation is legal, provided it occurs as a gift with no financial compensation. In 2005, there were 6,563 live donations in the US.
The number of live donations would have been much higher if it were not for the fact that, frequently, a potential donor 1 Data from the United Network for Organ Sharing [21]. 295 and his intended recipient are blood-type or tissue-type incompatible. In the past, the incompatible donor was sent home, leaving the patient to wait for a deceased-donor kidney. However, there are now a few regional kidney exchanges in the United States, in which patients can swap their incompatible donors with each other, in order to each obtain a compatible donor.
These markets are examples of barter exchanges. In a barter-exchange market, agents (patients) seek to swap their items (incompatible donors) with each other. These swaps consist of cycles of agents, with each agent receiving the item of the next agent in the cycle. Barter exchanges are ubiquitous: examples include Peerflix (DVDs) [11], Read It Swap It (books) [12], and Intervac (holiday houses) [9]. For many years, there has even been a large shoe exchange in the United States [10]. People with different-sized feet use this to avoid having to buy two pairs of shoes. Leg amputees have a separate exchange to share the cost of buying a single pair of shoes.
We can encode a barter exchange market as a directed graph G = (V, E) in the following way. Construct one vertex for each agent. Add a weighted edge e from one agent vi to another vj, if vi wants the item of vj. The weight we of e represents the utility to vi of obtaining vj"s item. A cycle c in this graph represents a possible swap, with each agent in the cycle obtaining the item of the next agent. The weight wc of a cycle c is the sum of its edge weights. An exchange is a collection of disjoint cycles. The weight of an exchange is the sum of its cycle weights. A social welfare maximizing exchange is one with maximum weight.
Figure 1 illustrates an example market with 5 agents, {v1, v2, . . . , v5}, in which all edges have weight 1. The market has 4 cycles, c1 = v1, v2 , c2 = v2, v3 , c3 = v3, v4 and c4 = v1, v2, v3, v4, v5 , and two (inclusion) maximal exchanges, namely M1 = {c4} and M2 = {c1, c3}. Exchange M1 has both maximum weight and maximum cardinality (i.e., it includes the most edges/vertices). v1 v2 v3 v4 v5 e1 e3 e5 c1 c2 c3 e8 e7 e6e4e2 c4 Figure 1: Example barter exchange market.
The clearing problem is to find a maximum-weight exchange consisting of cycles with length at most some small constant L. This cycle-length constraint arises naturally for several reasons. For example, in a kidney exchange, all operations in a cycle have to be performed simultaneously; otherwise a donor might back out after his incompatible partner has received a kidney. (One cannot write a binding contract to donate an organ.) This gives rise to a logistical constraint on cycle size: even if all the donors are operated on first and the same personnel and facilities are used to then operate on the donees, a k-cycle requires between 3k and 6k doctors, around 4k nurses, and almost 2k operating rooms.
Due to such resource constraints, the upcoming national kidney exchange market will likely allow only cycles of length 2 and 3. Another motivation for short cycles is that if the cycle fails to exchange, fewer agents are affected. For example, last-minute testing in a kidney exchange often reveals new incompatibilities that were not detected in the initial testing (based on which the compatibility graph was constructed). More generally, an agent may drop out of a cycle if his preferences have changed, or he/she simply fails to fulfill his obligations (such as sending a book to another agent in the cycle) due to forgetfulness.
In Section 3, we show that (the decision version of) the clearing problem is NP-complete for L ≥ 3. One approach then might be to look for a good heuristic or approximation algorithm. However, for two reasons, we aim for an exact algorithm based on an integer-linear program (ILP) formulation, which we solve using specialized tree search. • First, any loss of optimality could lead to unnecessary patient deaths. • Second, an attractive feature of using an ILP formulation is that it allows one to easily model a number of variations on the objective, and to add additional constraints to the problem. For example, if 3-cycles are believed to be more likely to fail than 2-cycles, then one can simply give them a weight that is appropriately lower than 3/2 the weight of a 2-cycle. Or, if for various (e.g., ethical) reasons one requires a maximum cardinality exchange, one can at least in a second pass find the solution (out of all maximum cardinality solutions) that has the fewest 3-cycles. Other variations one can solve for include finding various forms of fault tolerant (non-disjoint) collections of cycles in the event that certain pairs that were thought to be compatible turn out to be incompatible after all.
In this paper, we present the first algorithm capable of clearing these markets on a nationwide scale. Straight-forward ILP encodings are too large to even construct on current hardware - not to talk about solving them. The key then is incremental problem formulation. We adapt two paradigms for the task: constraint generation and column generation. For each, we develop a host of (mainly problemspecific) techniques that dramatically improve both runtime and memory usage.
Several recent papers have used simulations and marketclearing algorithms to explore the impact of a national kidney exchange [13, 20, 6, 14, 15, 17]. For example, using Edmond"s maximum-matching algorithm [4], [20] shows that a national pairwise-exchange market (using length-2 cycles only) would result in more transplants, reduced waiting time, and savings of $750 million in heath care costs over 5 years. Those results are conservative in two ways. Firstly, the simulated market contained only 4,000 initial patients, with 250 patients added every 3 months. It has been reported to us that the market could be almost double this size. Secondly, the exchanges were restricted to length-2 cycles (because that is all that can be modeled as maximum matching, and solved using Edmonds"s algorithm).
Allowing length-3 cycles leads to additional significant gains. This has been demonstrated on kidney exchange markets with 100 patients by using CPLEX to solve an integer-program encoding of the clearing problem [15]. In this paper, we 296 present an alternative algorithm for this integer program that can clear markets with over 10,000 patients (and that same number of willing donors).
Allowing cycles of length more than 3 often leads to no improvement in the size of the exchange [15]. (Furthermore, in a simplified theoretical model, any kidney exchange can be converted into one with cycles of length at most 4 [15].) Whilst this does not hold for general barter exchanges, or even for all kidney exchange markets, in Section 5.2.3 we make use of the observation that short cycles suffice to dramatically increase the speed of our algorithm.
At a high-level, the clearing problem for barter exchanges is similar to the clearing problem (aka winner determination problem) in combinatorial auctions. In both settings, the idea is to gather all the pertinent information about the agents into a central clearing point and to run a centralized clearing algorithm to determine the allocation. Both problems are NP-hard. Both are best solved using tree search techniques. Since 1999, significant work has been done in computer science and operations research on faster optimal tree search algorithms for clearing combinatorial auctions. (For a recent review, see [18].) However, the kidney exchange clearing problem (with a limit of 3 or more on cycle size) is different from the combinatorial auction clearing problem in significant ways. The most important difference is that the natural formulations of the combinatorial auction problem tend to easily fit in memory, so time is the bottleneck in practice. In contrast, the natural formulations of the kidney exchange problem (with L = 3) take at least cubic space in the number of patients to even model, and therefore memory becomes a bottleneck much before time does when using standard tree search, such as branch-andcut in CPLEX, to tackle the problem. (On a 1GB computer and a realistic standard instance generator, discussed later,
CPLEX 10.010 runs out of memory on five of the ten 900patient instances and ten of the ten 1,000-patient instances that we generated.) Therefore, the approaches that have been developed for combinatorial auctions cannot handle the kidney exchange problem.
The rest of the paper is organized as follows. Section 2 discusses the process by which we generate realistic kidney exchange market data, in order to benchmark the clearing algorithms. Section 3 contains a proof that the market clearing decision problem is NP-complete. Sections 4 and 5 each contain an ILP formulation of the clearing problem. We also detail in those sections our techniques used to solve those programs on large instances. Section 6 presents experiments on the various techniques. Section 7 discusses recent fielding of our algorithm. Finally, we present our conclusions in Section 8, and suggest future research directions.
INSTANCE GENERATOR We test the algorithms on simulated kidney exchange markets, which are generated by a process described in Saidman et al. [17]. This process is based on the extensive nationwide data maintained by the United Network for Organ Sharing (UNOS) [21], so it generates a realistic instance distribution. Several papers have used variations of this process to demonstrate the effectiveness of a national kidney exchange (extrapolating from small instances or restricting the clearing to 2-cycles) [6, 20, 14, 13, 15, 17].
Briefly, the process involves generating patients with a random blood type, sex, and probability of being tissue-type incompatible with a randomly chosen donor. These probabilities are based on actual real-world population data. Each patient is assigned a potential donor with a random blood type and relation to the patient. If the patient and potential donor are incompatible, the two are entered into the market. Blood type and tissue type information is then used to decide on which patients and donors are compatible. One complication, handled by the generator, is that if the patient is female, and she has had a child with her potential donor, then the probability that the two are incompatible increases. (This is because the mother develops antibodies to her partner during pregnancy.) Finally, although our algorithms can handle more general weight functions, patients have a utility of 1 for compatible donors, since their survival probability is not affected by the choice of donor [3]. This means that the maximum-weight exchange has maximum cardinality.
Table 1 gives lower and upper bounds on the size of a maximum-cardinality exchange in the kidney-exchange market. The lower bounds were found by clearing the market with length-2 cycles only, while the upper bounds had no restriction on cycle length. For each market size, the bounds were computed over 10 randomly generated markets. Note that there can be a large amount of variability in the markets - in one 5000 patient market, less than 1000 patients were in the maximum-cardinality exchange.
Maximum exchange size Length-2 cycles only Arbitrary cycles Patients Mean Max Mean Max 100 4.00e+1 4.60e+1 5.30e+1 6.10e+1 500 2.58e+2 2.80e+2 2.79e+2 2.97e+2 1000 5.35e+2 6.22e+2 5.61e+2 6.30e+2 2000 1.05e+3 1.13e+3 1.09e+3 1.16e+3 3000 1.63e+3 1.70e+3 1.68e+3 1.73e+3 4000 2.15e+3 2.22e+3 2.20e+3 2.27e+3 5000 2.53e+3 2.87e+3 2.59e+3 2.92e+3 6000 3.26e+3 3.32e+3 3.35e+3 3.39e+3 7000 3.80e+3 3.86e+3 3.89e+3 3.97e+3 8000 4.35e+3 4.45e+3 4.46e+3 4.55e+3 9000 4.90e+3 4.96e+3 5.01e+3 5.07e+3 10000 5.47e+3 5.61e+3 5.59e+3 5.73e+3 Table 1: Upper and lower bounds on exchange size.
Table 2 gives additional characteristics of the kidney-exchange market. Note that a market with 5000 patients can already have more than 450 million cycles of length 2 and 3.
Edges Length 2 & 3 cycles Patients Mean Max Mean Max 100 2.38e+3 2.79e+3 2.76e+3 5.90e+3 500 6.19e+4 6.68e+4 3.96e+5 5.27e+5 1000 2.44e+5 2.68e+5 3.31e+6 4.57e+6 2000 9.60e+5 1.02e+6 2.50e+7 3.26e+7 3000 2.19e+6 2.28e+6 8.70e+7 9.64e+7 4000 3.86e+6 3.97e+6 1.94e+8 2.14e+8 5000 5.67e+6 6.33e+6 3.60e+8 4.59e+8 6000 8.80e+6 8.95e+6 7000 1.19e+7 1.21e+7 8000 1.56e+7 1.59e+7 9000 1.98e+7 2.02e+7 10000 2.44e+7 2.51e+7 Table 2: Market characteristics. 297
In this section, we prove that (the decision version of) the market clearing problem with short cycles is NP-complete.
Theorem 1. Given a graph G = (V, E) and an integer L ≥ 3, the problem of deciding if G admits a perfect cycle cover containing cycles of length at most L is NP-complete.
Proof. It is clear that this problem is in NP. For NPhardness, we reduce from 3D-Matching, which is the problem of, given disjoint sets X, Y and Z of size q, and a set of triples T ⊆ X × Y × Z, deciding if there is a disjoint subset M of T with size q.
One straightforward idea is to construct a tripartite graph with vertex sets X ∪ Y ∪ Z and directed edges (xa, yb), (yb, zc), and (zc, xa) for each triple ti = {xa, yb, zc} ∈ T.
However, it is not too hard to see that this encoding fails because a perfect cycle cover may include a cycle with no corresponding triple.
Instead then, we use the following reduction. Given an instance of 3D-Matching, construct one vertex for each element in X, Y and Z. For each triple, ti = {xa, yb, zc} construct the gadget in Figure 2, which is a similar to one in Garey and Johnson [5, pp 68-69]. Note that the gadgets intersect only on vertices in X ∪ Y ∪ Z. It is clear that this construction can be done in polynomial time. 1 ... 2 3 y_b ... 2 3 z_c y_b^i z_c^i L−1 L−1 L−1 x_a^i x_a ... 2 31 1 Figure 2: NP-completeness gadget for triple ti and maximum cycle length L.
Let M be a perfect 3D-Matching. We will show the construction admits a perfect cycle cover by short cycles. If ti = {xa, yb, zc} ∈ M, add from ti"s gadget the three lengthL cycles containing xa, yb and zc respectively. Also add the cycle ª xi a, yi b, zi c « . Otherwise, if ti /∈ M, add the three lengthL cycles containing xi a, yi b and zi c respectively. It is clear that all vertices are covered, since M partitions X × Y × Z.
Conversely, suppose we have a perfect cover by short cycles. Note that the construction only has short cycles of lengths 3 and L, and no short cycle involves distinct vertices from two different gadgets. It is easy to see then that in a perfect cover, each gadget ti contributes cycles according to the cases above: ti ∈ M, or ti /∈ M. Hence, there exists a perfect 3D-Matching in the original instance.
AN EDGE FORMULATION In this section, we consider a formulation of the clearing problem as an ILP with one variable for each edge. This encoding is based on the following classical algorithm for solving the directed cycle cover problem with no cycle-length constraints.
Given a market G = (V, E), construct a bipartite graph with one vertex for each agent, and one vertex for each item.
Add an edge ev with weight 0 between each agent v and its own item. At this point, the encoding is a perfect matching.
Now, for each edge e = (vi, vj) in the original market, add an edge e with weight we between agent vi and the item of vj. Perfect matchings in this encoding correspond exactly with cycle covers, since whenever an agent"s item is taken, it must receive some other agent"s item. It follows that the unrestricted clearing problem can be solved in polynomial time by finding a maximum-weight perfect matching.
Figure 3 contains the bipartite graph encoding of the example market from Figure 1. The weight-0 edges are encoded by dashed lines, while the market edges are in bold.
Items Agents v1 v2 v3 v4 v5 e1 e3 e8 e2 v1 v2 v3 v4 v5 e7e6 e5 e4 Figure 3: Perfect matching encoding of the market in Figure 1.
Alternatively, we can solve the problem by encoding it as an ILP with one variable for each edge in the original market graph G. This ILP, given below, has the advantage that it can be extended naturally to deal with cycle length constraints. Therefore, for the rest of this section, this is the approach we will pursue. max e∈E wee such that for all vi ∈ V , the conservation constraint eout=(vi,vj ) eout − ein=(vj ,vi) ein = 0 and capacity constraint eout=(vi,vj ) eout ≤ 1 are satisfied.
If cycles are allowed to have length at most L, it is easy to see that we only need to make the following changes to the ILP. For each length-L path (throughout the paper, we do not include cycles in the definition of path) p = ep1 , ep2 , . . . , epL , add a constraint ep1 + ep2 + . . . + epL ≤ L − 1, which precludes path p from being in any feasible solution.
Unfortunately, in a market with only 1000 patients, the number of length-3 paths is in excess of 400 million, and so we cannot even construct this ILP without running out of memory.
Therefore, we use a tree search with an incremental formulation approach. Specifically, we use CPLEX, though 298 we add constraints as cutting planes during the tree search process. We begin with only a small subset of the constraints in the ILP. Since this ILP is small, CPLEX can solve its LP relaxation. We then check whether any of the missing constraints are violated by the fractional solution. If so, we generate a set of these constraints, add them to the ILP, and repeat. Even once all constraints are satisfied, there may be no integral solution matching the fractional upper bound, and even if there were, the LP solver might not find it.
In these cases, CPLEX branches on a variable (we used CPLEX"s default branching strategy), and generates one new search node corresponding to each of the children. At each node of the search tree that is visited, this process of solving the LP and adding constraints is repeated. Clearly, this approach yields an optimal solution once the tree search finishes.
We still need to explain the details of the constraint seeder (i.e., selecting which constraints to begin with) and the constraint generation (i.e., selecting which violated constraints to include). We describe these briefly in the next two subsections, respectively.
The main constraint seeder we developed forbids any path of length L − 1 that does not have an edge closing the cycle from its head to its tail. While it is computationally expensive to find these constraints, their addition focuses the search away from paths that cannot be in the final solution.
We also tried seeding the LP with a random collection of constraints from the ILP.
We experimented with several constraint generators. In each, given a fractional solution, we construct the subgraph of edges with positive value. This graph is much smaller than the original graph, so we can perform the following computations efficiently.
In our first constraint generator, we simply search for length-L paths with value sum more than L − 1. For any such path, we restrict its sum to be at most L − 1. Note that if there is a cycle c with length |c| > L, it could contain as many as |c| violating paths.
In our second constraint generator, we only add one constraint for such cycles: the sum of edges in the cycle can be at most |c|(L − 1)/L .
This generator made the algorithm slower, so we went in the other direction in developing our final generator. It adds one constraint per violating path p, and furthermore, it adds a constraint for each path with the same interior vertices (not counting the endpoints) as p. This improved the overall speed.
It turned out that even with these improvements, the edge formulation approach cannot clear a kidney exchange with 100 vertices in the time the cycle formulation (described later in Section 5) can clear one with 10,000 vertices. In other words, column generation based approaches turned out to be drastically better than constraint generation based approaches. Therefore, in the rest of the paper, we will focus on the cycle formulation and the column generation based approaches.
CYCLE FORMULATION In this section, we consider a formulation of the clearing problem as an ILP with one variable for each cycle. This encoding is based on the following classical algorithm for solving the directed cycle cover problem when cycles have length 2.
Given a market G = (V, E), construct a new graph on V with a weight wc edge for each cycle c of length 2. It is easy to see that matchings in this new graph correspond to cycle covers by length-2 cycles in the original market graph. Hence, the market clearing problem with L = 2 can be solved in polynomial time by finding a maximum-weight matching. c_1 v 1 v 2 v 3 v 4 c_3c_2 Figure 4: Maximum-weight matching encoding of the market in Figure 1.
We can generalize this encoding for arbitrary L. Let C(L) be the set of all cycles of G with length at most L. Then the following ILP finds the maximum-weight cycle cover by C(L) cycles: max c∈C(L) wcc subject to c:vi∈c c ≤ 1 ∀vi ∈ V with c ∈ {0, 1} ∀c ∈ C(L)
In this section, we consider the merits of the edge formulation and cycle formulation. The edge formulation can be solved in polynomial time when there are no constraints on the cycle size. The cycle formulation can be solved in polynomial time when the cycle size is at most 2.
We now consider the case of short cycles of length at most L, where L ≥ 3. Our tree search algorithms use the LP relaxation of these formulations to provide upper bounds on the optimal solution. These bounds help prune subtrees and guide the search in the usual ways.
Theorem 2. The LP relaxation of the cycle formulation weakly dominates the LP relaxation of the edge formulation.
Proof. Consider an optimal solution to the LP relaxation of the cycle formulation. We show how to construct an equivalent solution in the edge formulation. For each edge in the graph, set its value as the sum of values of all the cycles of which it is a member. Also, define the value of a vertex in the same manner. Because of the cycle constraints, the conservation and capacity constraints of the edge encoding are clearly satisfied. It remains to show that none of the path constraints are violated.
Let p be any length-L path in the graph. Since p has L−1 interior vertices (not counting the endpoints), the value sum of these interior vertices is at most L−1. Now, for any cycle c of length at most L, the number of edges it has in p, which we denote by ec(p), is at most the number of interior vertices it has in p, which we denote by vc(p). Hence,
È e∈p e = È c∈C(L) c∗ec(p) ≤ È c∈C(L) c∗vc(p) = È v∈p v = L−1. 299 The converse of this theorem is not true. Consider a graph which is simply a cycle with n edges. Clearly, the LP relaxation of the cycle formulation has optimal value 0, since there are no cycles of size at most L. However, the edge formulation has a solution of size n/2, with each edge having value 1/2.
Hence, the cycle formulation is tighter than the edge formulation. Additionally, for a graph with m edges, the edge formulation requires O(m3 ) constraints, while the cycle formulation requires only O(m2 ).
Table 2 shows how the number of cycles of length at most 3 grows with the size of the market. With one variable per cycle in the cycle formulation, CPLEX cannot even clear markets with 1,000 patients without running out of memory (see Figure 6). To address this problem, we used an incremental formulation approach.
The first step in LP-guided tree search is to solve the LP relaxation. Since the cycle formulation does not fit in memory, this LP stage would fail immediately without an incremental formulation approach. However, motivated by the observation that an exchange solution can include only a tiny fraction of the cycles, we explored the approach of using column (i.e., cycle) generation.
The idea of column generation is to start with a restricted LP containing only a small number of columns (variables, i.e., cycles), and then to repeatedly add columns until an optimal solution to this partially formulated LP is an optimal solution to the original (aka master) LP. We explain this further by way of an example.
Consider the market in Figure 1 with L = 2. Figure 5 gives the corresponding master LP, P, and its dual, D.
Primal P max 2c1 +2c2 +2c3 s.t. c1 ≤ 1 (v1) c1 +c2 ≤ 1 (v2) +c2 +c3 ≤ 1 (v3) +c3 ≤ 1 (v4) with c1, c2, c3 ≥ 0 Dual D min v1 +v2 +v3 +v4 s.t v1 +v2 ≥ 2 (c1) +v2 +v3 ≥ 2 (c2) +v3 +v4 ≥ 2 (c3) with v1, v2, v3, v4 ≥ 0 Figure 5: Cycle formulation.
Let P be the restriction of P containing columns c1 and c3 only. Let D be the dual of P , that is, D is just D without the constraint c2. Because P and D are small, we can solve them to obtain OPT(P ) = OPT(D ) = 4, with cOP T (P ) = c1 = c3 = 1 and vOP T (D ) = v1 = v2 = v3 = v4 = 1 .
While cOP T (P ) must be a feasible solution of P, it turns out (fortunately) that vOP T (D ) is feasible for D, so that OPT(D ) ≥ OPT(D). We can verify this by checking that vOP T (D ) satisfies the constraints of D not already in Di.e. constraint c2. It follows that OPT(P ) = OPT(D ) ≥ OPT(D) = OPT(P), and so vOP T (P ) is provably an optimal solution for P, even though P is contains a only strict subset of the columns of P.
Of course, it may turn out (unfortunately) that vOP T (D ) is not feasible for D. This can happen above if vOP T (D ) = v1 = 2, v2 = 0, v3 = 0, v4 = 2 . Although we can still see that OPT(D ) = OPT(D), in general we cannot prove this because D and P are too large to solve. Instead, because constraint c2 is violated, we add column c2 to P , update D , and repeat. The problem of finding a violated constraint is called the pricing problem. Here, the price of a column (cycle in our setting) is the difference between its weight, and the dual-value sum of the cycle"s vertices. If any column of P has a positive price, its corresponding constraint is violated and we have not yet proven optimality. In this case, we must continue generating columns to add to P .
For smaller instances, we can maintain an explicit collection of all feasible cycles. This makes the pricing problem easy and efficient to solve: we simply traverse the collection of cycles, and look for cycles with positive price. We can even find cycles with the most positive price, which are the ones most likely to improve the objective value of restricted LP [1]. This approach does not scale however. A market with 5000 patients can have as many as 400 million cycles of length at most 3 (see Table 2). This is too many cycles to keep in memory.
Hence, for larger instances, we have to generate feasible cycles while looking for one with a positive price. We do this using a depth-first search algorithm on the market graph (see Figure 1). In order to make this search faster, we explore vertices in non-decreasing value order, as these vertices are more likely to belong to cycles with positive weight. We also use several pruning rules to determine if the current search path can lead to a positive weight cycle. For example, at a given vertex in the search, we can prune based on the fact that every vertex we visit from this point onwards will have value at least as great the current vertex.
Even with these pruning rules, column generation is a bottleneck. Hence, we also implemented the following optimizations.
Whenever the search exhaustively proves that a vertex belongs to no positive price cycle, we mark the vertex and do not use it as the root of a depth-first search until its dual value decreases. In this way, we avoid unnecessarily repeating our computational efforts from a previous column generation iteration.
Finally, it can sometimes be beneficial for column generation to include several positive-price columns in one iteration, since it may be faster to generate a second column, once the first one is found. However, we avoid this for the following reason. If we attempt to find more positive-price columns than there are to be found, or if the columns are far apart in the search space, we end up having to generate and check a large part of the collection of feasible cycles. In our experiments, we have seen this occur in markets with hundreds of millions of cycles, resulting in prohibitively expensive computation costs.
Even if there is only a small gap to the master LP relaxation, column generation requires many iterations to improve the objective value of the restricted LP. Each of these 300 iterations is expensive, as we must solve the pricing problem, and re-solve the restricted LP. Hence, although we could begin with no columns in the restricted LP, it is much faster to seed the LP with enough columns that the optimal objective value is not too far from the master LP. Of course, we cannot include so many columns that we run out of memory.
We experimented with several column seeders. In one class of seeder, we use a heuristic to find an exchange, and then add the cycles of that exchange to the initial restricted LP. We implemented two heuristics. The first is a greedy algorithm: for each vertex in a random order, if it is uncovered, we attempt to include a cycle containing it and other uncovered vertices. The other heuristic uses specialized maximum-weight matching code [16] to find an optimal cover by length-2 cycles.
These heuristics perform extremely well, especially taking into account the fact that they only add a small number of columns. For example, Table 1 shows that an optimal cover by length-2 cycles has almost as much weight as the exchange with unrestricted cycle size. However, we have enough memory to include hundreds-of-thousands of additional columns and thereby get closer still to the upper bound.
Our best column seeder constructs a random collection of feasible cycles. Since a market with 5000 patients can have as many as 400 million feasible cycles, it takes too long to generate and traverse all feasible cycles, and so we do not include a uniformly random collection. Instead, we perform a random walk on the market graph (see, for example, Figure 1), in which, after each step of the walk, we test whether there is an edge back onto our path that forms a feasible cycle. If we find a cycle, it is included in the restricted LP, and we start a new walk from a random vertex. In our experiments (see Section 6), we use this algorithm to seed the LP with 400,000 cycles.
This last approach outperforms the heuristic seeders described above. However, in our algorithm, we use a combination that takes the union of all columns from all three seeders. In Figure 6, we compare the performance of the combination seeder against the combination without the random collection seeder. We do not plot the performance of the algorithm without any seeder at all, because it can take hours to clear markets we can otherwise clear in a few minutes.
Recall that our aim is to find an optimal solution to the master LP relaxation. Using column generation, we can prove that a restricted-primal solution is optimal once all columns have non-positive prices. Unfortunately though, our clearing problem has the so-called tailing-off effect [1,
Section 6.3], in which, even though the restricted primal is optimal in hindsight, a large number of additional iterations are required in order to prove optimality (i.e., eliminate all positive-price columns). There is no good general solution to the tailing-off effect.
However, to mitigate this effect, we take advantage of the following problem-specific observation. Recall from Section 1.1 that, almost always, a maximum-weight exchange with cycles of length at most 3 has the same weight as an unrestricted maximum-weight exchange. (This does not mean that the solver for the unrestricted case will find a solution with short cycles, however.) Furthermore, the unrestricted clearing problem can be solved in polynomial time (recall Section 4). Hence, we can efficiently compute an upper bound on the master LP relaxation, and, whenever the restricted primal achieves this upper bound, we have proven optimality without necessarily having to eliminate all positive-price columns! In order for this to improve the running time of the overall algorithm, we need to be able to clear the unrestricted market in less time than it takes column generation to eliminate all the positive-price cycles. Even though the first problem is polynomial-time solvable, this is not trivial for large instances. For example, for a market with 10,000 patients and 25 million edges, specialized maximum-weight matching code [16] was too slow, and CPLEX ran out of memory on the edge formulation encoding from Section 4. To make this idea work then, we used column generation to solve the edge formulation.
This involves starting with a small random subset of the edges, and then adding positive price edges one-by-one until none remain. We conduct this secondary column generation not in the original market graph G, but in the perfect matching bipartite graph of Figure 3. We do this so that we only need to solve the LP, not the ILP, since the integrality gap in the perfect matching bipartite graph is 1-i.e. there always exists an integral solution that achieves the fractional upper bound.
The resulting speedup to the overall algorithm is dramatic, as can be seen in Figure 6.
If the optimal value of the initial restricted LP P is far from the the master LP P, then a large number of columns are generated before the gap is closed. This leads to memory problems on markets with as few as 4,000 patients. Also, even before memory becomes an issue, the column generation iterations become slow because of the additional overhead of solving a larger LP.
To address these issues, we implemented a column management scheme to limit the size of the restricted LP.
Whenever we add columns to the LP, we check to see if it contains more than a threshold number of columns. If this is the case, we selectively remove columns until it is again below the threshold2 . As we discussed earlier, only a tiny fraction of all the cycles will end up in the final solution. It is unlikely that we delete such a cycle, and even if we do, it can always be generated again. Of course, we must not be too aggressive with the threshold, because doing so may offset the per-iteration performance gains by significantly increasing the number of iterations required to get a suitable column set in the LP at the same time.
There are some columns we never delete, for example those we have branched on (see Section 5.3.2), or those with a non-zero LP value. Amongst the rest, we delete those with the lowest price, since those correspond to the dual constraints that are most satisfied. This column management scheme works well and has enabled us to clear markets with 10,000 patients, as seen in Figure 6.
Given a large market clearing problem, we can successfully solve its LP relaxation to optimality by using the column generation enhancements described above. However, the solutions we find are usually fractional. Thus the next 2 Based on memory size, we set the threshold at 400,000. 301 step involves performing a branch-and-price tree search [1] to find an optimal integral solution.
Briefly, this is the idea of branch-and-price. Whenever we set a fractional variable to 0 or 1 (branch), both the master LP, and the restriction we are working with, are changed (constrained). By default then, we need to perform column generation (go through the effort of pricing) at each node of the search tree to prove that the constrained restriction is optimal for constrained master LP. (However, as discussed in Section 5.2.3, we compute the integral upper bound for the root node based on relaxing the cycle length constraint completely, and whenever any node"s LP in the tree achieves that value, we do not need to continue pricing columns at that node.) For the clearing problem with cycles of length at most 3, we have found that there is rarely a gap between the optimal integral and fractional solutions. This means we can largely avoid the expensive per node pricing step: whenever the constrained restricted LP has the same optimal value as its parent in the tree search, we can prove LP optimality, as in Section 5.2.3, without having to include any additional columns in the restricted LP.
Although CPLEX can solve ILPs, it does not support branch-and-price (for example, because there can be problemspecific complications involving the interaction between the branching rule and the pricing problem). Hence, we implemented our own branch-and-price algorithm, which explores the search tree in depth-first order. We also experimented with the A* node selection order [7, 2]. However, this search strategy requires significantly more memory, which we found was better employed in making the column generation phase faster (see Section 5.2.2). The remaining major components of the algorithm are described in the next two subsections.
Before branching on a fractional variable, we use primal heuristics to construct a feasible integral solution. These solutions are lower bounds on the final optimal integral solutions. Hence, whenever a restricted fractional solution is no better than the best integral solution found so far, we prune the current subtree. A primal heuristic is effective if it is efficient and constructs tight lower bounds.
We experimented with two primal heuristics. The first is a simple rounding algorithm [8]: include all cycles with fractional value at least 0.5, and then, ensuring feasibility, greedily add the remaining cycles. Whilst this heuristic is efficient, we found that the lower bounds it constructs rarely enable much pruning.
We also tried using CPLEX as a primal heuristic. At any given node of the search tree, we can convert the restricted LP relaxation back to an ILP by reintroducing the integrality constraints. CPLEX has several built-in primal heuristics, which we can apply to this ILP. Moreover, we can use CPLEX"s own tree search to find an optimal integral solution. In general, this tree search is much faster than our own.
If CPLEX finds an integral solution that matches the fractional upper bound at the root node, we are done.
Otherwise, no such integral solution exists, or we don"t yet have the right combination of cycles in the restricted LP. For kidney-exchange markets, it is usually the second reason that applies (see Sections 5.2.2 and 5.2.4). Hence, at some point in the tree search, once more columns have been generated as a result of branching, the CPLEX heuristic will find an optimal integral solution.
Although CPLEX tree search is faster than our own, it is not so fast that we can apply it to every node in our search tree. Hence, we make the following optimizations. Firstly, we add a constraint that requires the objective value of the ILP to be as large as the fractional target. If this is not the case, we want to abort and proceed to generate more columns with our branch-and-price search. Secondly, we limit the number of nodes in CPLEX"s search tree. This is because we have observed that no integral solution exists,
CPLEX can take a very long time to prove that. Finally, we only apply the CPLEX heuristic at a node if it has a sufficiently different set of cycles from its parent.
Using CPLEX as a primal heuristic has a large impact because it makes the search tree smaller, so all the computationally expensive pricing work is avoided at nodes that are not generated in this smaller tree.
We experimented with two branching strategies, both of which select one variable per node. The first strategy, branching by certainty, randomly selects a variable from those whose LP value is closest to 1. The second strategy, branching by uncertainty, randomly selects a variable whose LP value is closest to 0.5. In either case, two children of the node are generated corresponding to two subtrees, one in which the variable is set to 0, the other in which it is set to 1. Our depth-first search always chooses to explore first the subtree in which the value of the variable is closest to its fractional value.
For our clearing problem with cycles of length at most 3, we found branching by uncertainty to be superior, rarely requiring any backtracking.
All our experiments were performed in Linux (Red Hat
processor, and 1GB of RAM. Wherever we used CPLEX (e.g., in solving the LP and as a primal heuristic, as discussed in the previous sections), we used CPLEX 10.010.
Figure 6 shows the runtime performance of four clearing algorithms. For each market size listed, we randomly generated 10 markets, and attempted to clear them using each of the algorithms.
The first algorithm is CPLEX on the full cycle formulation. This algorithm fails to clear any markets with 1000 patients or more. Also, its running time on markets smaller than this is significantly worse than the other algorithms.
The other algorithms are variations of the incremental column generation approach described in Section 5. We begin with the following settings (all optimizations are switched on): Category Setting Column Seeder Combination of greedy exchange and maximum-weight matching heuristics, and random walk seeder (400,000 cycles).
Column Generation One column at a time.
Column Management On, with 400,000 column limit.
Optimality Prover On.
Primal Heuristic Rounding & CPLEX tree search.
Branching Rule Uncertainty. 302 The combination of these optimizations allows us to easily clear markets with over 10,000 patients. In each of the next two algorithms, we turn one of these optimizations off to highlight its effectiveness.
First, we restrict the seeder so that it only begins with 10,000 cycles. This setting is faster for smaller instances, since the LP relaxations are smaller, and faster to solve.
However, at 5000 vertices, this effect starts to be offset by the additional column generation that must be performed.
For larger instance, this restricted seeder is clearly worse.
Finally, we restore the seeder to its optimized setting, but this time, remove the optimality prover described in Section 5.2.3. As in many column generation problems, the tailing-off effect is substantial. By taking advantage of the properties of our problem, we manage to clear a market with 10,000 patients in about the same time it would otherwise have taken to clear a 6000 patient market.
Our algorithm and implementation replaced CPLEX as the clearing algorithm of the Alliance for Paired Donation, one of the leading kidney exchanges, in December 2006. We conduct a match run every two weeks, and the first transplants based on our solutions have already been conducted.
While there are (for political/inter-personal reasons) at least four kidney exchanges in the US currently, everyone understands that a unified unfragmented national exchange would save more lives. We are in discussions with additional kidney exchanges that are interested in adopting our technology. This way our technology (and the processes around it) will hopefully serve as a substrate that will eventually help in unifying the exchanges. At least computational scalability is no longer an obstacle.
In this work we have developed the most scalable exact algorithms for barter exchanges to date, with special focus on the upcoming national kidney-exchange market in which patients with kidney disease will be matched with compatible donors by swapping their own willing but incompatible donors. With over 70,000 patients already waiting for a cadaver kidney in the US, this market is seen as the only ethical way to significantly reduce the 4,000 deaths per year attributed to kidney disease.
Our work presents the first algorithm capable of clearing these markets on a nationwide scale. It optimally solves the kidney exchange clearing problem with 10,000 donordonee pairs. Thus there is no need to resort to approximate solutions. The best prior technology (vanilla CPLEX) cannot handle instances beyond about 900 donor-donee pairs because it runs out of memory. The key to our improvement is incremental problem formulation. We adapted two paradigms for the task: constraint generation and column generation. For each, we developed a host of techniques that substantially improve both runtime and memory usage. Some of the techniques use domain-specific observations while others are domain independent. We conclude that column generation scales dramatically better than constraint generation. For column generation in the LP, our enhancements include pricing techniques, column seeding techniques, techniques for proving optimality without having to bring in all positive-price columns (and using another column-generation process in a different formulation to do so), and column removal techniques. For the branch-andprice search in the integer program that surrounds the LP, our enhancements include primal heuristics and we also compared branching strategies. Undoubtedly, further parameter tuning and perhaps additional speed improvement techniques could be used to make the algorithm even faster.
Our algorithm also supports several generalizations, as desired by real-world kidney exchanges. These include multiple alternative donors per patient, weighted edges in the market graph (to encode differences in expected life years added based on degrees of compatibility, patient age and weight, etc., as well as the probability of last-minute incompatibility), angel-triggered chains (chains of transplants triggered by altruistic donors who do not have patients associated with them, each chain ending with a left-over kidney), and additional issues (such as different scores for saving different altruistic donors or left-over kidneys for future match runs based on blood type, tissue type, and likelihood that the organ would not disappear from the market by the donor getting second thoughts). Because we use an ILP methodology, we can also support a variety of side constraints, which often play an important role in markets in practice [19]. We can also support forcing part of the allocation, for example,
This acutely sick teenager has to get a kidney if possible.
Our work has treated the kidney exchange as a batch problem with full information (at least in the short run, kidney exchanges will most likely continue to run in batch mode every so often). Two important directions for future work are to explicitly address both online and limited-information aspects of the problem.
The online aspect is that donees and donors will be arriving into the system over time, and it may be best to not execute the myopically optimal exchange now, but rather save part of the current market for later matches. In fact, some work has been done on this in certain restricted settings [22, 24].
The limited-information aspect is that even in batch mode, the graph provided as input is not completely correct: a number of donor-donee pairs believed to be compatible turn out to be incompatible when more expensive last-minute tests are performed. Therefore, it would be desirable to perform an optimization with this in mind, such as outputting a low-degree robust subgraph to be tested before the final match is produced, or to output a contingency plan in case of failure. We are currently exploring a number of questions along these lines but there is certainly much more to be done.
Acknowledgments We thank economists Al Roth and Utku Unver, as well as kidney transplant surgeon Michael Rees, for alerting us to the fact that prior technology was inadequate for the clearing problem on a national scale, supplying initial data sets, and discussions on details of the kidney exchange process.
We also thank Don Sheehy for bringing to our attention the idea of shoe exchange. This work was supported in part by the National Science Foundation under grants IIS-0427858 and CCF-0514922.
[1] C. Barnhart, E. L. Johnson, G. L. Nemhauser,
M. W. P. Savelsbergh, and P. H. Vance. 303 0 500 1000 1500 2000 2500 3000 3500 4000 0 2000 4000 6000 8000 10000 Clearingtime(seconds) Number of patients Our algorithm Our algorithm with restricted column seeder Our algorithm with no optimality prover CPLEX cycle formulation Figure 6: Experimental results: average runtime with standard deviation bars.
Branch-and-price: Column generation for solving huge integer programs. Operations Research, 46:316-329,
May-June 1998. [2] R. Dechter and J. Pearl. Generalized best-first search strategies and the optimality of A*. Journal of the ACM, 32(3):505-536, 1985. [3] F. L. Delmonico. Exchanging kidneys - advances in living-donor transplantation. New England Journal of Medicine, 350:1812-1814, 2004. [4] J. Edmonds. Path, trees, and flowers. Canadian Journal of Mathematics, 17:449-467, 1965. [5] M. R. Garey and D. S. Johnson. Computers and Intractability; A Guide to the Theory of NP-Completeness. 1990. [6] S. E. Gentry, D. L. Segev, and R. A. Montgomery. A comparison of populations served by kidney paired donation and list paired donation. American Journal of Transplantation, 5(8):1914-1921, August 2005. [7] P. Hart, N. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100-107, 1968. [8] K. Hoffman and M. Padberg. Solving airline crew-scheduling problems by branch-and-cut.
Management Science, 39:657-682, 1993. [9] Intervac. http://intervac-online.com/. [10] National odd shoe exchange. http://www.oddshoe.org/. [11] Peerflix. http://www.peerflix.com. [12] Read it swap it. http://www.readitswapit.co.uk/. [13] A. E. Roth, T. Sonmez, and M. U. Unver. Kidney exchange. Quarterly Journal of Economics, 119(2):457-488, May 2004. [14] A. E. Roth, T. Sonmez, and M. U. Unver. A kidney exchange clearinghouse in New England. American Economic Review, 95(2):376-380, May 2005. [15] A. E. Roth, T. Sonmez, and M. U. Unver. Efficient kidney exchange: Coincidence of wants in a market with compatibility-based preferences. American Economic Review, forthcoming. [16] E. Rothberg. Gabow"s n3 maximum-weight matching algorithm: an implementation. The First DIMACS Implementation Challenge, 1990. [17] S. L. Saidman, A. E. Roth, T. Snmez, M. U. Unver, and F. L. Delmonico. Increasing the opportunity of live kidney donation by matching for two and three way exchanges. Transplantation, 81(5):773-782, 2006. [18] T. Sandholm. Optimal winner determination algorithms. In Combinatorial Auctions, Cramton,
Shoham, and Steinberg, eds. MIT Press, 2006. [19] T. Sandholm and S. Suri. Side constraints and non-price attributes in markets. In IJCAI-2001 Workshop on Distributed Constraint Reasoning, pages 55-61, Seattle, WA, 2001. To appear in Games and Economic Behavior. [20] D. L. Segev, S. E. Gentry, D. S. Warren, B. Reeb, and R. A. Montgomery. Kidney paired donation and optimizing the use of live donor organs. Journal of the American Medical Association, 293(15):1883-1890,

Markets have long been used as a medium for trade. As a side effect of trade, the participants in a market reveal something about their preferences and beliefs. For example, in a financial market, agents would buy shares which they think are undervalued, and sell shares which they think are overvalued. It has long been observed that, because the market price is influenced by all the trades taking place, it aggregates the private information of all the traders. Thus, in a situation in which future events are uncertain, and each trader might have a little information, the aggregated information contained in the market prices can be used to predict future events. This has motivated the creation of information markets, which are mechanisms for aggregating the traders" information about an uncertain event.
Information markets can be modeled as a game in which the participants bet on a number of possible outcomes, such as the results of a presidential election, by buying shares of the outcomes and receiving payoffs when the outcome is realized. As in financial markets, the participants aim to maximize their profit by buying low and selling high. In this way, the players" behavior transmits their personal information and beliefs about the possible outcomes, and can be used to predict the event more accurately. The benefit of well-designed information markets goes beyond information aggregation; they can also be used as a hedging instrument, to allow traders to insure against risk.
Recently, researchers have turned to the problem of designing market structures specifically to achieve better information aggregation properties than traditional markets.
Two designs for information markets have been proposed: the Dynamic Parimutuel Market (DPM) by Pennock [10] and the Market Scoring Rules (MSR) by Hanson [6]. Both the DPM and the MSR were designed with the goal of giving informed traders an incentive to trade, and to reveal their information as soon as possible, while also controlling the subsidy that the market designer needs to pump into the market.
The DPM was created as a combination of a pari-mutuel market (which is commonly used for betting on horses) and a continuous double auction, in order to simultaneously obtain the first one"s infinite buy-in liquidity and the latter"s ability to react continuously to new information. One version of the DPM was implemented in the Yahoo! Buzz market [8] to experimentally test the market"s prediction properties. The foundations of the MSR lie in the idea of a proper scoring rule, which is a technique to reward forecasters in a way that encourages them to give their best prediction. 316 The innovation in the MSR is to use these scoring rules as instruments that can be traded, thus providing traders who have new information an incentive to trade. The MSR was to be used in a policy analysis market in the Middle East [15], which was subsequently withdrawn.
Information markets rely on informed traders trading for their own profit, so it is critical to understand the strategic properties of these markets. This is not an easy task, because markets are complex, and traders can influence each other"s beliefs through their trades, and hence, can potentially achieve long term gains by manipulating the market.
For the MSR, it has been shown that, if we exclude the possibility of achieving gain through misleading other traders, it is optimal for each trader to honestly reflect her private belief in her trades. For the DPM, we are not aware of any prior strategic analysis of this nature; in fact, a strategic hole was discovered while testing the DPM in the Yahoo! Buzz market [8].
In this paper, we seek to develop an analytic method to guide the design and strategic analysis of information markets. Our central contribution is a new abstract betting game, the projection 1 game, that serves as a useful model for information markets. The projection game is conceptually simpler than the MSR and DPM, and thus it is easier to analyze. In addition it has an attractive geometric visualization, which makes the strategic moves and interactions more transparent. We present an analysis of the optimal strategies and profits in this game.
We then undertake an analysis of traders" costs and profits in the dynamic parimutuel market. Remarkably, we find that the cost of a sequence of trades in the DPM is identical to the cost of the corresponding moves in the projection game. Further, if we assume that the traders beliefs at the end of trading match the true probability of the event being predicted, the traders" payoffs and profits in the DPM are identical to their payoffs and profits in a corresponding projection game. We use the equivalence between the DPM and the projection game to prove that the DPM is arbitrage-free, deduce profitable strategies in the DPM, and demonstrate that constraints on the agents" trades are necessary to prevent a strategic breakdown.
We also prove an equivalence between the projection game and the MSR: We show that play in the MSR is strategically equivalent to play in a restricted projection game, at least for myopic strategies and small trades. In particular, the profitability of any move under the spherical scoring rule is exactly proportional to the profitability of the corresponding move in the projection game restricted to a circle, with slight distortion of the prior probabilities. This allows us to use the projection game as a conceptual model for market scoring rules.
We note that while the MSR with the spherical scoring rule somewhat resembles the projection game, due to the mathematical similarity of their profit expressions, the DPM model is markedly different and thus its equivalence to the projection game is especially striking. Further, because the restricted projection game corresponds to a DPM with a natural trading constraint, this sheds light on an intriguing connection between the MSR and the DPM. 1 In an earlier version of this paper, we called this the segment game.
Lastly, we illustrate how the projection game model can be used to analyze the potential for manipulation of information markets for long-term gain.2 We present an example scenario in which such manipulation can occur, and suggest additional rules that might mitigate the possibility of manipulation. We also illustrate another application to analyzing how a market maker can improve the prediction accuracy of a market in which traders will not trade unless their expected profit is above a threshold.
Numerous studies have demonstrated empirically that market prices are good predictors of future events, and seem to aggregate the collected wisdom of all the traders [2, 3, 12, 1, 5, 16]. This effect has also been demonstrated in laboratory studies [13, 14], and has theoretical support in the literature of rational expectations [9].
A number of recent studies have addressed the design of the market structure and trading rules for information markets, as well as the incentive to participate and other strategic issues. The two papers most closely related to our work are the papers by Hanson [6] and Pennock [10]. However, strategic issues in information markets have also been studied by Mangold et al. [8] and by Hanson, Oprea and Porter [7]. An upcoming survey paper [11] discusses costfunction formulations of automated market makers.
Organization of the paper The rest of this paper is organized as follows: In Section 2, we describe the projection game, and analyze the players" costs, profits, and optimal strategies in this game. In Section 3, we study the dynamic parimutuel market, and show that trade in a DPM is equivalent to a projection game. We establish a connection between the projection game and the MSR in Section 4.
In Section 5, we illustrate how the projection game can be used to analyze non-myopic, and potentially manipulative, actions. We present our conclusions, and suggestions for future work, in Section 6.
In this section, we describe an abstract betting game, the projection game; in the following sections, we will argue that both the MSR and the DPM are strategically similar to the projection game. The projection game is conceptually simpler than MSR and DPM, and hence should prove easier to analyze. For clarity of exposition, here and in the rest of the paper we assume the space is two dimensional, i.e., there are only two possible events. Our results easily generalize to more than two dimensions. We also assume throughout that players are risk-neutral.
Suppose there are two mutually exclusive and exhaustive events, A and B. (In other words, B is the same as not A.) There are n agents who may have information about the likelihood of A and B, and we (the designers) would like to aggregate their information. We invite them to play the game described below: At any point in the game, there is a current state described by a pair of parameters, (x, y), which we sometimes write in vector form as x. Intuitively, x corresponds to the 2 Here, we are referring only to manipulation of the information market for later gain from the market itself; we do not consider the possibility of traders having vested interests in the underlying events. 317 total holding of shares in A, and y corresponds to the holding of shares in B. In each move of the game, one player (say i) plays an arrow (or segment) from (x, y) to (x , y ).
We use the notation [(x, y) → (x , y )] or [x, x ] to denote this move. The game starts at (0, 0), but the market maker makes the first move; without loss of generality, we can assume the move is to (1, 1). All subsequent moves are made by players, in an arbitrary (and potentially repeating) sequence.
Each move has a cost associated with it, given by C[x, x ] = |x | − |x|, where | · | denotes the Euclidean norm, |x| = p x2 + y2.
Note that none of the variables are constrained to be nonnegative, and hence, the cost of a move can be negative.
The cost can be expressed in an alternative form, that is also useful. Suppose player i moves from (x, y) to (x , y ).
We can write (x , y ) as (x + lex, y + ley), such that l ≥ 0 and |ex|2 + |ey|2 = 1. We call l the volume of the move, and (ex, ey) the direction of the move. At any point (ˆx, ˆy), there is an instantaneous price charged, defined as follows: c((ˆx, ˆy), (ex, ey)) = ˆxex + ˆyey |(ˆx, ˆy)| = ˆx · e |ˆx| .
Note that the price depends only on the angle between the line joining the vector (ˆx, ˆy) and the segment [(x, y), (x , y )], and not the lengths. The total cost of the move is the price integrated over the segment [(x, y) → (x , y )], i.e.,
C[(x, y) → (x , y )] = Z l w=0 c((x+wex, y +wey), (ex, ey))dw We assume that the game terminates after a finite number of moves. At the end of the game, the true probability p of event A is determined, and the agents receive payoffs for the moves they made. Let q = (qx, qy) = (p,1−p) |(p,1−p)| . The payoff to agent i for a segment [(x, y) → (x , y )] is given by: P([(x, y) → (x , y )]) = qx(x − x) + qy(y − y) = q.(x − x) We call the line through the origin with slope (1 − p)/p = qy/qx the p-line. Note that the payoff, too, may be negative.
One drawback of the definition of a projection game is that implementing the payoffs requires us to know the actual probability p. This is feasible if the probability can eventually be determined statistically, such as when predicting the relative frequency of different recurring events, or vote shares. It is also feasible for one-off events in which there is reason to believe that the true probability is either 0 or 1. For other one-off events, it cannot be implemented directly (unlike scoring rules, which can be implemented in expectation). However, we believe that even in these cases, the projection game can be useful as a conceptual and analytical tool.
The moves, costs and payoffs have a natural geometric representation, which is shown in Figure 1 for three players with one move each. The players append directed line segments in turn, and the payoff player i finally receives for a move is the projection of her segment onto the line with slope (1 − p)/p. Her cost is the difference of distances of the endpoints of her move to the origin.
We begin our strategic analysis of the projection game by observing the following simple path-independence property. 1−p p 3"s m ove 1"s payoff M M m ove 1"s move 2"smove 3"s payoff 2"s payoff x y Figure 1: A projection game with three players Lemma 1. [Path-Independence] Suppose there is a sequence of moves leading from (x, y) to (x , y ). Then, the total cost of all the moves is equal to the cost of the single move [(x, y) → (x , y )], and the total payoff of all the moves is equal to the payoff of the single move [(x, y) → (x , y )].
Proof. The proof follows trivially from the definition of the costs and payoffs: If we consider a path from point x to point x , both the net change in the vector lengths and the net projection onto the p-line are completely determined by x and x .
Although simple, path independence of profits is vitally important, because it implies (and is implied by) the absence of arbitrage in the market. In other words, there is no sequence of moves that start and end at the same point, but result in a positive profit. On the other hand, if there were two paths from (x, y) to (x , y ) with different profits, there would be a cyclic path with positive profit.
For ease of reference, we summarize some more useful properties of the cost and payoff functions in the projection game.
Lemma 2.
the origin is 1 or −1, when the move is away or toward the origin respectively. The instantaneous price along a circle centered at the origin is 0.
point ¯x on the positive p-line, the corresponding payoff is P(x, ¯x) = |x| − x · q, and the cost is C[x, ¯x] = 0.
C[x, x ] = Z l w=0 cos(x + we, e)dw = |x |−|x| ∀x, x , where e is the unit vector giving the direction of move.
In addition, when x moves along the positive p-line, the payoff is equal to the cost, P(x, x ) = |x | − |x|.
Proof. 1. The instantaneous price is c(x, e) = x · e/|x| = cos(x, e), where e is the direction of movement, and the result follows.
P(x, ¯x) = q · (¯x − x) = |x| − x · q; the cost is 0 from the definition. 318
is C[x, 0] = Z l w=0 cos(x + we, e)dw = Z l w=0 (−1)dw = −|x|, where l = |x|, e = x/|x|. By the path-independence property,
C[x, x ] = C[x, 0] + C[0, x ] = |x | − |x|.
Finally, a point on the positive p-line gets projected to itself, namely q · x = |x| so when the movement is along the positive p-line, P(x, x ) = q · (x − x) = |x | − |x| = C[x, x ].
We now consider the question of which moves are profitable in this game. The eventual profit of a move [x, x ], where x = x + l.(ex, ey), is profit[x, x ] = P[x, x ] − C[x, x ] = lq.e − C[x, x ] Differentiating with respect to l, we get d(profit) dl = q.e − c(x + le, e) = q.e − x + le |x + le| .e We observe that this is 0 if p(y + ley) = (1 − p)(x + lex), in other words, when the vectors q and (x + le) are exactly aligned. Further, we observe that the price is non-decreasing with increasing l. Thus, along the direction e, the profit is maximized at the point of intersection with the p-line.
By Lemma 2, there is always a path from x to the positive p-line with 0 cost, which is given by an arc of the circle with center at the origin and radius |x|. Also, any movement along the p-line has 0 additional profit. Thus, for any point x, we can define the profit potential φ(x, p) by φ(x, p) = |x| − x · q.
Note, the potential is positive for x off the positive p-line and zero for x on the line. Next we show that a move to a lower potential is always profitable.
Lemma 3. The profit of a move [x, x ] is equal to the difference in potential φ(x, p) − φ(x , p).
Proof. Denote z = |x|q and z = |x |q, i.e., these are the points of intersection of the positive p-line with the circles centered at the origin with radii |x| and |x | respectively.
By the path-independence property and Lemma 2, the profit of move [x, x ] is profit(x, x ) = profit(x, z) + profit(z, z ) + profit(z , x ) = (|x| − x · q) + 0 + (x · q − |x |) = φ(x, p) − φ(x , p).
Thus, the profit of the move is equal to the change in profit potential between the endpoints.
This lemma offers another way of seeing that it is optimal to move to the point of lowest potential, namely to the p-line. p y 1−p x x x" z z" profit = |x|−x.q profit = x".q−|x"| profit = 0 Figure 2: The profit of move [x, x ] is equal to the change in profit potential from x to x .
The dynamic parimutuel market (DPM) was introduced by Pennock [10] as an information market structure that encourages informed traders to trade early, has guaranteed liquidity, and requires a bounded subsidy. This market structure was used in the Yahoo! Buzz market [8]. In this section, we show that the dynamic parimutuel market is also remarkably similar to the projection game. Coupled with section 4, this also demonstrates a strong connection between the DPM and MSR.
In a two-event DPM, users can place bets on either event A or B at any time, by buying a share in the appropriate event. The price of a share is variable, determined by the total amount of money in the market and the number of shares currently outstanding. Further, existing shares can be sold at the current price. After it is determined which event really happens, the shares are liquidated for cash. In the total-money-redistributed variant of DPM, which is the variant used in the Yahoo! market, the total money is divided equally among the shares of the winning event; shares of the losing event are worthless. Note that the payoffs are undefined if the event has zero outstanding shares; the DPM rules should preclude this possibility.
We use the following notation: Let x be the number of outstanding shares of A (totalled over all traders), and y be the number of outstanding shares in B. Let M denote the total money currently in the market. Let cA and cB denote the prices of shares in A and B respectively. The price of a share in the Yahoo! DPM is determined by the share-ratio principle: cA cB = x y (1) The form of the prices can be fully determined by stipulating that, for any given value of M, x, and y, there must be some probability pA such that, if a trader believes that pA is the probability that A will occur and the market will liquidate in the current state, she cannot expect to profit from either buying or selling either share. This gives us cA = pA hM x i cB = pB hM y i 319 Since pA + pB = 1, we have: xcA + ycB = M (2) Finally, combining Equations 1 and 2, we get cA = x M x2 + y2 cB = y M x2 + y2 Cost of a trade in the DPM Consider a trader who comes to a DPM in state (M, x, y), and buys or sells shares such that the eventual state is (M , x , y ). What is the net cost, M − M, of her move?
Theorem 4. The cost of the move from (x, y) to (x , y ) is M − M = M0[ p x 2 + y 2 − p x2 + y2] for some constant M0. In other words, it is a constant multiple of the corresponding cost in the projection game.
Proof. Consider the function G(x, y) = M0[ p x2 + y2].
The function G is differentiable for all x, y = 0, and it"s partial derivatives are: ∂G ∂x = M0[ x p x2 + y2 ] = x G(x, y) x2 + y2 ∂G ∂y = M0[ y p x2 + y2 ] = y G(x, y) x2 + y2 Now, compare these equations to the prices in the DPM, and observe that, as a trader buys or sells in the DPM, the instantaneous price is the derivative of the money. It follows that, if at any point of time the DPM is in a state (M, x, y) such that M = G(x, y), then, at all subsequent points of time, the state (M , x , y ) of the DPM will satisfy M = G(x , y ). Finally, note that we can pick the constant M0 such that the equation is satisfied for the initial state of the DPM, and hence, it will always be satisfied.
One important consequence of Theorem 4 is that the dynamic parimutuel market is arbitrage-free (using Lemma 1).
It is interesting to note that the original Yahoo! Buzz market used a different pricing rule, which did permit arbitrage; the price rule was changed to the share-ratio rule after traders started exploiting the arbitrage opportunities [8]. Another somewhat surprising consequence is that the numbers of outstanding shares x, y completely determines the total capitalization M of the DPM.
Constraints in the DPM Although it might seem, based on the costs, that any move in the projection game has an equivalent move in the DPM, the DPM places some constraints on trades. Firstly, no trader is allowed to have a net negative holding in either share. This is important, because it ensures that the total holdings in each share are always positive. However, this is a boundary constraint, and does not impact the strategic choices for a player with a sufficiently large positive holding in each share. Thus, we can ignore this constraint from a first-order strategic analysis of the DPM. Secondly, for practical reasons a DPM will probably have a minimum unit of trade, but we assume here that arbitrarily small quantities can be traded.
Payoffs in the DPM At some point, trading in the DPM ceases and shares are liquidated. We assume here that the true probability becomes known at liquidation time, and describe the payoffs in terms of the probability; however, if the probability is not revealed, only the event that actually occurs, these payoffs can be implemented in expectation.
Suppose the DPM terminates in a state (M, x, y), and the true probability of event A is p. When the dynamic parimutuel market is liquidated, the shares are paid off in the following way: Each owner of a share of A receives pM x , and each owner of a share of B receives (1 − p)M y , for each share owned.
The payoffs in the DPM, although given by a fairly simple form, are conceptually complex, because the payoff of a move depends on the subsequent moves before the market liquidates. Thus, a fully rational choice of move in the DPM for player i should take into account the actions of subsequent players, including player i himself.
Here, we restrict the analysis to myopic, infinitesimal strategies: Given the market position is (M, x, y), in which direction should a player make an infinitesimal move in order to maximize her profit? We show that the infinitesimal payoffs and profits of a DPM with true probability p correspond strategically to the infinitesimal payoffs and profits of a projection game with odds p p/(1 − p), in the following sense: Lemma 5. Suppose player i is about to make a move in a dynamic parimutuel market in a state (M, x, y), and the true probability of event A is p. Then, assuming the market is liquidated after i"s move, • If x y < q p 1−p , player i profits by buying shares in A , or selling shares in B. • If x y > q p 1−p , player i profits by selling shares in A, or buying shares in B.
Proof. Consider the cost and payoff of buying a small quantity Δx of shares in A. The cost is C[(x, y) → (x + Δx, y)] = Δx · x M x2+y2 , and the payoff is Δx · pM x . Thus, buying the shares is profitable iff Δx · x M x2 + y2 < Δx · p M x ⇔ x2 x2 + y2 < p ⇔ x2 + y2 x2 > 1 p ⇔ 1 + ( y x )2 > 1 p ⇔ y x > r 1 − p p ⇔ x y < r p 1 − p Thus, buying A is profitable if x y < q p 1−p , and selling A is profitable if x y > q p 1−p . The analysis for buying or selling B is similar, with p and (1 − p) interchanged.
It follows from Lemma 5 that it is myopically profitable for players to move towards the line with slope q 1−p p . Note that there is a one-to-one mapping between 1−p p and q 1−p p 320 in their respective ranges, so this line is uniquely defined, and each such line also corresponds to a unique p.
However, because the actual payoff of a move depends on the future moves, players must base their decisions on some belief about the final state of the market. In the light of Lemma 5, one natural, rational-expectation style assumption is that the final state (M, x∗ , y∗ ) will satisfy x∗ y∗ = q p 1−p . (In other words, one might assume that the traders" beliefs will ultimately converge to the true probability p; knowing p, the traders will drive the market state to satisfy x y = q p 1−p .) This is very plausible in markets (such as the Yahoo! Buzz market) in which trading is permitted right until the market is liquidated, at which point there is no remaining uncertainty about the relevant frequencies. Under this assumption, we can prove an even tighter connection between payoffs in the DPM (where the true probability is p) and payoffs in the projection game, with odds q p 1−p : Theorem 6. Suppose that the DPM ultimately terminates in a state (M, X, Y ) satisfying X Y = q p 1−p . Assume without loss of generality that the constant M0 = 1, so M =√ X2 + Y 2. Then, the final payoff for any move [x → x ] made in the course of trading is (x − x) · ( √ p, √ 1 − p), i.e., it is the same as the payoff in the projection game with oddsq p 1−p .
Proof. First, observe that X M = √ p and Y M = √ 1 − p.
The final payoff is the liquidation value of (x − x) shares of A and (y − y) shares of B, which is PayoffDP M [x − x] = p M X (x − x) + (1 − p) M Y (y − y) = p 1 √ p (x − x) + (1 − p) 1 √ 1 − p (y − y) = √ p(x − x) + p 1 − p(y − y).
Strategic Analysis for the DPM Theorems 4 and 6 give us a very strong equivalence between the projection game and the dynamic parimutuel market, under the assumption that the DPM converges to the optimal value for the true probability. A player playing in a DPM with true odds p/(1 − p), can imagine himself playing in the projection game with odds q p 1−p , because both the costs and the payoffs of any given move are identical.
Using this equivalence, we can transfer all the strategic properties proven for the projection game directly to the analysis of the dynamic parimutuel market. One particularly interesting conclusion we can draw is as follows: In the absence of any constraint that disallows it, it is always profitable for an agent to move towards the origin, by selling shares in both A and B while maintaining the ratio x/y. In the DPM, this is limited by forbidding short sales, so players can never have negative holdings in either share. As a result, when their holding in one share (say A) is 0, they can"t use the strategy of moving towards the origin. We can conclude that a rational player should never hold shares of both A and B simultaneously, regardless of her beliefs and the market position.
This discussion leads us to consider a modified DPM, in which this strategic loophole is addressed directly: Instead of disallowing all short sales, we place a constraint that no agent ever reduce the total market capitalization M (or, alternatively, that any agent"s total investment in the market is always non-negative). We call this the nondecreasing market capitalization constraint for the DPM. This corresponds to a restriction that no move in the projection game reduces the radius. However, we can conclude from the preceding discussion that players have no incentive to ever increase the radius. Thus, the moves of the projection game would all lie on the quarter circle in the positive quadrant, with radius determined by the market maker"s move. In section 4, we show that the projection game on this quarter circle is strategically equivalent (at least myopically) to trade in a Market Scoring Rule. Thus, the DPM and MSR appear to be deeply connected to each other, like different interfaces to the same underlying game.
The Market Scoring Rule (MSR) was introduced by Hanson [6]. It is based on the concept of a proper scoring rule, a technique which rewards forecasters to give their best prediction. Hanson"s innovation was to turn the scoring rules into instruments that can be traded, thereby providing traders who have new information an incentive to trade. One positive effect of this design is that a single trader would still have incentive to trade, which is equivalent to updating the scoring rule report to reflect her information, thereby eliminating the problem of thin markets and illiquidity. In this section, we show that, when the scoring rule used is the spherical scoring rule [4], there is a strong strategic equivalence between the projection game and the market scoring rule.
Proper scoring rules are tools used to reward forecasters who predict the probability distribution of an event. We describe this in the simple setting of two exhaustive, mutually exclusive events A and B. In the simple setting of two exhaustive, mutually exclusive events A and B, proper scoring rules are defined as follows. Suppose the forecaster predicts that the probabilities of the events are r = (rA, rB), with rA + rB = 1. The scoring rule is specified by functions sA(rA, rB) and sB(rA, rB), which are applied as follows: If the event A occurs, the forecaster is paid sA(rA, rB), and if the event B occurs, the forecaster is paid sB(rA, rB). The key property that a proper scoring rule satisfies is that the expected payment is maximized when the report is identical to the true probability distribution.
In this section, we focus on one specific scoring rule: the spherical scoring rule [4].
Definition 1. The spherical scoring rule [4] is defined by si(r) def = ri/||r||. For two events, this can be written as: sA(rA, rB) = rA p r2 A + r2 B ; sB(rA, rB) = rB p r2 A + r2 B The spherical scoring rule is known to be a proper scoring rule. The definition generalizes naturally to higher dimensions.
We now demonstrate a close connection between the projection game restricted to a circular arc and a market scoring rule that uses the spherical scoring rule. At this point, it is 321 convenient to use vector notation. Let x = (x, y) denote a position in the projection game. We consider the projection game restricted to the circle |x| = 1.
Restricted projection game Consider a move in this restricted projection game from x to x . Recall that q = ( p √ p2+(1−p)2 , 1−p √ p2+(1−p)2 ), where p is the true probability of the event. Then, the projection game profit of a move [x, x ] is q · [x − x] (noting that |x| = |x |).
We can extend this to an arbitrary collection3 of (not necessarily contiguous) moves X = {[x1, x1], [x2, x2], · · · , [xl, xl]}.
SEG-PROFITp(X ) = X [x,x ]∈X q · [x − x] = q · 2 4 X [x,x ]∈X [x − x] 3 5 Spherical scoring rule profit We now turn our attention to the MSR with the spherical scoring rule (SSR).
Consider a player who changes the report from r to r . Then, if the true probability of A is p, her expected profit is SSR-PROFIT([r, r ]) = p(sA(r )−sA(r))+(1−p)(sB(r )−sB(r)) Now, let us represent the initial and final position in terms of circular coordinates. For r = (rA, rB), define the corresponding coordinates x = ( rA√ r2 A+r2 B , rB√ r2 A+r2 B ). Note that the coordinates satisfy |x| = 1, and thus correspond to valid coordinates for the restricted projection game.
Now, let p denote the vector [p, 1 − p]. Then, expanding the spherical scoring functions sA, sB, the player"s profit for a move from r to r can be rewritten in terms of the corresponding coordinates x, x as: SSR-PROFIT([x, x ]) = p · (x − x) For any collection X of moves, the total payoff in the SSR market is given by: SSR-PROFITp(X ) = X [x,x ]∈X p · [x − x] = p · 2 4 X [x,x ]∈X [x − x] 3 5 Finally, we note that p and q are related by q = μpp, where μp = 1/ p p2 + (1 − p)2 is a scalar that depends only on p. This immediately gives us the following strong strategic equivalence for the restricted projection game and the SSR market: Theorem 7. Any collection of moves X yields a positive (negative) payoff in the restricted projection game iff X yields a positive (negative) payoff in the Spherical Scoring Rule market.
Proof. As derived above,
SEG-PROFITp(X ) = μpSSR-PROFITp(X ).
For all p, 1 ≤ μp ≤ √ 2, (or more generally for an ndimensional probability vector p, 1 ≤ μp = 1 |p| ≤ √ n, by the arithmetic mean-root mean square inequality), and the result follows immediately. 3 We allow the collection to contain repeated moves, i.e., it is a multiset.
Although theorem 7 is stated in terms of the sign of the payoff, it extends to relative payoffs of two collections of moves: Corollary 8. Consider any two collections of moves X ,
X . Then, X yields a greater payoff than X in the projection game iff X yields a greater payment than X in the SSR market.
Proof. Every move [x, x ] has a corresponding inverse move [x , x]. In both the projection game and the SSR, the inverse move profit is simply the negative profit of the move (the moves are reversible). We can define a collection of moves X = X − X by adding the inverse of X to X . Note that SEG-PROFITp(X ) = SEG-PROFITp(X )−SEG-PROFITp(X ) and SSR-PROFITp(X ) = SSR-PROFITp(X )−SSR-PROFITp(X ); applying theorem 7 completes the proof.
It follows that the ex post optimality of a move (or set of moves) is the same in both the projection game and the SSR market. On its own, this strong ex post equivalence is not completely satisfying, because in any non-trivial game there is uncertainty about the value of p, and the different scaling ratios for different p could lead to different ex ante optimal behavior. We can extend the correspondence to settings with uncertain p, as follows: Theorem 9. Consider the restricted projection game with some prior probability distribution F over possible values of p. Then, there is a probability distribution G with the same support as F, and a strictly positive constant c that depends only on F such that: • (i) For any collection X of moves, the expected profits are related by: EF (SEG-PROFIT(X )) = cEG(SSR-PROFIT(X )) • (ii) For any collection X , and any measurable information set I ⊆ [0, 1], the expected profits conditioned on knowing that p ∈ I satisfy EF (SEG-PROFIT(X )|p ∈ I) = cEG(SSR-PROFIT(X )|p ∈ I) The converse also holds: For any probability distribution G, there is a distribution F such that both these statements are true.
Proof. For simplicity, assume that F has a density function f. (The result holds even for non-continuous distributions). Then, let c = R 1 0 μpf(p)dp. Define the density function g of distribution G by g(p) = μpf(p) c Now, for a collection of moves X ,
EF (SEG-PROFIT(X )) = Z SEG-PROFITp(X )f(p)dp = Z SSR-PROFITp(X )μpf(p)dp = Z SSR-PROFITp(X )cg(p)dp = cEG(SSR-PROFIT(X )) 322 −1 −0.5 0 0.5 1 1.5 2 2.5 −1 −0.5 0
1
2
x y log scoring rule quadratic scoring rule Figure 3: Sample score curves for the log scoring rule si(r) = ai + b log ri and the quadratic scoring rule si(r) = ai + b(2ri − P k r2 k).
To prove part (ii), we simply restrict the integral to values in I. The converse follows similarly by constructing F from G.
Analysis of MSR strategies Theorem 9 provides the foundation for analysis of strategies in scoring rule markets.
To the extent that strategies in these markets are independent of the specific scoring rule used, we can use the spherical scoring rule as the market instrument. Then, analysis of strategies in the projection game with a slightly distorted distribution over p can be used to understand the strategic properties of the original market situation.
Implementation in expectation Another important consequence of Theorem 9 is that the restricted projection game can be implemented with a small distortion in the probability distribution over values of p, by using a Spherical Scoring Rule to implement the payoffs. This makes the projection game valuable as a design tool; for example, we can analyze new constraints and rules in the projection game, and then implement them via the SSR. Unfortunately, the result does not extend to unrestricted projection games, because the relative profit of moving along the circle versus changing radius is not preserved through this transformation. However, it is possible to extend the transformation to projection games in which the radius ri after the ith move is a fixed function of i (not necessarily constant), so that it is not within the strategic control of the player making the move; such games can also be strategically implemented via the spherical scoring rule (with distortion of priors).
In this section, we show a weaker similarity between the projection game and the MSR with other scoring rules. We prove an infinitesimal similarity between the restricted projection game and the MSR with log scoring rule; the result generalizes to all proper scoring rules that have a unique local and global maximum.
A geometric visualization of some common scoring rules in two dimensions is depicted in Figure 3. The score curves in the figure are defined by {(s1(r), s2(r)) | r = (r, 1 − r), r ∈ [0, 1]}. Similarly to the projection game, define the profit potential of a probability r in MSR to be the change in profit for moving from r to the optimum p, φMSR(s(r), p) = profitMSR[s(r), s(p)]. We will show that the profit potentials in the two games have analogous roles for analyzing the optimal strategies, in particular both potential functions have a global minimum 0 at r = p.
Theorem 10. Consider the projection game restricted to the non-negative unit circle where strategies x have the natural one-to-one correspondence to probability distributions r = (r, 1 − r) given by x = ( r |r| , 1−r |r| ). Trade in a log market scoring rule is strategically similar to trade in the projection game on the quarter-circle, in that d dr φ(s(r), p) < 0 for r < p d dr φ(s(r), p) > 0 for r > p, both for the projection game and MSR potentials φ(.).
Proof. (sketch) The derivative of the MSR potential is d dr φ(s(r), p) = −p · d dr s(r) = − X i pisi(r).
For the log scoring rule si(r) = ai + b log ri with b > 0, d dr φMSR(s(r), p) = −p · b r , − b 1 − r  = −b p r − 1 − p 1 − r  = b r − p r(1 − r) .
Since r = (r, 1 − r) is a probability distribution, this expression is positive for r > p and negative for r < p as desired.
Now, consider the projection game on the non-negative unit circle. The potential for any x = ( r |r| , 1−r |r| ) is given by φ(x(r), p) = |x| − q · x(r),
It is easy to show that d dr φ(x(r), p) < 0 for r < p and the derivative is positive for r > p, so the potential function along the circle is decreasing and then increasing with r similarly to an energy function, with a global minimum at r = p, as desired.
Theorem 10 establishes that the market log-scoring rule is strategically similar to the projection game played on a circle, in the sense that the optimal direction of movement at the current state is the same in both games. For example, if the current state is r < p, it is profitable to move to r+dr since the effective profit of that move is profit(r, r ) = φ(s(r), p) − φ(s(r + dr), p) > 0. Although stated for logscoring rules, the theorem holds for any scoring rules that induce a potential with a unique local and global minimum at p, such as the quadratic scoring rule and others.
The chief advantages of the projection game are that it is analytically tractable, and also easy to visualize. In Section 3, we used the projection-game model of the DPM to prove the absence of arbitrage, and to infer strategic properties that might have been difficult to deduce otherwise.
In this section, we provide two examples that illustrate the power of projection-game analysis for gaining insight about more complex strategic settings. 323
The standard analysis of the trader behavior in any of the market forms we have studied asserts that traders who disagree with the market probabilities will expect to gain from changing the probability, and thus have a strict incentive to trade in the market. The expected gain may, however, be very small. A plausible model of real trader behavior might include some form of inertia or -optimality: We assume that traders will trade if their expected profit is greater than some constant . We do not attempt to justify this model here; rather, we illustrate how the projection game may be used to analyze such situations, and shed some light on how to modify the trading rules to alleviate this problem.
Consider the simple projection game restricted to a circular arc with unit radius; as we have seen, this corresponds closely to the spherical market scoring rule, and to the dynamic parimutuel market under a reasonable constraint. Now, suppose the market probability is p, and a trader believes the true probability is p . Then, his expected gain can be calculated, as follows: Let q and q be the unit vectors in the directions of p and p respectively. The expected profit is given by E = φ(q, p ) = 1− q ·q . Thus, the trader will trade only if 1−q·q > . If we let θ and θ be the angles of the p-line and p -line respectively (from the x-axis), we get E = 1 − cos(θ − θ ); when θ is close to θ , a Taylor series approximation gives us that E ≈ (θ − θ )2 /2. Thus, we can derive a bound on the limit of the market accuracy: The market price will not change as long as (θ − θ )2 ≤ 2 .
Now, suppose a market operator faced with this situation wanted to sharpen the accuracy of the market. One natural approach is simply to multiply all payoffs by a constant.
This corresponds to using a larger circle in the projection game, and would indeed improve the accuracy. However, it will also increase the market-maker"s exposure to loss: the market-maker would have to pump in more money to achieve this.
The projection game model suggests a natural approach to improving the accuracy while retaining the same bounds on the market maker"s loss. The idea is that, instead of restricting all moves to being on the unit circle, we force each move to have a slightly larger radius than the previous move. Suppose we insist that, if the current radius is r, the next trader has to move to r + 1. Then, the trader"s expected profit would be E = r(1 − cos(θ − θ )). Using the same approximation as above, the trader would trade as long as (θ − θ )2 > 2 /r. Now, even if the market maker seeded the market with r = 1, it would increase with each trade, and the incentives to sharpen the estimate increase with every trade.
Up to this point, our analysis has been restricted to trader strategies that are myopic in the sense that traders do not consider the impact of their trades on other traders" beliefs. In practice, an informed trader can potentially profit by playing a suboptimal strategy to mislead other traders, in a way that allows her to profit later. In this section, we illustrate how the projection game can be used to analyze an instance of this phenomenon, and to design market rules that mitigate this effect.
The scenario we consider is as follows. There are two traders speculating on the probability of an event E, who each get a 1-bit signal. The optimal probability for each 2bit signal pair is as follows. If trader 1 gets the signal 0, and trader 2 gets signal 0, the optimal probability is 0.3. If trader 1 got a 0, but trader 2 got a 1, the optimal probability is
probability is 0.7. If trader 1 got a 0, but trader 2 got a 1, the optimal probability is 0.1. (Note that the impact of trader 2"s signal is in a different direction, depending on trader 1"s signal). Suppose that the prior distribution of the signals is that trader 1 is equally likely to get a 0 or a 1, but trader 2 gets a 0 with probability 0.55 and a 1 with probability 0.45.
The traders are playing the projection game restricted to a circular arc. This setup is depicted in Figure 4.
A B D C X Y Signals Opt. Pt 00 C D11 10 01 Event does not happenEventhappens B A Figure 4: Example illustrating non-myopic deception Suppose that, for some exogenous reason, trader 1 has the opportunity to trade, followed by trader 2. Then, trader 1 has the option of placing a last-minute trade just before the market closes. If traders were playing their myopically optimal strategies, here is how the market should run: If trader 1 sees a 0, he would move to some point Y that is between A and C, but closer to C. Trader 2 would then infer that trader 1 received a 0 signal and move to A or C if she got 1 or 0 respectively. Trader 1 has no reason to move again. If trader 1 had got a 1, he would move to a different point X instead, and trader 2 would move to D if she saw 1 and B if she saw 0. Again, trader 1 would not want to move again.
Using the projection game, it is easy to show that, if traders consider non-myopic strategies, this set of strategies is not an equilibrium. The exact position of the points does not matter; all we need is the relative position, and the observation that, because of the perfect symmetry in the setup, segments XY, BC, and AD are all parallel to each other. Now, suppose trader 1 got a 0. He could move to X instead of Y , to mislead trader 2 into thinking he got a
correct the rating to A. To show that this is a profitable deviation, observe that this strategy is equivalent to playing two additional moves over trader 1"s myopic strategy of moving to Y . The first move, Y X may either move toward or away from the optimal final position. The second move,
DA or BC, is always in the correct direction. Further, because DA and BC are longer than XY , and parallel to XY , their projection on the final p-line will always be greater 324 in absolute value than the projection of XY , regardless of what the true p-line is! Thus, the deception would result in a strictly higher expected profit for trader 1. Note that this problem is not specific to the projection game form: Our equivalence results show that it could arise in the MSR or DPM (perhaps with a different prior distribution and different numerical values). Observe also that a strategy profile in which neither trader moved in the first two rounds, and trader 1 moved to either X or Y would be a subgame-perfect equilibrium in this setup.
We suggest that one approach to mitigating this problem might be by reducing the radius at every move. This essentially provides a form of discounting that motivates trader 1 to take his profit early rather than mislead trader 2.
Graphically, the right reduction factor would make the segments AD and BC shorter than XY (as they are chords on a smaller circle), thus making the myopic strategy optimal.
We have presented a simple geometric game, the projection game, that can serve as a model for strategic behavior in information markets, as well as a tool to guide the design of new information markets. We have used this model to analyze the cost, profit, and strategies of a trader in a dynamic parimutuel market, and shown that both the dynamic parimutuel market and the spherical market scoring rule are strategically equivalent to the restricted projection game under slight distortion of the prior probabilities.
The general analysis was based on the assumption that traders do not actively try to mislead other traders for future profit. In section 5, however, we analyze a small example market without this assumption. We demonstrate that the projection game can be used to analyze traders" strategies in this scenario, and potentially to help design markets with better strategic properties.
Our results raise several very interesting open questions.
Firstly, the payoffs of the projection game cannot be directly implemented in situations in which the true probability is not ultimately revealed. It would be very useful to have an automatic transformation of a given projection game into another game in which the payoffs can be implemented in expectation without knowing the probability, and preserves the strategic properties of the projection game. Second, given the tight connection between the projection game and the spherical market scoring rule, it is natural to ask if we can find as strong a connection to other scoring rules or if not, to understand what strategic differences are implied by the form of the scoring rule used in the market. Finally, the existence of long-range manipulative strategies in information markets is of great interest. The example we studied in section 5 merely scratches the surface of this area. A general study of this class of manipulations, together with a characterization of markets in which it can or cannot arise, would be very useful for the design of information markets.

Buying or selling a financial security in effect is a wager on the security"s value. For example, buying a stock is a bet that the stock"s value is greater than its current price. Each trader evaluates his expected profit to decide the quantity to buy or sell according to his own information and subjective probability assessment. The collective interaction of all bets leads to an equilibrium that reflects an aggregation of all the traders" information and beliefs. In practice, this aggregate market assessment of the security"s value is often more accurate than other forecasts relying on experts, polls, or statistical inference [16, 17, 5, 2, 15].
Consider buying a security at price fifty-two cents, that pays $1 if and only if a Democrat wins the 2008 US Presidential election. The transaction is a commitment to accept a fifty-two cent loss if a Democrat does not win in return for a forty-eight cent profit if a Democrat does win. In this case of an event-contingent security, the price-the market"s value of the security-corresponds directly to the estimated probability of the event.
Almost all existing financial and betting exchanges pair up bilateral trading partners. For example, one trader willing to accept an x dollar loss if a Democrat does not win in return for a y dollar profit if a Democrat wins is matched up with a second trader willing to accept the opposite.
However in many scenarios, even if no bilateral agreements exist among traders, multilateral agreements may be possible. For example, if one trader bets that the Democratic candidate will receive more votes than the Republican candidate, a second trader bets that the Republican candidate will receive more votes than the Libertarian candidate, and a third trader bets that the Libertarian candidate will receive more votes than the Democratic candidate, then, depending on the odds they each offer, there may be a three-way agreeable match even though no two-way matches exist.
We propose an exchange where traders have considerable flexibility to naturally and succinctly express their wagers, 326 and examine the computational complexity of the auctioneer"s resulting matching problem of identifying bilateral and multilateral agreements. In particular, we focus on a setting where traders bet on the outcome of a competition among n candidates. For example, suppose that there are n candidates in an election (or n horses in a race, etc.) and thus n! possible orderings of candidates after the final vote tally.
Traders may like to bet on arbitrary properties of the final ordering, for example candidate D will win, candidate D will finish in either first place or last place, candidate D will defeat candidate R, candidates D and R will both defeat candidate L, etc. The goal of the exchange is to search among all the offers to find two or more that together form an agreeable match. As we shall see, the matching problem can be set up as a linear or integer program, depending on whether orders are divisible or indivisible, respectively.
Attempting to reduce the problem to a bilateral matching problem by explicitly creating n! securities, one for each possible final ordering, is both cumbersome for the traders and computationally infeasible even for modest sized n.
Moreover, traders" attention would be spread among n! independent choices, making the likelihood of two traders converging at the same time and place seem remote.
There is a tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem. We want to offer traders the most expressive bidding language possible while maintaining computational feasibility. We explore two bidding languages that seem natural from a trader perspective. Subset betting, described in Section 3.2, allows traders to bet on which positions in the ranking a candidate will fall, for example candidate D will finish in position 1, 3-5, or 10. Symetrically, traders can also bet on which candidates will fall in a particular position. In Section 4, we derive a polynomial-time algorithm for matching (divisible) subset bets. The key to the result is showing that the exponentially big linear program has a corresponding separation problem that reduces to maximum weighted bipartite matching and consequently we can solve it in time polynomial in the number of orders.
Pair betting, described in Section 3.3, allows traders to bet on the final ranking of any two candidates, for example candidate D will defeat candidate R. In Section 5, we show that optimal matching of (divisible or indivisible) pair bets is NP-hard, via a reduction from the unweighted minimum feedback arc set problem. We also provide a polynomiallyverifiable sufficient condition for the existence of a pairbetting match and show that a greedy algorithm offers poor approximation for indivisible pair bets.
We consider permutation betting, or betting on the outcome of a competition among n candidates. The final outcome or state s ∈ S is an ordinal ranking of the n candidates.
For example, the candidates could be horses in a race and the outcome the list of horses in increasing order of their finishing times. The state space S contains all n! mutually exclusive and exhaustive permutations of candidates.
In a typical horse race, people bet on properties of the outcome like horse A will win, horse A will show, or finish in either first or second place, or horses A and B will finish in first and second place, respectively. In practice at the racetrack, each of these different types of bets are processed in separate pools or groups. In other words, all the win bets are processed together, and all the show bets are processed together, but the two types of bets do not mix. This separation can hurt liquidity and information aggregation. For example, even though horse A is heavily favored to win, that may not directly boost the horse"s odds to show.
Instead, we describe a central exchange where all bets on the outcome are processed together, thus aggregating liquidity and ensuring that informational inference happens automatically.
Ideally, we"d like to allow traders to bet on any property of the final ordering they like, stated in exactly the language they prefer. In practice, allowing too flexible a language creates a computational burden for the auctioneer attempting to match willing traders. We explore the tradeoff between the expressiveness of the bidding language and the computational complexity of the matching problem.
We consider a framework where people propose to buy securities that pay $1 if and only if some property of the final ordering is true. Traders state the price they are willing to pay per share and the number of shares they would like to purchase. (Sell orders may not be explicitly needed, since buying the negation of an event is equivalent to selling the event.) A divisible order permits the trader to receive fewer shares than requested, as long as the price constraint is met; an indivisible order is an all-or-nothing order. The description of bets in terms of prices and shares is without loss of generality: we can also allow bets to be described in terms of odds, payoff vectors, or any of the diverse array of approaches practiced in financial and gambling circles.
In principle, we can do everything we want by explicitly offering n! securities, one for every state s ∈ S (or in fact any set of n! linearly independent securities). This is the so-called complete Arrow-Debreu securities market [1] for our setting. In practice, traders do not want to deal with low-level specification of complete orderings: people think more naturally in terms of high-level properties of orderings. Moreover, operating n! securities is infeasible in practice from a computational point of view as n grows.
A very simple bidding language might allow traders to bet only on who wins the competition, as is done in the win pool at racetracks. The corresponding matching problem is polynomial, however the language is not very expressive. A trader who believes that A will defeat B, but that neither will win outright cannot usefully impart his information to the market. The price space of the market reveals the collective estimates of win probabilities but nothing else. Our goal is to find languages that are as expressive and intuitive as possible and reveal as much information as possible, while maintaining computational feasibility.
Our work is in direct analogy to work by Fortnow et. al. [6]. Whereas we explore permutation combinatorics,
Fortnow et. al. explore Boolean combinatorics. The authors consider a state space of the 2n possible outcomes of n binary variables. Traders express bets in Boolean logic. The authors show that divisible matching is co-NP-complete and indivisible matching is Σp 2-complete.
Hanson [9] describes a market scoring rule mechanism which can allow betting on combinatorial number of outcomes. The market starts with a joint probability distribution across all outcomes. It works like a sequential version of a scoring rule. Any trader can change the probability distribution as long as he agrees to pay the most recent trader 327 according to the scoring rule. The market maker pays the last trader. Hence, he bears risk and may incur loss.
Market scoring rule mechanisms have a nice property that the worst-case loss of the market maker is bounded. However, the computational aspects on how to operate the mechanism have not been fully explored. Our mechanisms have an auctioneer who does not bear any risk and only matches orders.
Research on bidding languages and winner determination in combinatorial auctions [4, 14, 18] considers similar computational challenges in finding an allocation of items to bidders that maximizes the auctioneer"s revenue.
Combinatorial auctions allow bidders to place distinct values on bundles of goods rather than just on individual goods.
Uncertainty and risk are typically not considered and the central auctioneer problem is to maximize social welfare. Our mechanisms allow traders to construct bets for an event with n! outcomes. Uncertainty and risk are considered and the auctioneer problem is to explore arbitrage opportunities and risklessly match up wagers.
In this section, we define the matching and optimal matching problems that an auctioneer needs to solve in a general permutation betting market. We then illustrate the problem definitions in the context of the subset-betting and pairbetting markets.
Consider an event with n competing candidates where the outcome (state) is a ranking of the n candidates. The bidding language of a market offering securities in the future outcomes determines the type and number of securities available and directly affects what information can be aggregated about the outcome. A fully expressive bidding language can capture any possible information that traders may have about the final ranking; a less expressive language limits the type of information that can be aggregated though it may enable a more efficient solution to the matching problem. For any bidding language and number of securities in a permutation betting market, we can succinctly represent the problem of the auctioneer to risklessly match offers as follows.
Consider an index set of bets or orders O which traders submit to the auctioneer. Each order i ∈ O is a triple (bi, qi, φi), where bi denotes how much the trader is willing to pay for a unit share of security φi and qi is the number of shares of the security he wants to purchase at price bi.
Naturally, bi ∈ (0, 1) since a unit of the security pays off at most $1 when the event is realized. Since order i is defined for a single security φi, we will omit the security variable whenever it is clear from the context.
The auctioneer can accept or reject each order, or in a divisible world accept a fraction of the order. Let xi be the fraction of order i ∈ O accepted. In the indivisible version of the market xi = 0 or 1 while in the divisible version xi ∈ [0, 1]. Further let Ii(s) be the indicator variable for whether order i is winning in state s, that is Ii(s) = 1 if the order is paid back $1 in state s and Ii(s) = 0 otherwise.
There are two possible problems that the auctioneer may want to solve. The simpler one is to find a subset of orders that can be matched risk-free, namely a subset of orders which accepted together give a nonnegative profit to the auctioneer in every possible outcome. We call this problem the existence of a match or sometimes simply, the matching problem. The more complex problem is for the auctioneer to find the optimal match with respect to some criterion such as profit, trading volume, etc.
Definition 1 (Existence of match, indivisible orders).
Given a set of orders O, does there exist a set of xi ∈ {0, 1}, i ∈ O, with at least one xi = 1 such that i (bi − Ii(s))qixi ≥ 0, ∀s ∈ S? (1) Similarly we can define the existence of a match with divisible orders.
Definition 2 (Existence of match, divisible orders).
Given a set of orders O, does there exist a set of xi ∈ [0, 1], i ∈ O, with at least one xi > 0 such that i (bi − Ii(s))qixi ≥ 0, ∀s ∈ S? (2) The existence of a match is a decision problem. It only returns whether trade can occur at no risk to the auctioneer. In addition to the risk-free requirement, the auctioneer can optimize some criterion in determining the orders to accept. Some reasonable objectives include maximizing the total trading volume in the market or the worst-case profit of the auctioneer. The following optimal matching problems are defined for an auctioneer who maximizes his worst-case profit.
Definition 3 (Optimal match, indivisible orders).
Given a set of orders O, choose xi ∈ {0, 1} such that the following mixed integer programming problem achieves its optimality max xi,c c (3) s.t. i bi − Ii(s) qixi ≥ c, ∀s ∈ S xi ∈ {0, 1}, ∀i ∈ O.
Definition 4 (Optimal match, divisible orders).
Given a set of orders O, choose xi ∈ [0, 1] such that the following linear programming problem achieves its optimality max xi,c c (4) s.t. i bi − Ii(s) qixi ≥ c, ∀s ∈ S 0 ≤ xi ≤ 1, ∀i ∈ O.
The variable c is the worst-case profit for the auctioneer.
Note that, strictly speaking, the optimal matching problems do not require to solve the optimization problems (3) and (4), because only the optimal set of orders are needed. The optimal worst-case profit may remain unknown.
A subset betting market allows two different types of bets.
Traders can bet on a subset of positions a candidate may end up at, or they can bet on a subset of candidates that will occupy a particular position. A security α|Φ where Φ is a subset of positions pays off $1 if candidate α stands at a position that is an element of Φ and it pays $0 otherwise.
For example, security α|{2, 4} pays $1 when candidate α 328 is ranked second or fourth. Similarly, a security Ψ|j where Ψ is a subset of candidates pays off $1 if any of the candidates in the set Ψ ranks at position j. For instance, security {α, γ}|2 pays off $1 when either candidate α or candidate γ is ranked second.
The auctioneer in a subset betting market faces a nontrivial matching problem, that is to determine which orders to accept among all submitted orders i ∈ O. Note that although there are only n candidates and n possible positions, the number of available securities to bet on is exponential since a trader may bet on any of the 2n subsets of candidates or positions. With this, it is not immediately clear whether one can even find a trading partner or a match for trade to occur, or that the auctioneer can solve the matching problem in polynomial time. In the next section, we will show that somewhat surprisingly there is an elegant polynomial solution to both the matching and optimal matching problems, based on classic combinatorial problems.
When an order is accepted, the corresponding trader pays the submitted order price bi to the auctioneer and the auctioneer pays the winning orders $1 per share after the outcome is revealed. The auctioneer has to carefully choose which orders and what fractions of them to accept so as to be guaranteed a nonnegative profit in any future state.
The following example illustrates the matching problem for indivisible orders in the subset-betting market.
Example 1. Suppose n = 3. Objects α, β, and γ compete for positions 1, 2, and 3 in a competition. The auctioneer receives the following 4 orders: (1) buy 1 share α|{1} at price $0.6; (2) buy 1 share β|{1, 2} at price $0.7; (3) buy 1 share γ|{1, 3} at price $0.8; and (4) buy 1 share β|{3} at price $0.7. There are 6 possible states of ordering: αβγ, αγβ, βαγ, βγα, γαβ,and γβα. The corresponding statedependent profit of the auctioneer for each order can be calculated as the following vectors, c1 = (−0.4, −0.4, 0.6, 0.6, 0.6, 0.6) c2 = (−0.3, 0.7, −0.3, −0.3, 0.7, −0.3) c3 = (−0.2, 0.8, −0.2, 0.8, −0.2, −0.2) c4 = ( 0.7, −0.3, 0.7, 0.7, −0.3, 0.7). 6 columns correspond to the 6 future states. For indivisible orders, the auctioneer can either accept orders (2) and (4) and obtain profit vector c = (0.4, 0.4, 0.4, 0.4, 0.4, 0.4), or accept orders (2), (3), and (4) and has profit across state c = (0.2, 1.2, 0.2, 1.2, 0.2, 0.2).
A pair betting market allows traders to bet on whether one candidate will rank higher than another candidate, in an outcome which is a permutation of n candidates. A security α > β pays off $ 1 if candidate α is ranked higher than candidate β and $ 0 otherwise. There are a total of N(N −1) different securities offered in the market, each corresponding to an ordered pair of candidates.
Traders place orders of the form buy qi shares of α > β at price per share no greater than bi. bi in general should be between 0 and 1. Again the order can be either indivisible or divisible and the auctioneer needs to decide what fraction xi of each order to accept so as not to incur any loss, with A B C D E F .99 .99 .5 .5 .5 .99 .99 .99 .99 Figure 1: Every cycle has negative worst-case profit of −0.02 (for the cycles of length 4) or less (for the cycles of length 6), however accepting all edges in full gives a positive worst-case profit of 0.44. xi ∈ {0, 1} for indivisible and xi ∈ [0, 1] for divisible orders.
The same definitions for existence of a match and optimal match from Section 3.1 apply.
The orders in the pair-betting market have a natural interpretation as a graph, where the candidates are nodes in the graph and each order which ranks a pair of candidates α > β is represented by a directed edge e = (α, β) with price be and weight qe. With this interpretation, it is tempting to assume that a necessary condition for a match is to have a cycle in the graph with a nonnegative worst-case profit.
Assuming qe = 1 for all e, this is a cycle C with a total of |C| edges such that the worst-case profit for the auctioneer is e∈C be − (|C| − 1) ≥ 0, since in the worst-case state the auctioneer needs to pay $,1 to every order in the cycle except one. However, the example in Figure 1 shows that this is not the case: we may have a set of orders in which every single cycle has a negative worst-case profit, and yet there is a positive worstcase match overall. The edge labels in the figure are the prices be; both the optimal divisible and indivisible solution in this case accept all orders in full, xe = 1.
The matching problems of the auctioneer in any permutation market, including the subset betting market have n! constraints. Brute-force methods would take exponential time to solve. However, given the special form of the securities in the subset betting market, we can show that the matching problems for divisible orders can be solved in polynomial time.
Theorem 1. The existence of a match and the optimal match problems with divisible orders in a subset betting market can both be solved in polynomial time. 329 Proof. Consider the linear programming problem (4) for finding an optimal match. This linear program has |O| + 1 variables, one variable xi for each order i and the profit variable c. It also has exponentially many constraints. However, we can solve the program in time polynomial in the number of orders |O| by using the ellipsoid algorithm, as long as we can efficiently solve its corresponding separation problem in polynomial time [7, 8]. The separation problem for a linear program takes as input a vector of variable values and returns if the vector is feasible, or otherwise it returns a violated constraint.
For given values of the variables, a violated constraint in Eq. (4) asks whether there is a state or permutation s in which the profit is less than c, and can be rewritten as i Ii(s)qixi < i biqixi − c ∀s ∈ S. (5) Thus it suffices to show how to find efficiently a state s satisfying the above inequality (5) or verify that the opposite inequality holds for all states s.
We will show that the separation problem can be reduced to the maximum weighted bipartite matching1 problem [3].
The left hand side in Eq. (5) is the total money that the auctioneer needs to pay back to the winning traders in state s. The first term on the right hand side is the total money collected by the auctioneer and it is fixed for a given solution vector of xi"s and c. A weighted bipartite graph can be constructed between the set of candidates and the set of positions. For every order of the form α|Φ there are edges from candidate node α to every position node in Φ. For orders of the form Ψ|j there are edges from each candidate in Ψ to position j. For each order i we put weight qixi on each of these edges. All multi-edges with the same end points are then replaced with a single edge that carries the total weight of the multi-edge. Every state s then corresponds to a perfect matching in the bipartite graph. In addition, the auctioneer pays out to the winners the sum of all edge weights in the perfect matching since every candidate can only stand in one position and every position is taken by one candidate. Thus, the auctioneer"s worst-cast state and payment are the solution to the maximum weighted bipartite matching problem, which has known polynomial-time algorithms [12, 13]. Hence, the separation problem can be solved in polynomial time.
Naturally, if the optimal solution to (4) gives a worst-case profit of c∗ > 0, there exists a matching. Thus, the matching problem can be solved in polynomial time also.
In this section we show that a slight change of the bidding language may bring about a dramatic change in the complexity of the optimal matching problem of the auctioneer. In particular, we show that finding the optimal match in the pair betting market is NP-hard for both divisible and indivisible orders. We then identify a polynomially-verifiable sufficient condition for deciding the existence of a match.
The hardness results are surprising especially in light of the observation that a pair betting market has a seemingly more restrictive bidding language which only offers n(n−1) 1 The notion of perfect matching in a bipartite graph, which we use only in this proof, should not be confused with the notion of matching bets which we use throughout the paper. securities. In contrast, the subset betting market enables traders to bet on an exponential number of securities and yet had a polynomial time solution for finding the optimal match. Our hope is that the comparison of the complexities of the subset and pair betting markets would offer insight into what makes a bidding language expressive while at the same time enabling an efficient matching solution.
In all analysis that follows, we assume that traders submit unit orders in pair betting markets, that is qi = 1. A set of orders O received by the auctioneer in a pair betting market with unit orders can be represented by a directed graph, G(V, E), where the vertex set V contains candidates that traders bet on. An edge e ∈ E, denoted (α, β, be), represents an order to buy 1 share of the security α > β at price be. All edges have equal weight of 1.
We adopt the following notations throughout the paper: • G(V, E): original equally weighted directed graph for the set of unit orders O. • be: price of the order for edge e. • G∗ (V ∗ , E∗ ); a weighted directed graph of accepted orders for optimal matching, where edge weight xe is the quantity of order e accepted by the auctioneer. xe = 1 for indivisible orders and 0 < xe ≤ 1 for divisible orders. • H(V, E): a generic weighted directed graph of accepted orders. • k(H): solution to the unweighted minimum feedback arc set problem on graph H. k(H) is the minimum number of edges to remove so that H becomes acyclic. • l(H): solution to the weighted minimum feedback arc set problem on graph H. l(H) is the minimum total weights for the set of edges which, when removed, leave H acyclic. • c(H): worst-case profit of the auctioneer if he accepts all orders in graph H. • : a sufficiently small positive real number. Where not stated, < 1/(2|E|) for a graph H(V, E). In other cases, the value is determined in context.
A feedback arc set of a directed graph is a set of edges which, when removed from the graph, leave a directed acyclic graph (DAG). Unweighted minimum feedback arc set problem is to find a feedback arc set with the minimum cardinality, while weighted minimum feedback arc set problem seeks to find a feedback arc set with the minimum total edge weight. Both unweighted and weighted minimum feedback arc set problems have been shown to be NP-complete [10].
We will use this result in our complexity analysis on pair betting markets.
The auctioneer"s optimal indivisible matching problem is introduced in Definition 3 of Section 3. Assuming unit orders and considering the order graph G(V, E), we restate the auctioneer"s optimal matching problem in a pair betting market as picking a subset of edges to accept such that 330 worst-case profit is maximized in the following optimization problem, max xe,c c (6) s.t. e be − Ie(s) xe ≥ c, ∀s ∈ S xe ∈ {0, 1}, ∀e ∈ E.
Without lose of generality, we assume that there are no multi-edges in the order graph G.
We show that the optimal matching problem for indivisible orders is NP-hard via a reduction from the unweighted minimum feedback arc set problem. The latter takes as input a directed graph, and asks what is the minimum number of edges to delete from the graph so as to be left with a DAG.
Our hardness proof is based on the following lemmas.
Lemma 2. Suppose the auctioneer accepts all edges in an equally weighted directed graph H(V, E) with edge price be = (1 − ) and edge weight xe = 1. Then the worst-case profit is equal to k(H) − |E|, where k(H) is the solution to the unweighted minimum feedback arc problem on H.
Proof. If the order of an edge gets $1 payoff at the end of the market we call the edge a winning edge, otherwise it is called a losing edge. For any state s, all winning edges necessarily form a DAG. Conversely, for every DAG there is a state in which the DAG edges are winners (though the remaining edges in G are not necessarily losers).
Suppose that in state s there are ws winning edges and ls = |E| − ws losing edges. Then, ls is the cardinality of a feedback arc set that consists of all losing edges in state s.
The edges that remain after deleting the minimum feedback arc set form the maximum DAG for the graph H. Consider the state smax in which all edges of the maximum DAG are winners. This gives the maximum number of winning edges wmax. All other edges are necessarily losers in the state smax, since any edge which is not in the max DAG must form a cycle together with some of the DAG edges. The number of losing edges in state smax is the cardinality of the minimum feedback arc set of H, that is |E| − wmax = k(H).
The profit of the auctioneer in a state s is profit(s) = e∈E be − w = (1 − )|E| − w ≥ (1 − )|E| − wmax, where equality holds when s = smax. Thus, the worst-case profit is achieved at state smax, profit(smax) = (|E| − wmax) − |E| = k(H) − |E|.
Consider the graph of accepted orders for optimal matching, G∗ (V ∗ , E∗ ), which consists of the optimal subset of edges E∗ to be accepted by the auctioneer, that is edges with xe = 1 in the solution of the optimization problem (6).
We have the following lemma.
Lemma 3. If the edge prices are be = (1− ), then the optimal indivisible solution graph G∗ has the same unweighted minimum feedback arc set size as the graph of all orders G, that is k(G∗ ) = k(G). Furthermore, G∗ is the smallest such subgraph of G, i.e., it is the subgraph of G with the smallest number of edges, that has the same size of unweighted minimum feedback arc set as G.
Proof. G∗ is a subgraph of G, hence the minimum number of edges to break cycles in G∗ is no more than that in G, namely k(G∗ ) ≤ k(G).
Suppose k(G∗ ) < k(G). Since both k(G∗ ) and k(G) are integers, for any < 1 |E| we have that k(G∗ ) − |E∗ | < k(G)− |E|. Hence by Lemma 2, the auctioneer has a higher worst-case profit by accepting G than accepting G∗ , which contradicts the optimality of G∗ . Finally, the worst-case profit k(G) − |E∗ | is maximized when |E∗ | is minimized.
Hence, G∗ is the smallest subgraph of G such that k(G∗ ) = k(G).
The above two lemmas prove that the maximum worstcase profit in the optimal indivisible matching is directly related to the size of the minimum feedback arc set. Thus computing each automatically gives the other, hence computing the maximum worst-case profit in the indivisible pair betting problem is NP-hard.
Theorem 4. Computing the maximum worst-case profit in indivisible pair betting is NP-hard.
Proof. By Lemma 3, the maximum worst-case profit which is the optimum to the mixed integer programming problem (6), is k(G) − |E∗ |, where |E∗ | is the number of accepted edges. Since k(G) is integer and |E∗ | ≤ |E| < 1, solving (6) will automatically give us the cardinality of the minimum feedback arc set of G, k(G). Because the minimum feedback arc set problem is NP-complete [10], computing the maximum worst-case profit is NP-hard.
Theorem 4 states that solving the optimization problem is hard, because even if the optimal set of orders are provided computing the optimal worst-case profit from accepting those orders is NP-hard. However, it does not imply whether the optimal matching problem, i.e. finding the optimal set of orders to accept, is NP-hard. It is possible to be able to determine which edges in a graph participating in the optimal match, yet unable to compute the corresponding worst-case profit. Next, we prove that the indivisible optimal matching problem is actually NP-hard. We will use the following short fact repeatedly.
Lemma 5 (Edge removal lemma). Given a weighted graph H(V, E), removing a single edge e with weight xe from the graph decreases the weighted minimum feedback arc set solution l(H) by no more than xe and reduces the unweighted minimum feedback arc set solution k(H) by no more than 1.
Proof. Suppose the weighted minimum feedback arc set for the graph H − {e} is F. Then F ∪ {e} is a feedback arc set for H, and has total edge weight l(H−{e})+xe. Because l(H) is the solution to the weighted minimum feedback arc set problem on H, we have l(H) ≤ l(H −{e})+xe, implying that l(H − {e}) ≥ l(H) − xe.
Similarly, suppose the unweighted minimum feedback arc set for the graph H − {e} is F . Then F ∪ {e} is a feedback arc set for H, and has set cardinality k(H−{e})+1. Because k(H) is the solution to the unweighted minimum feedback arc set problem on H, we have k(H) ≤ k(H − {e}) + 1, giving that k(H − {e}) ≥ k(H) − 1.
Theorem 6. Finding the optimal match in indivisible pair betting is NP-hard. 331 Proof. We reduce from the unweighted minimum feedback arc set problem again, although with a slightly more complex polynomial transformation involving multiple calls to the optimal match oracle. Consider an instance graph G of the minimum feedback arc set problem. We are interested in computing k(G), the size of the minimum feedback arc set of G.
Suppose we have an oracle which solves the optimal matching problem. Denote by optimal match(G ) the output of the optimal matching oracle on graph G with prices be = (1− ) on all its edges. By Lemma 3, on input G , the oracle optimal match returns the subgraph of G with the smallest number of edges, that has the same size of minimum feedback arc set as G .
The following procedure finds k(G) by using polynomially many calls to the optimal match oracle on a sequence of subgraphs of G. set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal match(G ) if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) This procedure removes edges from the original graph G layer by layer until the graph is empty, while at the same time computing the minimum feedback arc set size k(G) of the original graph as the number of iterations. In each iteration, we start with a graph G and replace it with the smallest subgraph G that has the same k(G ). At this stage, removing an additional edge e necessarily results in k(G −{e}) = k(G )−1, because k(G −{e}) < k(G ) by the optimality of G , and k(G − {e}) ≥ k(G ) − 1 by the edgeremoval lemma. Therefore, in each iteration the cardinality of the minimum feedback arc set gets reduced exactly by 1.
Hence the number of iterations is equal to k(G).
Note that this procedure gives a polynomial transformation from the optimal matching problem to the unweighted minimum feedback arc set problem, which calls the optimal matching oracle exactly k(G) ≤ |E| times, where |E| is the number of edges of G. Hence the optimal matching problem is NP-hard.
When orders are divisible, the auctioneer"s optimal matching problem is described in Definition 4 of Section 3.
Assuming unit orders and considering the order graph G(V, E), we restate the auctioneer"s optimal matching problem for divisible orders as choosing quantity of orders to accept, xe ∈ [0, 1], such that worst-case profit is maximized in the following linear programming problem, max xe,c c (7) s.t. e be − Ie(s) xe ≥ c, ∀s ∈ S xe ∈ [0, 1], ∀e ∈ E.
We still assume that there are no multi-edges in the order graph G.
When orders are divisible, the auctioneer can be better off by accepting partial orders. Example 2 shows a situation when accepting partial orders generates higher worst-case profit than the optimal indivisible solution.
Example 2. We show that the linear program (7) sometimes has a non-integer optimal solution.
A B C D E F b b b b b b b b b Figure 2: An order graph. Letters on edges represent order prices.
Consider the graph in Figure 2. There are a total of five cycles in the graph: three four-edge cycles ABCD, ABEF,
CDEF, and two six-edge cycles ABCDEF and ABEFCD.
Suppose each edge has price b such that 4b − 3 > 0 and 6b − 5 < 0, namely b ∈ (.75, .80), for example b = .78. With this, the optimal indivisible solution consists of at most one four-edge cycle, with worst case profit (4b−3). On the other hand, taking 1 2 fraction of each of the three four-edge cycles would yield higher worst-case profit of 3 2 (4b − 3).
Despite the potential profit increase for accepting divisible orders, the auctioneer"s optimal matching problem remains to be NP-hard for divisible orders, which is presented below via several lemmas and theorems.
Lemma 7. Suppose the auctioneer accept orders described by a weighted directed graph H(V, E) with edge weight xe to be the quantity accepted for edge order e. The worst-case profit for the auctioneer is c(H) = e∈E (be − 1)xe + l(H). (8) Proof. For any state s, the winning edges form a DAG.
Thus, the worst-case profit for the auctioneer achieves at the state(s) when the total quantity of losing orders is minimized. The minimum total quantity of losing orders is the solution to weighted minimal feedback arc set problem on H, that is l(H).
Consider the graph of accepted orders for optimal divisible matching, G∗ (V ∗ , E∗ ), which consists of the optimal subset of edges E∗ to be accepted by the auctioneer, with edge weight xe > 0 getting from the optimal solution of the linear program (7). We have the following lemmas. 332 Lemma 8. l(G∗ ) ≤ k(G∗ ) ≤ k(G).
Proof. l(G∗ ) is the solution of the weighted minimum feedback arc set problem on G∗ , while k(G∗ ) is the solution of the unweighted minimum feedback arc set problem on G∗ . When all edge weights in G∗ are 1, l(G∗ ) = k(G∗ ).
When xe"s are less than 1, l(G∗ ) can be less than or equal to k(G∗ ). Since G∗ is a subgraph of G but possibly with different edge weights, k(G∗ ) ≤ k(G). Hence, we have the above relation.
Lemma 9. There exists some such that when all edge prices be"s are (1 − ), l(G∗ ) = k(G).
Proof. From lemma 8, l(G∗ ) ≤ k(G). We know that the auctioneer"s worst-case profit when accepting G∗ is c(G∗ ) = e∈E∗ (be − 1)xe + l(G∗ ) = l(G∗ ) − e∈E∗ xe.
When he accepts the original order graph G in full, his worstcase profit is c(G) = e∈E (be − 1) + k(G) = k(G) − |E|.
Suppose l(G∗ ) < k(G). If |E| − e∈E∗ xe = 0, it means that G∗ is G. Hence, l(G∗ ) = k(G) regardless of , which contradicts with the assumption l(G∗ ) < k(G). If |E| − e∈E∗ xe > 0, then when < k(G) − l(G∗ ) |E| − e∈E∗ xe , c(G) is strictly greater than c(G∗ ), contradicting with the optimality of c(G∗ ). Because xe"s are less than 1, l(G∗ ) > k(G) is impossible. Thus, l(G∗ ) = k(G).
Theorem 10. Finding the optimal worst-case profit in divisible pair betting is NP-hard.
Proof. Given the optimal set of partial orders to accept for G when edge weights are (1 − ), if we can calculate the optimal worst-case profit, by lemma 9 we can solve the unweighted minimum feedback arc set problem on G, which is NP-hard. Hence, finding the optimal worst-case profit is NP-hard.
Theorem 10 states that solving the linear program (7) is NP-hard. Similarly to the indivisible case, we still need to prove that just finding the optimal divisible match is hard, as opposed to being able to compute the optimal worstcase profit. Since in the divisible case the edges do not necessarily have unit weights, the proof in Theorem 6 does not apply directly. However, with an additional property of the divisible case, we can augment the procedure from the indivisible hardness proof to compute the unweighted minimum feedback arc set size k(G) here as well.
First, note that the optimal divisible subgraph G∗ of a graph G is the weighted subgraph with minimum weighted feedback arc set size l(G∗ ) = k(G) and smallest sum of edge weights e∈E∗ xe, since its corresponding worst case profit is k(G) − e∈E∗ xe according to lemmas 7 and 9.
Lemma 11. Suppose graph H satisfies l(H) = k(H) and we remove edge e from it with weight xe < 1. Then, k(H − {e}) = k(H).
Proof. Assume the contrary, namely k(H−{e}) < k(H).
Then by Lemma 5, k(H − {e}) = k(H) − 1. Since removing a single edge cannot reduce the minimum feedback arc set by more than the edge weight, l(H) − xe ≤ l(H − {e}). (9) On the other hand H − {e} ⊂ H so we have, l(H − {e}) ≤ k(H − {e}) = k(H) − 1 = l(H) − 1. (10) Combining (9) and (10), we get xe ≥ 1. The contradiction arises. Therefore, removing any edge with less than unit weight from an optimal divisible graph does not change k(H), the minimal feedback arc set size of the unweighted version of the graph.
We now can augment the procedure for the indivisible case in Theorem 6, to prove hardness of the divisible version, as follows.
Theorem 12. Finding the optimal match in divisible pair betting is NP-hard.
Proof. We reduce from the unweighted minimum feedback arc set problem for graph G. Suppose we have an oracle for the optimal divisible problem called optimal divisible match, which on input graph H computes edge weights xe ∈ (0, 1] for the optimal subgraph H∗ of H, satisfying l(H∗ ) = k(H).
The following procedure outputs k(G). set G := G iterations := 0 while (G has nonempty edge set) reset G := optimal divisible match(G ) while (G has edges with weight < 1) remove an edge with weight < 1 from G reset G by setting all edge weights to 1 reset G := optimal divisible match(G ) end while if (G has nonempty edge set) increment iterations by 1 reset G by removing any edge e end if end while return (iterations) As in the proof of the corresponding Theorem 6 for the indivisible case, we compute k(G) by iteratively removing edges and recomputing the optimal divisible solution on the remaining subgraph, until all edges are deleted. In each iteration of the outer while loop, the minimum feedback arc set is reduced by 1, thus the number of iterations is equal to k(G).
It remains to verify that each iteration reduces k(G) by exactly 1. Starting from a graph at the beginning of an iteration, we compute its optimal divisible subgraph. We then keep removing one non-unit weight edge at a time and recomputing the optimal divisible subgraph, until the latter contains only edges with unit weight. By Lemma 11 throughout the iteration so far the minimum feedback arc set of the corresponding unweighted graph remains unchanged.
Once the oracle returns a graph G with unit edge weights, removing any edge would reduce the minimum feedback arc set: otherwise G is not optimal since G − {e} would have 333 the same minimum feedback arc set but smaller total edge weight. By Lemma 5 removing a single edge cannot reduce the minimum feedback arc set by more than one, thus as all edges have unit weight, k(G ) gets reduced by exactly one. k(G) is equal to the returned value from the procedure.
Hence, the optimal matching problem for divisible orders is NP-hard.
Knowing that the optimal matching problem is NP-hard for both indivisible and divisible orders in pair betting, we check whether the auctioneer can identify the existence of a match with ease. Lemma 13 states a sufficient condition for the matching problem with both indivisible and divisible orders.
Lemma 13. A sufficient condition for the existence of a match for pair betting is that there exists a cycle C in G such that, e∈C be ≥ |C| − 1, (11) where |C| is the number of edges in the cycle C.
Proof. The left-hand side of the inequality (11) represents the total payment that the auctioneer receives by accepting every unit orders in the cycle C in full. Because the direction of an edge represents predicted ordering of the two connected nodes in the final ranking, forming a cycle meaning that there is some logical contradiction on the predicted orderings of candidates. Hence, whichever state is realized, not all of the edges in the cycle can be winning edges. The worst-case for the auctioneer corresponds to a state where every edge in the cycle gets paid by $ 1 except one, with |C| − 1 be the maximum payment to traders. Hence, if inequality (11) is satisfied, the auctioneer has non-negative worst-case profit by accepting the orders in the cycle.
It can be shown that identifying such a non-negative worstcase profit cycle in an order graph G can be achieved in polynomial time.
Lemma 14. It takes polynomial time to find a cycle in an order graph G(V, E) that has the highest worst-case profit, that is max C∈C e∈C be − (|C| − 1) , where C is the set of all cycles in G.
Proof. Because e∈C be − (|C| − 1) = e∈C (be − 1) + 1 = 1 − e∈C (1 − be), finding the cycle that gives the highest worst-case profit in the original order graph G is equivalent to finding the shortest cycle in a converted graph H(V, E), where H is achieved by setting the weight for edge e in G to be (1 − be).
Finding the shortest cycle in graph H can be done within polynomial time by resorting to the shortest path problem.
For any vertex v in V , we consider every neighbor vertex w such that (v, w) ∈ E. We then find the shortest path from w to v, denoted as path(w, v). The shortest cycle that passes vertex v is found by choosing the w such that e(v,w) + path(w, v) is minimized. Comparing the shortest cycle found for every vertex, we then can determine the shortest overall cycle for the graph H. Because the short path problem can be solved in polynomial time [3], we can find the solution to our problem in polynomial time.
If the worst-case profit for the optimal cycle is non-negative, we know that there exists a match in G. However, the condition in lemma 13 is not a necessary condition for the existence of a match. Even if all single cycles in the order graph have negative worst-case profit, the auctioneer may accept multiple interweaving cycles to have positive worstcase profit. Figure 1 exhibits such a situation.
If the optimal indivisible match consists only of edge disjoint cycles, a natural greedy algorithm can find the cycle that gives the highest worst-case profit, remove its edges from the graph, and proceed until no more cycles exist.
However, we show that such greedy algorithm can give a very poor approximation. √ n + 1 √ n + 1 √ n √ n + 1 √ n + 1 √ n + 1 √ n + 1 Figure 3: Graph with n vertices and n + √ n edges on which the greedy algorithm finds only two cycles, the dotted cycle in the center and the unique remaining cycle. The labels in the faces give the number of edges in the corresponding cycle.
Lemma 15. The greedy algorithm gives at most an O( √  n)approximation to the maximum number of disjoint cycles.
Proof. Consider the graph in Figure 3 consisting of a cycle with √ n edges, each of which participates in another (otherwise disjoint) cycle with √ n + 1 edges. Suppose all edge weights are (1 − ). The maximum number of disjoint cycles is clearly √ n, taking all cycles with length √ n + 1.
Because smaller cycles gives higher worst-case profit, the greedy algorithm would first select the cycle of length √ n, after which there would be only one remaining cycle of length n. Thus the total number of cycles selected by greedy is 2 and the approximation factor in this case is √ n/2.
In light of Lemma 15, one may expect that greedy algorithms would give √ n-approximations at best.
Approxima334 tion algorithms for finding the maximum number of edgedisjoint cycles have been considered by Krivelevich,
Nutov and Yuster [11, 19]. Indeed, for the case of directed graphs, the authors show that a greedy algorithm gives a√ n-approximation [11]. When the optimal match does not consist of edge-disjoint cycles as in the example of Figure 3, greedy algorithm trying to finding optimal single cycles fails obviously.
We consider a permutation betting scenario, where traders wager on the final ordering of n candidates. While it is unnatural and intractable to allow traders to bet directly on the n! different final orderings, we propose two expressive betting languages, subset betting and pair betting. In a subset betting market, traders can bet either on a subset of positions that a candidate stands or on a subset of candidates who occupy a specific position in the final ordering.
Pair betting allows traders bet on whether one given candidate ranks higher than another given candidate.
We examine the auctioneer problem of matching orders without incurring risk. We find that in a subset betting market an auctioneer can find the optimal set and quantity of orders to accept such that his worst-case profit is maximized in polynomial time if orders are divisible. The complexity changes dramatically for pair betting. We prove that the optimal matching problem for the auctioneer is NP-hard for pair betting with both indivisible and divisible orders via reductions to the minimum feedback arc set problem. We identify a sufficient condition for the existence of a match, which can be verified in polynomial time. A natural greedy algorithm has been shown to give poor approximation for indivisible pair betting.
Interesting open questions for our permutation betting include the computational complexity of optimal indivisible matching for subset betting and the necessary condition for the existence of a match in pair betting markets. We are interested in further exploring better approximation algorithms for pair betting markets.
We thank Ravi Kumar, Yishay Mansour, Amin Saberi,
Andrew Tomkins, John Tomlin, and members of Yahoo! Research for valuable insights and discussions.
[1] K. J. Arrow. The role of securities in the optimal allocation of risk-bearing. Review of Economic Studies, 31(2):91-96, 1964. [2] J. E. Berg, R. Forsythe, F. D. Nelson, and T. A.
Rietz. Results from a dozen years of election futures markets research. In C. A. Plott and V. Smith, editors, Handbook of Experimental Economic Results (forthcoming). 2001. [3] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms (Second Edition).
MIT Press and McGraw-Hill, 2001. [4] P. Cramton, Y. Shoham, and R. Steinberg.
Combinatorial Auctions. MIT Press, Cambridge, MA,
[5] R. Forsythe, T. A. Rietz, and T. W. Ross. Wishes, expectations, and actions: A survey on price formation in election stock markets. Journal of Economic Behavior and Organization, 39:83-110, 1999. [6] L. Fortnow, J. Kilian, D. M. Pennock, and M. P.
Wellman. Betting boolean-style: A framework for trading in securities based on logical formulas.
Decision Support Systems, 39(1):87-104, 2004. [7] M. Gr¨otschel, L. Lov´asz, and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1(2):169-197, 1981. [8] M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization.
Springer-Verlag, Berlin Heidelberg, 1993. [9] R. D. Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1):107-119,
[10] R. M. Karp. Reducibility among combinatorial problems. In Complexity of computer computations (Proc. Sympos., IBM Thomas J. Watson Res. Center,
Yorktown Heights, N.Y.), pages 85-103. Plenum, New York, 1972. [11] M. Krivelevich, Z. Nutov, and R. Yuster.
Approximation algorithms for cycle packing problems.
Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, pages 556-561,
[12] H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistic Quarterly, 2:83-97, 1955. [13] J. Munkres. Algorithms for the assignment and transportation problems. Journal of the Society of Industrial and Applied Mathematics, 5(1):32-38, 1957. [14] N. Nisan. Bidding and allocation in combinatorial auctions. In Proceedings of the 2nd ACM Conference on Electronic Commerce (EC"00), Minneapolis, MN,
[15] D. M. Pennock, S. Lawrence, C. L. Giles, and F. A.

In a set system auction there is a single buyer and many vendors that can provide various services. It is assumed that the buyer"s requirements can be satisfied by various subsets of the vendors; these subsets are called the feasible sets. A widely-studied class of setsystem auctions is path auctions, where each vendor is able to sell access to a link in a network, and the feasible sets are those sets whose links contain a path from a given source to a given destination; the study of these auctions has been initiated in the seminal paper by Nisan and Ronen [19] (see also [1, 10, 9, 6, 15, 7, 20]).
We assume that each vendor has a cost of providing his services, but submits a possibly larger bid to the auctioneer. Based on these bids, the auctioneer selects a feasible subset of vendors, and makes payments to the vendors in this subset. Each selected vendor enjoys a profit of payment minus cost. Vendors want to maximise profit, while the buyer wants to minimise the amount he pays. A natural goal in this setting is to design a truthful auction, in which vendors have an incentive to bid their true cost. This can be achieved by paying each selected vendor a premium above her bid in such a way that the vendor has no incentive to overbid. An interesting question in mechanism design is how much the auctioneer will have to overpay in order to ensure truthful bids.
In the context of path auctions this topic was first addressed by Archer and Tardos [1]. They define the frugality ratio of a mechanism as the ratio between its total payment and the cost of the cheapest path disjoint from the path selected by the mechanism.
They show that, for a large class of truthful mechanisms for this problem, the frugality ratio is as large as the number of edges in the shortest path. Talwar [21] extends this definition of frugality ratio to general set systems, and studies the frugality ratio of the classical VCG mechanism [22, 4, 14] for many specific set systems, such as minimum spanning trees and set covers.
While the definition of frugality ratio proposed by [1] is wellmotivated and has been instrumental in studying truthful mechanisms for set systems, it is not completely satisfactory. Consider, for example, the graph of Figure 1 with the costs cAB = cBC = A B C D Figure 1: The diamond graph 336 cCD = 0, cAC = cBD = 1. This graph is 2-connected and the VCG payment to the winning path ABCD is bounded. However, the graph contains no A-D path that is disjoint from ABCD, and hence the frugality ratio of VCG on this graph remains undefined.
At the same time, there is no monopoly, that is, there is no vendor that appears in all feasible sets. In auctions for other types of set systems, the requirement that there exist a feasible solution disjoint from the selected one is even more severe: for example, for vertex-cover auctions (where vendors correspond to the vertices of some underlying graph, and the feasible sets are vertex covers) the requirement means that the graph must be bipartite. To deal with this problem, Karlin et al. [16] suggest a better benchmark, which is defined for any monopoly-free set system. This quantity, which they denote by ν, intuitively corresponds to the value of a cheapest Nash equilibrium. Based on this new definition, the authors construct new mechanisms for the shortest path problem and show that the overpayment of these mechanisms is within a constant factor of optimal.
Vertex cover auctions We propose a truthful polynomial-time auction for vertex cover that outputs a solution whose cost is within a factor of 2 of optimal, and whose frugality ratio is at most 2Δ, where Δ is the maximum degree of the graph (Theorem 4). We complement this result by proving (Theorem 5) that for any Δ and n, there are graphs of maximum degree Δ and size Θ(n) for which any truthful mechanism has frugality ratio at least Δ/2. This means that the solution quality of our auction is with a factor of 2 of optimal and the frugality ratio is within a factor of 4 of the best possible bound for worst-case inputs. To the best of our knowledge, this is the first auction for this problem that enjoys these properties. Moreover, we show how to transform any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio.
Frugality ratios Our vertex cover results naturally suggest two modifications of the definition of ν in [16]. These modifications can be made independently of each other, resulting in four different payment bounds TUmax, TUmin, NTUmax, and NTUmin, where NTUmin is equal to the original payment bound ν of in [16].
All four payment bounds arise as Nash equilibria of certain games (see the full version of this paper [8]); the differences between them can be seen as the price of initiative and the price of cooperation (see Section 3). While our main result about vertex cover auctions (Theorem 4) is with respect to NTUmin = ν, we make use of the new definitions by first comparing the payment of our mechanism to a weaker bound NTUmax, and then bootstrapping from this result to obtain the desired bound.
Inspired by this application, we embark on a further study of these payment bounds. Our results here are as follows:
always obey a particular order that is independent of the choice of the set system and the cost vector, namely, TUmin ≤ NTUmin ≤ NTUmax ≤ TUmax. We provide examples (Proposition 5 and Corollaries 1 and 2) showing that for the vertex cover problem any two consecutive bounds can differ by a factor of n − 2, where n is the number of agents. We then show (Theorem 2) that this separation is almost best possible for general set systems by proving that for any set system TUmax/TUmin ≤ n. In contrast, we demonstrate (Theorem 3) that for path auctions TUmax/TUmin ≤ 2.
We provide examples (Propositions 2, 3 and 4) showing that this bound is tight. We see this as an argument for the study of vertexcover auctions, as they appear to be more representative of the general team -selection problem than the widely studied path auctions.
vector for which TUmin and NTUmin differ by a factor of α, there is another cost vector that separates NTUmin and NTUmax by the same factor and vice versa; the same is true for the pairs (NTUmin, NTUmax) and (NTUmax, TUmax). This symmetry is quite surprising, since, e.g., TUmin and NTUmax are obtained from NTUmin by two very different transformations. This observation suggests that the four payment bounds should be studied in a unified framework; moreover, it leads us to believe that the bootstrapping technique of Theorem 4 may have other applications.
to a checklist of desirable features. In particular, we note that the payment bound ν = NTUmin of [16] exhibits some counterintuitive properties, such as nonmonotonicity with respect to adding a new feasible set (Proposition 7), and is NP-hard to compute (Theorem 6), while some of the other payment bounds do not suffer from these problems. This can be seen as an argument in favour of using weaker but efficiently computable bounds NTUmax and TUmax.
Related work Vertex-cover auctions have been studied in the past by Talwar [21] and Calinescu [5]. Both of these papers are based on the definition of frugality ratio used in [1]; as mentioned before, this means that their results only apply to bipartite graphs. Talwar [21] shows that the frugality ratio of VCG is at most Δ. However, since finding the cheapest vertex cover is an NP-hard problem, the VCG mechanism is computationally infeasible. The first (and, to the best of our knowledge, only) paper to investigate polynomial-time truthful mechanisms for vertex cover is [5]. This paper studies an auction that is based on the greedy allocation algorithm, which has an approximation ratio of log n. While the main focus of [5] is the more general set cover problem, the results of [5] imply a frugality ratio of 2Δ2 for vertex cover. Our results improve on those of [21] as our mechanism is polynomial-time computable, as well as on those of [5], as our mechanism has a better approximation ratio, and we prove a stronger bound on the frugality ratio; moreover, this bound also applies to the mechanism of [5].
In most of this paper, we discuss auctions for set systems. A set system is a pair (E, F), where E is the ground set, |E| = n, and F is a collection of feasible sets, which are subsets of E. Two particular types of set systems are of interest to us - shortest path systems, in which the ground set consists of all edges of a network, and the feasible sets are paths between two specified vertices s and t, and vertex cover systems, in which the elements of the ground set are the vertices of a graph, and the feasible sets are vertex covers of this graph.
In set system auctions, each element e of the ground set is owned by an independent agent and has an associated non-negative cost ce.
The goal of the centre is to select (purchase) a feasible set. Each element e in the selected set incurs a cost of ce. The elements that are not selected incur no costs.
The auction proceeds as follows: all elements of the ground set make their bids, the centre selects a feasible set based on the bids and makes payments to the agents. Formally, an auction is defined by an allocation rule A : Rn → F and a payment rule P : Rn → Rn . The allocation rule takes as input a vector of bids and decides which of the sets in F should be selected. The payment rule also takes as input a vector of bids and decides how much to pay to each agent. The standard requirements are individual rationality, i.e., the payment to each agent should be at least as high as his incurred cost (0 for agents not in the selected set and ce for agents in the 337 selected set) and incentive compatibility, or truthfulness, i.e., each agent"s dominant strategy is to bid his true cost.
An allocation rule is monotone if an agent cannot increase his chance of getting selected by raising his bid. Formally, for any bid vector b and any e ∈ E, if e ∈ A(b) then e ∈ A(b1, . . . , be, . . . , bn) for any be > be. Given a monotone allocation rule A and a bid vector b, the threshold bid te of an agent e ∈ A(b) is the highest bid of this agent that still wins the auction, given that the bids of other participants remain the same. Formally, te = sup{be ∈ R | e ∈ A(b1, . . . , be, . . . , bn)}. It is well known (see, e.g. [19, 13]) that any auction that has a monotone allocation rule and pays each agent his threshold bid is truthful; conversely, any truthful auction has a monotone allocation rule.
The VCG mechanism is a truthful mechanism that maximises the social welfare and pays 0 to the losing agents. For set system auctions, this simply means picking a cheapest feasible set, paying each agent in the selected set his threshold bid, and paying 0 to all other agents. Note, however, that the VCG mechanism may be difficult to implement, since finding a cheapest feasible set may be intractable.
If U is a set of agents, c(U) denotes P w∈U cw. Similarly, b(U) denotes P w∈U bw.
We start by reproducing the definition of the quantity ν from [16,
Definition 4].
Let (E, F) be a set system and let S be a cheapest feasible set with respect to the true costs ce. Then ν(c, S) is the solution to the following optimisation problem.
Minimise B = P e∈S be subject to (1) be ≥ ce for all e ∈ E (2) P e∈S\T be ≤ P e∈T \S ce for all T ∈ F (3) for every e ∈ S, there is a Te ∈ F such that e ∈ Te andP e ∈S\Te be = P e ∈Te\S ce The bound ν(c, S) can be seen as an outcome of a two-stage process, where first each agent e ∈ S makes a bid be stating how much it wants to be paid, and then the centre decides whether to accept these bids. The behaviour of both parties is affected by the following considerations. From the centre"s point of view, the set S must remain the most attractive choice, i.e., it must be among the cheapest feasible sets under the new costs ce = ce for e ∈ S, ce = be for e ∈ S (condition (2)). The reason for that is that if (2) is violated for some set T, the centre would prefer T to S.
On the other hand, no agent would agree to a payment that does not cover his costs (condition (1)), and moreover, each agent tries to maximise his profit by bidding as high as possible, i.e., none of the agents can increase his bid without violating condition (2) (condition (3)). The centre wants to minimise the total payout, so ν(c, S) corresponds to the best possible outcome from the centre"s point of view.
This definition captures many important aspects of our intuition about ‘fair" payments. However, it can be modified in two ways, both of which are still quite natural, but result in different payment bounds.
First, we can consider the worst rather than the best possible outcome for the centre. That is, we can consider the maximum total payment that the agents can extract by jointly selecting their bids subject to (1), (2), and (3). Such a bound corresponds to maximising B subject to (1), (2), and (3) rather than minimising it. If it is the agents who make the original bids (rather than the centre), this kind of bidding behaviour is plausible. On the other hand, in a game in which the centre proposes payments to the agents in S and the agents accept them as long as (1), (2) and (3) are satisfied, we would be likely to observe a total payment of ν(c, S). Hence, the difference between these two definitions can be seen as the price of initiative.
Second, the agents may be able to make payments to each other.
In this case, if they can extract more money from the centre by agreeing on a vector of bids that violates individual rationality (i.e., condition (1)) for some bidders, they might be willing to do so, as the agents who are paid below their costs will be compensated by other members of the group. The bids must still be realistic, i.e., they have to satisfy be ≥ 0. The resulting change in payments can be seen as the price of co-operation and corresponds to replacing condition (1) with the following weaker condition (1∗ ): be ≥ 0 for all e ∈ E. (1∗ ) By considering all possible combinations of these modifications, we obtain four different payment bounds, namely • TUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1∗ ), (2), and (3). • TUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1∗ ), (2), and (3). • NTUmin(c, S), which is the solution to the optimisation problem Minimise B subject to (1), (2), and (3). • NTUmax(c, S), which is the solution to the optimisation problem Maximise B subject to (1), (2), (3).
The abbreviations TU and NTU correspond, respectively, to transferable utility and non-transferable utility, i.e., the agents" ability/inability to make payments to each other. For concreteness, we will take TUmin(c) to be TUmin(c, S) where S is the lexicographically least amongst the cheapest feasible sets. We define TUmax(c), NTUmin(c), NTUmax(c) and ν(c) similarly, though we will see in Section 6.3 that, in fact, NTUmin(c, S) and NTUmax(c, S) are independent of the choice of S. Note that the quantity ν(c) from [16] is NTUmin(c).
The second modification (transferable utility) is more intuitively appealing in the context of the maximisation problem, as both assume some degree of co-operation between the agents. While the second modification can be made without the first, the resulting payment bound TUmin(c, S) is too strong to be a realistic benchmark, at least for general set systems. In particular, it can be smaller than the total cost of the cheapest feasible set S (see Section 6).
Nevertheless, we provide the definition as well as some results about TUmin(c, S) in the paper, both for completeness and because we believe that it may help to understand which properties of the payment bounds are important for our proofs. Another possibility would be to introduce an additional constraint P e∈S be ≥P e∈S ce in the definition of TUmin(c, S) (note that this condition holds automatically for TUmax(c, S), as TUmax(c, S) ≥ NTUmax(c, S)); however, such a definition would have no direct game-theoretic interpretation, and some of our results (in particular, the ones in Section 4) would no longer be true.
REMARK 1. For the payment bounds that are derived from maximisation problems, (i.e., TUmax(c, S) and NTUmax(c, S)), constraints of type (3) are redundant and can be dropped. Hence,
TUmax(c, S) and NTUmax(c, S) are solutions to linear programs, and therefore can be computed in polynomial time as long as we have a separation oracle for constraints in (2). In contrast, 338 NTUmin(c, S) can be NP-hard to compute even if the size of F is polynomial (see Section 6).
The first and third inequalities in the following observation follow from the fact that condition (1∗ ) is strictly weaker than condition (1).
PROPOSITION 1.
TUmin(c, S) ≤ NTUmin(c, S) ≤ NTUmax(c, S) ≤ TUmax(c, S).
Let M be a truthful mechanism for (E, F). Let pM(c) denote the total payments of M when the actual costs are c. A frugality ratio of M with respect to a payment bound is the ratio between the payment of M and this payment bound. In particular, φTUmin(M) = sup c pM(c)/TUmin(c), φTUmax(M) = sup c pM(c)/TUmax(c), φNTUmin(M) = sup c pM(c)/NTUmin(c), φNTUmax(M) = sup c pM(c)/NTUmax(c).
We conclude this section by showing that there exist set systems and respective cost vectors for which all four payment bounds are different. In the next section, we quantify this difference, both for general set systems, and for specific types of set systems, such as path auctions or vertex cover auctions.
EXAMPLE 1. Consider the shortest-path auction on the graph of Figure 1. The cheapest feasible sets are all paths from A to D. It can be verified, using the reasoning of Propositions 2 and 3 below, that for the cost vector cAB = cCD = 2, cBC = 1, cAC = cBD = 5, we have • TUmax(c) = 10 (with bAB = bCD = 5, bBC = 0), • NTUmax(c) = 9 (with bAB = bCD = 4, bBC = 1), • NTUmin(c) = 7 (with bAB = bCD = 2, bBC = 3), • TUmin(c) = 5 (with bAB = bCD = 0, bBC = 5).
We start by showing that for path auctions any two consecutive payment bounds can differ by at least a factor of 2.
PROPOSITION 2. There is an instance of the shortest-path problem for which we have NTUmax(c)/NTUmin(c) ≥ 2.
PROOF. This construction is due to David Kempe [17].
Consider the graph of Figure 1 with the edge costs cAB = cBC = cCD = 0, cAC = cBD = 1. Under these costs, ABCD is the cheapest path. The inequalities in (2) are bAB + bBC ≤ cAC = 1, bBC + bCD ≤ cBD = 1. By condition (3), both of these inequalities must be tight (the former one is the only inequality involving bAB, and the latter one is the only inequality involving bCD).
The inequalities in (1) are bAB ≥ 0, bBC ≥ 0, bCD ≥ 0. Now, if the goal is to maximise bAB + bBC + bCD, the best choice is bAB = bCD = 1, bBC = 0, so NTUmax(c) = 2. On the other hand, if the goal is to minimise bAB + bBC + bCD, one should set bAB = bCD = 0, bBC = 1, so NTUmin(c) = 1.
PROPOSITION 3. There is an instance of the shortest-path problem for which we have TUmax(c)/NTUmax(c) ≥ 2.
PROOF. Again, consider the graph of Figure 1. Let the edge costs be cAB = cCD = 0, cBC = 1, cAC = cBD = 1. ABCD is the lexicographically-least cheapest path, so we can assume that S = {AB, BC, CD}. The inequalities in (2) are the same as in the previous example, and by the same argument both of them are, in fact, equalities. The inequalities in (1) are bAB ≥ 0, bBC ≥ 1, bCD ≥ 0. Our goal is to maximise bAB + bBC + bCD. If we have to respect the inequalities in (1), we have to set bAB = bCD = 0, bBC = 1, so NTUmax(c) = 1. Otherwise, we can set bAB = bCD = 1, bBC = 0, so TUmax(c) ≥ 2.
PROPOSITION 4. There is an instance of the shortest-path problem for which we have NTUmin(c)/TUmin(c) ≥ 2.
PROOF. This construction is also based on the graph of Figure 1.
The edge costs are cAB = cCD = 1, cBC = 0, cAC = cBD =
assume that S = {AB, BC, CD}. Again, the inequalities in (2) are the same, and both are, in fact, equalities. The inequalities in (1) are bAB ≥ 1, bBC ≥ 0, bCD ≥ 1. Our goal is to minimise bAB + bBC +bCD. If we have to respect the inequalities in (1), we have to set bAB = bCD = 1, bBC = 0, so NTUmin(c) = 2. Otherwise, we can set bAB = bCD = 0, bBC = 1, so TUmin(c) ≤ 1.
In Section 4.4 (Theorem 3), we show that the separation results in Propositions 2, 3, and 4 are optimal.
The separation results for path auctions are obtained on the same graph using very similar cost vectors. It turns out that this is not coincidental. Namely, we can prove the following theorem.
THEOREM 1. For any set system (E, F), and any feasible set S, max c TUmax(c, S) NTUmax(c, S) = max c NTUmax(c, S) NTUmin(c, S) , max c NTUmax(c, S) NTUmin(c, S) = max c NTUmin(c, S) TUmin(c, S) , where the maximum is over all cost vectors c for which S is a cheapest feasible set.
The proof of the theorem follows directly from the four lemmas proved below; more precisely, the first equality in Theorem 1 is obtained by combining Lemmas 1 and 2, and the second equality is obtained by combining Lemmas 3 and 4. We prove Lemma 1 here; the proofs of Lemmas 2- 4 are similar and can be found in the full version of this paper [8].
LEMMA 1. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and TUmax(c, S)/NTUmax(c, S) = α. Then there is a cost vector c such that S is a cheapest feasible set and NTUmax(c , S)/NTUmin(c , S) ≥ α.
PROOF. Suppose that TUmax(c, S) = X and NTUmax(c, S) = Y where X/Y = α. Assume without loss of generality that S consists of elements 1, . . . , k, and let b1 = (b1 1, . . . , b1 k) and b2 = (b2 1, . . . , b2 k) be the bid vectors that correspond to TUmax(c, S) and NTUmax(c, S), respectively.
Construct the cost vector c by setting ci = ci for i ∈ S, ci = min{ci, b1 i } for i ∈ S. Clearly, S is a cheapest set under c .
Moreover, as the costs of elements outside of S remained the same, the right-hand sides of all constraints in (2) did not change, so any bid vector that satisfies (2) and (3) with respect to c, also satisfies them with respect to c . We will construct two bid vectors b3 and b4 that satisfy conditions (1), (2), and (3) for the cost vector c , and 339 X X X X X 0 X 12 3 X 4 5 6 Figure 2: Graph that separates payment bounds for vertex cover, n = 7 have P i∈S b3 i = X,
P i∈S b4 i = Y . As NTUmax(c , S) ≥ X and NTUmin(c , S) ≤ Y , this implies the lemma.
We can set b3 i = b1 i : this bid vector satisfies conditions (2) and (3) since b1 does, and we have b1 i ≥ min{ci, b1 i } = ci, which means that b3 satisfies condition (1). Furthermore, we can set b4 i = b2 i . Again, b4 satisfies conditions (2) and (3) since b2 does, and since b2 satisfies condition (1), we have b2 i ≥ ci ≥ ci, which means that b4 satisfies condition (1).
LEMMA 2. Suppose c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmax(c, S)/NTUmin(c, S) = α.
Then there is a cost vector c such that S is a cheapest feasible set and TUmax(c , S)/NTUmax(c , S) ≥ α.
LEMMA 3. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmax(c, S)/NTUmin(c, S) = α. Then there is a cost vector c such that S is a cheapest feasible set and NTUmin(c , S)/TUmin(c , S) ≥ α.
LEMMA 4. Suppose that c is a cost vector for (E, F) such that S is a cheapest feasible set and NTUmin(c, S)/TUmin(c, S) = α. Then there is a cost vector c such that S is a cheapest feasible set and NTUmax(c , S)/NTUmin(c , S) ≥ α.
In contrast to the case of path auctions, for vertex-cover auctions the gap between NTUmin(c) and NTUmax(c) (and hence between NTUmax(c) and TUmax(c), and between TUmin(c) and NTUmin(c)) can be proportional to the size of the graph.
PROPOSITION 5. For any n ≥ 3, there is a an n-vertex graph and a cost vector c for which TUmax(c)/NTUmax(c) ≥ n − 2.
PROOF. The underlying graph consists of an (n − 1)-clique on the vertices X1, . . . , Xn−1, and an extra vertex X0 adjacent to Xn−1. The costs are cX1 = cX2 = · · · = cXn−2 = 0, cX0 = cXn−1 = 1. We can assume that S = {X0, X1, . . . , Xn−2} (this is the lexicographically first vertex cover of cost 1). For this set system, the constraints in (2) are bXi + bX0 ≤ cXn−1 = 1 for i = 1, . . . , n − 2. Clearly, we can satisfy conditions (2) and (3) by setting bXi = 1 for i = 1, . . . , n − 2, bX0 = 0. Hence,
TUmax(c) ≥ n − 2. For NTUmax(c), there is an additional constraint bX0 ≥ 1, so the best we can do is to set bXi = 0 for i = 1, . . . , n − 2, bX0 = 1, which implies NTUmax(c) = 1.
Combining Proposition 5 with Lemmas 1 and 3, we derive the following corollaries.
COROLLARY 1. For any n ≥ 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmax(c)/NTUmin(c) ≥ n − 2.
COROLLARY 2. For any n ≥ 3, we can construct an instance of the vertex cover problem on a graph of size n that satisfies NTUmin(c)/TUmin(c) ≥ n − 2. j+2ix ij P \ P ij+2P \ P yijixix j j+1 i j+2ij+1 y y i i j+2ie j e j+1 e ij+1 P \ P Figure 3: Proof of Theorem 3: constraints for ˆPij and ˆPij+2 do not overlap
It turns out that the lower bound proved in the previous subsection is almost tight. More precisely, the following theorem shows that no two payment bounds can differ by more than a factor of n; moreover, this is the case not just for the vertex cover problem, but for general set systems. We bound the gap between TUmax(c) and TUmin(c). Since TUmin(c) ≤ NTUmin(c) ≤ NTUmax(c) ≤ TUmax(c), this bound applies to any pair of payment bounds.
THEOREM 2. For any set system (E, F) and any cost vector c, we have TUmax(c)/TUmin(c) ≤ n.
PROOF. Assume wlog that the winning set S consists of elements 1, . . . , k. Let c1, . . . , ck be the true costs of elements in S, let b1, . . . , bk be their bids that correspond to TUmin(c), and let b1 , . . . , bk be their bids that correspond to TUmax(c).
Consider the conditions (2) and (3) for S. One can pick a subset L of at most k inequalities in (2) so that for each i = 1, . . . , k there is at least one inequality in L that is tight for bi. Suppose that the jth inequality in L is of the form bi1 + · · · + bit ≤ c(Tj \ S). For bi, all inequalities in L are, in fact, equalities. Hence, by adding up all of them we obtain k P i=1,...,k bi ≥ P j=1,...,k c(Tj \ S).
On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e.,
P i=1,...,k bi ≤ P j=1,...,k c(Tj \ S).
Combining these two inequalities, we obtain nTUmin(c) ≥ kTUmin(c) ≥ TUmax(c).
REMARK 2. The final line of the proof of Theorem 2 shows that, in fact, the upper bound on TUmax(c)/TUmin(c) can be strengthened to the size of the winning set, k. Note that in Proposition 5, as well as in Corollaries 1 and 2, k = n−1, so these results do not contradict each other.
For path auctions, this upper bound can be improved to 2, matching the lower bounds of Section 4.1.
THEOREM 3. For any instance of the shortest path problem,
TUmax(c) ≤ 2 TUmin(c).
PROOF. Given a network (G, s, t), assume without loss of generality that the lexicographically-least cheapest s-t path, P, in G is {e1, . . . , ek}, where e1 = (s, v1), e2 = (v1, v2), . . . , ek = (vk−1, t). Let c1, . . . , ck be the true costs of e1, . . . , ek, and let b = (b1, . . . , bk) and b = (b1 , . . . , bk ) be bid vectors that correspond to TUmin(c) and TUmax(c), respectively.
For any i = 1, . . . , k, there is a constraint in (2) that is tight for bi with respect to the bid vector b , i.e., an s-t path Pi that avoids ei and satisfies b (P \Pi) = c(Pi \P). We can assume without loss of generality that Pi coincides with P up to some vertex xi, then deviates from P to avoid ei, and finally returns to P at a vertex 340 yi and coincides with P from then on (clearly, it might happen that s = xi or t = yi). Indeed, if Pi deviates from P more than once, one of these deviations is not necessary to avoid ei and can be replaced with the respective segment of P without increasing the cost of Pi. Among all paths of this form, let ˆPi be the one with the largest value of yi, i.e., the rightmost one. This path corresponds to an inequality Ii of the form bxi+1 + · · · + byi ≤ c( ˆPi \ P).
As in the proof of Theorem 2, we construct a set of tight constraints L such that every variable bi appears in at least one of these constraints; however, now we have to be more careful about the choice of constraints in L. We construct L inductively as follows.
Start by setting L = {I1}. At the jth step, suppose that all variables up to (but not including) bij appear in at least one inequality in L. Add Iij to L.
Note that for any j we have yij+1 > yij . This is because the inequalities added to L during the first j steps did not cover bij+1 .
See Figure 3. Since yij+2 > yij+1 , we must also have xij+2 > yij : otherwise, ˆPij+1 would not be the rightmost constraint for bij+1 . Therefore, the variables in Iij+2 and Iij do not overlap, and hence no bi can appear in more than two inequalities in L.
Now we follow the argument of the proof of Theorem 2 to finish.
By adding up all of the (tight) inequalities in L for bi we obtain 2 P i=1,...,k bi ≥ P j=1,...,k c( ˆPj \ P). On the other hand, all these inequalities appear in condition (2), so they must hold for bi , i.e.,
P i=1,...,k bi ≤ P j=1,...,k c( ˆPj \ P), so TUmax(c) ≤ 2TUmin(c).
VERTEX COVER Recall that for a vertex-cover auction on a graph G = (V, E), an allocation rule is an algorithm that takes as input a bid bv for each vertex and returns a vertex cover ˆS of G. As explained in Section 2, we can combine a monotone allocation rule with threshold payments to obtain a truthful auction.
Two natural examples of monotone allocation rules are Aopt, i.e., the algorithm that finds an optimal vertex cover, and the greedy algorithm AGR. However, Aopt cannot be guaranteed to run in polynomial time unless P = NP and AGR has approximation ratio of log n.
Another approximation algorithm for vertex cover, which has approximation ratio 2, is the local ratio algorithm ALR [2, 3]. This algorithm considers the edges of G one by one. Given an edge e = (u, v), it computes = min{bu, bv} and sets bu = bu − , bv = bv − . After all edges have been processed, ALR returns the set of vertices {v | bv = 0}. It is not hard to check that if the order in which the edges are considered is independent of the bids, then this algorithm is monotone as well. Hence, we can use it to construct a truthful auction that is guaranteed to select a vertex cover whose cost is within a factor of 2 from the optimal.
However, while the quality of the solution produced by ALR is much better than that of AGR, we still need to show that its total payment is not too high. In the next subsection, we bound the frugality ratio of ALR (and, more generally, all algorithms that satisfy the condition of local optimality, defined later) by 2Δ, where Δ is the maximum degree of G. We then prove a matching lower bound showing that for some graphs the frugality ratio of any truthful auction is at least Δ/2.
We say that an allocation rule is locally optimal if whenever bv >P w∼v bw, the vertex v is not chosen. Note that for any such rule the threshold bid of v satisfies tv ≤ P w∼v bw.
CLAIM 1. The algorithms Aopt, AGR, and ALR are locally optimal.
THEOREM 4. Any vertex cover auction M that has a locally optimal and monotone allocation rule and pays each agent his threshold bid has frugality ratio φNTUmin(M) ≤ 2Δ.
To prove Theorem 4, we first show that the total payment of any locally optimal mechanism does not exceed Δc(V ). We then demonstrate that NTUmin(c) ≥ c(V )/2. By combining these two results, the theorem follows.
LEMMA 5. Consider a graph G = (V, E) with maximum degree Δ. Let M be a vertex-cover auction on G that satisfies the conditions of Theorem 4. Then for any cost vector c, the total payment of M satisfies pM(c) ≤ Δc(V ).
PROOF. First note that any such auction is truthful, so we can assume that each agent"s bid is equal to his cost. Let ˆS be the vertex cover selected by M. Then by local optimality pM(c) = X v∈ ˆS tv ≤ X v∈ ˆS X w∼v cw ≤ X w∈V Δcw = Δc(V ).
We now derive a lower bound on TUmax(c); while not essential for the proof of Theorem 4, it helps us build the intuition necessary for that proof.
LEMMA 6. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, TUmax(c, S) ≥ c(V \ S) PROOF. For a vertex w with at least one neighbour in S, let d(w) denote the number of neighbours that w has in S. Consider the bid vector b in which, for each v ∈ S, bv = P w∼v,w∈S cw d(w) .
Then P v∈S bv = P v∈S P w∼v,w∈S cw/d(w) = P w /∈S cw = c(V \ S). To finish we want to show that b is feasible in the sense that it satisfies (2). Consider a vertex cover T, and extend the bid vector b by assigning bv = cv for v /∈ S. Then b(T) = c(T \S)+b(S∩T) ≥ c(T \S)+ X v∈S∩T X w∈S∩T :w∼v cw d(w) , and since all edges between S ∩ T and S go to S ∩ T, the righthand-side is equal to c(T \S)+ X w∈S∩T cw = c(T \S)+c(S ∩T) = c(V \S) = b(S).
Next, we prove a lower bound on NTUmax(c, S); we will then use it to obtain a lower bound on NTUmin(c).
LEMMA 7. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, NTUmax(c, S) ≥ c(V \ S) PROOF. If c(S) ≥ c(V \ S), by condition (1) we are done.
Therefore, for the rest of the proof we assume that c(S) < c(V \ S). We show how to construct a bid vector (be)e∈S that satisfies conditions (1) and (2) such that b(S) ≥ c(V \ S); clearly, this implies NTUmax(c, S) ≥ c(V \ S).
Recall that a network flow problem is described by a directed graph Γ = (VΓ, EΓ), a source node s ∈ VΓ, a sink node t ∈ VΓ, and a vector of capacity constraints ae, e ∈ EΓ. Consider a network (VΓ, EΓ) such that VΓ = V ∪{s, t}, EΓ = E1 ∪E2 ∪E3, where E1 = {(s, v) | v ∈ S}, E2 = {(v, w) | v ∈ S, w ∈ 341 V \ S, (v, w) ∈ E}, E3 = {(w, t) | w ∈ V \ S}. Since S is a vertex cover for G, no edge of E can have both of its endpoints in V \ S, and by construction, E2 contains no edges with both endpoints in S. Therefore, the graph (V, E2) is bipartite with parts (S, V \ S).
Set the capacity constraints for e ∈ EΓ as follows: a(s,v) = cv, a(w,t) = cw, a(v,w) = +∞ for all v ∈ S, w ∈ V \ S.
Recall that a cut is a partition of the vertices in VΓ into two sets C1 and C2 so that s ∈ C1, t ∈ C2; we denote such a cut by C = (C1, C2). Abusing notation, we write e = (u, v) ∈ C if u ∈ C1, v ∈ C2 or u ∈ C2, v ∈ C1, and say that such an edge e = (u, v) crosses the cut C. The capacity of a cut C is computed as cap(C) = P (v,w)∈C a(v,w). We have cap(s, V ∪{t}) = c(S), cap({s} ∪ V, t) = c(V \ S).
Let Cmin = ({s} ∪ S ∪ W , {t} ∪ S ∪ W ) be a minimum cut in Γ, where S , S ⊆ S, W , W ⊆ V \ S. See Figure 4. As cap(Cmin) ≤ cap(s, V ∪ {t}) = c(S) < +∞, and any edge in E2 has infinite capacity, no edge (u, v) ∈ E2 crosses Cmin.
Consider the network Γ = (VΓ , EΓ ), where VΓ = {s} ∪ S ∪ W ∪ {t}, EΓ = {(u, v) ∈ EΓ | u, v ∈ VΓ }. Clearly,
C = ({s} ∪ S ∪ W , {t}) is a minimum cut in Γ (otherwise, there would exist a smaller cut for Γ). As cap(C ) = c(W ), we have c(S ) ≥ c(W ).
Now, consider the network Γ = (VΓ , EΓ ), where VΓ = {s} ∪ S ∪ W ∪ {t}, EΓ = {(u, v) ∈ EΓ | u, v ∈ VΓ }.
Similarly, C = ({s}, S ∪ W ∪ {t}) is a minimum cut in Γ , cap(C ) = c(S ). As the size of a maximum flow from s to t is equal to the capacity of a minimum cut separating s and t, there exists a flow F = (fe)e∈EΓ of size c(S ). This flow has to saturate all edges between s and S , i.e., f(s,v) = cv for all v ∈ S . Now, increase the capacities of all edges between s and S to +∞. In the modified network, the capacity of a minimum cut (and hence the size of a maximum flow) is c(W ), and a maximum flow F = (fe)e∈EΓ can be constructed by greedily augmenting F.
Set bv = cv for all v ∈ S , bv = f(s,v) for all v ∈ S . As F is constructed by augmenting F, we have bv ≥ cv for all v ∈ S, i.e., condition (1) is satisfied.
Now, let us check that no vertex cover T ⊆ V can violate condition (2). Set T1 = T ∩ S , T2 = T ∩ S , T3 = T ∩ W ,
T4 = T ∩ W ; our goal is to show that b(S \ T1) + b(S \ T2) ≤ c(T3)+c(T4). Consider all edges (u, v) ∈ E such that u ∈ S \T1.
If (u, v) ∈ E2 then v ∈ T3 (no edge in E2 can cross the cut), and if u, v ∈ S then v ∈ T1∪T2. Hence, T1∪T3∪S is a vertex cover for G, and therefore c(T1)+ c(T3)+ c(S ) ≥ c(S) = c(T1)+ c(S \ T1) + c(S ). Consequently, c(T3) ≥ c(S \ T1) = b(S \ T1).
Now, consider the vertices in S \T2. Any edge in E2 that starts in one of these vertices has to end in T4 (this edge has to be covered by T, and it cannot go across the cut). Therefore, the total flow out of S \T2 is at most the total flow out of T4, i.e., b(S \T2) ≤ c(T4).
Hence, b(S \ T1) + b(S \ T2) ≤ c(T3) + c(T4).
Finally, we derive a lower bound on the payment bound that is of interest to us, namely, NTUmin(c).
LEMMA 8. For a vertex cover instance G = (V, E) in which S is a minimum vertex cover, NTUmin(c, S) ≥ c(V \ S) PROOF. Suppose for contradiction that c is a cost vector with minimum-cost vertex cover S and NTUmin(c, S) < c(V \S). Let b be the corresponding bid vector and let c be a new cost vector with cv = bv for v ∈ S and cv = cv for v ∈ S. Condition (2) guarantees that S is an optimal solution to the cost vector c . Now compute a bid vector b corresponding to NTUmax(c , S). We S" W"" S"" W" s t T1 T3 T2 T4 0 00 1 11 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 00 00 11 11 0 00 1 11 0 00 1 11 0 00 1 11 0000000 00000000000000 00000000000000 0000000 00000000000000 00000000000000 0000000 00000000000000 1111111 11111111111111 11111111111111 1111111 11111111111111 11111111111111 1111111 11111111111111 0000 00000000 00000000 0000 00000000 00000000 0000 00000000 1111 11111111 11111111 1111 11111111 11111111 1111 11111111 00 0000 0000 00 0000 0000 00 0000 11 1111 1111 11 1111 1111 11 1111 00 0000 0000 00 0000 0000 00 0000 11 1111 1111 11 1111 1111 11 1111 00000 0000000000 0000000000 00000 0000000000 0000000000 00000 0000000000 11111 1111111111 1111111111 11111 1111111111 1111111111 11111 1111111111 0000000 00000000000000 00000000000000 0000000 00000000000000 00000000000000 0000000 00000000000000 1111111 11111111111111 11111111111111 1111111 11111111111111 11111111111111 1111111 11111111111111 0000000 00000000000000 00000000000000 0000000 00000000000000 00000000000000 0000000 00000000000000 1111111 11111111111111 11111111111111 1111111 11111111111111 11111111111111 1111111 11111111111111 0000 00000000 00000000 0000 00000000 00000000 0000 00000000 1111 11111111 11111111 1111 11111111 11111111 1111 11111111 00 0000 0000 00 0000 0000 00 0000 11 1111 1111 11 1111 1111 11 1111000 000000 000000 000 000000 000000 000 000000 000000 000 000 111 111111 111111 111 111111 111111 111 111111 111111 111 111 0000000 00000000000000 00000000000000 0000000 00000000000000 00000000000000 0000000 00000000000000 00000000000000 0000000 00000000000000 1111111 11111111111111 11111111111111 1111111 11111111111111 11111111111111 1111111 11111111111111 11111111111111 1111111 11111111111111 000 000000 000000 000 000000 000000 000 000000 000000 000 000000 111 111111 111111 111 111111 111111 111 111111 111111 111 111111 000000 000000000000 000000000000 000000 000000000000 000000000000 000000 000000000000 000000000000 000000 000000000000 111111 111111111111 111111111111 111111 111111111111 111111111111 111111 111111111111 111111111111 111111 111111111111 000000 000000000000 000000000000 000000 000000000000 000000000000 000000 000000000000 000000000000 000000 000000000000 111111 111111111111 111111111111 111111 111111111111 111111111111 111111 111111111111 111111111111 111111 111111111111 000 000000 000000 000 000000 000000 000 000000 000000 000 000000 111 111111 111111 111 111111 111111 111 111111 111111 111 111111 Figure 4: Proof of Lemma 7. Dashed lines correspond to edges in E \ E2 claim that bv = cv for any v ∈ S. Indeed, suppose that bv > cv for some v ∈ S (bv = cv for v ∈ S by construction). As b satisfies conditions (1)-(3), among the inequalities in (2) there is one that is tight for v and the bid vector b. That is, b(S \ T) = c(T \ S). By the construction of c , c (S \ T) = c (T \ S). Now since bw ≥ cw for all w ∈ S, bv > cv implies b (S \T) > c (S \T) = c (T \S).
But this violates (2). So we now know b = c . Hence, we have NTUmax(c , S) = P v∈S bv = NTUmin(c, S) < c(V \ S), giving a contradiction to the fact that NTUmax(c , S) ≥ c (V \S) which we proved in Lemma 7.
As NTUmin(c, S) satisfies condition (1), it follows that we have NTUmin(c, S) ≥ c(S). Together will Lemma 8, this implies NTUmin(c, S) ≥ max{c(V \ S), c(S)} ≥ c(V )/2. Combined with Lemma 5, this completes the proof of Theorem 4.
REMARK 3. As NTUmin(c) ≤ NTUmax(c) ≤ TUmax(c), our bound of 2Δ extends to the smaller frugality ratios that we consider, i.e., φNTUmax(M) and φTUmax(M). It is not clear whether it extends to the larger frugality ratio φTUmin(M). However, the frugality ratio φTUmin(M) is not realistic because the payment bound TUmin(c) is inappropriately low - we show in Section 6 that TUmin(c) can be significantly smaller than the total cost of a cheapest vertex cover.
Extensions We can also apply our results to monotone vertex-cover algorithms that do not necessarily output locally-optimal solutions. To do so, we simply take the vertex cover produced by any such algorithm and transform it into a locally-optimal one, considering the vertices in lexicographic order and replacing a vertex v with its neighbours whenever bv > P u∼v bu. Note that if a vertex u has been added to the vertex cover during this process, it means that it has a neighbour whose bid is higher than bu, so after one pass all vertices in the vertex cover satisfy bv ≤ P u∼v bu. This procedure is monotone in bids, and it can only decrease the cost of the vertex cover.
Therefore, using it on top of a monotone allocation rule with approx342 imation ratio α, we obtain a monotone locally-optimal allocation rule with approximation ratio α. Combining it with threshold payments, we get an auction with φNTUmin ≤ 2Δ. Since any truthful auction has a monotone allocation rule, this procedure transforms any truthful mechanism for the vertex-cover problem into a frugal one while preserving the approximation ratio.
In this subsection, we prove that the upper bound of Theorem 4 is essentially optimal. Our proof uses the techniques of [9], where the authors prove a similar result for shortest-path auctions.
THEOREM 5. For any Δ > 0 and any n, there exist a graph G of maximum degree Δ and size N > n such that for any truthful mechanism M on G we have φNTUmin(M) ≥ Δ/2.
PROOF. Given n and Δ, set k = n/2Δ . Let G be the graph that consists of k blocks B1, . . . , Bk of size 2Δ each, where each Bi is a complete bipartite graph with parts Li and Ri, |Li| = |Ri| = Δ.
We will consider two families of cost vectors for G. Under a cost vector x ∈ X, each block Bi has one vertex of cost 1; all other vertices cost 0. Under a cost vector y ∈ Y , there is one block that has two vertices of cost 1, one in each part, all other blocks have one vertex of cost 1, and all other vertices cost 0. Clearly, |X| = (2Δ)k , |Y | = k(2Δ)k−1 Δ2 . We will now construct a bipartite graph W with the vertex set X ∪ Y as follows.
Consider a cost vector y ∈ Y that has two vertices of cost 1 in Bi; let these vertices be vl ∈ Li and vr ∈ Ri. By changing the cost of either of these vertices to 0, we obtain a cost vector in X.
Let xl and xr be the cost vectors obtained by changing the cost of vl and vr, respectively. The vertex cover chosen by M(y) must either contain all vertices in Li or it must contain all vertices in Ri.
In the former case, we put in W an edge from y to xl and in the latter case we put in W an edge from y to xr (if the vertex cover includes all of Bi, W contains both of these edges).
The graph W has at least k(2Δ)k−1 Δ2 edges, so there must exist an x ∈ X of degree at least kΔ/2. Let y1, . . . , ykΔ/2 be the other endpoints of the edges incident to x, and for each i = 1, . . . , kΔ/2, let vi be the vertex whose cost is different under x and yi; note that all vi are distinct.
It is not hard to see that NTUmin(x) ≤ k: the cheapest vertex cover contains the all-0 part of each block, and we can satisfy conditions (1)-(3) by letting one of the vertices in the all-0 part of each block to bid 1, while all other the vertices in the cheapest set bid 0.
On the other hand, by monotonicity of M we have vi ∈ M(x) for i = 1, . . . , kΔ/2 (vi is in the winning set under yi, and x is obtained from yi by decreasing the cost of vi), and moreover, the threshold bid of each vi is at least 1, so the total payment of M on x is at least kΔ/2. Hence, φNTUmin(M) ≥ M(x)/NTUmin(x) ≥ Δ/2.
REMARK 4. The lower bound of Theorem 5 can be generalised to randomised mechanisms, where a randomised mechanism is considered to be truthful if it can be represented as a probability distribution over truthful mechanisms. In this case, instead of choosing the vertex x ∈ X with the highest degree, we put both (y, xl) and (y, xr) into W , label each edge with the probability that the respective part of the block is chosen, and pick x ∈ X with the highest weighted degree. The argument can be further extended to a more permissive definition of truthfulness for randomised mechanisms, but this discussion is beyond the scope of this paper.
In this section we consider several desirable properties of payment bounds and evaluate the four payment bounds proposed in this paper with respect to them. The particular properties that we are interested in are independence of the choice of S (Section 6.3), monotonicity (Section 6.4.1), computational hardness (Section 6.4.2), and the relationship with other reasonable bounds, such as the total cost of the cheapest set (Section 6.1), or the total VCG payment (Section 6.2).
Our first requirement is that a payment bound should not be less than the total cost of the selected set. Payment bounds are used to evaluate the performance of set-system auctions. The latter have to satisfy individual rationality, i.e., the payment to each agent must be at least as large as his incurred costs; it is only reasonable to require the payment bound to satisfy the same requirement.
Clearly, NTUmax(c) and NTUmin(c) satisfy this requirement due to condition (1), and so does TUmax(c), since TUmax(c) ≥ NTUmax(c). However, TUmin(c) fails this test. The example of Proposition 4 shows that for path auctions, TUmin(c) can be smaller than the total cost by a factor of 2. Moreover, there are set systems and cost vectors for which TUmin(c) is smaller than the cost of the cheapest set S by a factor of Ω(n). Consider, for example, the vertex-cover auction for the graph of Proposition 5 with the costs cX1 = · · · = cXn−2 = cXn−1 = 1, cX0 = 0. The cost of a cheapest vertex cover is n − 2, and the lexicographically first vertex cover of cost n−2 is {X0, X1, . . . , Xn−2}. The constraints in (2) are bXi + bX0 ≤ cXn−1 = 1. Clearly, we can satisfy conditions (2) and (3) by setting bX1 = · · · = bXn−2 = 0, bX0 = 1, which means that TUmin(c) ≤ 1. This observation suggests that the payment bound TUmin(c) is too strong to be realistic, since it can be substantially lower than the cost of the cheapest feasible set.
Nevertheless, some of the positive results that were proved in [16] for NTUmin(c) go through for TUmin(c) as well. In particular, one can show that if the feasible sets are the bases of a monopolyfree matroid, then φTUmin(VCG) = 1. To show that φTUmin(VCG) is at most 1, one must prove that the VCG payment is at most TUmin(c). This is shown for NTUmin(c) in the first paragraph of the proof of Theorem 5 in [16]. Their argument does not use condition (1) at all, so it also applies to TUmin(c). On the other hand, φTUmin(VCG) ≥ 1 since φTUmin(VCG) ≥ φNTUmin(VCG) and φNTUmin(VCG) ≥ 1 by Proposition 7 of [16] (and also by Proposition 6 below).
Another measure of suitability for payment bounds is that they should not result in frugality ratios that are less then 1 for wellknown truthful mechanisms. If this is indeed the case, the payment bound may be too weak, as it becomes too easy to design mechanisms that perform well with respect to it. It particular, a reasonable requirement is that a payment bound should not exceed the total payment of the classical VCG mechanism.
The following proposition shows that NTUmax(c), and therefore also NTUmin(c) and TUmin(c), do not exceed the VCG payment pVCG(c). The proof essentially follows the argument of Proposition 7 of [16] and can be found in the full version of this paper [8].
PROPOSITION 6. φNTUmax(VCG) ≥ 1.
Proposition 6 shows that none of the payment bounds TUmin(c),
NTUmin(c) and NTUmax(c) exceeds the payment of VCG.
However, the payment bound TUmax(c) can be larger that the total 343 VCG payment. In particular, for the instance in Proposition 5, the VCG payment is smaller than TUmax(c) by a factor of n − 2. We have already seen that TUmax(c) ≥ n − 2. On the other hand, under VCG, the threshold bid of any Xi, i = 1, . . . , n − 2, is 0: if any such vertex bids above 0, it is deleted from the winning set together with X0 and replaced with Xn−1. Similarly, the threshold bid of X0 is 1, because if X0 bids above 1, it can be replaced with Xn−1. So the VCG payment is 1.
This result is not surprising: the definition of TUmax(c) implicitly assumes there is co-operation between the agents, while the computation of VCG payments does not take into account any interaction between them. Indeed, co-operation enables the agents to extract higher payments under VCG. That is, VCG is not groupstrategyproof. This suggests that as a payment bound, TUmax(c) may be too liberal, at least in a context where there is little or no co-operation between agents. Perhaps TUmax(c) can be a good benchmark for measuring the performance of mechanisms designed for agents that can form coalitions or make side payments to each other, in particular, group-strategyproof mechanisms.
Another setting in which bounding φTUmax is still of some interest is when, for the underlying problem, the optimal allocation and VCG payments are NP-hard to compute. In this case, finding a polynomial-time computable mechanism with good frugality ratio with respect to TUmax(c) is a non-trivial task, while bounding the frugality ratio with respect to more challenging payment bounds could be too difficult. To illustrate this point, compare the proofs of Lemma 6 and Lemma 7: both require some effort, but the latter is much more difficult than the former.
All payment bounds defined in this paper correspond to the total bid of all elements in the cheapest feasible set, where ties are broken lexicographically. While this definition ensures that our payment bounds are well-defined, the particular choice of the drawresolution rule appears arbitrary, and one might wonder if our payment bounds are sufficiently robust to be independent of this choice.
It turns out that is indeed the case for NTUmin(c) and NTUmax(c), i.e., these bounds do not depend on the draw-resolution rule. To see this, suppose that two feasible sets S1 and S2 have the same cost. In the computation of NTUmin(c, S1), all vertices in S1 \S2 would have to bid their true cost, since otherwise S2 would become cheaper than S1. Hence, any bid vector for S1 can only have be = ce for e ∈ S1 ∩ S2, and hence constitutes a valid bid vector for S2 and vice versa. A similar argument applies to NTUmax(c).
However, for TUmin(c) and TUmax(c) this is not the case.
For example, consider the set system E = {e1, e2, e3, e4, e5},
F = {S1 = {e1, e2}, S2 = {e2, e3, e4}, S3 = {e4, e5}} with the costs c1 = 2, c2 = c3 = c4 = 1, c5 = 3. The cheapest sets are S1 and S2. Now TUmax(c, S1) ≤ 4, as the total bid of the elements in S1 cannot exceed the total cost of S3. On the other hand, TUmax(c, S2) ≥ 5, as we can set b2 = 3, b3 = 0, b4 = 2.
Similarly, TUmin(c, S1) = 4, because the inequalities in (2) are b1 ≤ 2 and b1 + b2 ≤ 4. But TUmin(c, S2) ≤ 3 as we can set b2 = 1, b3 = 2, b4 = 0.
The results in [16] and our vertex cover results are proved for the frugality ratio φNTUmin. Indeed, it can be argued that φNTUmin is the best definition of frugality ratio, because among all reasonable payment bounds (i.e., ones that are at least as large as the cost of the cheapest feasible set), it is most demanding of the algorithm.
However, NTUmin(c) is not always the easiest or the most natural payment bound to work with. In this subsection, we discuss several disadvantages of NTUmin(c) (and also TUmin(c)) as compared to NTUmax(c) and TUmax(c).
The first problem with NTUmin(c) is that it is not monotone with respect to F, i.e., it may increase when one adds a feasible set to F. (It is, however, monotone in the sense that a losing agent cannot become a winner by raising his cost.) Intuitively, a good payment bound should satisfy this monotonicity requirement, as adding a feasible set increases the competition, so it should drive the prices down. Note that this indeed the case for NTUmax(c) and TUmax(c) since a new feasible set adds a constraint in (2), thus limiting the solution space for the respective linear program.
PROPOSITION 7. Adding a feasible set to F can increase the value of NTUmin(c) by a factor of Ω(n).
PROOF. Let E = {x, xx, y1, . . . , yn, z1, . . . , zn}. Set Y = {y1, . . . , yn}, S = Y ∪ {x}, Ti = Y \ {yi} ∪ {zi}, i = 1, . . . , n, and suppose that F = {S, T1, . . . , Tn}. The costs are cx = 0, cxx = 0, cyi = 0, czi = 1 for i = 1, . . . , n. Note that S is the cheapest feasible set. Let F = F ∪ {T0}, where T0 = Y ∪ {xx}. For F, the bid vector by1 = · · · = byn = 0, bx = 1 satisfies (1), (2), and (3), so NTUmin(c) ≤ 1. For F , S is still the lexicographically-least cheapest set. Any optimal solution has bx = 0 (by constraint in (2) with T0). Condition (3) for yi implies bx + byi = czi = 1, so byi = 1 and NTUmin(c) = n.
For path auctions, it has been shown [18] that NTUmin(c) is non-monotone in a slightly different sense, i.e., with respect to adding a new edge (agent) rather than a new feasible set (a team of existing agents).
REMARK 5. We can also show that NTUmin(c) is non-monotone for vertex cover. In this case, adding a new feasible set corresponds to deleting edges from the graph. It turns out that deleting a single edge can increase NTUmin(c) by a factor of n − 2; the construction is similar to that of Proposition 5.
Another problem with NTUmin(c, S) is that it is NP-hard to compute even if the number of feasible sets is polynomial in n.
Again, this puts it at a disadvantage compared to NTUmax(c, S) and TUmax(c, S) (see Remark 1).
THEOREM 6. Computing NTUmin(c) is NP-hard, even when the lexicographically-least feasible set S is given in the input.
PROOF. We reduce EXACT COVER BY 3-SETS(X3C) to our problem. An instance of X3C is given by a universe G = {g1, . . . , gn} and a collection of subsets C1, . . . , Cm, Ci ⊂ G, |Ci| = 3, where the goal is to decide whether one can cover G by n/3 of these sets.
Observe that if this is indeed the case, each element of G is contained in exactly one set of the cover.
LEMMA 9. Consider a minimisation problem P of the following form: Minimise P i=1,...,n bi under conditions (1) bi ≥ 0 for all i = 1, . . . , n; (2) for any j = 1, . . . , k we have P bi∈Sj bi ≤ aj, where Sj ⊆ {b1, . . . , bn}; (3) for each bj , one of the constraints in (2) involving it is tight. For any such P, one can construct a set system S and a vector of costs c such that NTUmin(c) is the optimal solution to P.
PROOF. The construction is straightforward: there is an element of cost 0 for each bi, an element of cost aj for each aj, the feasible solutions are {b1, . . . , bn}, or any set obtained from {b1, . . . , bn} by replacing the elements in Sj by aj. 344 By this lemma, all we have to do to prove Theorem 6 is to show how to solve X3C by using the solution to a minimisation problem of the form given in Lemma 9. We do this as follows. For each Ci, we introduce 4 variables xi, ¯xi, ai, and bi. Also, for each element gj of G there is a variable dj. We use the following set of constraints: • In (1), we have constraints xi ≥ 0, ¯xi ≥ 0, ai ≥ 0, bi ≥ 0, dj ≥ 0 for all i = 1, . . . , m, j = 1, . . . , n. • In (2), for all i = 1, . . . , m, we have the following 5 constraints: xi + ¯xi ≤ 1, xi +ai ≤ 1, ¯xi +ai ≤ 1, xi +bi ≤ 1, ¯xi + bi ≤ 1. Also, for all j = 1, . . . , n we have a constraint of the form xi1 + · · · + xik + dj ≤ 1, where Ci1 , . . . , Cik are the sets that contain gj.
The goal is to minimize z = P i(xi + ¯xi + ai + bi) + P j dj.
Observe that for each j, there is only one constraint involving dj , so by condition (3) it must be tight.
Consider the two constraints involving ai. One of them must be tight, and therefore xi +¯xi +ai +bi ≥ xi +¯xi +ai ≥ 1. Hence, for any feasible solution to (1)-(3) we have z ≥ m. Now, suppose that there is an exact set cover. Set dj = 0 for j = 1, . . . , n. Also, if Ci is included in this cover, set xi = 1, ¯xi = ai = bi = 0, otherwise set ¯xi = 1, xi = ai = bi = 0. Clearly, all inequalities in (2) are satisfied (we use the fact that each element is covered exactly once), and for each variable, one of the constraints involving it is tight. This assignment results in z = m.
Conversely, suppose there is a feasible solution with z = m.
As each addend of the form xi + ¯xi + ai + bi contributes at least 1, we have xi + ¯xi + ai + bi = 1 for all i, dj = 0 for all j.
We will now show that for each i, either xi = 1 and ¯xi = 0, or xi = 0 and ¯xi = 1. For the sake of contradiction, suppose that xi = δ < 1, ¯xi = δ < 1. As one of the constraints involving ai must be tight, we have ai ≥ min{1 − δ, 1 − δ }. Similarly, bi ≥ min{1 − δ, 1 − δ }. Hence, xi + ¯xi + ai + bi = 1 = δ +δ +2 min{1−δ, 1−δ } > 1. To finish the proof, note that for each j = 1, . . . , m we have xi1 + · · · + xik + dj = 1 and dj = 0, so the subsets that correspond to xi = 1 constitute a set cover.
REMARK 6. In the proofs of Proposition 7 and Theorem 6 all constraints in (1) are of the form be ≥ 0. Hence, the same results are true for TUmin(c).
REMARK 7. For shortest-path auctions, the size of F can be superpolynomial. However, there is a polynomial-time separation oracle for constraints in (2) (to construct one, use any algorithm for finding shortest paths), so one can compute NTUmax(c) and TUmax(c) in polynomial time. On the other hand, recently and independently it was shown [18] that computing NTUmin(c) for shortest-path auctions is NP-hard.

Securities markets effectively allow traders to place bets on the outcomes of uncertain future propositions. Examples include stock markets like NASDAQ, options markets like CBOE [17], futures markets like CME [30], other derivatives markets, insurance markets, political stock markets [11, 12], sports betting markets [7, 13, 32], horse racing markets [33], idea futures markets [16], decision markets [14] and even market games [4, 24, 25]. The economic value of securities markets is two-fold. First, they allow traders to hedge risk, or to insure against undesirable outcomes. For example, the owner of a stock might buy a put option (the right to sell the stock at a particular price) in order to insure against a stock downturn. Or the owner of a house may purchase an insurance contract to hedge against unforeseen damage to the house. Second, securities markets allow traders to speculate, or to obtain a subjective expected profit when market prices do not reflect their assessment of the likelihood of future outcomes. For example, a trader might buy a call option if he believes that the likelihood is high that the price of the underlying stock will go up, regardless of risk exposure to changes in the stock price. Because traders stand to earn a profit if they can make effective probability assessments, often prices in financial markets yield very accurate aggregate forecasts of future events [10, 29, 27, 28].
Real securities markets have complex payoff structures with various triggers. However, these can all be modeled as collections of more basic or atomic Arrow-Debreu securities [1, 8, 20]. One unit of one Arrow-Debreu security pays off one dollar if and only if (iff) a corresponding binary event occurs; it pays nothing if the event does not occur. So, for example, one unit of a security denoted Acme100 might pay $1 iff Acme"s stock is above $100 on January 4, 2004.
An Acme stock option as it would be defined on a finan144 cial exchange can be though of as a portfolio of such atomic securities.1 In this paper, we develop and analyze a framework for trading in compound securities markets with payoffs contingent on arbitrary logical combinations of events, including conditionals. For example, given binary events A, B, and C, one trader might bid to buy three units of a security denoted A ∧ ¯B ∨ C that pays off $1 iff the compound event A ∧ ¯B ∨ C occurs for thirty cents each. Another trader may bid to sell six units of a security A|C that pays off $1 iff A occurs for fifty-five cents each, conditional on event C occurring, meaning that the transaction is revoked if C does not occur (i.e., no payoff is given and the price of the security is refunded) [5]. Bids may also be divisible, meaning that bidders are willing to accept less than the requested quantity, or indivisible, meaning that bids must be fulfilled either completely or not at all. Given a set of such bids, the auctioneer faces a complex matching problem to decide which bids are accepted for how many units at what price.
Typically, the auctioneer seeks to take on no risk of its own, only matching up agreeable trades among the bidders, but we also consider alternative formulations where the auctioneer acts as a market maker willing to accept some risk.
We examine the computational complexity of the auctioneer"s matching problem. Let the length of the description of all the available securities be O(n). With n events, the matching problem is co-NP-complete in the divisible case and Σp 2-complete in the indivisible case. This Σp 2-complete hardness holds even when the bidding language is significantly restricted. With log n events, the problem is polynomial in the divisible case and NP-complete in the indivisible case.
Section 2 presents some necessary background information, motivation, and related work. Section 3 formally describes our framework for compound securities, and defines the auctioneer"s matching problem. Section 4 briefly discusses natural algorithms for solving the matching problem.
Section 5 proves our central computational complexity results. Section 6 discusses the possibility of tractable special cases. Section 7 concludes with a summary and some ideas of future directions.
Imagine a world where there are only two future uncertain events of any consequence: (1) the event that one"s house is struck by lightning by December 31, 2003, denoted struck, and (2) the event that Acme"s stock price goes above $100 by January 4, 2004, denoted acme100. In this simple world there are four possible future states-all possible combinations of the binary events" outcomes: struck ∧ acme100, struck ∧ acme100, struck ∧ acme100, struck ∧ acme100.
Hedging risk can be thought of as an action of moving money between various possible future states. For example, insur1 Technically, an option is a portfolio of infinitely many atomic securities, though it can be approximately modeled with a finite number. ing one"s house transfers money from future states where struck is not true to states where it is. Selling a security denoted acme100 -that pays off $1 iff the event acme100 occurs-transfers money from future states where Acme"s price is above $100 on January 4 to states where it"s not.
Speculating is also an act of transferring money between future states, though usually associated with maximizing expected return rather than reducing risk. For example, betting on a football team moves money from the team loses state to the team wins state. In practice, agents engage in a mixture of hedging and speculating, and there is no clear dividing line between the two [18].
All possible future outcomes form a state space Ω, consisting of mutually exclusive and exhaustive states ω ∈ Ω.
Often a more natural way to think of possible future outcomes is as an event space A of linearly independent events A ∈ A that may overlap arbitrarily. So in our toy example struck ∧ acme100 is one of the four disjoint states, while struck is one of the two events. Note that a set of n linearly independent events defines a state space Ω of size 2n consisting of all possible combinations of event outcomes.
Conversely, any state space Ω can be factored into log |Ω| events.
Suppose that A exhaustively covers all meaningful future outcomes (i.e., covers all eventualities that agents may wish to hedge against and/or speculate upon). Then the existence of 2n linearly independent securities-called a complete market-allows agents to distribute their wealth arbitrarily across future states.2 An agent may create any hedge or speculation it desires. Under classical conditions, agents trading in a complete market form an equilibrium where risk is allocated Pareto optimally. If the market is incomplete, meaning it consists of fewer than 2n linearly independent securities, then in general agents cannot construct arbitrary hedges and equilibrium allocations may be nonoptimal [1, 8, 19, 20].
In real-world settings, the number of meaningful events n is large and thus the number of securities required for completeness is intractable. No truly complete market exists or will ever exist. One motivation behind compound securities markets is to provide a mechanism that supports the most transfer of risk using the least number of transactions possible. Compound securities allow a high degree of expressivity in constructing bids. The tradeoff for increased expressivity is increased computational complexity, from both the bidder"s and auctioneer"s point of view.
The quest to reduce the number of financial instruments required to support an optimal allocation of risk dates to Arrow"s original work [1]. The requirement stated above of only 2n linearly-independent securities is itself a reduction from the most straightforward formulation. In an economy with k standard goods, the most straightforward complete market contains k·2n securities, each paying off in one good under one state realization. Arrow [1] showed that a market where securities and goods are essentially separated, with 2n securities paying off in a single numeraire good plus k spot markets in the standard goods, is also complete. For our purposes, we need consider only the securities market. 2 By linearly independent securities, we mean that the vectors of payoffs in all future states of these securities are linearly independent. 145 Varian [34] shows that a complete market can be constructed using fewer than 2n securities, replacing the missing securities with options. Still, the number of linearly independent financial instruments-securities plus optionsmust be 2n to guarantee completeness.
Though the requirement of 2n financial instruments cannot be relaxed if one wants to guarantee completeness in all circumstances, Pennock and Wellman [26] explore conditions under which a smaller securities market may be operationally complete, meaning that its equilibrium is Pareto optimal with respect to the agents involved, even if the market contains less than 2n securities. The authors show that in some cases the market can be structured and compacted in analogy to Bayesian network representations of joint probability distributions [23]. They show that, if all agents" risk-neutral independencies agree with the independencies encoded in the market structure, then the market is operationally complete. For collections of agents all with constant absolute risk aversion, agreement on Markov independencies is sufficient.
Bossaerts, Fine, and Ledyard [2] develop a mechanism they call combined-value trading (CVT) that allows traders to order an arbitrary portfolio of securities in one bid, rather than breaking up the order into a sequence of bids on individual securities. If the portfolio order is accepted, all of the implied trades on individual securities are executed simultaneously, thus eliminating so-called execution risk that prices will change in the middle of a planned sequence of orders. The authors conduct laboratory experiments showing that, even in thin markets where ordinary sequential trading breaks down, CVT supports efficient pricing and allocation. Note that CVT differs significantly from compound securities trading. CVT allows instantaneous trading of any linear combination of securities, while compound securities allow more expressive securities that can encode nonlinear boolean combinations of events. For example, CVT may allow an agent to order securities A and B in a bundle that pays off as a linear combination of A and B,3 but CVT won"t allow the construction of a compound security A ∧ B that pays off $1 iff both A and B occur, or a compound security A|B .
Related to CVT are combinatorial auctions [6, 21] and exchanges [31], mechanisms that have recently received quite a bit of attention in the economics and computer science literatures. Combinatorial auctions allow bidders to place distinct values on all possible bundles of goods rather than just on individual goods. In this way bidders can express substitutability and complementarity relationships among goods that cannot be expressed in standard parallel or sequential auctions. Compound securities differ from combinatorial auctions in concept and complexity. Compound securities allow bidders to construct an arbitrary bet on any of the 22n possible compound events expressible as logical functions of the n base events, conditional on any other of the 22n  compound events. Agents optimize based on their own subjective probabilities and risk attitude (and in general, their beliefs about other agents" beliefs and utilities, ad infinitum).
The central auctioneer problem is identifying arbitrage opportunities: that is, to match bets together without taking on any risk. Combinatorial auctions, on the other hand, allow bids on any of the 2n bundles of n goods. Typically, 3 Specifically, one unit of each pays off $2 iff both A and B occur, $1 iff A or B occurs (but not both), and $0 otherwise. uncertainty-and thus risk-is not considered. The central auctioneer problem is to maximize social welfare. Also note that the problems lie in different complexity classes. While clearing a combinatorial auction is polynomial in the divisible case and NP-complete in the indivisible case, matching in a compound securities market is NP-complete in the divisible case and Σp 2-complete in the indivisible case. In fact, even the problem of deciding whether two bids on compound securities match, even in the divisible case, is NP-complete (see Section 5.2).
There is a sense in which it is possible to translate our matching problem for compound securities into an analogous problem for clearing two-sided combinatorial exchanges [31] of exponential size. Specifically, if we regard payoff in a particular state as a good, then compound securities can be viewed as bundles of (fractional quantities of) such goods.
The material balance constraint facing the combinatorial auctioneer corresponds to a restriction that the compoundsecurity auctioneer be disallowed from assuming any risk.
Note that this translation is not at all useful for addressing the compound-security matching problem, as the resulting combinatorial exchange has an exponential number of goods.
Hanson [15] develops a market mechanism called a market scoring rule that is especially well suited for allowing bets on a combinatorial number of outcomes. The mechanism maintains a joint probability distribution over all 2n states, either explicitly or implicitly using a Bayesian network or other compact representation. At any time any trader who believes the probabilities are wrong can change any part of the distribution by accepting a lottery ticket that pays off according to a scoring rule (e.g., the logarithmic scoring rule) [35], as long as that trader also agrees to pay off the most recent person to change the distribution. In the limit of a single trader, the mechanism behaves like a scoring rule, suitable for polling a single agent for its probability distribution. In the limit of many traders, it produces a combined estimate. Since the market essentially always has a complete set of posted prices for all possible outcomes, the mechanism avoids the problem of thin markets, or illiquidity, that necessarily plagues any market containing an exponential number of alternative investments. The mechanism requires a patron to pay off the final person to change the distribution, though the patron"s payment is bounded.
Though Hanson offers some initial suggestions, several open problems remain, including efficient methods for representing and updating the joint distribution and recording traders positions and portfolios, without resorting to exponential time and space algorithms.
Fagin, Halpern, and Megiddo [9] give a sound and complete axiomatization for deciding whether sets of probabilistic inequalities are consistent. Bids for compound securities can be thought of as expressions of probabilistic inequalities: for example, a bid to buy A ∧ B at price 0.3 is a statement that the probability of A ∧ B is greater than 0.3.
If a set of single-unit bids correspond to a set of inconsistent probabilistic inequalities, then there is a match. However, because they are interested in a much different framework,
Fagin et al. do not consider several complicating factors specific to the securities market framework: namely, handling multi-unit or fractional bid quantities, identifying matches, choosing among multiple matches, and optimizing based on probabilities and risk attitudes. We address these issues below. 146
COMPOUND SECURITIES
Common knowledge among agents is the set of events A.
There are no predefined securities. Instead, agents offer to buy or sell securities of their own design that pay off contingent on logical combinations of events and event negations.
Combination operators may include conjunctions, disjunctions, and conditionals.
For all practical purposes, it is impossible for agents to trade in enough securities (2n ) to form a complete market, so agents must devise their best tradeoff between the number and complexity of their bids, and the extent to which their risks are hedged and desirable bets are placed. In its most general form, the problem is game-theoretic in nature, since what an agent should offer depends on what it believes other agents will accept. At the other end of the spectrum, a simplified version of the problem is to optimize bids only on currently available securities at current prices. In between these two formulations are other possible interesting optimization problems. Approximation algorithms might also be pursued.
The auctioneer faces a nontrivial problem of matching buy and sell orders to maximize surplus (the cash and securities left over after accepted bids are fulfilled). For example, offers to sell A1A2 at $0.2 and A1 ¯A2 at $0.1 can match with an offer to buy A1 at $0.4, with surplus $0.1. Or an offer to sell A1 at $0.3 can match with an offer to buy A1A2 at $0.4, with surplus $0.1 in cash and A1 ¯A2 in securities. In general, a single security might qualify for multiple matches, but only one can be transacted. So the auctioneer must find the optimal set of matches that maximizes surplus, which could be measured in a number of ways. Again, approximation algorithms might be considered. In another formulation, the auctioneer functions as a market maker willing to take on a certain amount of risk.
Informally, our motivation is to provide a mechanism that allows a very high degree of expressivity in placing hedges and bets, and is also capable of approximating the optimal (complete-market) allocation of risk, trading off the number and complexity of securities and transactions needed.
We use φ and ψ to denote arbitrary boolean formulas, or logical combinations of events in A. We denote securities φ|ψ . Securities pay off $1 if and only if (iff) φ and ψ are true, pay off $0 iff φ is false and ψ is true, and are canceled (i.e., any price paid is refunded) iff ψ is false. We define T ≡ Ω to be the event true and F ≡ ∅ to be the event false. We abbreviate φ|T as φ .
Agents place orders, denoted o, of the form q units of φ|ψ at price p per unit, where q > 0 implies a buy order and q < 0 implies a sell order. We assume agents submitting buy (sell) orders will accept any price p∗ ≤ p (p∗ ≥ p). We distinguish between divisible and indivisible orders. Agents submitting divisible orders will accept any quantity αq where 0 < α ≤ 1. Agents submitting indivisible orders will accept only exactly q units, or none at all.
We believe that, given the nature of what is being traded (state-contingent dollars), most agents will be content to trade using divisible orders.
Every order o can be translated into a payoff vector Υ across all states ω ∈ Ω. The payoff Υ ω in state ω is q · 1ω∈ψ(1ω∈φ − p), where 1ω∈E equals 1 iff ω ∈ E and zero otherwise. Recall that the 2n states correspond to the 2n possible combinations of event outcomes. We index multiple orders with subscripts (e.g., oi and Υi). Let the set of all orders be O and the set of all corresponding payoff vectors be P.
Example 1. (Translating orders into payoff vectors) Suppose that |A| = 3. Consider an order to buy two units of A2 ∨ A3|A1 at price $0.8. The corresponding payoff vector is: Υ = Υ A1A2A3 , Υ A1A2 ¯A3 , Υ A1 ¯A2A3 , . . . , Υ ¯A1 ¯A2 ¯A3 = 2 · 0.2, 0.2, 0.2, −0.8, 0, 0, 0, 0 2
The auctioneer"s task, called the matching problem, is to determine which orders to accept among all orders o ∈ O.
Let αi be the fraction of order oi accepted by the auctioneer (in the indivisible case, αi must be either 0 or 1; in the divisible case, αi can range from 0 to 1). If αi = 0, then order oi is considered rejected and no transactions take place concerning this order. For accepted orders (αi > 0), the auctioneer receives the money lost by bidders and pays out the money won by bidders, so the auctioneer"s payoff vector is: Υauc = X Υi∈P −αiΥi.
We also call the auctioneer"s payoff vector the surplus vector, since it is the (possibly state-contingent) money left over after all accepted orders are filled.
Assume that the auctioneer wants to choose a set of orders so that it is guaranteed not to lose any money in any future state, but that the auctioneer does not necessarily insist on obtaining a positive benefit from the transaction (i.e., the auctioneer is content to break even).
Definition 1. (Matching problem, indivisible case) Given a set of orders O, does there exist αi ∈ {0, 1} with at least one αi = 1 such that ∀ω, Υ ω auc ≥ 0?
In other words, does there exist a nonempty subset of orders that the auctioneer can accept without risk? 2 If ∀ω, Υ ω auc = c where c is nonnegative, then the surplus leftover after processing this match is c dollars. Let m = minω[Υ ω auc]. In general, processing a match leaves m dollars in cash and Υ ω auc − m in state-contingent dollars, which can then be translated into securities.
Example 2. (Indivisible order matching) Suppose |A| =
147 corresponding payoff vectors are: Υ1 = Υ A1A2 1 ,Υ A1 ¯A2 1 ,Υ ¯A1A2 1 ,Υ ¯A1 ¯A2 1 = 0.6, −0.4, −0.4, −0.4 Υ2 = −0.7, −0.7, 0.3, 0.3 The auctioneer"s payoff vector (the negative of the componentwise sum of the above two vectors) is: Υauc = −Υ1 − Υ2 = 0.1, 1.1, 0.1, 0.1 .
Since all components are nonnegative, the two orders match.
The auctioneer can process both orders, leaving a surplus of $0.1 in cash and one unit of A1 ¯A2 in securities. 2 Now consider the divisible case, where order can be partially filled.
Definition 2. (Matching problem, divisible case) Given a set of orders O, does there exist αi ∈ [0, 1] with at least one αi > 0 such that ∀ω, Υ ω auc ≥ 0, 2 Example 3. (Divisible order matching) Suppose |A| = 2.
Consider an order to sell one unit of A1 at price $0.5, an order to buy one unit of A1A2|A1 ∨ A2 at price $0.5, and an order to buy one unit of A1| ¯A2 at price $0.4. The corresponding payoff vectors are: Υ1 = Υ A1A2 1 ,Υ A1 ¯A2 1 ,Υ ¯A1A2 1 ,Υ ¯A1 ¯A2 1 = −0.5, −0.5, 0.5, 0.5 Υ2 = 0.5, −0.5, −0.5, 0 Υ3 = 0, 0.6, 0, −0.4 It is clear by inspection that no non-empty subset of whole orders constitutes a match: in all cases where αi ∈ {0, 1} (other than all αi = 0), at least one state sums to a positive amount (negative for the auctioneer). However, if α1 = α2 = 3/5 and α3 = 1, then the auctioneer"s payoff vector is: Υauc = − 3 5 Υ1 − 3 5 Υ2 − Υ3 = 0, 0, 0, 0.1 , constituting a match. The auctioneer can process 3/5 of the first and second orders, and all of the third order, leaving a surplus of 0.1 units of ¯A1 ¯A2 . In this example, a divisible match exists even though an indivisible match is not possible; we examine the distinction in detail in Section 5, where we separate the two matching problems into distinct complexity classes. 2 The matching problems defined above are decision problems: the task is only to show the existence or nonexistence of a match. However, there may be multiple matches from which the auctioneer can choose. Sometimes the choices are equivalent from the auctioneer"s perspective; alternatively, an objective function can be used to find an optimal match according to that objective.
Example 4. (Auctioneer alternatives I) Suppose |A| =
an order to sell one unit of A2 at price $0.7, an order to buy one unit of A1A2 at price $0.4, an order to buy one unit of A1 ¯A2 at price $0.4, and an order to buy one unit of ¯A1A2 at price $0.4. The corresponding payoff vectors are: Υ1 = −0.3,−0.3, 0.7, 0.7 Υ2 = −0.3, 0.7,−0.3, 0.7 Υ3 = 0.6,−0.4,−0.4,−0.4 Υ4 = −0.4, 0.6,−0.4,−0.4 Υ5 = −0.4,−0.4, 0.6,−0.4 Consider the indivisible case. The auctioneer could choose to accept bids 1, 3, and 4 together, or the auctioneer could choose to accept bids 2, 3, and 5 together. Both constitute matches, and in fact both yield identical payoffs (Υauc =
Example 5. (Auctioneer alternatives II) Suppose |A| =
an order to buy one unit of A1A2 at price $0.3, and an order to buy one unit of A1 ¯A2 at price $0.5. The corresponding payoff vectors are: Υ1 = −0.4,−0.4, 0.6, 0.6 Υ2 = 0.7,−0.3,−0.3,−0.3 Υ3 = −0.5, 0.5,−0.5,−0.5 Consider the divisible case. The auctioneer could choose to accept one unit each of all three bids, yielding a payoff to the auctioneer of $0.2 in cash (Υauc = 0.2, 0.2, 0.2, 0.2 ).
Alternatively, the auctioneer could choose to accept 4/3 units of bid 1, and one unit each of bids 2 and 3, yielding a payoff to the auctioneer of 1/3 units of security A1 . Both choices constitute matches (in fact, accepting any number of units of bid 1 between 1 and 4/3 can be part of a match), though depending on the auctioneer"s objective, one choice might be preferred over another. For example, if the auctioneer believes that A1 is very likely to occur, it may prefer to accept 4/3 units of bid 1. 2 There are many possible criteria for the auctioneer to decide among matches, all of which seem reasonable in some circumstances. One natural quantity to maximize is the volume of trade among bidders; another is the auctioneer"s utility, either with or without the arbitrage constraint.
Definition 3. (Trade maximization problem) Given a set of indivisible (divisible) orders O, choose αi ∈ {0, 1} (αi ∈ [0, 1]) to maximize X i αiqi, under the constraint that ∀ω, Υ ω auc ≥ 0. 2 Another reasonable variation is to maximize the total percent of orders filled, or P i αi, under the same (risk-free) constraint that ∀ω, Υ ω auc ≥ 0.
Definition 4. (Auctioneer risk-free utility-maximization problem) Let the auctioneer"s subjective probability for each state ω be Pr(ω), and let the auctioneer"s utility for y dollars be u(y). Given a set of indivisible (divisible) orders O, choose αi ∈ {0, 1} (αi ∈ [0, 1]) to maximize X ω∈Ω Pr(ω)u(Υ ω auc), 148 under the constraint that ∀ω, Υ ω auc ≥ 0. 2 Definition 5. (Auctioneer standard utility-maximization problem) Let the auctioneer"s subjective probability for each state ω be Pr(ω), and let the auctioneer"s utility for y dollars be u(y). Given a set of indivisible (divisible) orders O, choose αi ∈ {0, 1} (αi ∈ [0, 1]) to maximize X ω∈Ω Pr(ω)u  Υ ω auc  . 2 This last objective function drops the risk-free (arbitrage) constraint. In this case, the auctioneer is a market maker with beliefs about the likelihood of outcomes, and the auctioneer may actually lose money is some outcomes.
Still other variations and other optimization criteria seem reasonable, including social welfare, etc. It also seems reasonable to suppose that the surplus be shared among bidders and the auctioneer, rather than retained solely by the auctioneer. This is analogous to choosing a common transaction price in a double auction (e.g., the midpoint between the bid and ask prices), rather than the buyer paying the bid price and the seller receiving the ask price, with the difference going to the auctioneer. The problem becomes more complicated when dividing surplus securities, in part because they are valued differently by different agents. Formulating reasonable sharing rules and examining the resulting incentive properties seems a rich and promising avenue for further investigation.
The straightforward algorithm for solving the divisible matching problem is linear programming; we set up an appropriate linear program in Section 5.1. The straightforward algorithm for solving the indivisible matching problem is integer programming. With n events, to set up the appropriate linear or integer programs, simply writing out the payoff vectors in the straightforward way requires O(2n ) space.
There is some hope that specialized algorithms that exploit structure among bids can perform better in terms of average-case time and space complexity. For example, in some cases matches can be identified using logical reduction techniques, without writing down the full payoff vectors. So a match between the following bids: • sell 1 of A1A2 at $0.2 • sell 1 of A1 ¯A2 at $0.1 • buy 1 of A1 at $0.4 can be identified by reducing the first two bids to an equivalent offer to sell A1 at $0.3 that clearly matches with the third bid. Formalizing a logical-reduction algorithm for matching, or other algorithms that can exploit special structure among the bids, is a promising avenue for future work.
OF MATCHING In this section we examine the computational complexity of the auctioneer"s matching problem. Here n refers to the problem"s input size that includes descriptions of all of the buy and sell orders. We also assume that n bounds the number of base securities.
We consider four cases based on two parameters:
possibilities: (a) O(log n) base securities yielding a polynomial number of states. (b) An unlimited number of base securities yielding an exponential number of states.
We show the following results.
Theorem 1. The matching problem is
securities with divisible orders.
orders.
indivisible orders.
2-complete for unlimited securities with indivisible orders.
orders We can build a linear program based on Definition 2. We have variables αi. For each i, we have 0 ≤ αi ≤ 1 and for each state ω in Ω we have the constraint Υ ω auc = X i −αiΥ ω i ≥ 0.
Given these constraints we maximize X i αi.
A set of orders has a matching exactly when P i αi > 0.
With O(log n) base securities, we have |Ω| bounded by a polynomial so we can solve this linear program in polynomial time.
Note that one might argue that one should maximize some linear combination of the −Υ ω i s to maximize the surplus.
However this approach will not find matchings that have zero surplus.
orders With unlimited base securities, the linear program given in Section 5.1 has an exponential number of constraint equations. Each constraint is short to describe and easily computable given ω. 149 Let m ≤ n be the total number of buy and sell orders.
By the theory of linear programming, an upper bound on the objective function can be forced by a collection of m + 1 constraints. So if no matching exists there must exist m + 1 constraints that force all the αi to zero. In nondeterministic polynomial-time we can guess these constraints and solve the reduced linear program. This shows that matching is in co-NP.
To show co-NP-completeness we reduce the NP-complete problem of Boolean formula satisfiability to the nonexistence of a matching. Fix a formula φ. Let the base securities be the variables of φ and consider the single security φ with a buy order of 0.5. If the formula φ is satisfiable then there is some state with payoff 0.5 and no fractional unit of the security φ is a matching. If the formula φ is not satisfiable then every state has an auctioneer"s payoff of 0.5 and a single unit of the security φ is a matching.
One could argue that if the formula φ is not satisfiable then no fully rational buyer would want to buy φ for a cost of 0.5. We can get around this problem by adding auxiliary base securities, A and B, and defining two securities τ = (φ ∧ A) ∨ (A ∧ B) τ = (φ ∧ A) ∨ (A ∧ B) with separate buy orders of 0.5 on each.
If φ were satisfiable then in the state corresponding to the satisfying assignment and both A and B to be true, τ and τ both have an auctioneer"s payoff of −0.5 so even no divisible matching can exist.
If φ were not satisfiable then one unit of each would be a matching since in every state at least one of τ or τ are false.
orders This case is easily seen to be in NP: Just nondeterministically guess a nonempty subset S of orders and check for each state ω in Ω that Υ ω auc = X i∈S −Υ ω i ≥ 0.
Since |Ω| and |S| are bounded by a polynomial in n, the verification can be done in polynomial time.
To show that matching is NP-complete we reduce the NPcomplete problem EXACT COVER BY 3-SETS (X3C) to a matching of securities.
The input to X3C consists of a set X and a collection C of 3-element subsets of X. The input (X, C) is in X3C if C contains an exact cover of X, i.e., there is a subcollection C of C such that every element of X occurs in exactly one member of C . Karp showed that X3C is NP-complete.
Suppose we have an instance (X, C) with the vector X = {x1, . . . , x3q} and C = {c1, . . . , cm}.
We set Ω = {e1, . . . , e3q, r, s} and define securities labelled φ1 , . . . , φm , ψ1 , . . . , ψq and τ , as follows: • Security φi is true in state r, and is true in state ek if k is not in ci. • Security ψj is true only in state s. • Security τ is true in each state ek but not r or s.
We have buy orders on each φi and ψj security for
8q and a buy order on τ for 0.5.
We claim that a matching exists if and only if (X, C) is in X3C.
If (X, C) is in X3C, let C be the subcollection that covers each element of X exactly once. Note that |C | = q.
We claim the collection consisting of φi for each ci in C , every ψj and τ has a matching.
In each state ek we have an auctioneer"s payoff of (.5 − 1 8q ) + (q − 1)(−.5 − 1 8q ) + q(.5 − 1 8q ) − .5 = .5 − 2q 1 8q = .25 ≥ 0.
In states r and s the auctioneer"s payoffs are −q(.5 + 1 8q ) + −q(−.5 + 1 8q ) + .5 = −5 − 2q 1 8q = .25 ≥ 0.
Suppose now that (X, C) is not in X3C but there is a matching. Consider the number q of the φi in that matching and q the number of ψj in the matching. Since a matching requires a nonempty subset of the orders and τ by itself is not a matching we have q + q > 0.
We have three cases. q > q: In state r, the auctioneer"s payoff is −q (.5 + 1 8q ) − q(−.5 + 1 8q ) + .5 ≤ −(q + q) 1 8q < 0. q > q : In state s, the auctioneer"s payoff is −q (.5 + 1 8q ) − q (−.5 + 1 8q ) + .5 ≤ −(q + q ) 1 8q < 0. q ≤ q ≤ q: Consider the set C consisting of the ci where φi is in the matching. There must be some state ek not in any of the ci or C would be an exact cover. The auctioneer"s payoff in ek is at most −q (.5 + 1 8q ) − q (−.5 + 1 8q ) ≤ −(q + q ) 1 8q < 0.
Indivisible Orders The class Σp 2 is the second level of the polynomial-time hierarchy. A language L is in Σp 2 if there exists a polynomial p and a set A in P such that x is in L if and only if there is a y with |y| = p(|x|) such that for all z, with |z| = p(|x|), (x, y, z) is in A. The class Σp 2 contains both NP and coNP. Unless the polynomial-time hierarchy collapses (which is considered unlikely), a problem that is complete for Σp 2 is not contained in NP or co-NP.
We will show that computing a matching is Σp 2-complete, and remains so even for quite restricted types of securities, and hence is (likely) neither in NP or co-NP. While it may seem that being NP-complete or co-NP-complete is hard enough, there are further practical consequences of being outside of NP and co-NP. If the matching problem were in NP, one could use heuristics to search for and verify a match if it exists; even if such heuristics fail in the worst case, they may succeed for most examples in practice. Similarly, if the matching problem were in co-NP, one might hope to at least heuristically rule out the possibility of matching. But for problems outside of NP or co-NP, there is no framework for verifying that a heuristically derived answer is correct.
Less formally, for NP (or co-NP)-complete problems, you have to be lucky; for Σp 2-complete problems, you can"t even tell if you"ve been lucky. 150 We note that the existence of a matching is in Σp 2: We use y to choose a subset of the orders and z to represent a state ω with (x, y, z) in A if the set of orders has a total nonpositive auctioneer"s payoff in state ω.
We prove a stronger theorem which implies that matching is in Σp
security Si has cost ci and pays off pi whenever formula Ci is satisfied. The 0 − 1-matching problem asks whether one can, by accepting either 0 or 1 of each security, guarantee a worst-case payoff strictly larger than the total cost.
Theorem 2. The 0−1-matching problem is Σp 2-complete.
Furthermore, the problem remains Σp 2-complete under the following two special cases:
negations), pi = 1, and ci = cj for all i and j.
securities (or their negations).
These hardness results hold even if there is a promise that no subset of the securities guarantees a worst-case payoff identical to their cost.
To prove Theorem 2, we reduce from the standard Σp 2 problem that we call T∃∀BF. Given a boolean formula φ with variables x1, . . . , xn and y1, . . . , yn is the following fullyquantified formula true ∃x1 . . . ∃xn∀y1 . . . ∀yn φ(x1, . . . , xn, y1, . . . , yn)?
The problem remains Σp 2-complete when φ(x1, . . . , xn, y1, . . . , yn) is restricted to being a disjunction of conjunctions of at most 3 variables (or their negations), e.g., φ(x1, . . . , xn, y1, . . . , yn) = (x1 ∧ ¯x3 ∧ y2) ∨ (x2 ∧ y3 ∧ y7) ∨ · · · .
This form, without the bound on the conjunction size, is known as disjunctive normal form (DNF); the restriction to conjunctions of 3 variables is 3-DNF.
We reduce T∃∀BF to finding a matching. For the simplest reduction, we consider the matching problem where one has a set of Arrow-Debreu securities whose payoff events are conjunctions of the base securities, or their negations. The auctioneer has the option of accepting either 0 or 1 of each of the given securities.
We first reduce to the case where the payoff events are conjunctions of arbitrarily many base events (or their negations). By a standard trick we can reduce the number of base events in each conjunction to 3, and with a slight twist we can even ensure that all securities have the same price as well as the same payoff. Finally, we show that the problem remains hard even if only conjunctions of 2 variables are allowed, though with securities that deviate slightly from Arrow-Debreu securities in that they may have varying, non unit payoffs.
Before describing the securities, we give some intuition.
The T∃∀BFproblem may be viewed as a game between a selector and an adversary. The selector sets the xi variables, and then the adversary sets the yi variables so as to falsify the formula φ. We can view the 0 − 1-matching problem as one in which the auctioneer is a buyer who buys securities corresponding to disjunctions of the base events, and then the adversary sets the values of the base events to minimize the payoff from the securities.
We construct our securities so that the optimal buying strategy is to buy n expensive securities along with a set of cheap securities, of negligible cost (for some cases we can modify the construction so that all securities have the same cost). The total cost of the securities will be just under 1, and each security pays off 1, so the adversary must ensure that none of the securities pays off. Each expensive security forces the adversary to set some variable, xi to a particular value to prevent the security from paying off; this corresponds to setting the xi variables in the original game.
The cheap securities are such that preventing every one of of these securities from paying off is equivalent to falsifying φ in the original game.
Among the technical difficulties we face is how to prevent the buyer from buying conflicting securities, e.g., one that forces xi = 0 and the other that forces xi = 1, allowing for a trivial arbitrage. Secondly, for our analysis we need to ensure that a trader cannot spend more to get more, say by spending 1.5 for a set of securities with the property that at least 2 securities pay off under all possible events.
For each of the variables {xi}, {yi} in φ, we add a corresponding base security (with the same labels). For each existential variable xi we add additional base securities, ni and zi. We also include a base security Q.
In our basic construction, each expensive security costs C and each cheap security costs ; all securities pay off 1. We require that Cn+ (|cheap securities|) < 1 and C(n+1) > 1.
That is, one can buy n expensive securities and all of the cheap securities for less than 1, but one cannot buy n + 1 expensive securities for less than 1. We at times refer to a security by its payoff clause.
Remark: We may loosely think of as 0. However, this would allow one to buy a security for nothing that pays (in the worst case) nothing. By making > 0 , we can show it hard to distinguish portfolios that guarantee a positive profit from those that risk a positive loss. Setting > 0 will also allow us to show hardness results for the case where all securities have the same cost.
For 1 ≤ i ≤ n, we have two expensive securities with payoff clauses (¯xi ∧Q) and (¯ni ∧Q) and two cheap securities with payoff clauses (xi ∧ ¯zi) and (ni ∧ ¯zi).
For each clause C ∈ φ, we convert every negated variable ¯xi into ni and add the conjunction z1 ∧ · · · ∧ zn. Thus, for a clause C = (x2 ∧ ¯x7 ∧ ¯y5) we construct a cheap security SC, with payoff clause (z1 ∧ · · · ∧ zn ∧ x2 ∧ n7 ∧ ¯y5).
Finally, we have a cheap security with payoff clause ( ¯Q).
We now argue that a matching exists iff ∃x1 . . . ∃xn∀y1 . . . ∀yn φ(x1, . . . , xn, y1, . . . , yn).
We do this by successively constraining the buyer and the adversary, eliminating behaviors that would cause the other player to win. The resulting reasonable strategies correspond exactly to the game version of T∃∀BF.
First, observe that if the adversary sets all of the base securities to false (0), then only the ( ¯Q) security will pay off. 151 Thus, no buyer can buy more than n expensive securities and guarantee a profit. The problem is thus whether one can buy n expensive securities and all the cheap securities, so that at for any setting of the base events at least one security will pay off.
Clearly, the adversary must make Q hold, or the ( ¯Q) security will pay off. Next, we claim that for each i, 1 ≤ i ≤ i, the auctioneer must buy at least one of the (¯xi ∧ Q) and (¯ni ∧ Q) securities. This follow from the fact that if the adversary sets xi, ni and zi to be false, and every other base event to be true, then only the (¯xi ∧ Q) and (¯ni ∧ Q) securities will pay off. As no auctioneer can buy more than n expensive securities, it must therefore buy exactly one of (¯xi ∧ Q) or (¯ni ∧ Q), for each i, 1 ≤ i ≤ n. For the rest of the analysis, we assume that the auctioneer follows this constraint.
Suppose that the buyer buys (¯xi ∧Q). Then the adversary must set xi to be true (since it must set Q to be true), or the security will pay off. It must then set zi to be true or (xi∧¯zi) will pay off. Since the buyer doesn"t buy (¯ni ∧ Q) (by the above constraint), and all the other securities pay the same or less when ni is made false, we can assume without loss of generality that the adversary sets ni to be false. Similarly, if the buyer buys (¯ni ∧ Q), then the adversary must set ni and zi to be true, and we can assume without loss of generality that the adversary sets xi to be false. Note that the adversary must in all cases set each zi event to be true.
Summarizing the preceding argument, there is an exact correspondence between the rational strategies of the buyer and settings for the xi variables forced on the adversary.
Furthermore, the adversary is also constrained to set the variables Q, z1, . . . , zn to be true, and without loss of generality may be assumed to set ni = ¯xi. Under these constraints, those securities not corresponding to clauses in φ are guaranteed to not pay off.
The adversary also decides the value of the y1, . . . , ym base events. Recall that for each clause C ∈ φ there is a corresponding security SC. Given that zi is true and ni = ¯xi (without loss of generality), it follows from the construction of SC that the setting of the yis causes SC to pay off iff it satisfies C. This establishes the reduction from T∃∀BF to the matching problem, when the securities are constrained to be a conjunction of polynomially many base events or their negations.
There are standard methods for reducing DNF formulae to 3-DNF formulae, which are trivially modifiable to our securities framework; we include the reduction for completeness.
Given a security S whose payoff clause is C = (v1 ∧ v2 ∧ · · · ∧ vk) (variable negations are irrelevant to this discussion), cost c and payoff p, introduce a new auxiliary variable, w, and replace the security with two securities, S1 and S2, with payoff clauses,
C1 = (v1 ∧ v2 ∧ w) and C2 = ( ¯w ∧ v3 ∧ · · · ∧ vk).
The securities both have payoff p, and their costs can be any positive values that sum to c. Note that at most one of the securities can pay off at a time. If only one security is bought, then the adversary can always set w so that it won"t pay off; hence the auctioneer will buy either both or neither, for a total cost of c (here we use the fact that one is only allowed to buy either 0 or 1 shares of each security). Then, it may be verified that, given the ability to set w arbitrarily, the adversary can cause C to be unsatisfied iff it can cause both C1 and C2 to be unsatisfied. Hence, owning one share each of S1 and S2 is equivalent to owning one share of S.
Note that C1 has three variables and C2 has k−1 variables.
By applying the transformation successively, one obtains an equivalent set of securities, of polynomial size, whose payoff clauses have at most 3 variables.
We note that in the basic construction, all of the clauses with more than 3 variables are associated with cheap securities (cost ). Instead of subdividing costs, we can simply make all of the resulting securities have cost ; the constraints on C and must reflect the new, larger number of cheap securities.
One can ensure that all of the payoff clauses have exactly 3 variables, with a similar construction. A security S with cost c, payoff p and defining clause (x ∧ y) can be replaced by securities S1 and S2 with cost c/2, payoff p and defining clauses (x∧y∧w) and (x∧y∧ ¯w), where w is a new auxiliary variable. Essentially the same analysis as given above applies to this case. The case of single-variable payoff clauses is handled by two applications of this technique.
By setting C and appropriately, one can ensure that in the basic reduction every security costs a polynomially bounded integer multiple of ; call this ratio r. We now show how to reduce this case to the case where every security costs . Recall that the expensive securities have payoff clauses (¯xi ∧ Q) or (¯ni ∧ Q). Assume that security S has payoff clause (¯xi ∧ Q) (the other case is handled identically). Replace S with security S , with payoff clause (¯xi ∧ Q ∧ w1) (w1, . . . , wr−1 are auxiliary variables; fresh variables are chosen for each clause), and also S1, . . . , Sr−1, with payoff clauses ( ¯w1 ∧ w2), ( ¯w2 ∧ w3), . . . , ( ¯wr−2 ∧ wr−1), and( ¯wr−1 ∧ ¯w1).
Clearly, buying none of the new securities is equivalent to not buying the original security. We show that buying all of the new securities is equivalent to buying the original security, and that buying a proper, nonempty subset of the securities is irrational.
We first note that if the buyer buys securities S1, . . . , Sr−1, then the adversary must set w1 to be true, or one of the securities will pay off. To see this, note that if wi is set to false, then ( ¯wi ∧wi+1) will be true unless wi+1 is set to false; thus, setting w1 to false forces the adversary to set wr−1 to false, causing the final clause to be true. Having set w1 true, the adversary can set w2, . . . , wr−1 to false, ensuring that none of the securities S1, . . . , Sr−1 pays out. If wi is true, then (¯xi ∧ Q ∧ w1) is equivalent to (¯xi ∧ Q). So buying all of the replacement securities for each is equivalent to buying the original security for r.
It remains to show that buying a proper, nonempty subset of the securities is irrational. If one doesn"t buy S , then the adversary can set the w variables so that none of S1, . . . , Sr−1 will pay off; any money spent on these securities is wasted. If one doesn"t buy Sr−1, the adversary can set all w to false, in which case none of the new clauses will pay off, regardless of the value of xi and Q. Similarly, if one 152 doesn"t buy Si, for 1 ≤ i ≤ r −2, the adversary can set wi+1 to be true, all the other w variables to be false, and again there is no payoff, regardless of the value of xi and Q. Thus, buying a proper subset of these securities will not increase ones payoff.
We note that this reduction can be combined trivially with the reduction that ensures that all of the defining clauses have 3 or fewer variables. With a slightly messier argument, all of the defining clauses can be set up to have exactly 3 variables.
If we allow securities to have variable payoffs and prices, we can reduce to the case where each security"s payoff clause is a conjunction of at most 2 variables or their negations.
Given a security s with payoff clause (X ∧ Y ∧ Z), cost c and payoff 1, we introduce fresh auxiliary variables, w1, w2 and w3 (new variables are used for each clause) and replace S with the following securities: • Securities S1, S2 and S3, each with cost c/3 and payoff 1, with respective payoff clauses (X ∧ w1), (Y ∧ w2) and (Z ∧ w3). • Securities S1, . . . , S6, each with cost 4 and payoff 24 − 2, with payoff clauses, (w1 ∧ w2) (w1 ∧ w3) (w2 ∧ w3) ( ¯w1 ∧ ¯w2) ( ¯w1 ∧ ¯w3) ( ¯w2 ∧ ¯w3) Here, 2 is a tiny positive quantity, described later. By a simple case analysis, we have the following.
Observations:
that of the S securities only Si pays off.
S securities will pay off.
will pay off.
to be 0, will cause exactly one of the S securities to pay off.
By Observation 1, there is no point in buying a nonempty proper subset of the S securities: The adversary can ensure that none of the bought securities will pay off, and even if all the S securities pay off, it will not be sufficient to recoup the cost of buying a single S security. By Observation 2, if one buys all the S securities, one is guaranteed to almost make back ones investment (except for 2), in which case by Observations 3 and 4, the adversaries optimal strategy is to make exactly one of w1, w2 or w3 true. We set C, and 2 so that Cn + (|cheap securities|) + 2(|clauses|) < 1.
Thus, the accumulated losses of 2 can never spell the difference between making a guaranteed profit and making no profit at all. Note also that by making 2 positive we prevent the existence of break-even buying strategies in which the buyer only purchases S securities.
Summarizing the previous argument, we may assume without loss of generality that the buyer buys all of the S securities (for all clauses), and that for each clause the adversary sets exactly one of that clause"s auxiliary variables w1, w2 or w3 to be true. For the rest of the discussion, we assume that the players follow these constraints.
We next claim that a rational buyer will either buy all of S1, S2 or S3, or none of them. If the buyer doesn"t buy S1, then if the adversary makes w1 true and w2 and w3 false, neither S2 nor S3 will pay off, regardless of how the adversary sets X, Y and Z. Hence, there is no point in buying either S2 or S3 if one doesn"t buy S1. Applying the same argument to S2 and S3 establishes the claim.
Clearly, buying none of S1, S2 and S3 has, up to negligible 2 factors, the same price/payoff behavior as not buying S.
We next argue that, subject to the established constraints put on the players" behaviors, buying all of S1, S2 and S3 has the same price/payoff behavior (again ignoring 2 factors) as buying S, regardless of how the adversary sets X, Y and Z.
First, in both cases, the cost is c. If the adversary makes X, Y and Z true, then S pays off 1, and (assuming that exactly one of w1, w2 and w3 is true), exactly one of S1, S2 or S3 will pay off 1. If X is false, then S doesn"t pay off, and the adversary can set w1 true (and w2 and w3 false), ensuring that none of S1, S2 and S3 pays off. The same argument holds if Y or Z are false.
The logical question to ask in light of these complexity results is whether further, more severe restrictions on the space of securities can enable tractable matching algorithms.
Although we have not systematically explored the possibilities, the potential for useful tractable cases certainly exists.
Suppose, for example, that bids are limited to unit quantities of securities of the following two forms:
Let p be the price offered for a disjunction A1 ∨ · · · ∨ Ak , and qi the maximal price offered for the respective negated disjuncts. This disjunction bid is part of a match iff p +P i qi ≥ k. Evaluating whether this condition is satisfied by a subset of bids is quite straightforward.
Although this example is contrived, its application is not entirely implausible. For example, the disjunctions may correspond to insurance customers, who want an insurance contract to cover all the potential causes of their asset loss. The atomic securities are sold by insurers, each of whom specialize in a different form of disaster cause.
FUTURE DIRECTIONS We have analyzed the computational complexity of matching for securities based on logical formulas. Many possible avenues for future work exist, including
• How to choose quantities and bid/ask prices for a collection of securities to maximizes one"s expected utility, both for linear and nonlinear utility functions. 153 • How to choose securities; that is, deciding on what collection of boolean formulas to offer to trade, subject to constraints or penalties on the number or complexity of bids. • How do make the above choices in a game theoretically sound way, taking into account the choices of other traders, their reasoning about other traders, etc.
heuristics that achieve matches in many cases or approximate a matching?
incentive properties of the resulting mechanisms.
equilibrium if no computationally-bounded player can find a strategy that increases utility. With few exceptions [3, 22], little is known about computational equilibriums. A natural question is to determine whether a market can achieve a computational equilibrium that is not a true equilibrium, and under what circumstances this may occur.
Acknowledgments We thank Rahul Sami for his help with Section 5.4.4. We thank Rahul, Joan Feigenbaum and Robin Hanson for useful discussions.
[1] Kenneth J. Arrow. The role of securities in the optimal allocation of risk-bearing. Review of Economic Studies, 31(2):91-96, 1964. [2] Peter Bossaerts, Leslie Fine, and John Ledyard.
Inducing liquidity in thin financial markets through combined-value trading mechanisms. European Economic Review, 46:1671-1695, 2002. [3] Paul J. Brewer. Decentralized computation procurement and computational robustness in a smart market. Economic Theory, 13:41-92, 1999. [4] Kay-Yut Chen, Leslie R. Fine, and Bernardo A.
Huberman. Forecasting uncertain events with small groups. In Third ACM Conference on Electronic Commerce (EC"01), pages 58-64, 2001. [5] Bruno de Finetti. Theory of Probability: A Critical Introductory Treatment, volume 1. Wiley, New York,
[6] Sven de Vries and Rakesh V. Vohra. Combinatorial auctions: A survey. INFORMS J. of Computing, 2003. [7] Sandip Debnath, David M. Pennock, C. Lee Giles, and Steve Lawrence. Information incorporation in online in-game sports betting markets. In Fourth ACM Conference on Electronic Commerce (EC"03), 2003. [8] Jacques H. Dreze. Market allocation under uncertainty. In Essays on Economic Decisions under Uncertainty, pages 119-143. Cambridge University Press, 1987. [9] Ronald Fagin, Joseph Y. Halpern, and Nimrod Megiddo. A logic for reasoning about probabilities.
Information and Computation, 87(1/2):78-128, 1990. [10] Robert Forsythe and Russell Lundholm. Information aggregation in an experimental market. Econometrica, 58(2):309-347, 1990. [11] Robert Forsythe, Forrest Nelson, George R. Neumann, and Jack Wright. Anatomy of an experimental political stock market. American Economic Review, 82(5):1142-1161, 1992. [12] Robert Forsythe, Thomas A. Rietz, and Thomas W.
Ross. Wishes, expectations, and actions: A survey on price formation in election stock markets. Journal of Economic Behavior and Organization, 39:83-110,
[13] John M. Gandar, William H. Dare, Craig R. Brown, and Richard A. Zuber. Informed traders and price variations in the betting market for professional basketball games. Journal of Finance,
LIII(1):385-401, 1998. [14] Robin Hanson. Decision markets. IEEE Intelligent Systems, 14(3):16-19, 1999. [15] Robin Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1), 2002. [16] Robin D. Hanson. Could gambling save science?
Encouraging an honest consensus. Social Epistemology, 9(1):3-33, 1995. [17] Jens Carsten Jackwerth and Mark Rubinstein.
Recovering probability distributions from options prices. Journal of Finance, 51(5):1611-1631, 1996. [18] Joseph B. Kadane and Robert L. Winkler. Separating probability elicitation from utilities. Journal of the American Statistical Association, 83(402):357-363,
[19] Michael Magill and Martine Quinzii. Theory of Incomplete Markets, Vol. 1. MIT Press, 1996. [20] Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory. Oxford University Press, New York, 1995. [21] Noam Nisan. Bidding and allocation in combinatorial auctions. In Second ACM Conference on Electronic Commerce (EC"00), pages 1-12, 2000. [22] Noam Nisan and Amir Ronen. Computationally feasible VCG mechanisms. In Second ACM Conference on Electronic Commerce (EC"00), pages 242-252,
[23] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988. [24] David M. Pennock, Steve Lawrence, C. Lee Giles, and Finn ˚Arup Nielsen. The real power of artificial markets. Science, 291:987-988, February 9 2001. [25] David M. Pennock, Steve Lawrence, Finn ˚Arup Nielsen, and C. Lee Giles. Extracting collective probabilistic forecasts from web games. In Seventh International Conference on Knowledge Discovery and Data Mining, pages 174-183, 2001. [26] David M. Pennock and Michael P. Wellman. Compact securities markets for Pareto optimal reallocation of risk. In Sixteenth Conference on Uncertainty in Artificial Intelligence, pages 481-488, 2000. [27] C. R. Plott, J. Wit, and W. C. Yang. Parimutuel betting markets as information aggregation devices: Experimental results. Social Science Working Paper 986, California Institute of Technology, April 1997. 154 [28] Charles R. Plott. Markets as information gathering tools. Southern Economic Journal, 67(1):1-15, 2000. [29] Charles R. Plott and Shyam Sunder. Rational expectations and the aggregation of diverse information in laboratory security markets.
Econometrica, 56(5):1085-1118, 1988. [30] R. Roll. Orange juice and weather. American Economic Review, 74(5):861-880, 1984. [31] Tuomas Sandholm, Subhash Suri, Andrew Gilpin, and David Levine. Winner determination in combinatorial auction generalizations. In First International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), July 2002. [32] Carsten Schmidt and Axel Werwatz. How accurate do markets predict the outcome of an event? the Euro 2000 soccer championships experiment. Technical Report 09-2002, Max Planck Institute for Research into Economic Systems, 2002. [33] Richard H. Thaler and William T. Ziemba. Anomalies: Parimutuel betting markets: Racetracks and lotteries.
Journal of Economic Perspectives, 2(2):161-174, 1988. [34] Hal R. Varian. The arbitrage principle in financial economics. J. Economic Perspectives, 1(2):55-72,

One of the most striking characteristics of modern computer networks - in particular the Internet - is that different parts of it are owned and operated by different individuals, firms, and organizations. The analysis and design of protocols for this environment thus naturally needs to take into account the different selfish economic interests of the different participants. Indeed, the last few years have seen much work addressing this issue using game-theoretic notions (see [7] for an influential survey). A significant part of the difficulty stems from underlying asymmetries of information: one participant may not know everything that is known or done by another. In particular, the field of algorithmic mechanism design [6] uses appropriate incentives to extract the private information from the participants.
This paper deals with the complementary lack of knowledge, that of hidden actions. In many cases the actual behaviors - actions - of the different participants are hidden from others and only influence the final outcome indirectly.
Hidden here covers a wide range of situations including not precisely measurable, costly to determine, or even non-contractible - meaning that it can not be formally used in a legal contract.
An example that was discussed in [3] is Quality of Service routing in a network: every intermediate link or router may exert a different amount of effort (priority, bandwidth, ...) when attempting to forward a packet of information. While the final outcome of whether a packet reached its destination is clearly visible, it is rarely feasible to monitor the exact amount of effort exerted by each intermediate link - how can we ensure that they really do exert the appropriate amount of effort? Many other complex resource allocation problems exhibit similar hidden actions, e.g., a task that runs on a collection of shared servers may be allocated, by each server, an unknown percentage of the CPU"s processing power or of the physical memory. How can we ensure that the right combination of allocations is actually made by the different servers? A related class of examples concerns security issues: each link in a complex system may exert different levels of effort for protecting some desired security property of the system. How can we ensure that the desired level of 18 collective security is obtained?
Our approach to this problem is based on the well studied principal-agent problem in economic theory: How can a principal motivate a rational agent to exert costly effort towards the welfare of the principal? The crux of the model is that the agent"s action (i.e. whether he exerts effort or not) is invisible to the principal and only the final outcome, which is probabilistic and also influenced by other factors, is visible. This problem is well studied in many contexts in classical economic theory and we refer the readers to introductory texts on economic theory such as [5] Chapter 14.
The solution is based on the observation that a properly designed contract, in which the payments are contingent upon the final outcome, can influence a rational agent to exert the required effort.
In this paper we initiate a general study of handling combinations of agents rather than a single agent. While much work was already done on motivating teams of agents [4], our emphasis is on dealing with the complex combinatorial structure of dependencies between agents" actions. In the general case, each combination of efforts exerted by the n different agents may result in a different expected gain for the principal. The general question asks which conditional payments should the principal offer to which agents as to maximize his net utility? In our setting and unlike in previous work (see, e.g., [12]), the main challenge is to determine the optimal amount of effort desired from each agent.
This paper suggest models for and provides some interesting initial results about this combinatorial agency problem. We believe that we have only scratched the surface and leave many open questions, conjectures, and directions for further research.
We believe that this type of analysis may also find applications in regular economic activity. Consider for example a firm that sub-contracts a family of related tasks to many individuals (or other firms). It will often not be possible to exactly monitor the actual effort level of each sub-contractor (e.g., in cases of public-relations activities, consulting activities, or any activities that require cooperation between different sub-contractors.) When the dependencies between the different subtasks are complex, we believe that combinatorial agency models can offer a foundation for the design of contracts with appropriate incentives.
It may also be useful to view our work as part of a general research agenda stemming from the fact that all types of economic activity are increasingly being handled with the aid of sophisticated computer systems. In general, in such computerized settings, complex scenarios involving multiple agents and goods can naturally occur, and they need to be algorithmically handled. This calls for the study of the standard issues in economic theory in new complex settings.
The principal-agent problem is a prime example where such complex settings introduce new challenges.
We start by presenting a general model: in this model each of n agents has a set of possible actions, the combination of actions by the players results in some outcome, where this happens probabilistically. The main part of the specification of a problem in this model is a function that specifies this distribution for each n-tuple of agents" actions.
Additionally, the problem specifies the principal"s utility for each possible outcome, and for each agent, the agent"s cost for each possible action. The principal motivates the agents by offering to each of them a contract that specifies a payment for each possible outcome of the whole project1 . Key here is that the actions of the players are non-observable and thus the contract cannot make the payments directly contingent on the actions of the players, but rather only on the outcome of the whole project.
Given a set of contracts, the agents will each optimize his own utility: i.e. will choose the action that maximizes his expected payment minus the cost of his action. Since the outcome depends on the actions of all players together, the agents are put in a game and are assumed to reach a Nash equilibrium2 . The principal"s problem, our problem in this paper, is of designing an optimal set of contracts: i.e. contracts that maximize his expected utility from the outcome, minus his expected total payment. The main difficulty is that of determining the required Nash equilibrium point.
In order to focus on the main issues, the rest of the paper deals with the basic binary case: each agent has only two possible actions exert effort and shirk and there are only two possible outcomes success and failure. It seems that this case already captures the main interesting ingredients3 .
In this case, each agent"s problem boils down to whether to exert effort or not, and the principal"s problem boils down to which agents should be contracted to exert effort. This model is still pretty abstract, and every problem description contains a complete table specifying the success probability for each subset of the agents who exert effort.
We then consider a more concrete model which concerns a subclass of problem instances where this exponential size table is succinctly represented. This subclass will provide many natural types of problem instances. In this subclass every agent performs a subtask which succeeds with a low probability γ if the agent does not exert effort and with a higher probability δ > γ, if the agent does exert effort. The whole project succeeds as a deterministic Boolean function of the success of the subtasks. This Boolean function can now be represented in various ways. Two basic examples are the AND function in which the project succeeds only if all subtasks succeed, and the OR function which succeeds if any of the subtasks succeeds. A more complex example considers a communication network, where each agent controls a single edge, and success of the subtask means that a message is forwarded by that edge. Effort by the edge increases this success probability. The complete project succeeds if there is a complete path of successful edges between a given source and sink. Complete definitions of the models appear in Section 2.
1 One could think of a different model in which the agents have intrinsic utility from the outcome and payments may not be needed, as in [10, 11]. 2 In this paper our philosophy is that the principal can suggest a Nash equilibrium point to the agents, thus focusing on the best Nash equilibrium. One may alternatively study the worst case equilibrium as in [12], or alternatively, attempt modeling some kind of an extensive game between the agents, as in [9, 10, 11]. 3 However, some of the more advanced questions we ask for this case can be viewed as instances of the general model. 19 We address a host of questions and prove a large number of results. We believe that despite the large amount of work that appears here, we have only scratched the surface. In many cases we were not able to achieve the general characterization theorems that we desired and had to settle for analyzing special cases or proving partial results. In many cases, simulations reveal structure that we were not able to formally prove. We present here an informal overview of the issues that we studied, what we were able to do, and what we were not. The full treatment of most of our results appears only in the extended version [2], and only some are discussed, often with associated simulation results, in the body of the paper.
Our first object of study is the structure of the class of sets of agents that can be contracted for a given problem instance. Let us fix a given function describing success probabilities, fix the agent"s costs, and let us consider the set of contracted agents for different values of the principal"s associated value from success. For very low values, no agent will be contracted since even a single agent"s cost is higher than the principal"s value. For very high values, all agents will always be contracted since the marginal contribution of an agent multiplied by the principal"s value will overtake any associated payment. What happens for intermediate principal"s values?
We first observe that there is a finite number of transitions between different sets, as the principal"s project value increases. These transitions behave very differently for different functions. For example, we show that for the AND function only a single transition occurs: for low enough values no agent will be contracted, while for higher values all agents will be contracted - there is no intermediate range for which only some of the agents are contracted. For the OR function, the situation is opposite: as the principal"s value increases, the set of contracted agents increases one-by-one.
We are able to fully characterize the types of functions for which these two extreme types of transitions behavior occur.
However, the structure of these transitions in general seems quite complex, and we were not able to fully analyze them even in simple cases like the Majority function (the project succeeds if a majority of subtasks succeeds) or very simple networks. We do have several partial results, including a construction with an exponential number of transitions.
During the previous analysis we also study what we term the price of unaccountability: How much is the social utility achieved under the optimal contracts worse than what could be achieved in the non-strategic case4 , where the socially optimal actions are simply dictated by the principal?
We are able to fully analyze this price for the AND function, where it is shown to tend to infinity as the number of agents tends to infinity. More general analysis remains an open problem.
Our analysis of these questions sheds light on the difficulty of the various natural associated algorithmic problems. In particular, we observe that the optimal contract can be found in time polynomial in the explicit representation of the probability function. We prove a lower bound that shows that the optimal contract cannot be found in number of queries that is polynomial just in the number of agents, in a general black-box model. We also show that when the probability function is succinctly represented as 4 The non-strategic case is often referred to as the case with contractible actions or the principal"s first-best solution. a read-once network, the problem becomes #P-hard. The status of some algorithmic questions remains open, in particular that of finding the optimal contract for technologies defined by serial-parallel networks.
In a follow-up paper [1] we deal with equilibria in mixed strategies and show that the principal can gain from inducing a mixed-Nash equilibrium between the agents rather than a pure one. We also show cases where the principal can gain by asking agents to reduce their effort level, even when this effort comes for free. Both phenomena can not occur in the non-strategic setting.
A principal employs a set of agents N of size n. Each agent i ∈ N has a possible set of actions Ai, and a cost (effort) ci(ai) ≥ 0 for each possible action ai ∈ Ai (ci : Ai → +). The actions of all players determine, in a probabilistic way, a contractible outcome o ∈ O, according to a success function t : A1×, . . . × An → Δ(O) (where Δ(O) denotes the set of probability distributions on O). A technology is a pair, (t, c), of a success function, t, and cost functions, c = (c1, c2, . . . , cn). The principal has a certain value for each possible outcome, given by the function v : O → .
As we will only consider risk-neutral players in this paper5 , we will also treat v as a function on Δ(O), by taking simple expected value. Actions of the players are invisible, but the final outcome o is visible to him and to others (in particular the court), and he may design enforceable contracts based on the final outcome. Thus the contract for agent i is a function (payment) pi : O → ; again, we will also view pi as a function on Δ(O).
Given this setting, the agents have been put in a game, where the utility of agent i under the vector of actions a = (a1, . . . , an) is given by ui(a) = pi(t(a))−ci(ai). The agents will be assumed to reach Nash equilibrium, if such equilibrium exists. The principal"s problem (which is our problem in this paper) is how to design the contracts pi as to maximize his own expected utility u(a) = v(t(a)) − P i pi(t(a)), where the actions a1, . . . , an are at Nash-equilibrium. In the case of multiple Nash equilibria we let the principal choose the equilibrium, thus focusing on the best Nash equilibrium. A variant, which is similar in spirit to strong implementation in mechanism design would be to take the worst Nash equilibrium, or even, stronger yet, to require that only a single equilibrium exists. Finally, the social welfare for a ∈ A is u(a) + P i∈N ui(a) = v(t(a)) − P i∈N ci(ai).
We wish to concentrate on the complexities introduced by the combinatorial structure of the success function t, we restrict ourselves to a simpler setting that seems to focus more clearly on the structure of t. A similar model was used in [12]. We first restrict the action spaces to have only two states (binary-action): 0 (low effort) and 1 (high effort). The cost function of agent i is now just a scalar ci > 0 denoting the cost of exerting high effort (where the low effort has cost 0). The vector of costs is c = (c1, c2, . . . , cn), 5 The risk-averse case would obviously be a natural second step in the research of this model, as has been for noncombinatorial scenarios. 20 and we use the notation (t, c) to denote a technology in such a binary-outcome model. We then restrict the outcome space to have only two states (binary-outcome): 0 (project failure) and 1 (project success). The principal"s value for a successful project is given by a scalar v > 0 (where the value of project failure is 0). We assume that the principal can pay the agents but not fine them (known as the limited liability constraint). The contract to agent i is thus now given by a scalar value pi ≥ 0 that denotes the payment that i gets in case of project success. If the project fails, the agent gets 0. When the lowest cost action has zero cost (as we assume), this immediately implies that the participation constraint holds.
At this point the success function t becomes a function t : {0, 1}n → [0, 1], where t(a1, . . . , an) denotes the probability of project success where players with ai = 0 do not exert effort and incur no cost, and players with ai = 1 do exert effort and incur a cost of ci.
As we wish to concentrate on motivating agents, rather than on the coordination between agents, we assume that more effort by an agent always leads to a better probability of success, i.e. that the success function t is strictly monotone. Formally, if we denote by a−i ∈ A−i the (n − 1)dimensional vector of the actions of all agents excluding agent i. i.e., a−i = (a1, . . . , ai−1, ai+1, . . . , an), then a success function must satisfy: ∀i ∈ N, ∀a−i ∈ A−i t(1, a−i) > t(0, a−i) Additionally, we assume that t(a) > 0 for any a ∈ A (or equivalently, t(0, 0, . . . , 0) > 0).
Definition 1. The marginal contribution of agent i, denoted by Δi, is the difference between the probability of success when i exerts effort and when he shirks.
Δi(a−i) = t(1, a−i) − t(0, a−i) Note that since t is monotone, Δi is a strictly positive function. At this point we can already make some simple observations. The best action, ai ∈ Ai, of agent i can now be easily determined as a function of what the others do, a−i ∈ A−i, and his contract pi.
Claim 1. Given a profile of actions a−i, agent i"s best strategy is ai = 1 if pi ≥ ci Δi(a−i) , and is ai = 0 if pi ≤ ci Δi(a−i) . (In the case of equality the agent is indifferent between the two alternatives.) As pi ≥ ci Δi(a−i) if and only if ui(1, a−i) = pi ·t(1, a−i)−ci ≥ pi ·t(0, a−i) = ui(0, a−i), i"s best strategy is to choose ai = 1 in this case.
This allows us to specify the contracts that are the principal"s optimal, for inducing a given equilibrium.
Observation 1. The best contracts (for the principal) that induce a ∈ A as an equilibrium are pi = 0 for agent i who exerts no effort (ai = 0), and pi = ci Δi(a−i) for agent i who exerts effort (ai = 1).
In this case, the expected utility of agent i who exerts effort is ci ·  t(1,a−i) Δi(a−i) − 1  , and 0 for an agent who shirk. The principal"s expected utility is given by u(a, v) = (v−P)·t(a), where P is the total payment in case of success, given by P = P i|ai=1 ci Δi(a−i) .
We say that the principal contracts with agent i if pi > 0 (and ai = 1 in the equilibrium a ∈ A). The principal"s goal is to maximize his utility given his value v, i.e. to determine the profile of actions a∗ ∈ A, which gives the highest value of u(a, v) in equilibrium. Choosing a ∈ A corresponds to choosing a set S of agents that exert effort (S = {i|ai = 1}).
We call the set of agents S∗ that the principal contracts with in a∗ (S∗ = {i|a∗ i = 1}) an optimal contract for the principal at value v. We sometimes abuse notation and denote t(S) instead of t(a), when S is exactly the set of agents that exert effort in a ∈ A.
A natural yardstick by which to measure this decision is the non-strategic case, i.e. when the agents need not be motivated but are rather controlled directly by the principal (who also bears their costs). In this case the principal will simply choose the profile a ∈ A that optimizes the social welfare (global efficiency), t(a) · v − P i|ai=1 ci. The worst ratio between the social welfare in this non-strategic case and the social welfare for the profile a ∈ A chosen by the principal in the agency case, may be termed the price of unaccountability.
Given a technology (t, c), let S∗ (v) denote the optimal contract in the agency case and let S∗ ns(v) denote an optimal contract in the non-strategic case, when the principal"s value is v. The social welfare for value v when the set S of agents is contracted is t(S) · v − P i∈S ci (in both the agency and non-strategic cases).
Definition 2. The price of unaccountability POU(t, c) of a technology (t, c) is defined as the worst ratio (over v) between the total social welfare in the non-strategic case and the agency case: POU(t, c) = Supv>0 t(S∗ ns(v)) · v − P i∈S∗ ns(v) ci t(S∗(v)) · v − P i∈S∗(v) ci In cases where several sets are optimal in the agency case, we take the worst set (i.e., the set that yields the lowest social welfare).
When the technology (t, c) is clear in the context we will use POU to denote the price of unaccountability for technology (t, c). Note that the POU is at least 1 for any technology.
As we would like to focus on results that derived from properties of the success function, in most of the paper we will deal with the case where all agents have an identical cost c, that is ci = c for all i ∈ N. We denote a technology (t, c) with identical costs by (t, c). For the simplicity of the presentation, we sometimes use the term technology function to refer to the success function of the technology.
In order to be more concrete, we will especially focus on technology functions whose structure can be described easily as being derived from independent agent tasks - we call these structured technology functions. This subclass will first give us some natural examples of technology function, and will also provide a succinct and natural way to represent the technology functions.
In a structured technology function, each individual succeeds or fails in his own task independently. The project"s success or failure depends, possibly in a complex way, on the set of successful sub-tasks. Thus we will assume a monotone Boolean function f : {0, 1}n → {0, 1} which denotes 21 whether the project succeeds as a function of the success of the n agents" tasks (and is not determined by any set of n−1 agents). Additionally there are constants 0 < γi < δi < 1, where γi denotes the probability of success for agent i if he does not exert effort, and δi (> γi) denotes the probability of success if he does exert effort. In order to reduce the number of parameters, we will restrict our attention to the case where γ1 = . . . = γn = γ and δ1 = . . . = δn = 1 − γ thus leaving ourselves with a single parameter γ s.t. 0 < γ < 1 2 .
Under this structure, the technology function t is defined by t(a1, . . . , an) being the probability that f(x1, . . . , xn) = 1 where the bits x1, . . . , xn are chosen according to the following distribution: if ai = 0 then xi = 1 with probability γ and xi = 0 with probability 1 − γ; otherwise, i.e. if ai = 1, then xi = 1 with probability 1 − γ and xi = 0 with probability γ.
We denote x = (x1, . . . , xn).
The question of the representation of the technology function is now reduced to that of representing the underlying monotone Boolean function f. In the most general case, the function f can be given by a general monotone Boolean circuit. An especially natural sub-class of functions in the structured technologies setting would be functions that can be represented as a read-once network - a graph with a given source and sink, where every edge is labeled by a different player. The project succeeds if the edges that belong to player"s whose task succeeded form a path between the source and the sink6 .
A few simple examples should be in order here:
conjunction of xi (f(x) = V i∈N xi). Thus the project succeeds only if all agents succeed in their tasks. This is shown graphically as a read-once network in Figure 1(a). If m agents exert effort ( P i ai = m), then t(a) = tm = γn−m (1 − γ)m . E.g. for two players, the technology function t(a1a2) = ta1+a2 is given by t0 = t(00) = γ2 , t1 = t(01) = t(10) = γ(1 − γ), and t2 = t(11) = (1 − γ)2 .
disjunction of xi (f(x) = W i∈N xi). Thus the project succeeds if at least one of the agents succeed in their tasks. This is shown graphically as a read-once network in Figure 1(b). If m agents exert effort, then tm = 1 − γm (1 − γ)n−m .E.g. for two players, the technology function is given by t(00) = 1 − (1 − γ)2 , t(01) = t(10) = 1 − γ(1 − γ), and t(11) = 1 − γ2 .
logical disjunction of conjunctions. In the simplest case of equal-length clauses (denote by nc the number of clauses and by nl their length), f(x) = Wnc j=1( Vnl k=1 xj k).
Thus the project succeeds if in at least one clause all agents succeed in their tasks. This is shown graphically as a read-once network in Figure 2(a). If mi agents on path i exert effort, then t(m1, ..., mnc ) = 1 − Q i(1 − γnl−mi (1 − γ)mi ). E.g. for four players, the technology function t(a1 1 a1 2, a2 1 a2 2) is given by t(00, 00) = 1 − (1 − γ2 )2 , t(01, 00) = t(10, 00) = t(00, 01) = t(00, 10) = 1 − (1 − γ(1 − γ))(1 − γ2 ), and so on. 6 One may view this representation as directly corresponding to the project of delivering a message from the source to the sink in a real network of computers, with the edges being controlled by selfish agents.
Figure 1: Graphical representations of (a) AND and (b) OR technologies.
Figure 2: Graphical representations of (a) OOA and (b) AOO technologies.
logical conjunction of disjunctions. In the simplest case of equal-length clauses (denote by nl the number of clauses and by nc their length), f(x) = Vnl j=1( Wnc k=1 xj k).
Thus the project succeeds if at least one agent from each disjunctive-form-clause succeeds in his tasks. This is shown graphically as a read-once network in Figure 2(b). If mi agents on clause i exert effort, then t(m1, ..., mnc ) = Q i(1 − γmi (1 − γ)nc−mi ). E.g. for four players, the technology function t(a1 1 a1 2, a2 1 a2 2) is given by t(00, 00) = (1 − (1 − γ)2 )2 , t(01, 00) = t(10, 00) = t(00, 01) = t(00, 10) = (1 − γ(1 − γ))(1 − (1 − γ)2 ), and so on.
the values xi are 1. Thus the project succeeds if most players succeed. The majority function, even on 3 inputs, can not be represented by a read-once network, but is easily represented by a monotone Boolean formula maj(x, y, z) = xy+yz+xz. In this case the technology function is given by t(000) = 3γ2 (1 − γ) + γ3 , t(001) = t(010) = t(100) = γ3 +2(1−γ)2 γ +γ2 (1−γ), etc.
TECHNOLOGIES A success function t is called anonymous if it is symmetric with respect to the players. I.e. t(a1, . . . , an) depends only on P i∈N ai (the number of agents that exert effort). A technology (t, c) is anonymous if t is anonymous and the cost c is identical to all agents. Of the examples presented above, the AND, OR, and majority technologies were anonymous (but not AOO and OOA). As for an anonymous t only the number of agents that exert effort is important, we can shorten the notations and denote tm = t(1m , 0n−m ), Δm = tm+1 − tm, pm = c Δm−1 and um = tm · (v − m · pm), for the case of identical cost c. 22 v 3 0 gamma 200 150
100 50
0
2 1 0 3 12000 6000 8000 4000 2000 gamma 0
10000
Figure 3: Number of agents in the optimal contract of the AND (left) and OR (right) technologies with 3 players, as a function of γ and v. AND technology: either 0 or 3 agents are contracted, and the transition value is monotonic in γ. OR technology: for any γ we can see all transitions.
Let us start with a direct and full analysis of the AND and OR technologies for two players for the case γ = 1/4 and c = 1.
Example 1. AND technology with two agents, c = 1, γ = 1/4: we have t0 = γ2 = 1/16, t1 = γ(1 − γ) = 3/16, and t2 = (1 − γ)2 = 9/16 thus Δ0 = 1/8 and Δ1 = 3/8.
The principal has 3 possibilities: contracting with 0, 1, or 2 agents. Let us write down the expressions for his utility in these 3 cases: • 0 Agents: No agent is paid thus and the principal"s utility is u0 = t0 · v = v/16. • 1 Agent: This agent is paid p1 = c/Δ0 = 8 on success and the principal"s utility is u1 = t1(v − p1) = 3v/16− 3/2. • 2 Agents: each agent is paid p2 = c/Δ1 = 8/3 on success, and the principal"s utility is u2 = t2(v−2p2) = 9v/16 − 3.
Notice that the option of contracting with one agent is always inferior to either contracting with both or with none, and will never be taken by the principal. The principal will contract with no agent when v < 6, with both agents whenever v > 6, and with either non or both for v = 6.
This should be contrasted with the non-strategic case in which the principal completely controls the agents (and bears their costs) and thus simply optimizes globally. In this case the principal will make both agents exert effort whenever v ≥
(non-strategic case) would give a global utility of 6 · 9/16 − 2 = 11/8 while the principal"s decision (in the agency case) would give a global utility of 3/8, giving a ratio of 11/3.
It turns out that this is the worst price of unaccountability in this example, and it is obtained exactly at the transition point of the agency case, as we show below.
Example 2. OR technology with two agents, c = 1, γ = 1/4: we have t0 = 1 − (1 − γ)2 = 7/16, t1 = 1 − γ(1 − γ) = 13/16, and t2 = 1 − γ2 = 15/16 thus Δ0 = 3/8 and Δ1 = 1/8. Let us write down the expressions for the principal"s utility in these three cases: • 0 Agents: No agent is paid and the principal"s utility is u0 = t0 · v = 7v/16. • 1 Agent: This agent is paid p1 = c/Δ0 = 8/3 on success and the principal"s utility is u1 = t1(v − p1) = 13v/16 − 13/6. • 2 Agents: each agent is paid p2 = c/Δ1 = 8 on success, and the principal"s utility is u2 = t2(v − 2p2) = 15v/16 − 15/2.
Now contracting with one agent is better than contracting with none whenever v > 52/9 (and is equivalent for v = 52/9), and contracting with both agents is better than contracting with one agent whenever v > 128/3 (and is equivalent for v = 128/3), thus the principal will contract with no agent for 0 ≤ v ≤ 52/9, with one agent for 52/9 ≤ v ≤ 128/3, and with both agents for v ≥ 128/3.
In the non-strategic case, in comparison, the principal will make a single agent exert effort for v > 8/3, and the second one exert effort as well when v > 8.
It turns out that the price of unaccountability here is 19/13, and is achieved at v = 52/9, which is exactly the transition point from 0 to 1 contracted agents in the agency case. This is not a coincidence that in both the AND and OR technologies the POU is obtained for v that is a transition point (see full proof in [2]).
Lemma 1. For any given technology (t, c) the price of unaccountability POU(t, c) is obtained at some value v which is a transition point, of either the agency or the non-strategic cases.
Proof sketch: We look at all transition points in both cases.
For any value lower than the first transition point, 0 agents are contracted in both cases, and the social welfare ratio is 1. Similarly, for any value higher than the last transition point, n agents are contracted in both cases, and the social welfare ratio is 1. Thus, we can focus on the interval between the first and last transition points. Between any pair of consecutive points, the social welfare ratio is between two linear functions of v (the optimal contracts are fixed on such a segment). We then show that for each segment, the suprimum ratio is obtained at an end point of the segment (a transition point). As there are finitely many such points, the global suprimum is obtained at the transition point with the maximal social welfare ratio. 2 We already see a qualitative difference between the AND and OR technologies (even with 2 agents): in the first case either all agents are contracted or none, while in the second case, for some intermediate range of values v, exactly one agent is contracted. Figure 3 shows the same phenomena for AND and OR technologies with 3 players.
Theorem 1. For any anonymous AND technology7 : • there exists a value8 v∗ < ∞ such that for any v < v∗ it is optimal to contract with no agent, for v > v∗ it is optimal to contract with all n agents, and for v = v∗, both contracts (0, n) are optimal. 7 AND technology with any number of agents n and any γ, and any identical cost c. 8 v∗ is a function of n, γ, c. 23 • the price of unaccountability is obtained at the transition point of the agency case, and is POU = ` 1 γ − 1 ´n−1 + (1 − γ 1 − γ ) Proof sketch: For any fixed number of contracted agents, k, the principal"s utility is a linear function in v, where the slope equals the success probability under k contracted agents. Thus, the optimal contract corresponds to the maximum over a set of linear functions. Let v∗ denote the point at which the principal is indifferent between contracting with 0 or n agents. In [2] we show that at v∗, the principal"s utility from contracting with 0 (or n) agents is higher than his utility when contracting with any number of agents k ∈ {1, . . . , n − 1}. As the number of contracted agents is monotonic non-decreasing in the value (due to Lemma 3), for any v < v∗, contracting with 0 agents is optimal, and for any v > v∗, contracting with n agents is optimal. This is true for both the agency and the non-strategic cases.
As in both cases there is a single transition point, the claim about the price of unaccountability for AND technology is proved as a special case of Lemma 2 below. For AND technology tn−1 t0 = (1−γ)n−1 ·γ γn =  1 γ − 1 n−1 and tn−1 tn = (1−γ)n−1 ·γ (1−γ)n = γ 1−γ , and the expressions for the POU follows. 2 In [2] we present a general characterization of technologies with a single transition in the agency and the non-strategic cases, and provide a full proof of Theorem 1 as a special case.
The property of a single transition occurs in both the agency and the non-strategic cases, where the transition occurs at a smaller value of v in the non-strategic case. Notice that the POU is not bounded across the AND family of technologies (for various n, γ) as POU → ∞ either if γ → 0 (for any given n ≥ 2) or n → ∞ (for any fixed γ ∈ (0, 1 2 )).
Next we consider the OR technology and show that it exhibits all n transitions.
Theorem 2. For any anonymous OR technology, there exist finite positive values v1 < v2 < . . . < vn such that for any v s.t. vk < v < vk+1, contracting with exactly k agents is optimal (for v < v1, no agent is contracted, and for v > vn, all n agents are contracted). For v = vk, the principal is indifferent between contracting with k − 1 or k agents.
Proof sketch: To prove the claim we define vk to be the value for which the principal is indifferent between contracting with k − 1 agents, and contracting with k agents. We then show that for any k, vk < vk+1. As the number of contracted agents is monotonic non-decreasing in the value (due to Lemma 3), v1 < v2 < . . . < vn is a sufficient condition for the theorem to hold. 2 The same behavior occurs in both the agency and the nonstrategic case. This characterization is a direct corollary of a more general characterization given in [2].
While in the AND technology we were able to fully determine the POU analytically, the OR technology is more difficult to analyze.
Open Question 1. What is the POU for OR with n > 2 agents? Is it bounded by a constant for every n?
We are only able to determine the POU of the OR technology for the case of two agents [2]. Even for the 2 agents case we already observe a qualitative difference between the POU in the AND and OR technologies.
Observation 2. While in the AND technology the POU for n = 2 is not bounded from above (for γ → 0), the highest POU in OR technology with two agents is 2 (for γ → 0).
Theorems 1 and 2 say that both the AND and OR technologies exhibit the same transition behavior (changes of the optimal contract) in the agency and the non-strategic cases.
However, this is not true in general. In [2] we provide a full characterization of the sufficient and necessary conditions for general anonymous technologies to have a single transition and all n transitions. We find that the conditions in the agency case are different than the ones in the non-strategic case.
We are able to determine the POU for any anonymous technology that exhibits a single transition in both the agency and the non-strategic cases (see full proof in [2]).
Lemma 2. For any anonymous technology that has a single transition in both the agency and the non-strategic cases, the POU is given by: POU = 1 + tn−1 t0 − tn−1 tn and it is obtained at the transition point of the agency case.
Proof sketch: Since the payments in the agency case are higher than in the non-strategic case, the transition point in the agency case occurs for a higher value than in the non-strategic case. Thus, there exists a region in which the optimal numbers of contracted agents in the agency and the non-strategic cases are 0 and n, respectively. By Lemma 1 the POU is obtained at a transition point. As the social welfare ratio is decreasing in v in this region, the POU is obtained at the higher value, that is, at the transition point of the agency case. The transition point in the agency case is the point at which the principal is indifferent between contracting with 0 and with n agents, v∗ = c·n tn−t0 · tn tn−tn−1 .
Substituting the transition point of the agency case into the POU expression yields the required expression.
POU = v∗ · tn − c · n v∗ · t0 = 1 + tn−1 t0 − tn−1 tn 2
The project under the MAJORITY function succeeds if the majority of the agents succeed in their tasks (see Section 2.3). We are unable to characterize the transition behavior of the MAJORITY technology analytically. Figure 4 presents the optimal number of contracted agents as a function of v and γ, for n = 5. The phenomena that we observe in this example (and others that we looked at) leads us to the following conjecture.
Conjecture 1. For any Majority technology (any n, γ and c), there exists l, 1 ≤ l ≤ n/2 such that the first transition is from 0 to l agents, and then all the remaining n − l transitions exist. 24 4 5 3 1 0 2 400 0
100 gamma
300
200 v 500
Figure 4: Simulations results showing the number of agents in the optimal contract of the MAJORITY technology with 5 players, as a function of γ and v. As γ decreases the first transition is at a lower value and to a higher number of agents. For any sufficiently small γ, the first transition is to 3 = 5/2 agents, and for any sufficiently large γ, the first transition is to 1 agents. For any γ, the first transition is never to more than 3 agents, and after the first transition we see all following possible transitions.
Moreover, for any fixed c, n, l = 1 when γ is close enough to 1 2 , l is a non-decreasing function of γ (with image {1, . . . , n/2 }), and l = n/2 when γ is close enough to 0.
In non-anonymous technologies (even with identical costs), we need to talk about the contracted set of agents and not only about the number of contracted agents. In this section, we identify the sets of agents that can be obtained as the optimal contract for some v. These sets construct the orbit of a technology.
Definition 3. For a technology t, a set of agents S is in the orbit of t if for some value v, the optimal contract is exactly with the set S of agents (where ties between different S"s are broken according to a lexicographic order9 ). The korbit of t is the collection of sets of size exactly k in the orbit.
Observe that in the non-strategic case the k-orbit of any technology with identical cost c is of size at most 1 (as all sets of size k has the same cost, only the one with the maximal probability can be on the orbit). Thus, the orbit of any such technology in the non-strategic case is of size at most n + 1.
We show that the picture in the agency case is very different.
A basic observation is that the orbit of a technology is actually an ordered list of sets of agents, where the order is determined by the following lemma.
Lemma 3. ( Monotonicity lemma) For any technology (t, c), in both the agency and the non-strategic cases, the 9 This implies that there are no two sets with the same success probability in the orbit. expected utility of the principal at the optimal contracts, the success probability of the optimal contracts, and the expected payment of the optimal contract, are all monotonically nondecreasing with the value.
Proof. Suppose the sets of agents S1 and S2 are optimal in v1 and v2 < v1, respectively. Let Q(S) denote the expected total payment to all agents in S in the case that the principal contracts with the set S and the project succeeds (for the agency case, Q(S) = t(S) · P i∈S ci t(S)−t(S\i) , while for the non-strategic case Q(S) = P i∈S ci). The principal"s utility is a linear function of the value, u(S, v) = t(S)·v−Q(S). As S1 is optimal at v1, u(S1, v1) ≥ u(S2, v1), and as t(S2) ≥ 0 and v1 > v2, u(S2, v1) ≥ u(S2, v2). We conclude that u(S1, v1) ≥ u(S2, v2), thus the utility is monotonic non-decreasing in the value.
Next we show that the success probability is monotonic non-decreasing in the value. S1 is optimal at v1, thus: t(S1) · v1 − Q(S1) ≥ t(S2) · v1 − Q(S2) S2 is optimal at v2, thus: t(S2) · v2 − Q(S2) ≥ t(S1) · v2 − Q(S1) Summing these two equations, we get that (t(S1) − t(S2)) · (v1 − v2) ≥ 0, which implies that if v1 > v2 than t(S1) ≥ t(S2).
Finally we show that the expected payment is monotonic non-decreasing in the value. As S2 is optimal at v2 and t(S1) ≥ t(S2), we observe that: t(S2) · v2 − Q(S2) ≥ t(S1) · v2 − Q(S1) ≥ t(S2) · v2 − Q(S1) or equivalently, Q(S2) ≤ Q(S1), which is what we wanted to show.
We begin our discussion of non-anonymous technologies with two examples; the And-of-Ors (AOO) and Or-of-Ands (OOA) technologies.
The AOO technology (see figure 2) is composed of multiple OR-components that are Anded together.
Theorem 3. Let h be an anonymous OR technology, and let f = Vnc j=1 h be the AOO technology that is obtained by a conjunction of nc of these OR-components on disjoint inputs. Then for any value v, an optimal contract contracts with the same number of agents in each OR-component.
Thus, the orbit of f is of size at most nl + 1, where nl is the number of agents in h.
Part of the proof of the theorem (for the complete proof see [2]), is based on such AOO technology being a special case of a more general family of technologies, in which disjoint anonymous technologies are And-ed together, as explained in the next section. We conjecture that a similar result holds for the OOA technology.
Conjecture 2. In an OOA technology which is a disjunction of the same anonymous paths (with the same number of agents, γ and c, but over disjoint inputs), for any value v the optimal contract is constructed from some number of fully-contracted paths. Moreover, there exist v1 < . . . < vnl such that for any v, vi ≤ v ≤ vi+1, exactly i paths are contracted.
We are unable to prove it in general, but can prove it for the case of an OOA technology with two paths of length two (see [2]). 25
The AOO is an example of a technology whose orbit size is linear in its number of agents. If conjecture 2 is true, the same holds for the OOA technology. What can be said about the orbit size of a general non-anonymous technology?
In case of identical costs, it is impossible for all subsets of agents to be on the orbit. This holds by the observation that the 1-orbit (a single agent that exerts effort) is of size at most 1. Only the agent that gives the highest success probability (when only he exerts effort) can be on the orbit (as he also needs to be paid the least). Nevertheless, we next show that the orbit can have exponential size.
A collection of sets of k elements (out of n) is admissible, if every two sets in the collection differ by at least 2 elements (e.g. for k=3, 123 and 234 can not be together in the collection, but 123 and 345 can be).
Theorem 4. Every admissible collection can be obtained as the k − orbit of some t.
Proof sketch: The proof is constructive. Let S be some admissible collection of k-size sets. For each set S ∈ S in the collection we pick S, such that for any two admissible sets Si = Sj, Si = Sj . We then define the technology function t as follows: for any S ∈ S, t(S) = 1/2 − S and ∀i ∈ S, t(S \ i) = 1/2 − 2 S. Thus, the marginal contribution of every i ∈ S is S. Note that since S is admissible, t is well defined, as for any two sets S, S ∈ S and any two agents i, j, S \ i = S \ j. For any other set Z, we define t(Z) in a way that ensures that the marginal contribution of each agent in Z is a very small (the technical details appear in the full version). This completes the definition of t.
We show that each admissible set S ∈ S is optimal at the value vS = ck 2 2 S . We first show that it is better than any other S ∈ S. At the value vS = ck 2 2 S , the set S that corresponds to S maximizes the utility of the principal. This result is obtained by taking the derivative of u(S, v).
Therefore S yields a higher utility than any other S ∈ S. We also pick the range of S to ensure that at vS, S is better than any other set S \ i s.t. S ∈ S. Now we are left to show that at vS, the set S yields a higher utility than any other set Z ∈ S. The construction of t(Z) ensures this since the marginal contribution of each agent in Z is such a small , that the payment is too high for the set to be optimal. 2 In [2] we present the full proof of the theorem, as well as the full proofs of all other claims presented in this section without such a proof. We next show that there exist very large admissible collections.
Lemma 4. For any n ≥ k, there exists an admissible collection of k-size sets of size Ω( 1 n · `n k ´ ).
Proof sketch: The proof is based on an error correcting code that corrects one bit. Such a code has a distance ≥ 3, thus admissible. It is known that there are such codes with Ω(2n /n) code words. To ensure that an appropriate fraction of these code words have weight k, we construct a new code by XOR-ing each code word with a random word r. The properties of XOR ensure that the new code remains admissible. Each code word is now uniformly mapped to the whole cube, and thus its probability of having weight k is `n k ´ /2n . Thus the expected number of weight k words is Ω( `n k ´ /n), and for some r this expectation is achieved or exceeded. 2 For k = n/2 we can construct an exponential size admissible collection, which by Theorem 4 can be used to build a technology with exponential size orbit.
Corollary 1. There exists a technology (t, c) with orbit of size Ω( 2n n √ n ).
Thus, we are able to construct a technology with exponential orbit, but this technology is not a network technology or a structured technology.
Open Question 2. Is there a Read Once network with exponential orbit? Is there a structured technology with exponential orbit?
Nevertheless, so far, we have not seen examples of seriesparallel networks whose orbit size is larger than n + 1.
Open Question 3. How big can the orbit size of a seriesparallel network be?
We make the first step towards a solution of this question by showing that the size of the orbit of a conjunction of two disjoint networks (taking the two in a serial) is at most the sum of the two networks" orbit sizes.
Let g and h be two Boolean functions on disjoint inputs and let f = g V h (i.e., take their networks in series). The optimal contract for f for some v, denoted by S, is composed of some agents from the h-part and some from the g-part, call them T and R respectively.
Lemma 5. Let S be an optimal contract for f = g V h on v. Then, T is an optimal contract for h on v · tg(R), and R is an optimal contract for g on v · th(T).
Proof sketch: We exress the pricipal"s utility u(S, v) from contracting with the set S when his value is v. We abuse notation and use the function to denote the technology as well. Let Δf i (S \ i) denote the marginal contribution of agent i ∈ S. Then, for any i ∈ T, Δf i (S \ i) = g(R) · Δh i (T \ i), and for any i ∈ R, Δf i (S \ i) = h(T) · Δg i (R \ i).
By substituting these expressions and f(S) = h(T) · g(R), we derive that u(S, v) = h(T)  g(R) · v − P i∈T ci Δh i (T \i)  + g(R) · P i∈R ci Δ g i (R\i) . The first term is maximized at a set T that is optimal for h on the value g(R) · v, while the second term is independent of T and h. Thus, S is optimal for f on v if and only if T is an optimal contract for h on v · tg(R).
Similarly, we show that R is an optimal contract for g on v · th(T). 2 Lemma 6. The real function v → th(T), where T is the h − part of an optimal contract for f on v, is monotone non-decreasing (and similarly for the function v → tg(R)).
Proof. Let S1 = T1 ∪ R1 be the optimal contract for f on v1, and let S2 = T2 ∪R2 be the optimal contract for f on v2 < v1. By Lemma 3 f(S1) ≥ f(S2), and since f = g · h, f(S1) = h(T1) · g(R1) ≥ h(T2) · g(R2) = f(S2). Assume in contradiction that h(T1) < h(T2), then since h(T1)·g(R1) ≥ h(T2)·g(R2) this implies that g(R1) > g(R2). By Lemma 5,
T1 is optimal for h on v1 · g(R1), and T2 is optimal for h on v2 ·g(R2). As v1 > v2 and g(R1) > g(R2), T1 is optimal for h on a larger value than T2, thus by Lemma 3, h(T1) ≥ h(T2), a contradiction. 26 Based on Lemma 5 and Lemma 6, we obtain the following Lemma. For the full proof, see [2].
Lemma 7. Let g and h be two Boolean functions on disjoint inputs and let f = g V h (i.e., take their networks in series). Suppose x and y are the respective orbit sizes of g and h; then, the orbit size of f is less or equal to x + y − 1.
By induction we get the following corollary.
Corollary 2. Assume that {(gj, cj )}m j=1 is a set of anonymous technologies on disjoint inputs, each with identical agent cost (all agents of technology gj has the same cost cj). Then the orbit of f = Vm j=1 gj is of size at most ( Pm j=1 nj ) − 1, where nj is the number of agents in technology gj (the orbit is linear in the number of agents).
In particular, this holds for AOO technology where each OR-component is anonymous.
It would also be interesting to consider a disjunction of two Boolean functions.
Open Question 4. Does Lemma 7 hold also for the Boolean function f = g W h (i.e., when the networks g, h are taken in parallel)?
We conjecture that this is indeed the case, and that the corresponding Lemmas 5 and 7 exist for the OR case as well.
If this is true, this will show that series-parallel networks have polynomial size orbit.
Our analysis throughout the paper sheds some light on the algorithmic aspects of computing the best contract. In this section we state these implications (for the proofs see [2]). We first consider the general model where the technology function is given by an arbitrary monotone function t (with rational values), and we then consider the case of structured technologies given by a network representation of the underlying Boolean function.
Technologies Here we assume that we are given a technology and value v as the input, and our output should be the optimal contract, i.e. the set S∗ of agents to be contracted and the contract pi for each i ∈ S∗ . In the general case, the success function t is of size exponential in n, the number of agents, and we will need to deal with that. In the special case of anonymous technologies, the description of t is only the n+1 numbers t0, . . . , tn, and in this case our analysis in section 3 completely suffices for computing the optimal contract.
Proposition 1. Given as input the full description of a technology (the values t0, . . . , tn and the identical cost c for an anonymous technology, or the value t(S) for all the 2n possible subsets S ⊆ N of the players, and a vector of costs c for non-anonymous technologies), the following can all be computed in polynomial time: • The orbit of the technology in both the agency and the non-strategic cases. • An optimal contract for any given value v, for both the agency and the non-strategic cases. • The price of unaccountability POU(t, c).
Proof. We prove the claims for the non-anonymous case, the proof for the anonymous case is similar.
We first show how to construct the orbit of the technology (the same procedure apply in both cases). To construct the orbit we find all transition points and the sets that are in the orbit. The empty contract is always optimal for v = 0.
Assume that we have calculated the optimal contracts and the transition points up to some transition point v for which S is an optimal contract with the highest success probability.
We show how to calculate the next transition point and the next optimal contract.
By Lemma 3 the next contract on the orbit (for higher values) has a higher success probability (there are no two sets with the same success probability on the orbit). We calculate the next optimal contract by the following procedure.
We go over all sets T such that t(T) > t(S), and calculate the value for which the principal is indifferent between contracting with T and contracting with S. The minimal indifference value is the next transition point and the contract that has the minimal indifference value is the next optimal contract. Linearity of the utility in the value and monotonicity of the success probability of the optimal contracts ensure that the above works. Clearly the above calculation is polynomial in the input size.
Once we have the orbit, it is clear that an optimal contract for any given value v can be calculated. We find the largest transition point that is not larger than the value v, and the optimal contract at v is the set with the higher success probability at this transition point.
Finally, as we can calculate the orbit of the technology in both the agency and the non-strategic cases in polynomial time, we can find the price of unaccountability in polynomial time. By Lemma 1 the price of unaccountability POU(t) is obtained at some transition point, so we only need to go over all transition points, and find the one with the maximal social welfare ratio.
A more interesting question is whether if given the function t as a black box, we can compute the optimal contract in time that is polynomial in n. We can show that, in general this is not the case: Theorem 5. Given as input a black box for a success function t (when the costs are identical), and a value v, the number of queries that is needed, in the worst case, to find the optimal contract is exponential in n.
Proof. Consider the following family of technologies. For some small > 0 and k = n/2 we define the success probability for a given set T as follows. If |T| < k, then t(T) = |T| · . If |T| > k, then t(T) = 1 − (n − |T|) · . For each set of agents ˆT of size k, the technology t ˆT is defined by t( ˆT) = 1 − (n − | ˆT|) · and t(T) = |T| · for any T = ˆT of size k.
For the value v = c·(k + 1/2), the optimal contract for t ˆT is ˆT (for the contract ˆT the utility of the principal is about v −c·k = 1/2·c > 0, while for any other contract the utility is negative).
If the algorithm queries about at most ` n n/2 ´ − 2 sets of size k, then it cannot always determine the optimal contract (as any of the sets that it has not queried about might be the optimal one). We conclude that ` n n/2 ´ − 1 queries are needed to determine the optimal contract, and this is exponential in n. 27
In this section we will consider the natural representation of read-once networks for the underlying Boolean function.
Thus the problem we address will be: The Optimal Contract Problem for Read Once Networks: Input: A read-once network G = (V, E), with two specific vertices s, t; rational values γe, δe for each player e ∈ E (and ce = 1), and a rational value v.
Output: A set S of agents who should be contracted in an optimal contract.
Let t(E) denote the probability of success when each edge succeeds with probability δe. We first notice that even computing the value t(E) is a hard problem: it is called the network reliability problem and is known to be #P − hard [8]. Just a little effort will reveal that our problem is not easier: Theorem 6. The Optimal Contract Problem for Read Once Networks is #P-hard (under Turing reductions).
Proof. We will show that an algorithm for this problem can be used to solve the network reliability problem. Given an instance of a network reliability problem < G, {ζe}e∈E > (where ζe denotes e"s probability of success), we define an instance of the optimal contract problem as follows: first define a new graph G which is obtained by Anding G with a new player x, with γx very close to 1 2 and δx = 1 − γx.
For the other edges, we let δe = ζe and γe = ζe/2. By choosing γx close enough to 1 2 , we can make sure that player x will enter the optimal contract only for very large values of v, after all other agents are contracted (if we can find the optimal contract for any value, it is easy to find a value for which in the original network the optimal contract is E, by keep doubling the value and asking for the optimal contract.
Once we find such a value, we choose γx s.t. c 1−2γx is larger than that value). Let us denote βx = 1 − 2γx.
The critical value of v where player x enters the optimal contract of G , can be found using binary search over the algorithm that supposedly finds the optimal contract for any network and any value. Note that at this critical value v, the principal is indifferent between the set E and E ∪ {x}.
Now when we write the expression for this indifference, in terms of t(E) and Δt i(E) , we observe the following. t(E) · γx · v − X i∈E c γx · Δt i(E \ i) ! = t(E)(1 − γx) v − X i∈E c (1 − γx) · Δt i(E \ i) − c t(E) · βx ! if and only if t(E) = (1 − γx) · c (βx)2 · v thus, if we can always find the optimal contract we are also able to compute the value of t(E).
In conclusion, computing the optimal contract in general is hard. These results suggest two natural research directions. The first avenue is to study families of technologies whose optimal contracts can be computed in polynomial time. The second avenue is to explore approximation algorithms for the optimal contract problem.
A possible candidate for the first direction is the family of series-parallel networks, for which the network reliability problem (computing the value of t) is polynomial.
Open Question 5. Can the optimal contract problem for Read Once series-parallel networks be solved in polynomial time?
We can only handle the non-trivial level of AOO networks: Lemma 8. Given a Read Once AND-of-OR network such that each OR-component is an anonymous technology, the optimal contract problem can be solved in polynomial time.
Acknowledgments. This work is supported by the Israel Science Foundation, the USA-Israel Binational Science Foundation, the Lady Davis Fellowship Trust, and by a National Science Foundation grant number ANI-0331659.
[1] M. Babaioff, M. Feldman, and N. Nisan. The Price of Purity and Free-Labor in Combinatorial Agency. In Working Paper, 2005. [2] M. Babaioff, M. Feldman, and N. Nisan.
Combinatorial agency, 2006. www.sims.berkeley.edu/˜moshe/comb-agency.pdf. [3] M. Feldman, J. Chuang, I. Stoica, and S. Shenker.
Hidden-action in multi-hop routing. In EC"05, pages 117-126, 2005. [4] B. Holmstrom. Moral Hazard in Teams. Bell Journal of Economics, 13:324-340, 1982. [5] A. Mass-Colell, M. Whinston, and J. Green.
Microeconomic Theory. Oxford University Press, 1995. [6] N. Nisan and A. Ronen. Algorithmic mechanism design. Games and Economic Behaviour, 35:166 - 196,
[7] C. Papadimitriou. Algorithms, Games, and the Internet. In Proceedings of 33rd STOC, pages 749-753,
[8] J. S. Provan and M. O. Ball. The complexity of counting cuts and of computing the probability that a graph is connected. SIAM J. Comput., 12(4):777-788,
[9] A. Ronen and L. Wahrmann. Prediction Games.

A market is an institution by which economic agents meet and make transactions. Classical economic theory explains the incentives of the agents to engage in this behavior through the agents" preference over the set of available bundles indicating that agents attempt to replace their current bundle with bundles that are both more preferred and attainable if such bundles exist. The preference relation is therefore the key factor in understanding consumer behavior.
One of the common assumptions in this theory is that the preference relation is represented by a utility function and that agents strive to maximize their utility given a budget constraint. This pattern of behavior is the essence of supply and demand, general equilibria and other aspects of consumer theory. Furthermore, as we elaborate in section 2, basic observations on market demand behavior suggest that utility functions are monotone and concave.
This brings us to the question, first raised by Samuelson [18], to what degree is this theory refutable? Given observations of price and demand, under what circumstances can we conclude that the data is consistent with the behavior of a utility maximizing agent equipped with a monotone concave utility function and subject to a budget constraint?
Samuelson gave a necessary but insufficient condition on the underlying preference known as the weak axiom of revealed preference. Uzawa [16] and Mas-Colell [10, 11] introduced a notion of income-Lipschitz and showed that demand functions with this property are rationalizable. These properties do not require any parametric assumptions and are technically refutable, but they do assume knowledge of the entire demand function and rely heavily on the differential properties of demand functions. Hence, an infinite amount of information is needed to refute the theory.
It is often the case that apart form the demand observations there is additional information on the system and it is sensible to make parametric assumptions, namely, to stipulate some functional form of utility. Consistency with utility maximization would then depend on fixing the parameters of the utility function to be consistent with the observations and with a set of equations called the Slutski equations. If such parameters exist, we conclude that the stipulated utility form is consistent with the observations. This approach is useful when there is reason to make these stipulations, it gives an explicit utility function which can be used to make precise forecasts on demand for unob36 served prices. The downside of this approach is that real life data is often inconsistent with convenient functional forms.
Moreover, if the observations are inconsistent it is unclear whether this is a refutation of the stipulated functional form or of utility maximization.
Addressing these issues Houthakker [7] noted that an observer can see only finite quantities of data. He askes when can it be determined that a finite set of observations is consistent with utility maximization without making parametric assumptions? He showes that rationalizability of a finite set of observations is equivalent to the strong axiom of revealed preference. Richter [15] showes that strong axiom of revealed preference is equivalent to rationalizability by a strictly concave monotone utility function. Afriat [1] gives another set of rationalizability conditions the observations must satisfy. Varian [20] introduces the generalized axiom of revealed preference (GARP), an equivalent form of Afriat"s consistency condition that is easier to verify computationally. It is interesting to note that these necessary and sufficient conditions for rationalizability are essentially versions of the well known Farkas lemma [6] (see also [22]).
Afriat [1] proved his theorem by an explicit construction of a utility function witnessing consistency. Varian [20] took this one step further progressing from consistency to forecasting. Varian"s forecasting algorithm basically rules out bundles that are revealed inferior to observed bundles and finds a bundle from the remaining set that together with the observations is consistent with GARP. Furthermore, he introduces Samuelson"s money metric as a canonical utility function and gives upper and lower envelope utility functions for the money metric. Knoblauch [9] shows these envelopes can be computed efficiently. Varian [21] provides an up to date survey on this line of research.
A different approach is presented by Blundell et al. [3, 4].
These papers introduce a model where an agent observes prices and Engel curves for these prices. This gives an improvement on Varian"s original bounds, though the basic idea is still to rule out demands that are revealed inferior.
This model is in a sense a hybrid between Mas-Colell and Afriat"s aproaches. The former requires full information for all prices, the latter for a finite number of prices. On the other hand the approach taken by Blundell et al. requires full information only on a finite number of price trajectories. The motivation for this crossover is to utilize income segmentation in the population to restructure econometric information. Different segments of the population face the same prices with different budgets, and as much as aggregate data can testify on individual preferences, show how demand varies with the budget. Applying non parametric statistical methods, they reconstruct a trajectory from the observed demands of different segments and use it to obtain tighter bounds.
Both these methods would most likely give a good forecast for a fixed demand function after sufficiently many observations assuming they were spread out in a reasonable manner.
However, these methods do not consider the complexity of the demand functions and do not use any probabilistic model of the observations. Therefore, they are unable to provide any estimate of the number of observations that would be sufficient for a good forecast or the degree of confidence in such a forecast.
In this paper we examine the feasibility of demand forecasting with a high degree of confidence using Afriat"s conditions. We formulate the question in terms of whether the class of demand functions derived from monotone concave utilities is efficiently PAC-learnable. Our first result is negative. We show, by computing the fat shattering dimension, that without any prior assumptions, the set of all demand functions induced by monotone concave utility functions is too rich to be efficiently PAC-learnable. However, under some prior assumptions on the set of demand functions we show that the fat shattering dimension is finite and therefore the corresponding sets are PAC-learnable. In these cases, assuming the probability distribution by which the observed price-demand pairs are generated is fixed, we are in a position to offer a forecast and a probabilistic estimate on its accuracy.
In section 2 we briefly discuss the basic assumptions of demand theory and their implications. In section 3 we present a new proof to Afriat"s theorem incorporating an algorithm for efficiently generating a forecasting function due to Teo and Vohra [17]. We show that this algorithm is computationally efficient and can be used as a learning algorithm.
In section 4 we give a brief introduction to PAC learning including several modifications to learning real vector valued functions. We introduce the notion of fat shattering dimension and use it to devise a lower bound on the sample complexity. We also sketch results on upper bounds. In section 5 we study the learnability of demand functions and directly compute the fat shattering dimension of the class of all demand functions and a class of income-Lipschitzian demand functions with a bounded global income-Lipschitz constant.
A utility function u : Rn + → R is a function relating bundles of goods to a cardinal in a manner reflecting the preferences over the bundles. A rational agent with a budget that w.l.g equals 1 facing a price vector p ∈ Rn + will choose from her budget set B(p) = {x ∈ Rn + : p · x ≤ 1} a bundle x ∈ Rn + that maximizes her private utility.
The first assumption we make is that the function is monotone increasing, namely, if x ≥ y, in the sense that the inequality holds coordinatewise, then u(x) ≥ u(y). This reflects the assumption that agents will always prefer more of any one good. This, of course, does not necessarily hold in practice, as in many cases excess supply may lead to storage expenses or other externalities. However, in such cases the demand will be an interior point of the budget set and the less preferred bundles won"t be observed. The second assumption we make on the utility is that all the marginals (partial derivatives) are monotone decreasing. This is the law of diminishing marginal utility which assumes that the larger the excess of one good over the other the less we value each additional good of one kind over the other. These assumptions imply that the utility function is concave and monotone on the observations.
The demand function of the agent is the correspondence fu : Rn + → Rn + satisfying f(p) = argmax{u(x) : p · x ≤ I} In general this correspondence is not necessarily single valued, but it is implicit in the proof of Afriat"s theorem that any set of observations can be rationalized by a demand function that is single valued for unobserved prices. 37 Since large quantities of any good are likely to create utility decreasing externalities, we assume the prices are limited to a compact set. W.l.g. we assume u has marginal utility zero outside [0, 1]d . Any budget set that is not a subset of the support is maximized on any point outside the support and it is therefore difficult to forecast for these prices. We are thus interested in forecasts for prices below the simplex ∆d = conv{(0, . . . , 1, . . . , 0)}. For these prices we take the metric dP (p, p ) = max{| 1 pi − 1 pi | : i = 1, . . . , d} for p, p ∈ ∆d. Note that with this metric ∆d is compact. A demand function is L-income-Lipschitz, for L ∈ R+, if ||f(p) − f(p )||∞ dP (p, p ) ≤ L for any p, p ∈ ∆d. This property reflects an assumption that preferences and demands have some sort of stability. It rules out different demands for the similar prices. We may therefore assume from here on that demand functions are single valued.
A sequence of prices and demands (p1, x1), . . . , (pn, xn) is rationalizable if there exists a utility function u such that xi = fu(pi) for i = 1, . . . , n. We begin with a trivial observation, if pi · xj ≤ pi · xi and xi = f(pi) then xi is preferred over xj since the latter is in the budget set when the former was chosen. It is therefore revealed that u(xj) ≤ u(xi) implying pj · xj ≤ pj · xi.
Suppose there is a sequence (pi1 , xi1 ), . . . , (pik , xik ) such that pij · (xij − xij+1 ) ≤ 0 for j = 1 . . . k − 1 and pik · (xik − xi1 ) ≤ 0. Then the same reasoning shows that u(xi1 ) = u(xi2 ) = . . . = u(xik ) implying pi1 · (xi1 − xi2 ) = pi2 · (xi2 − xi3 ) = . . . = pik−1 ·(xik−1 −xik ) = 0. We call the latter condition the Afriat condition (AC). This argument shows that AC is necessary for rationalizability; the surprising result in Afriat"s theorem is that this condition is also sufficient.
Let A be an n × n matrix with entries aij = pi · (xj − xi) (aij and aji are independent), aii = 0 and let D(A) be the weighted digraph associated with A. The matrix satisfies AC if every cycle with negative total weight includes at least one edge with positive weight.
Theorem 1. There exists y = (y1, . . . , yn) ∈ Rn and s = (s1, . . . , sn) ∈ Rn + satisfying the set of inequalities L(A), yj ≤ yi + siaij i = j 1 ≤ i, j ≤ n iff D(A) satisfies AC.
Proof : If L(A) is feasible then it is easy to see that u(x) = min i {yi + sipi(x − xi)} is a concave utility function that is consistent with the observations, and from our previous remark it follows that D(A) satisfies AC.
In the other direction it is shown by explicit construction that Afriat"s condition for D(A) implies L(A) is feasible. The construction provides a utility function that is consistent with the observations. Teo and Vohra [17] give a strongly polynomial time algorithm for this construction which will be the heart of our learning algorithm.
The construction is executed in two steps. First, the algorithm finds s ∈ Rn + such that the weighted digraph D(A, s) defined by the matrix ˜aij = siaij has no cycle with negative total weight if D(A) satisfies AC and returns a negative cycle otherwise.
The dual of a shortest path problem is given by the constraints: yj − yi ≤ siaij i = j It is a standard result (see [14] p 109) that the system is feasible iff D(A, s) has no negative cycles. Thus, in the second step, if D(A) satisfies AC, the algorithm calls a SHORTEST PATH algorithm to find y ∈ Rn satisfying the constraints.
Now we describe how to choose the si"s. Define S = {(i, j) : aij < 0}, E = {(i, j) : aij = 0} and T = {(i, j) : aij > 0} and let G = ([n], S ∪ E) be a digraph with weights wij = −1 if (i, j) ∈ S and wij = 0 otherwise. D(A) has no negative cycles, hence G is acyclic and breadth first search can assign potentials φi such that φj ≤ φi + wij for (i, j) ∈ S ∪ E. We relabel the vertices so that φ1 ≥ φ2 ≥ . . . ≥ φn.
Let δi = (n − 1) max(i,j)∈S(−aij) min(i,j)∈T aij if φi < φi−1 and δi = 1 otherwise, and define si = iY j=2 δj = δi · si−1 .
We show that for this choice of s, D(A, s) contains no negative weight cycle. Suppose C = (i1, . . . , ik) is a cycle in D(A, s). If φ is constant on C then aij ij+1 = 0 for j = 1, . . . , k and we are done. Otherwise let iv ∈ C be the vertex with smallest potential satisfying w.l.o.g. φ(iv) < φ(iv+1).
For any cycle C in the digraph D(A, s), let (v, u) be an edge in C such that (i) v has the smallest potential among all vertices in C, and (ii) φu > φv. Such an edge exists, otherwise φi is identical for all vertices i in C. In this case, all edges in C have non-negative edge weight in D(A, s).
If (iv, iv+1) ∈ S ∪ E, then we have φ(iv+1) ≤ φ(iv) + wiv,iv+1 ≤ φ(iv) a contradiction. Hence (iv, iv+1) ∈ T. Now, note that all vertices q in C with the same potential as iv must be incident to an edge (q, t) in C such that φ(t) ≥ φ(q). Hence the edge (q, t) must have non-negative weight. i.e., aq,t ≥ 0. Let p denote a vertex in C with the second smallest potential.
Now, C has weight svavu+ X (k,l)∈C\(v,u) skak,l ≥ svav,u+sp(n−1) max (i,j)∈S {aij } ≥ 0, i.e., C has non-negative weight ✷ Algorithm 1 returns in polynomial time a hypothesis that is a piecewise linear function and agrees with the labeling of the observation namely sample error zero. To use this function to forecast demand for unobserved prices we need algorithm 2 which maximizes the function on a given budget set. Since u(x) = mini{yi + sipi(x − xi)} this is a linear program and can be solved in time polynomial in d, n as well as the size of the largest number in the input. 38 Algorithm 1 Utility Algorithm Input (x1, p1), . . . , (xn, pn) S ← {(i, j) : aij < 0} E ← {(i, j) : aij = 0} for all (i, j) ∈ S do wij ← −1 end for for all (i, j) ∈ E do wij ← 0 end for while there exist unvisited vertices do visit new vertex j assign potential to φj end while reorder indices so φ1 ≤ φ2 . . . ≤ φn for all 1 ≤ i ≤ n do δi ← (n − 1) max(i,j)∈S (−aij ) min(i,j)∈T aij si ← Qi j=2 δj end for SHORTEST PATH(yj − yi ≤ siaij) Return y1, . . . , yn ∈ Rd and s1, . . . , sn ∈ R+ Algorithm 2 Evaluation Input y1, . . . , yn ∈ Rd and s1, . . . , sn ∈ R+ max z z ≤ yi + sipi(x − xi) for i = 1, . . . , n px ≤ 1 Return x for which z is maximized
In a supervised learning problem, a learning algorithm is given a finite sample of labeled observations as input and is required to return a model of the functional relationship underlying the labeling. This model, referred to as a hypothesis, is usually a computable function that is used to forecast the labels of future observations. The labels are usually binary values indicating the membership of the observed points in the set that is being learned. However, we are not limited to binary values and, indeed, in the demand functions we are studying the labels are real vectors.
The learning problem has three major components: estimation, approximation and complexity. The estimation problem is concerned with the tradeoff between the size of the sample given to the algorithm and the degree of confidence we have in the forecast it produces. The approximation problem is concerned with the ability of hypotheses from a certain class to approximate target functions from a possibly different class. The complexity problem is concerned with the computational complexity of finding a hypothesis that approximates the target function.
A parametric paradigm assumes that the underlying functional relationship comes from a well defined family, such as the Cobb-Douglas production functions; the system must learn the parameters characterizing this family. Suppose that a learning algorithm observes a finite set of production data which it assumes comes from a Cobb-Douglas production function and returns a hypothesis that is a polynomial of bounded degree. The estimation problem in this case would be to assess the sample size needed to obtain a good estimate of the coefficients. The approximation problem would be to assess the error sustained from approximating a rational function by a polynomial. The complexity problem would be the assessment of the time required to compute the polynomial coefficients.
In the probably approximately correct (PAC) paradigm, the learning of a target function is done by a class of hypothesis functions, that does or does not include the target function itself; it does not necessitate any parametric assumptions on this class. It is also assumed that the observations are generated independently by some distribution on the domain of the relation and that this distribution is fixed. If the class of target functions has finite "dimensionality" then a function in the class is characterized by its values on a finite number of points. The basic idea is to observe the labeling of a finite number of points and find a function from a class of hypotheses which tends to agree with this labeling. The theory tells us that if the sample is large enough then any function that tends to agree with the labeling will, with high probability, be a good approximation of the target function for future observations. The prime objective of PAC theory is to develop the relevant notion of dimensionality and to formalize the tradeoff between dimensionality, sample size and the level of confidence in the forecast.
In the revealed preference setting, our objective is to use a set of observations of prices and demand to forecast demand for unobserved prices. Thus the target function is a mapping from prices to bundles, namely f : Rd + → Rd +. The theory of PAC learning for real valued functions is concerned predominantly with functions from Rd to R. In this section we introduce modifications to the classical notions of PAC learning to vector valued functions and use them to prove a lower bound for sample complexity. An upper bound on the sample complexity can also be proved for our definition of fat shattering, but we do not bring it here as the proof is much more tedious and analogous to the proof of theorem 4.
Before we can proceed with the formal definition, we must clarify what we mean by forecast and tend to agree. In the case of discrete learning, we would like to obtain a function h that with high probability agrees with f. We would then take the probability Pσ(f(x) = h(x)) as the measure of the quality of the estimation. Demand functions are real vector functions and we therefore do not expect f and h to agree with high probability. Rather we are content with having small mean square errors on all coordinates. Thus, our measure of estimation error is given by: erσ(f, h) = Z (||f − h||∞)2 dσ.
For given observations S = {(p1, x1), . . . , (pn, xn)} we measure the agreement by the sample error erS(S, h) = X j (||xj − h(pj)||∞)2 .
A sample error minimization (SEM) algorithm is an algorithm that finds a hypothesis minimizing erS(S, h). In the case of revealed preference, there is a function that takes the sample error to zero. Nevertheless, the upper bounds theorem we use does not require the sample error to be zero.
Definition 1. A set of demand functions C is probably approximately correct (PAC) learnable by hypotheses set H if for any ε, δ > 0, f ∈ C and distribution σ on the prices 39 there exists an algorithm L that for a set of observations of length mL = mL(ε, δ) = Poly(1 δ , 1 ε ) finds a function h from H such that erσ(f, h) < ε with probability 1 − δ.
There may be several learning algorithms for C with different sample complexities. The minimal mL is called the sample complexity of C.
Note that in the definition there is no mention of the time complexity to find h in H and evaluating h(p). A set C is efficiently PAC-learnable if there is a Poly(1 δ , 1 ε ) time algorithm for choosing h and evaluating h(p).
For discrete function sets, sample complexity bounds may be derived from the VC-dimension of the set (see [19, 8]).
An analog to this notion of dimension for real functions is the fat shattering dimension. We use an adaptation of this notion to real vector valued function sets. Let Γ ⊂ Rd + and let C be a set of real functions from Γ to Rd +.
Definition 2. For γ > 0, a set of points p1, . . . , pn ∈ Γ is γ-shattered by a class of real functions C if there exists x1, . . . , xn ∈ Rd + and parallel affine hyperplanes H0, H1 ⊂ Rd such that 0 ∈ H− 0 ∩ H+ 1 , dist(H0, H1) > γ and for each b = (b1, . . . , bn) ∈ {0, 1}n there exists a function fb ∈ C such that fb(pi) ∈ xi + H+ 0 if bi = 0 and f(pi) ∈ xi + H− 1 if bi = 1.
We define the γ-fat shattering dimension of C, denoted fatC(γ) as the maximal size of a γ-shattered set in Γ. If this size is unbounded then the dimension is infinite.
To demonstrate the usefulness of the this notion we use it to derive a lower bound on the sample complexity.
Lemma 2. Suppose the functions {fb : b ∈ {0, 1}n } witness the shattering of {p1, . . . , pn}. Then, for any x ∈ Rd + and labels b, b ∈ {0, 1}n such that bi = bi either ||fb(pi) − x||∞ > γ 2d or ||fb (pi) − x||∞ > γ 2d .
Proof : Since the max exceeds the mean, it follows that if fb and fb correspond to labels such that bi = bi then ||fb(pi) − fb (pi)||∞ ≥ 1 d ||fb(pi) − fb (pi)||2 > γ d .
This implies that for any x ∈ Rd + either ||fb(pi) − x||∞ > γ 2d or ||fb (pi) − x||∞ > γ 2d ✷ Theorem 3. Suppose that C is a class of functions mapping from Γ to Rd +. Then any learning algorithm L for C has sample complexity satisfying mL(ε, δ) ≥ 1 2 fatC(4dε) An analog of this theorem for real valued functions with a tighter bound can be found in [2], this version will suffice for our needs.
Proof : Suppose n = 1 2 fatC(4dε) then there exists a set ΓS = {p1, . . . , p2n} that is shattered by C. It suffices to show that at least one distribution requires large sample.
We construct such a distribution. Let σ be the uniform distribution on ΓS and CS = {fb : b ∈ {0, 1}2n } be the set of functions that witness the shattering of {p1. . . . , pn}.
Let fb be a function chosen uniformly at random from CS.
It follows from lemma 2 (with γ = 2d ) that for any fixed function h the probability that ||fb(p) − h(p)||∞ > 2ε for p ∈ ΓS is at least as high as getting heads on a fair coin toss. Therefore Eb(||fb(p) − h(p)||∞) > 2ε.
Suppose for a sequence of observations z = ((pi1 , x1), . . . , (pin , xn)) a learning algorithm L finds a function h. The observation above and Fubini imply Eb(erσ(h, fb)) > ε.
Randomizing on the sample space we get Eb,z(erσ(h, fb)) > ε.
This shows Eh,z(erσ(h, fb0 )) > ε for some fb0 . W.l.g we may assume the error is bounded (since we are looking at what is essentially a finite set) therefore the probability that erσ(h, fb0 ) > ε cannot be too small, hence fb0 is not PAClearnable with a sample of size n ✷ The following theorem gives an upper bound on the sample complexity required for learning a set of functions with finite fat shattering dimension. The theorem is proved in [2] for real valued functions, the proof for the real vector case is analogous and so omitted.
Theorem 4. Let C be a set of real-valued functions from X to [0, 1] with fatC(γ) < ∞. Let A be approximate-SEM algorithm for C and define L(z) = A(z, ε0 6 ) for z ∈ Zm and ε0 = 16√ m . Then L is a learning algorithm for C with sample complexity given by: mL(ε, δ) = O „ 1 ε2 (ln2 ( 1 ε )fatC(ε) + ln( 1 δ )) « for any ε, δ > 0.
PREFERENCE Algorithm 1 is an efficient learning algorithm in the sense that it finds a hypothesis with sample error zero in time polynomial in the number of observations. As we have seen in section 4 the number of observations required to PAC learn the demand depends on the fat shattering dimension of the class of demand functions which in turn depends on the class of utility functions they are derived from. We compute the fat shattering dimension for two classes of demands. The first is the class of all demand functions, we show that this class has infinite shattering dimension (we give two proofs) and is therefore not PAC learnable. The second class we consider is the class of demand functions derived from utilities with bounded support and income-Lipschitz. We show that the class has a finite fat shattering dimension that depends on the support and the income-Lipschitz constant.
Theorem 5. Let C be a set of demand functions from Rd + to Rd + then fatC(γ) = ∞ Proof 1: For ε > 0 let pi = 2−i p for i = 1, . . . , n be a set of price vectors inducing parallel budget sets Bi and let x1, . . . , xn be the intersection of these hyperplanes with an orthogonal line passing through the center. Let H0 and H1 be hyperplanes that are not parallel to p and let xi ∈ Bi ∩ (xi + H+ 0 ) and xi ∈ Bi ∩ (xi + H− 1 ) for i = 1 . . . n (see figure 1).
For any labeling b = (b1, . . . , bn) ∈ {0, 1}n let y = y(b) = (y1, . . . , yn) be a set of demands such that yi = xi if bi = 0 and yi = xi if bi = 1 (we omit an additional index b in y for notational convenience). To show that p1, . . . , pn is shattered it suffices to find for every b a demand function fb supported by concave utility such that fb(pi) = yb i . To show that such a function exists it suffices to show that Afriat"s conditions are satisfied. Since yi are in the budget 40 set yi · 2−i p = 1, therefore pi · (yj − yi) = 2j−i − 1. This shows that pi · (yj − yi) ≤ 0 iff j < i hence there can be no negative cycles and the condition is met. ✷ Proof 2: The utility functions satisfying Afriat"s condition in the first proof could be trivial assigning the same utility to xi as to xi . In fact, pick a utility function whose level sets are parallel to the budget constraint. Therefore the shattering of the prices p1, . . . , pn is the result of indifference rather than genuine preference. To avoid this problem we reprove the theorem by constructing utility functions u such that u(xi) = u(xi ) for all i and therefore a distinct utility function is associated with each labeling.
For i = 1, . . . n let pi1, . . . , pid be price vectors satisfying the following conditions:
i are supporting hyperplanes of a convex polytope Λi
1, . . . , n Finally let yi1, . . . , yid be points on the facets of Λi that intersect yi, such that ||pjr||1 · ||yi − yis||∞ = o(1) for all j, s and r. We call the set of points yi, yi1, . . . , yid the level i demand and pi, pi1, . . . , pid level i prices. Applying H¨olders inequality we get |pir · yjs − pi · yj | ≤ |(pir − pi) · yj| + |pir · (yjs − yj)| ||pir − pi||∞ · ||yj ||1 + ||yjs − yj||∞ · ||pir||1. = o(1) This shows that pir · (yjs − yir) = pi · (ys − yi) + o(1) = 2j−i − 1 + o(1) therefore pir · (yjs − yir) ≤ 0 iff j < i or i = j. This implies that if there is a negative cycle then all the points in the cycle must belong to the same level. The points of any one level lie on the facets of a polytope Λi and the prices pis are supporting hyperplanes of the polytope. Thus, the polytope defines a utility function for which these demands are utility maximizing. The other direction of Afriat"s theorem therefore implies there can be no negative cycles within points on the same level.
It follows that there are no negative cycles for the union of observations from all levels hence the sequence of observations (y1, p1), (y11, p11), (y12, p12), . . . , (ynd, pnd) is consistent with monotone concave utility function maximization and again by Afriat"s theorem there exists u supporting a demand function fb ✷ The proof above relies on the fact that an agent have high utility and marginal utility for very large bundles. In many cases it is reasonable to assume that the marginal for very large bundles is very small, or even that the utility or the marginal utility have compact support. Unfortunately, rescaling the previous example shows that even a compact set may contain a large shattered set. We notice however, that in this case we obtain a utility function that yield demand functions that are very sensitive to small price changes. We show that the class of utility functions that have marginal utilities with compact support and for which the relevant demand functions are income-Lipschitzian has finite fat shattering dimension. ✲ ✻ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ ❅ (0,0)               H0               H1r x1 r x1 ❈ ❈ ❜ ❜❜ r x2 r x2 ❚ ❚ ❚ ❚ ❚ ❚ ❚ ❚ ❚ ❚ ❚ ❜ ❜❜ Figure 1: Utility function shattering x1 and x2 Theorem 6. Let C be a set of L-income-Lipschitz demand functions from ∆d to Rd + for some global constant L ∈ R. Then fatC(γ) ≤ ( L γ )d Proof : Let p1, . . . , pn ∈ ∆d be a shattered set with witnesses x1, . . . , xn ∈ Rd +. W.l.g. xi+H+ 0 ∩xj +H− 0 = ∅ implying xi + H− 1 ∩ xj + H+ 1 = ∅, for a labeling b = (b1, . . . , bn) ∈ {0, 1}n such that bi = 0 and bj = 1, ||fb(pi) − fb(pj)||∞ > γ hence ||pi − pj||∞ > γ L . A standard packing argument implies n ≤ (L γ )d ✷
The authors would like to thank Eli Shamir, Ehud Kalai,
Julio Gonz´alez D´ıaz, Rosa Matzkin, Gad Allon and Adam Galambos for helpful discussions and suggestions.

In this paper we present a fully polynomial-time approximation scheme for the single-good multi-unit auction problem. Our scheme is both approximately efficient and approximately strategyproof. The auction settings considered in our paper are motivated by recent trends in electronic commerce; for instance, corporations are increasingly using auctions for their strategic sourcing. We consider both a reverse auction variation and a forward auction variation, and propose a compact and expressive bidding language that allows marginal-decreasing piecewise constant curves.
In the reverse auction, we consider a single buyer with a demand for M units of a good and n suppliers, each with a marginal-decreasing piecewise-constant cost function. In addition, each supplier can also express an upper bound, or capacity constraint on the number of units she can supply. The reverse variation models, for example, a procurement auction to obtain raw materials or other services (e.g. circuit boards, power suppliers, toner cartridges), with flexible-sized lots.
In the forward auction, we consider a single seller with M units of a good and n buyers, each with a marginal-decreasing piecewise-constant valuation function. A buyer can also express a lower bound, or minimum lot size, on the number of units she demands. The forward variation models, for example, an auction to sell excess inventory in flexible-sized lots.
We consider the computational complexity of implementing the Vickrey-Clarke-Groves [22, 5, 11] mechanism for the multiunit auction problem. The Vickrey-Clarke-Groves (VCG) mechanism has a number of interesting economic properties in this setting, including strategyproofness, such that truthful bidding is a dominant strategy for buyers in the forward auction and sellers in the reverse auction, and allocative efficiency, such that the outcome maximizes the total surplus in the system. However, as we discuss in Section 2, the application of the VCG-based approach is limited in the reverse direction to instances in which the total payments to the sellers are less than the value of the outcome to the buyer. Otherwise, either the auction must run at a loss in these instances, or the buyer cannot be expected to voluntarily choose to participate. This is an example of the budget-deficit problem that often occurs in efficient mechanism design [17].
The computational problem is interesting, because even with marginal-decreasing bid curves, the underlying allocation problem turns out to (weakly) intractable. For instance, the classic 0/1 knapsack is a special case of this problem.1 We model the 1 However, the problem can be solved easily by a greedy scheme if we remove all capacity constraints from the seller and all 166 allocation problem as a novel and interesting generalization of the classic knapsack problem, and develop a fully polynomialtime approximation scheme, computing a (1 + )-approximation in worst-case time T = O(n3 /ε), where each bid has a fixed number of piecewise constant pieces.
Given this scheme, a straightforward computation of the VCG payments to all n agents requires time O(nT). We compute approximate VCG payments in worst-case time O(αT log(αn/ε)), where α is a constant that quantifies a reasonable no-monopoly assumption. Specifically, in the reverse auction, suppose that C(I) is the minimal cost for procuring M units with all sellers I, and C(I \ i) is the minimal cost without seller i. Then, the constant α is defined as an upper bound for the ratio C(I \i)/C(I), over all sellers i. This upper-bound tends to 1 as the number of sellers increases.
The approximate VCG mechanism is ( ε 1+ε )-strategyproof for an approximation to within (1 + ) of the optimal allocation.
This means that a bidder can gain at most ( ε 1+ε )V from a nontruthful bid, where V is the total surplus from the efficient allocation. As such, this is an example of a computationally-tractable ε-dominance result.2 In practice, we can have good confidence that bidders without good information about the bidding strategies of other participants will have little to gain from attempts at manipulation.
Section 2 formally defines the forward and reverse auctions, and defines the VCG mechanisms. We also prove our claims about ε-strategyproofness. Section 3 provides the generalized knapsack formulation for the multi-unit allocation problems and introduces the fully polynomial time approximation scheme.
Section 4 defines the approximation scheme for the payments in the VCG mechanism. Section 5 concludes.
There has been considerable interest in recent years in characterizing polynomial-time or approximable special cases of the general combinatorial allocation problem, in which there are multiple different items. The combinatorial allocation problem (CAP) is both NP-complete and inapproximable (e.g. [6]). Although some polynomial-time cases have been identified for the CAP [6, 20], introducing an expressive exclusive-or bidding language quickly breaks these special cases. We identify a non-trivial but approximable allocation problem with an expressive exclusiveor bidding language-the bid taker in our setting is allowed to accept at most one point on the bid curve.
The idea of using approximations within mechanisms, while retaining either full-strategyproofness or ε-dominance has received some previous attention. For instance, Lehmann et al. [15] propose a greedy and strategyproof approximation to a single-minded combinatorial auction problem. Nisan & Ronen [18] discussed approximate VCG-based mechanisms, but either appealed to particular maximal-in-range approximations to retain full strategyproofness, or to resource-bounded agents with information or computational limitations on the ability to compute strategies.
Feigenminimum-lot size constraints from the buyers. 2 However, this may not be an example of what Feigenbaum & Shenker refer to as a tolerably-manipulable mechanism [8] because we have not tried to bound the effect of such a manipulation on the efficiency of the outcome. VCG mechanism do have a natural self-correcting property, though, because a useful manipulation to an agent is a reported value that improves the total value of the allocation based on the reports of other agents and the agent"s own value. baum & Shenker [8] have defined the concept of strategically faithful approximations, and proposed the study of approximations as an important direction for algorithmic mechanism design. Schummer [21] and Parkes et al [19] have previously considered ε-dominance, in the context of economic impossibility results, for example in combinatorial exchanges.
Eso et al. [7] have studied a similar procurement problem, but for a different volume discount model. This earlier work formulates the problem as a general mixed integer linear program, and gives some empirical results on simulated data.
Kalagnanam et al. [12] address double auctions, where multiple buyers and sellers trade a divisible good. The focus of this paper is also different: it investigates the equilibrium prices using the demand and supply curves, whereas our focus is on efficient mechanism design. Ausubel [1] has proposed an ascending-price multi-unit auction for buyers with marginal-decreasing values [1], with an interpretation as a primal-dual algorithm [2].
VCG AUCTIONS In this section, we first describe the marginal-decreasing piecewise bidding language that is used in our forward and reverse auctions. Continuing, we introduce the VCG mechanism for the problem and the ε-dominance results for approximations to VCG outcomes. We also discuss the economic properties of VCG mechanisms in these forward and reverse auction multi-unit settings.
We provide a piecewise-constant and marginal-decreasing bidding language. This bidding language is expressive for a natural class of valuation and cost functions: fixed unit prices over intervals of quantities. See Figure 1 for an example. In addition, we slightly relax the marginal-decreasing requirement to allow: a bidder in the forward auction to state a minimal purchase amount, such that she has zero value for quantities smaller than that amount; a seller in the reverse auction to state a capacity constraint, such that she has an effectively infinite cost to supply quantities in excess of a particular amount.
Reverse Auction Bid 7 5 10 20 25 10 8 Quantity Price 7 5 10 20 25 10 8 Quantity Price Forward Auction Bid Figure 1: Marginal-decreasing, piecewise constant bids. In the forward auction bid, the bidder offers $10 per unit for quantity in the range [5, 10), $8 per unit in the range [10, 20), and $7 in the range [20, 25]. Her valuation is zero for quantities outside the range [10, 25]. In the reverse auction bid, the cost of the seller is ∞ outside the range [10, 25].
In detail, in a forward auction, a bid from buyer i can be written as a list of (quantity-range, unit-price) tuples, ((u1 i , p1 i ), (u2 i , p2 i ), . . . , (umi−1 i , pmi−1 i )), with an upper bound umi i on the quantity. The interpretation is that the bidder"s valuation in the 167 (semi-open) quantity range [uj i , uj+1 i ) is pj i for each unit.
Additionally, it is assumed that the valuation is 0 for quantities less than u1 i as well as for quantities more than um i . This is implemented by adding two dummy bid tuples, with zero prices in the range [0, u1 i ) and (umi i , ∞). We interpret the bid list as defining a price function, pbid,i(q) = qpj i , if uj i ≤ q < uj+1 i , where j = 1, 2, . . . , mi −1. In order to resolve the boundary condition, we assume that the bid price for the upper bound quantity umi i is pbid,i(umi i ) = umi i pmi−1 i .
A seller"s bid is similarly defined in the reverse auction. The interpretation is that the bidder"s cost in the (semi-open) quantity range [uj i , uj+1 i ) is pj i for each unit. Additionally, it is assumed that the cost is ∞ for quantities less than u1 i as well as for quantities more than um i . Equivalently, the unit prices in the ranges [0, u1 i ) and (um i , ∞) are infinity. We interpret the bid list as defining a price function, pask,i(q) = qpj i , if uj i ≤ q < uj+1 i .
We construct the tractable and approximately-strategyproof multiunit auctions around a VCG mechanism. We assume that all agents have quasilinear utility functions; that is, ui(q, p) = vi(q)− p, for a buyer i with valuation vi(q) for q units at price p, and ui(q, p) = p − ci(q) for a seller i with cost ci(q) at price p. This is a standard assumption in the auction literature, equivalent to assuming risk-neutral agents [13]. We will use the term payoff interchangeably for utility.
In the forward auction, there is a seller with M units to sell.
We assume that this seller has no intrinsic value for the items.
Given a set of bids from I agents, let V (I) denote the maximal revenue to the seller, given that at most one point on the bid curve can be selected from each agent and no more than M units of the item can be sold. Let x∗ = (x∗ 1, . . . , x∗ N ) denote the solution to this winner- determination problem, where x∗ i is the number of units sold to agent i. Similarly, let V (I \ i) denote the maximal revenue to the seller without bids from agent i. The VCG mechanism is defined as follows:
constraints from all the buyers.
that solves the winner-determination problem with all buyers.
i ) − [V (I) − V (I \ i)] from each buyer, and pass the payments to the seller.
In this forward auction, the VCG mechanism is strategyproof for buyers, which means that truthful bidding is a dominant strategy, i.e. utility maximizing whatever the bids of other buyers.
In addition, the VCG mechanism is allocatively-efficient, and the payments from each buyer are always positive.3 Moreover, each buyer pays less than its value, and receives payoff V (I)−V (I \ i) in equilibrium; this is precisely the marginal-value that buyer i contributes to the economic efficiency of the system.
In the reverse auction, there is a buyer with M units to buy, and n suppliers. We assume that the buyer has value V > 0 to purchase all M units, but zero value otherwise. To simplify the mechanism design problem we assume that the buyer will truthfully announce this value to the mechanism.4 The winner3 In fact, the VCG mechanism maximizes the expected payoff to the seller across all efficient mechanisms, even allowing for Bayesian-Nash implementations [14]. 4 Without this assumption, the Myerson-Satterthwaite [17] impossibility result would already imply that we should not expect an efficient trading mechanism in this setting. determination problem in the reverse auction is to determine the allocation, x∗ , that minimizes the cost to the buyer, or forfeits trade if the minimal cost is greater than value, V .
Let C(I) denote the minimal cost given bids from all sellers, and let C(I \i) denote the minimal cost without bids from seller i. We can assume, without loss of generality, that there is an efficient trade and V ≥ C(I). Otherwise, then the efficient outcome is no trade, and the outcome of the VCG mechanism is no trade and no payments.
The VCG mechanism implements the outcome x∗ that minimizes cost based on bids from all sellers, and then provides payment pvcg,i = pask,i(x∗ i )+[V −C(I)−max(0, V −C(I\i))] to each seller. The total payment is collected from the buyer. Again, in equilibrium each seller"s payoff is exactly the marginal-value that the seller contributes to the economic efficiency of the system; in the simple case that V ≥ C(I \ i) for all sellers i, this is precisely C(I \ i) − C(I).
Although the VCG mechanism remains strategyproof for sellers in the reverse direction, its applicability is limited to cases in which the total payments to the sellers are less than the buyer"s value. Otherwise, there will be instances in which the buyer will not choose to voluntarily participate in the mechanism, based on its own value and its beliefs about the costs of sellers. This leads to a loss in efficiency when the buyer chooses not to participate, because efficient trades are missed. This problem with the size of the payments, does not occur in simple single-item reverse auctions, or even in multi-unit reverse auctions with a buyer that has a constant marginal-valuation for each additional item that she procures.5 Intuitively, the problem occurs in the reverse multi-unit setting because the buyer demands a fixed number of items, and has zero value without them. This leads to the possibility of the trade being contingent on the presence of particular, so-called pivotal sellers. Define a seller i as pivotal, if C(I) ≤ V but C(I\i) > V . In words, there would be no efficient trade without the seller. Any time there is a pivotal seller, the VCG payments to that seller allow her to extract all of the surplus, and the payments are too large to sustain with the buyer"s value unless this is the only winning seller.
Concretely, we have this participation problem in the reverse auction when the total payoff to the sellers, in equilibrium, exceeds the total payoff from the efficient allocation: V − C(I) ≥ i [V − C(I) − max(0, V − C(I \ i))] As stated above, first notice that we require V > C(I \ i) for all sellers i. In other words, there must be no pivotal sellers.
Given this, it is then necessary and sufficient that: V − C(I) ≥ i (C(I \ i) − C(I)) (1) 5 To make the reverse auction symmetric with the forward direction, we would need a buyer with a constant marginal-value to buy the first M units, and zero value for additional units. The payments to the sellers would never exceed the buyer"s value in this case. Conversely, to make the forward auction symmetric with the reverse auction, we would need a seller with a constant (and high) marginal-cost to sell anything less than the first M units, and then a low (or zero) marginal cost. The total payments received by the seller can be less than the seller"s cost for the outcome in this case. 168 In words, the surplus of the efficient allocation must be greater than the total marginal-surplus provided by each seller.6 Consider an example with 3 agents {1, 2, 3}, and V = 150 and C(123) = 50. Condition (1) holds when C(12) = C(23) = 70 and C(13) = 100, but not when C(12) = C(23) = 80 and C(13) = 100. In the first case, the agent payoffs π = (π0, π1, π2, π3), where 0 is the seller, is (10, 20, 50, 20). In the second case, the payoffs are π = (−10, 30, 50, 30).
One thing we do know, because the VCG mechanism will maximize the payoff to the buyer across all efficient mechanisms [14], is that whenever Eq. 1 is not satisfied there can be no efficient auction mechanism.7
We now consider the same VCG mechanism, but with an approximation scheme for the underlying allocation problem. We derive an ε-strategyproofness result, that bounds the maximal gain in payoff that an agent can expect to achieve through a unilateral deviation from following a simple truth-revealing strategy.
We describe the result for the forward auction direction, but it is quite a general observation.
As before, let V (I) denote the value of the optimal solution to the allocation problem with truthful bids from all agents, and V (I \i) denote the value of the optimal solution computed without bids from agent i. Let ˆV (I) and ˆV (I \ i) denote the value of the allocation computed with an approximation scheme, and assume that the approximation satisfies: (1 + ) ˆV (I) ≥ V (I) for some > 0. We provide such an approximation scheme for our setting later in the paper. Let ˆx denote the allocation implemented by the approximation scheme.
The payoff to agent i, for announcing valuation ˆvi, is: vi(ˆxi) + j=i ˆvj (ˆxj) − ˆV (I \ i) The final term is independent of the agent"s announced value, and can be ignored in an incentive-analysis. However, agent i can try to improve its payoff through the effect of its announced value on the allocation ˆx implemented by the mechanism. In particular, agent i wants the mechanism to select ˆx to maximize the sum of its true value, vi(ˆxi), and the reported value of the other agents,
Èj=i ˆvj (ˆxj). If the mechanism"s allocation algorithm is optimal, then all the agent needs to do is truthfully state its value and the mechanism will do the rest. However, faced with an approximate allocation algorithm, the agent can try to improve its payoff by announcing a value that corrects for the approximation, and causes the approximation algorithm to implement the allocation that exactly maximizes the total reported value of the other agents together with its own actual value [18]. 6 This condition is implied by the agents are substitutes requirement [3], that has received some attention in the combinatorial auction literature because it characterizes the case in which VCG payments can be supported in a competitive equilibrium. Useful characterizations of conditions that satisfy agents are substitutes, in terms of the underlying valuations of agents have proved quite elusive. 7 Moreover, although there is a small literature on maximallyefficient mechanisms subject to requirements of voluntaryparticipation and budget-balance (i.e. with the mechanism neither introducing or removing money), analytic results are only known for simple problems (e.g. [16, 4]).
We can now analyze the best possible gain from manipulation to an agent in our setting. We first assume that the other agents are truthful, and then relax this. In both cases, the maximal benefit to agent i occurs when the initial approximation is worst-case. With truthful reports from other agents, this occurs when the value of choice ˆx is V (I)/(1 + ε). Then, an agent could hope to receive an improved payoff of: V (I) − V (I) 1 + ε = ε 1 + ε V (I) This is possible if the agent is able to select a reported type to correct the approximation algorithm, and make the algorithm implement the allocation with value V (I). Thus, if other agents are truthful, and with a (1 + ε)-approximation scheme to the allocation problem, then no agent can improve its payoff by more than a factor ε/(1 + ε) of the value of the optimal solution.
The analysis is very similar when the other agents are not truthful. In this case, an individual agent can improve its payoff by no more than a factor /(1 + ) of the value of the optimal solution given the values reported by the other agents.
Let V in the following theorem define the total value of the efficient allocation, given the reported values of agents j = i, and the true value of agent i.
THEOREM 1. A VCG-based mechanism with a (1 + ε)allocation algorithm is (1+ −V ) strategyproof for agent i, and agent i can gain at most this payoff through some non-truthful strategy.
Notice that we did not need to bound the error on the allocation problems without each agent, because the -strategyproofness result follows from the accuracy of the first-term in the VCG payment and is independent of the accuracy of the second-term.
However, the accuracy of the solution to the problem without each agent is important to implement a good approximation to the revenue properties of the VCG mechanism.
PROBLEM In this section, we design a fully polynomial approximation scheme for the generalized knapsack, which models the winnerdetermination problem for the VCG-based multi-unit auctions.
We describe our results for the reverse auction variation, but the formulation is completely symmetric for the forward-auction.
In describing our approximation scheme, we begin with a simple property (the Anchor property) of an optimal knapsack solution. We use this property to develop an O(n2 ) time 2-approximation for the generalized knapsack. In turn, we use this basic approximation to develop our fully polynomial-time approximation scheme (FPTAS).
One of the major appeals of our piecewise bidding language is its compact representation of the bidder"s valuation functions.
We strive to preserve this, and present an approximation scheme that will depend only on the number of bidders, and not the maximum quantity, M, which can be very large in realistic procurement settings.
The FPTAS implements an (1 + ε) approximation to the optimal solution x∗ , in worst-case time T = O(n3 /ε), where n is the number of bidders, and where we assume that the piecewise bid for each bidder has O(1) pieces. The dependence on the number of pieces is also polynomial: if each bid has a maximum 169 of c pieces, then the running time can be derived by substituting nc for each occurrence of n.
Before we begin, let us recall the classic 0/1 knapsack problem: we are given a set of n items, where the item i has value vi and size si, and a knapsack of capacity M; all sizes are integers. The goal is to determine a subset of items of maximum value with total size at most M. Since we want to focus on a reverse auction, the equivalent knapsack problem will be to choose a set of items with minimum value (i.e. cost) whose size exceeds M. The generalized knapsack problem of interest to us can be defined as follows: Generalized Knapsack: Instance: A target M, and a set of n lists, where the ith list has the form Bi = (u1 i , p1 i ), . . . , (umi−1 i , pmi−1 i ), (umi i (i), ∞) , where uj i are increasing with j and pj i are decreasing with j, and uj i , pj i , M are positive integers.
Problem: Determine a set of integers xj i such that
i is non-zero for any i,
i = 0 implies xj i ∈ [uj i , uj+1 i ),
Èi Èj xj i ≥ M, and
Èi Èj pj i xj i is minimized.
This generalized knapsack formulation is a clear generalization of the classic 0/1 knapsack. In the latter, each list consists of a single point (si, vi).8 The connection between the generalized knapsack and our auction problem is transparent. Each list encodes a bid, representing multiple mutually exclusive quantity intervals, and one can choose any quantity in an interval, but at most one interval can be selected. Choosing interval [uj i , uj+1 i ) has cost pj i per unit.
The goal is to procure at least M units of the good at minimum possible cost. The problem has some flavor of the continuous knapsack problem. However, there are two major differences that make our problem significantly more difficult: (1) intervals have boundaries, and so to choose interval [uj i , uj+1 i ) requires that at least uj i and at most uj+1 i units must be taken; (2) unlike the classic knapsack, we cannot sort the items (bids) by value/size, since different intervals in one list have different unit costs.
We begin with a definition. Given an instance of the generalized knapsack, we call each tuple tj i = (uj i , pj i ) an anchor.
Recall that these tuples represent the breakpoints in the piecewise constant curve bids. We say that the size of an anchor tj i is uj i , 8 In fact, because of the one per list constraint, the generalized problem is closer in spirit to the multiple choice knapsack problem [9], where the underling set of items is partitioned into disjoint subsets U1, U2, . . . , Uk, and one can choose at most one item from each subset. PTAS do exist for this problem [10], and indeed, one can convert our problem into a huge instance of the multiple choice knapsack problem, by creating one group for each list; put a (quantity, price) point tuple (x, p) for each possible quantity for a bidder into his group (subset). However, this conversion explodes the problem size, making it infeasible for all but the most trivial instances. the minimum number of units available at this anchor"s price pj i .
The cost of the anchor tj i is defined to be the minimum total price associated with this tuple, namely, cost(tj i ) = pj i uj i if j < mi, and cost(tmi i ) = pmi−1 i umi i .
In a feasible solution {x1, x2, . . . , xn} of the generalized knapsack, we say that an element xi = 0 is an anchor if xi = uj i , for some anchor uj i . Otherwise, we say that xi is midrange. We observe that an optimal knapsack solution can always be constructed so that at most one solution element is midrange. If there are two midrange elements x and x , for bids from two different agents, with x ≤ x , then we can increment x and decrement x, until one of them becomes an anchor. See Figure 2 for an example.
LEMMA 1. [Anchor Property] There exists an optimal solution of the generalized knapsack problem with at most one midrange element. All other elements are anchors. 1 midrange bid 5 20 15 10 25 5 25 30201510 35 3 2 1 Price Quantity 5 20 15 10 25 5 25 30201510 35 3 2 1 Price Quantity (i) Optimal solution with 2 midrange bids (ii) Optimal soltution with Figure 2: (i) An optimal solution with more than one bid not anchored (2,3); (ii) an optimal solution with only one bid (3) not anchored.
We use the anchor property to first obtain a polynomial-time 2-approximation scheme. We do this by solving several instances of a restricted generalized-knapsack problem, which we call iKnapsack, where one element is forced to be midrange for a particular interval.
Specifically, suppose element x for agent l is forced to lie in its jth range, [uj , uj+1 ), while all other elements, x1, . . . , xl−1, xl+1, xn, are required to be anchors, or zero. This corresponds to the restricted problem iKnapsack( , j), in which the goal is to obtain at least M − uj units with minimum cost. Element x is assumed to have already contributed uj units. The value of a solution to iKnapsack( , j) represents the minimal additional cost to purchase the rest of the units.
We create n − 1 groups of potential anchors, where ith group contains all the anchors of the list i in the generalized knapsack.
The group for agent l contains a single element that represents the interval [0, uj+1 −uj ), and the associated unit-price pj . This interval represents the excess number of units that can be taken from agent l in iKnapsack( , j), in addition to uj , which has already been committed. In any other group, we can choose at most one anchor.
The following pseudo-code describes our algorithm for this restriction of the generalized knapsack problem. U is the union of all the tuples in n groups, including a tuple t for agent l. The size of this special tuple is defined as uj+1 − uj , and the cost is defined as pj l (uj+1 −uj ). R is the number of units that remain to be acquired. S is the set of tuples accepted in the current tentative 170 solution. Best is the best solution found so far. Variable Skip is only used in the proof of correctness.
Algorithm Greedy( , j)
case of ties, sort in ascending order of unit quantities.
Initialize R = M − uj , S = Best = Skip = ∅.
tuple is tk i , i.e. the kth anchor from agent i.
If mark(i) = 1, ignore this tuple; otherwise do the following steps: • if size(tk i ) > R and i = return min {cost(S) + Rpj , cost(Best)}; • if size(tk i ) > R and cost(tk i ) ≤ cost(S) return min {cost(S) + cost(tk i ), cost(Best)}; • if size(tk i ) > R and cost(tk i ) > cost(S) Add tk i to Skip; Set Best to S ∪ {tk i } if cost improves; • if size(tk i ) ≤ R then add tk i to S; mark(i) = 1; subtract size(tk i ) from R.
The approximation algorithm is very similar to the approximation algorithm for knapsack. Since we wish to minimize the total cost, we consider the tuples in order of increasing per unit cost. If the size of tuple tk i is smaller than R, then we add it to S, update R, and delete from U all the tuples that belong to the same group as tk i . If size(tk i ) is greater than R, then S along with tk i forms a feasible solution. However, this solution can be far from optimal if the size of tk i is much larger than R. If total cost of S and tk i is smaller than the current best solution, we update Best. One exception to this rule is the tuple t . Since this tuple can be taken fractionally, we update Best if the sum of S"s cost and fractional cost of t is an improvement.
The algorithm terminates in either of the first two cases, or when all tuples are scanned. In particular, it terminates whenever we find a tk i such that size(tk i ) is greater than R but cost(tk i ) is less than cost(S), or when we reach the tuple representing agent l and it gives a feasible solution.
LEMMA 2. Suppose A∗ is an optimal solution of the generalized knapsack, and suppose that element (l, j) is midrange in the optimal solution. Then, the cost V (l, j), returned by Greedy( , j), satisfies: V ( , j) + cost(tj ) ≤ 2cost(A∗ ) PROOF. Let V ( , j) be the value returned by Greedy( , j) and let V ∗ ( , j) be an optimal solution for iKnapsack( , j). Consider the set Skip at the termination of Greedy( , j). There are two cases to consider: either some tuple t ∈ Skip is also in V ∗ ( , j), or no tuple in Skip is in V ∗ ( , j). In the first case, let St be the tentative solution S at the time t was added to Skip. Because t ∈ Skip then size(t) > R, and St together with t forms a feasible solution, and we have: V ( , j) ≤ cost(Best) ≤ cost(St) + cost(t).
Again, because t ∈ Skip then cost(t) > cost(St), and we have V ( , j) < 2cost(t). On the other hand, since t is included in V ∗ ( , j), we have V ∗ ( , j) ≥ cost(t). These two inequalities imply the desired bound: V ∗ ( , j) ≤ V ( , j) < 2V ∗ ( , j).
In the second case, imagine a modified instance of iKnapsack( , j), which excludes all the tuples of the set Skip. Since none of these tuples were included in V ∗ ( , j), the optimal solution for the modified problem should be the same as the one for the original. Suppose our approximation algorithm returns the value V ( , j) for this modified instance. Let t be the last tuple considered by the approximation algorithm before termination on the modified instance, and let St be the corresponding tentative solution set in that step. Since we consider tuples in order of increasing per unit price, and none of the tuples are going to be placed in the set Skip, we must have cost(St ) < V ∗ ( , j) because St is the optimal way to obtain size(St ).
We also have cost(t ) ≤ cost(St ), and the following inequalities: V ( , j) ≤ V ( , j) ≤ cost(St ) + cost(t ) < 2V ∗ ( , j) The inequality V ( , j) ≤ V ( , j) follows from the fact that a tuple in the Skip list can only affect the Best but not the tentative solutions. Therefore, dropping the tuples in the set Skip can only make the solution worse.
The above argument has shown that the value returned by Greedy( , j) is within a factor 2 of the optimal solution for iKnapsack( , j).
We now show that the value V ( , j) plus cost(tj ) is a 2-approximation of the original generalized knapsack problem.
Let A∗ be an optimal solution of the generalized knapsack, and suppose that element xj is midrange. Let x− to be set of the remaining elements, either zero or anchors, in this solution.
Furthermore, define x = xj − uj . Thus, cost(A∗ ) = cost(xl) + cost(tj l ) + cost(x−l) It is easy to see that (x− , x ) is an optimal solution for iKnapsack( , j).
Since V ( , j) is a 2-approximation for this optimal solution, we have the following inequalities: V ( , j) + cost(tj ) ≤ cost(tj ) + 2(cost(x ) + cost(x− )) ≤ 2(cost(x ) + cost(tj ) + cost(x− )) ≤ 2cost(A∗ ) This completes the proof of Lemma 2.
It is easy to see that, after an initial sorting of the tuples in U, the algorithm Greedy( , j) takes O(n) time. We have our first polynomial approximation algorithm.
THEOREM 2. A 2-approximation of the generalized knapsack problem can be found in time O(n2 ), where n is number of item lists (each of constant length).
PROOF. We run the algorithm Greedy( , j) once for each tuple (l, j) as a candidate for midrange. There are O(n) tuples, and it suffices to sort them once, the total cost of the algorithm is O(n2 ). By Lemma 1, there is an optimal solution with at most one midrange element, so our algorithm will find a 2-approximation, as claimed.
The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time is O((nc)2 ). 171
We now use the 2-approximation algorithm presented in the preceding section to develop a fully polynomial approximation (FPTAS) for the generalized knapsack problem. The high level idea is fairly standard, but the details require technical care. We use a dynamic programming algorithm to solve iKnapsack( , j) for each possible midrange element, with the 2-approximation algorithm providing an upper bound on the value of the solution and enabling the use of scaling on the cost dimension of the dynamic programming (DP) table.
Consider, for example, the case that the midrange element is x , which falls in the range [uj , uj+1 ). In our FPTAS, rather than using a greedy approximation algorithm to solve iKnapsack( , j), we construct a dynamic programming table to compute the minimum cost at which at least M − uj+1 units can be obtained using the remaining n − 1 lists in the generalized knapsack.
Suppose G[i, r] denotes the maximum number of units that can be obtained at cost at most r using only the first i lists in the generalized knapsack. Then, the following recurrence relation describes how to construct the dynamic programming table: G[0, r] = 0 G[i, r] = max ´ G[i − 1, r] max j∈β(i,r) {G[i − 1, r − cost(tj i )] + uj i } µ where β(i, r) = {j : 1 ≤ j ≤ mi, cost(tj i ) ≤ r}, is the set of anchors for agent i. As convention, agent i will index the row, and cost r will index the column.
This dynamic programming algorithm is only pseudo-polynomial, since the number of column in the dynamic programming table depends upon the total cost. However, we can convert it into a FPTAS by scaling the cost dimension.
Let A denote the 2-approximation to the generalized knapsack problem, with total cost, cost(A). Let ε denote the desired approximation factor. We compute the scaled cost of a tuple tj i , denoted scost(tj i ), as scost(tj i ) = n cost(tj i ) εcost(A) (2) This scaling improves the running time of the algorithm because the number of columns in the modified table is at most n ε , and independent of the total cost. However, the computed solution might not be an optimal solution for the original problem. We show that the error introduced is within a factor of ε of the optimal solution.
As a prelude to our approximation guarantee, we first show that if two different solutions to the iKnapsack problem have equal scaled cost, then their original (unscaled) costs cannot differ by more than εcost(A).
LEMMA 3. Let x and y be two distinct feasible solutions of iKnapsack( , j), excluding their midrange elements. If x and y have equal scaled costs, then their unscaled costs cannot differ by more than εcost(A).
PROOF. Let Ix and Iy, respectively, denote the indicator functions associated with the anchor vectors x and y-there is 1 in position Ix[i, k] if the xk i > 0. Since x and y has equal scaled cost, i= k scost(tk i )Ix[i, k] = i= k scost(tk i )Iy[i, k] (3) However, by (2), the scaled costs satisfy the following inequalities: (scost(tk i ) − 1)εcost(A) n ≤ cost(tk i ) ≤ scost(tk i )εcost(A) n (4) Substituting the upper-bound on scaled cost from (4) for cost(x), the lower-bound on scaled cost from (4) for cost(y), and using equality (3) to simplify, we have: cost(x) − cost(y) ≤ εcost(A) n i= k Iy[i, k] ≤ εcost(A),
The last inequality uses the fact that at most n components of an indicator vector are non-zero; that is, any feasible solution contains at most n tuples.
Finally, given the dynamic programming table for iKnapsack( , j), we consider all the entries in the last row of this table, G[n−1, r].
These entries correspond to optimal solutions with all agents except l, for different levels of cost. In particular, we consider the entries that provide at least M − uj+1 units. Together with a contribution from agent l, we choose the entry in this set that minimizes the total cost, defined as follows: cost(G[n − 1, r]) + max {uj , M − G[n − 1, r]}pj , where cost() is the original, unscaled cost associated with entry G[n−1, r]. It is worth noting, that unlike the 2-approximation scheme for iKnapsack( , j), the value computed with this FPTAS includes the cost to acquire uj l units from l.
The following lemma shows that we achieve a (1+ε)-approximation.
LEMMA 4. Suppose A∗ is an optimal solution of the generalized knapsack problem, and suppose that element (l, j) is midrange in the optimal solution. Then, the solution A(l, j) from running the scaled dynamic-programming algorithm on iKnapsack( , j) satisfies cost(A(l, j)) ≤ (1 + 2ε)cost(A∗ ) PROOF. Let x− denote the vector of the elements in solution A∗ without element l. Then, by definition, cost(A∗ ) = cost(x− ) + pj xj . Let r = scost(x− ) be the scaled cost associated with the vector x− . Now consider the dynamic programming table constructed for iKnapsack( , j), and consider its entry G[n − 1, r]. Let A denote the 2-approximation to the generalized knapsack problem, and A(l, j) denote the solution from the dynamic-programming algorithm.
Suppose y− is the solution associated with this entry in our dynamic program; the components of the vector y− are the quantities from different lists. Since both x− and y− have equal scaled costs, by Lemma 3, their unscaled costs are within εcost(A) of each other; that is, cost(y− ) − cost(x− ) ≤ εcost(A).
Now, define yj = max{uj , M − Èi= Èj yj i }; this is the contribution needed from to make (y− , yj ) a feasible solution.
Among all the equal cost solutions, our dynamic programming tables chooses the one with maximum units. Therefore, i= j yj i ≥ i= j xj i 172 Therefore, it must be the case that yj ≤ xj . Because (yj , y− ) is also a feasible solution, if our algorithm returns a solution with cost cost(A(l, j)), then we must have cost(A(l, j)) ≤ cost(y− ) + pj yj ≤ cost(x− ) + εcost(A) + pj xj ≤ (1 + 2ε)cost(A∗ ), where we use the fact that cost(A) ≤ 2cost(A∗ ).
Putting this together, our approximation scheme for the generalized knapsack problem will iterate the scheme described above for each choice of the midrange element (l, j), and choose the best solution from among these O(n) solutions.
For a given midrange, the most expensive step in the algorithm is the construction of dynamic programming table, which can be done in O(n2 /ε) time assuming constant intervals per list. Thus, we have the following result.
THEOREM 3. We can compute an (1 + ε) approximation to the solution of a generalized knapsack problem in worst-case time O(n3 /ε).
The dependence on the number of pieces is also polynomial: if each bid has a maximum of c pieces, then the running time can be derived by substituting cn for each occurrence of n.
We now consider the related problem of computing the VCG payments for all the agents. A naive approach requires solving the allocation problem n times, removing each agent in turn. In this section, we show that our approximation scheme for the generalized knapsack can be extended to determine all n payments in total time O(αT log(αn/ε)), where 1 ≤ C(I\i)/C(I) ≤ α, for a constant upper bound, α, and T is the complexity of solving the allocation problem once. This α-bound can be justified as a no monopoly condition, because it bounds the marginal value that a single buyer brings to the auction. Similarly, in the reverse variation we can compute the VCG payments to each seller in time O(αT log(αn/ε)), where α bounds the ratio C(I\ i)/C(I) for all i.
Our overall strategy will be to build two dynamic programming tables, forward and backward, for each midrange element (l, j) once. The forward table is built by considering the agents in the order of their indices, where as the backward table is built by considering them in the reverse order. The optimal solution corresponding to C(I \ i) can be broken into two parts: one corresponding to first (i − 1) agents and the other corresponding to last (n − i) agents. As the (i − 1)th row of the forward table corresponds to the sellers with first (i−1) indices, an approximation to the first part will be contained in (i − 1)th row of the forward table. Similarly, (n− i)th row of the backward table will contain an approximation for the second part. We first present a simple but an inefficient way of computing the approximate value of C(I \ i), which illustrates the main idea of our algorithm. Then we present an improved scheme, which uses the fact that the elements in the rows are sorted, to compute the approximate value more efficiently.
In the following, we concentrate on computing an allocation with xj being midrange, and some agent i = l removed. This will be a component in computing an approximation to C(I \ i), the value of the solution to the generalized knapsack without bids from agent i. We begin with the simple scheme.
We implement the scaled dynamic programming algorithm for iKnapsack( , j) with two alternate orderings over the other sellers, k = l, one with sellers ordered 1, 2, . . . , n, and one with sellers ordered n, n − 1, . . . , 1. We call the first table the forward table, and denote it F , and the second table the backward table, and denote it Bl. The subscript reminds us that the agent is midrange.9 In building these tables, we use the same scaling factor as before; namely, the cost of a tuple tj i is scaled as follows: scost(tj i ) = ncost(tj i ) εcost(A) where cost(A) is the upper bound on C(I), given by our 2approximation scheme. In this case, because C(I \ i) can be α times C(I), the scaled value of C(I \ i) can be at most nα/ε.
Therefore, the cost dimension of our dynamic program"s table will be nα/ε.
FlTable F (i−1)l 2 3 1 2 i−1 1 m−1 m n−1 g 2 31 m−1 m B (n−i) n−1 n−2 n−i 1 lh Table Bl Figure 3: Computing VCG payments. m = nα ε Now, suppose we want to compute a (1 + )-approximation to the generalized knapsack problem restricted to element (l, j) midrange, and further restricted to remove bids from some seller i = l. Call this problem iKnapsack−i ( , j).
Recall that the ith row of our DP table stores the best solution possible using only the first i agents excluding agent l, all of them either cleared at zero, or on anchors. These first i agents are a different subset of agents in the forward and the backward tables. By carefully combining one row of Fl with one row of Bl we can compute an approximation to iKnapsack−i ( , j). We consider the row of Fl that corresponds to solutions constructed from agents {1, 2, . . . , i − 1}, skipping agent l. We consider the row of Bl that corresponds to solutions constructed from agents {i+1, i+2, . . . , n}, again skipping agent l. The rows are labeled Fl(i − 1) and Bl(n − i) respectively.10 The scaled costs for acquiring these units are the column indices for these entries. To solve iKnapsack−i ( , j) we choose one entry from row F (i−1) and one from row B (n−i) such that their total quantity exceeds M − uj+1 and their combined cost is minimum over all such combinations. Formally, let g ∈ Fl(i − 1), and h ∈ Bl(n − 1) denote entries in each row, with size(g), size(h), denoting the number of units and cost(g) and cost(h) denoting the unscaled cost associated with the entry. We compute the following, subject 9 We could label the tables with both and j, to indicate the jth tuple is forced to be midrange, but omit j to avoid clutter. 10 To be precise, the index of the rows are (i − 2) and (n − i) for Fl and Bl when l < i, and (i − 1) and (n − i − 1), respectively, when l > i. 173 to the condition that g and h satisfy size(g) + size(h) > M − uj+1 : min g∈F (i−1),h∈B (n−i) Òcost(g) + cost(h) + pj · max{uj , M − size(g) − size(h)} Ó (5) LEMMA 5. Suppose A−i is an optimal solution of the generalized knapsack problem without bids from agent i, and suppose that element (l, j) is the midrange element in the optimal solution. Then, the expression in Eq. 5, for the restricted problem iKnapsack−i ( , j), computes a (1 + ε)-approximation to A−i .
PROOF. From earlier, we define cost(A−i ) = C(I \ i). We can split the optimal solution, A−i , into three disjoint parts: xl corresponds to the midrange seller, xi corresponds to first i − 1 sellers (skipping agent l if l < i), and x−i corresponds to last n − i sellers (skipping agent l if l > i). We have: cost(A−i ) = cost(xi) + cost(x−i) + pj xj Let ri = scost(xi) and r−i = scost(x−i). Let yi and y−i be the solution vectors corresponding to scaled cost ri and r−i in F (i − 1) and B (n − i), respectively. From Lemma 3 we conclude that, cost(yi) + cost(y−i) − cost(xi) − cost(x−i) ≤ εcost(A) where cost(A) is the upper-bound on C(I) computed with the 2-approximation.
Among all equal scaled cost solutions, our dynamic program chooses the one with maximum units. Therefore we also have, (size(yi) ≥ size(xi)) and (size(y−i) ≥ size(x−i)) where we use shorthand size(x) to denote total number of units in all tuples in x.
Now, define yj l = max(uj l , M −size(yi)−size(y−i)). From the preceding inequalities, we have yj l ≤ xj l . Since (yj l , yi, y−i) is also a feasible solution to the generalized knapsack problem without agent i, the value returned by Eq. 5 is at most cost(yi) + cost(y−i) + pj l yj l ≤ C(I \ i) + εcost(A) ≤ C(I \ i) + 2cost(A∗ )ε ≤ C(I \ i) + 2C(I \ i)ε This completes the proof.
A naive implementation of this scheme will be inefficient because it might check (nα/ε)2 pairs of elements, for any particular choice of (l, j) and choice of dropped agent i. In the next section, we present an efficient way to compute Eq. 5, and eventually to compute the VCG payments.
Our improved approximation scheme for the winner-determination problem without agent i uses the fact that elements in F (i − 1) and B (n − i) are sorted; specifically, both, unscaled cost and quantity (i.e. size), increases from left to right. As before, let g and h denote generic entries in F (i − 1) and B (n − i) respectively. To compute Eq. 5, we consider all the tuple pairs, and first divide the tuples that satisfy condition size(g) + size(h) > M − uj+1 l into two disjoint sets. For each set we compute the best solution, and then take the best between the two sets. [case I: size(g) + size(h) ≥ M − uj l ] The problem reduces to min g∈F (i−1), h∈B (n−i) Òcost(g) + cost(h) + pj l uj Ó (6) We define a pair (g, h) to be feasible if size(g) + size(h) ≥ M − uj l . Now to compute Eq. 6, we do a forward and backward walk on F (i − 1) and B (n − i) respectively. We start from the smallest index of F (i − 1) and move right, and from the highest index of B (n − i) and move left. Let (g, h) be the current pair. If (g, h) is feasible, we decrement B"s pointer (that is, move backward) otherwise we increment F"s pointer. The feasible pairs found during the walk are used to compute Eq. 6.
The complexity of this step is linear in size of F (i − 1), which is O(nα/ε). [case II: M − uj+1 l ≤ size(g) + size(h) ≤ M − uj l ] The problem reduces to min g∈F (i−1), h∈B (n−i) Òcost(g) + cost(h) + pj l (M − size(g) − size(h)) Ó To compute the above equation, we transform the above problem to another problem using modified cost, which is defined as: mcost(g) = cost(g) − pj l · size(g) mcost(h) = cost(h) − pj l · size(h) The new problem is to compute min g∈F (i−1), h∈B (n−i) Òmcost(g) + mcost(h) + pj l M Ó (7) The modified cost simplifies the problem, but unfortunately the elements in F (i − 1) and B (n − i) are no longer sorted with respect to mcost. However, the elements are still sorted in quantity and we use this property to compute Eq. 7. Call a pair (g, h) feasible if M − uj+1 l ≤ size(g) + size(h) ≤ M − uj l .
Define the feasible set of g as the elements h ∈ B (n − i) that are feasible given g. As the elements are sorted by quantity, the feasible set of g is a contiguous subset of B (n − i) and shifts left as g increases. 2 3 4 5 10 20 30 40 50 60 Begin End B (n−i)15 20 25 30 35 40 65421 3 1 6 F (i−1)l l Figure 4: The feasible set of g = 3, defined on B (n − i), is {2, 3, 4} when M − uj+1 l = 50 and M − uj l = 60. Begin and End represent the start and end pointers to the feasible set.
Therefore, we can compute Eq. 7 by doing a forward and backward walk on F (i − 1) and B (n − i) respectively. We walk on B (n − i), starting from the highest index, using two pointers,
Begin and End, to indicate the start and end of the current feasible set. We maintain the feasible set as a min heap, where the key is modified cost. To update the feasible set, when we increment F"s pointer(move forward), we walk left on B, first using End to remove elements from feasible set which are no longer 174 feasible and then using Begin to add new feasible elements. For a given g, the only element which we need to consider in g"s feasible set is the one with minimum modified cost which can be computed in constant time with the min heap. So, the main complexity of the computation lies in heap updates. Since, any element is added or deleted at most once, there are O(nα ε ) heap updates and the time complexity of this step is O(nα ε log nα ε ).
The algorithm works as follows. First, using the 2 approximation algorithm, we compute an upper bound on C(I). We use this bound to scale down the tuple costs. Using the scaled costs, we build the forward and backward tables corresponding to each tuple (l, j). The forward tables are used to compute C(I). To compute C(I \ i), we iterate over all the possible midrange tuples and use the corresponding forward and backward tables to compute the locally optimal solution using the above scheme.
Among all the locally optimal solutions we choose one with the minimum total cost.
The most expensive step in the algorithm is computation of C(I \ i). The time complexity of this step is O(n2 α ε log nα ε ) as we have to iterate over all O(n) choices of tj l , for all l = i, and each time use the above scheme to compute Eq. 5. In the worst case, we might need to compute C(I \ i) for all n sellers, in which case the final complexity of the algorithm will be O(n3 α ε log nα ε ).
THEOREM 4. We can compute an /(1+ )-strategyproof approximation to the VCG mechanism in the forward and reverse multi-unit auctions in worst-case time O(n3 α ε log nα ε ).
It is interesting to recall that T = O(n3 ε ) is the time complexity of the FPTAS to the generalized knapsack problem with all agents. Our combined scheme computes an approximation to the complete VCG mechanism, including payments to O(n) agents, in time complexity O(T log(n/ε)), taking the no-monopoly parameter, α, as a constant. Thus, our algorithm performs much better than the naive scheme, which computes the VCG payment for each agent by solving a new instance of generalized knapsack problem. The speed up comes from the way we solve iKnapsack−i ( , j). Time complexity of computing iKnapsack−i ( , j) by creating a new dynamic programming table will be O(n2 ε ) but by using the forward and backward tables, the complexity is reduced to O(n ε log n ε ). We can further improve the time complexity of our algorithm by computing Eq. 5 more efficiently.
Currently, the algorithm uses heap, which has logarithmic update time. In worst case, we can have two heap update operations for each element, which makes the time complexity super linear.
If we can compute Eq. 5 in linear time then the complexity of computing the VCG payment will be same as the complexity of solving a single generalized knapsack problem.
We presented a fully polynomial-time approximation scheme for the single-good multi-unit auction problem, using marginal decreasing piecewise constant bidding language. Our scheme is both approximately efficient and approximately strategyproof within any specified factor ε > 0. As such it is an example of computationally tractable ε-dominance result, as well as an example of a non-trivial but approximable allocation problem. It is particularly interesting that we are able to compute the payments to n agents in a VCG-based mechanism in worst-case time O(T log n), where T is the time complexity to compute the solution to a single allocation problem.
[1] L M Ausubel and P R Milgrom. Ascending auctions with package bidding. Frontiers of Theoretical Economics, 1:1-42, 2002. [2] S Bikchandani, S de Vries, J Schummer, and R V Vohra. Linear programming and Vickrey auctions. Technical report, Anderson Graduate School of Management, U.C.L.A., 2001. [3] S Bikchandani and J M Ostroy. The package assignment model.
Journal of Economic Theory, 2002. Forthcoming. [4] K Chatterjee and W Samuelson. Bargaining under incomplete information. Operations Research, 31:835-851, 1983. [5] E H Clarke. Multipart pricing of public goods. Public Choice, 11:17-33, 1971. [6] S de Vries and R V Vohra. Combinatorial auctions: A survey.
Informs Journal on Computing, 2002. Forthcoming. [7] M Eso, S Ghosh, J R Kalagnanam, and L Ladanyi. Bid evaluation in procurement auctions with piece-wise linear supply curves.
Technical report, IBM TJ Watson Research Center, 2001. in preparation. [8] J Feigenbaum and S Shenker. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions. In Proceedings of the 6th International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications, pages 1-13,

Online search is now ubiquitous and Internet search companies such as Google, Yahoo! and MSN let companies and individuals advertise based on search queries posed by users.
Conventional media outlets, such as TV stations or newspapers, price their ad slots individually, and the advertisers buy the ones they can afford. In contrast, Internet search companies find it difficult to set a price explicitly for the advertisements they place in response to user queries. This difficulty arises because supply (and demand) varies widely and unpredictably across the user queries, and they must price slots for billions of such queries in real time. Thus, they rely on the market to determine suitable prices by using auctions amongst the advertisers. It is a challenging problem to set up the auction in order to effect a stable market in which all the parties (the advertisers, users as well as the Internet search company) are adequately satisfied. Recently there has been systematic study of the issues involved in the game theory of the auctions [5, 1, 2], revenue maximization [10], etc.
The perspective in this paper is not of the Internet search company that displays the advertisements, but rather of the advertisers. The challenge from an advertiser"s point of view is to understand and interact with the auction mechanism.
The advertiser determines a set of keywords of their interest and then must create ads, set the bids for each keyword, and provide a total (often daily) budget. When a user poses a search query, the Internet search company determines the advertisers whose keywords match the query and who still have budget left over, runs an auction amongst them, and presents the set of ads corresponding to the advertisers who win the auction. The advertiser whose ad appears pays the Internet search company if the user clicks on the ad.
The focus in this paper is on how the advertisers bid.
For the particular choice of keywords of their interest1 , an advertiser wants to optimize the overall effect of the advertising campaign. While the effect of an ad campaign in any medium is a complicated phenomenon to quantify, one commonly accepted (and easily quantified) notion in searchbased advertising on the Internet is to maximize the number of clicks. The Internet search companies are supportive to1 The choice of keywords is related to the domain-knowledge of the advertiser, user behavior and strategic considerations.
Internet search companies provide the advertisers with summaries of the query traffic which is useful for them to optimize their keyword choices interactively. We do not directly address the choice of keywords in this paper, which is addressed elsewhere [13]. 40 wards advertisers and provide statistics about the history of click volumes and prediction about the future performance of various keywords. Still, this is a complex problem for the following reasons (among others): • Individual keywords have significantly different characteristics from each other; e.g., while fishing is a broad keyword that matches many user queries and has many competing advertisers, humane fishing bait is a niche keyword that matches only a few queries, but might have less competition. • There are complex interactions between keywords because a user query may match two or more keywords, since the advertiser is trying to cover all the possible keywords in some domain. In effect the advertiser ends up competing with herself.
As a result, the advertisers face a challenging optimization problem. The focus of this paper is to solve this optimization problem.
We present a short discussion and formulation of the optimization problem faced by advertisers; a more detailed description is in Section 2.
A given advertiser sees the state of the auctions for searchbased advertising as follows. There is a set K of keywords of interest; in practice, even small advertisers typically have a large set K. There is a set Q of queries posed by the users. For each query q ∈ Q, there are functions giving the clicksq(b) and costq(b) that result from bidding a particular amount b in the auction for that query, which we model more formally in the next section. There is a bipartite graph G on the two vertex sets representing K and Q. For any query q ∈ Q, the neighbors of q in K are the keywords that are said to match the query q.2 The budget optimization problem is as follows. Given graph G together with the functions clicksq(·) and costq(·) on the queries, as well as a budget U, determine the bids bk for each keyword k ∈ K such that P q clicksq(bq) is maximized subject to P q costq(bq) ≤ U, where the effective bid bq on a query is some function of the keyword bids in the neighborhood of q.
While we can cast this problem as a traditional optimization problem, there are different challenges in practice depending on the advertiser"s access to the query and graph information, and indeed the reliability of this information (e.g., it could be based on unstable historical data). Thus it is important to find solutions to this problem that not only get many clicks, but are also simple, robust and less reliant on the information. In this paper we define the notion of a uniform strategy which is essentially a strategy that bids uniformly on all keywords. Since this type of strategy obviates the need to know anything about the particulars of the graph, and effectively aggregates the click and cost functions on the queries, it is quite robust, and thus desirable in practice. What is surprising is that uniform strategy actually performs well, which we will prove. 2 The particulars of the matching rule are determined by the Internet search company; here we treat the function as arbitrary.
We present positive and negative results for the budget optimization problem. In particular, we show: • Nearly all formulations of the problem are NP-Hard. In cases slightly more general than the formulation above, where the clicks have weights, the problem is inapproximable better than a factor of 1 − 1 e , unless P=NP. • We give a (1−1/e)-approximation algorithm for the budget optimization problem. The strategy found by the algorithm is a two-bid uniform strategy, which means that it randomizes between bidding some value b1 on all keywords, and bidding some other value b2 on all keywords until the budget is exhausted3 . We show that this approximation ratio is tight for uniform strategies. We also give a (1/2)-approximation algorithm that offers a single-bid uniform strategy, only using one value b1. (This is tight for single-bid uniform strategies.) These strategies can be computed in time nearly linear in |Q| + |K|, the input size.
Uniform strategies may appear to be naive in first consideration because the keywords vary significantly in their click and cost functions, and there may be complex interaction between them when multiple keywords are relevant to a query. After all, the optimum can configure arbitrary bids on each of the keywords. Even for the simple case when the graph is a matching, the optimal algorithm involves placing different bids on different keywords via a knapsack-like packing (Section 2). So, it might be surprising that a simple two-bid uniform strategy is 63% or more effective compared to the optimum. In fact, our proof is stronger, showing that this strategy is 63% effective against a strictly more powerful adversary who can bid independently on the individual queries, i.e., not be constrained by the interaction imposed by the graph G.
Our proof of the 1 − 1/e approximation ratio relies on an adversarial analysis. We define a factor-revealing LP (Section 4) where primal solutions correspond to possible instances, and dual solutions correspond to distributions over bidding strategies. By deriving the optimal solution to this LP, we obtain both the proof of the approximation ratio, and a tight worst-case instance.
We have conducted simulations using real auction data from Google. The results of these simulations, which are highlighted at the end of Section 4, suggest that uniform bidding strategies could be useful in practice. However, important questions remain about (among other things) alternate bidding goals, on-line or stochastic bidding models [11], and game-theoretic concerns [3], which we briefly discuss in Section 8.
We describe an auction from an advertiser"s point of view.
An advertiser bids on a keyword, which we can think of as a word or set of words. Users of the search engine submit queries. If the query matches a keyword that has been bid on by an advertiser, then the advertiser is entered into an auction for the available ad slots on the results page.
What constitutes a match varies depending on the search engine. 3 This type of strategy can also be interpreted as bidding one value (on all keywords) for part of the day, and a different value for the rest of the day. 41 An advertiser makes a single bid for a keyword that remains in effect for a period of time, say one day. The keyword could match many different user queries throughout the day. Each user query might have a different set of advertisers competing for clicks. The advertiser could also bid different amounts on multiple keywords, each matching a (possibly overlapping) set of user queries.
The ultimate goal of an advertiser is to maximize traffic to their website, given a certain advertising budget. We now formalize a model of keyword bidding and define an optimization problem that captures this goal.
We begin by considering the case of a single keyword that matches a single user query. In this section we define the notion of a query landscape that describes the relationship between the advertiser"s bid and what will happen on this query as a result of this bid[9]. This definition will be central to the discussion as we continue to more general cases.
The search results page for a query contains p possible positions in which our ad can appear. We denote the highest (most favorable) position by 1 and lowest by p.
Associated with each position i is a value α[i] that denotes the click-through rate (ctr) of the ad in position i. The ctr is a measure of how likely it is that our ad will receive a click if placed in position i. The ctr can be measured empirically using past history. We assume throughout this work that that α[i] ≤ α[j] if j < i, that is, higher positions receive at least as many clicks as lower positions.
In order to place an ad on this page, we must enter the auction that is carried out among all advertisers that have submitted a bid on a keyword that matches the user"s query.
We will refer to such an auction as a query auction, to emphasize that there is an auction for each query rather than for each keyword. We assume that the auction is a generalized second price (GSP) auction [5, 7]: the advertisers are ranked in decreasing order of bid, and each advertiser is assigned a price equal to the amount bid by the advertiser below them in the ranking.4 In sponsored search auctions, this advertiser pays only if the user actually clicks on the ad.
Let (b[1], . . . , b[p]) denote the bids of the top p advertisers in this query auction. For notational convenience, we assume that b[0] = ∞ and b[p] = α[p] = 0. Since the auction is a generalized second price auction, higher bids win higher positions; i.e. b[i] ≥ b[i + 1]. Suppose that we bid b on some keyword that matches the user"s query, then our position is defined by the largest b[i] that is at most b, that is, pos(b) = arg max i (b[i] : b[i] ≤ b). (1) Since we only pay if the user clicks (and that happens with probability α[i]), our expected cost for winning position i 4 Google, Yahoo! and MSN all use some variant of the GSP auction. In the Google auction, the advertisers" bids are multiplied by a quality score before they are ranked; our results carry over to this case as well, which we omit from this paper for clarity. Also, other auctions besides GSP have been considered; e.g., the Vickrey Clark Groves (VCG) auction [14, 4, 7]. Each auction mechanism will result in a different sort of optimization problem. In the conclusion we point out that for the VCG auction, the bidding optimization problem becomes quite easy. would be cost[i] = α[i] · b[i], where i = pos(b). We use costq(b) and clicksq(b) to denote the expected cost and clicks that result from having a bid b that qualifies for a query auction q, and thus costq(b) = α[i] · b[i] where i = pos(b), (2) clicksq(b) = α[i] where i = pos(b). (3) The following observations about cost and clicks follow immediately from the definitions and equations (1), (2) and (3).
We use R+ to denote the nonnegative reals.
Observation 1. For b ∈ R+,
set of values Vq = {(cost[1], α[1]), . . . , (cost[p], α[p])}.
functions of b. Also, cost-per-click (cpc) costq(b)/clicksq(b) is non-decreasing in b.
For bids (b[1], . . . , b[p]) that correspond to the bids of other advertisers, we have: costq(b[i])/clicksq(b[i]) = b[i], i ∈ [p]. When the context is clear, we drop the subscript q.
We can summarize the data contained in the functions cost(b) and clicks(b) as a collection of points in a plot of cost vs. clicks, which we refer to as a landscape. For example, for a query with four slots, a landscape might look like Table 1. bid range cost per click cost clicks [$2.60,∞) $2.60 $1.30 .5 [$2.00,$2.60) $2.00 $0.90 .45 [$1.60,$2.00) $1.60 $0.40 .25 [$0.50,$1.60) $0.50 $0.10 .2 [$0,$0.50) $0 $0 0 Table 1: A landscape for a query It is convenient to represent this data graphically as in Figure 1 (ignore the dashed line for now). Here we graph clicks as a function of cost. Observe that in this graph, the cpc (cost(b)/clicks(b)) of each point is the reciprocal of the slope of the line from the origin to the point. Since cost(b), clicks(b) and cost(b)/clicks(b) are non-decreasing, the slope of the line from the origin to successive points on the plot decreases. This condition is slightly weaker than concavity.
Suppose we would like to solve the budget optimization problem for a single query landscape.5 As we increase our bid from zero, our cost increases and our expected number of clicks increases, and so we simply submit the highest bid such that we remain within our budget.
One problem we see right away is that since there are only a finite set of points in this landscape, we may not be able to target arbitrary budgets efficiently. Suppose in the example from Table 1 and Figure 1 that we had a budget 5 Of course it is a bit unrealistic to imagine that an advertiser would have to worry about a budget if only one user query was being considered; however one could imagine multiple instances of the same query and the problem scales. 42 $0.50 $1.00 $1.50 .1 .2 .3 .4 .5 Clicks Cost Figure 1: A bid landscape. of $1.00. Bidding between $2.00 and $2.60 uses only $0.90, and so we are under-spending. Bidding more than $2.60 is not an option, since we would then incur a cost of $1.30 and overspend our budget.
To rectify this problem and better utilize our available budget, we allow randomized bidding strategies. Let B be a distribution on bids b ∈ R+. Now we define cost(B) = Eb∼B[cost(b)] and clicks(B) = Eb∼B[clicks(b)]. Graphically, the possible values of (cost(B), clicks(B)) lie in the convex hull of the landscape points. This is represented in Figure 1 by the dashed line.
To find a bid distribution B that maximizes clicks subject to a budget, we simply draw a vertical line on the plot where the cost is equal to the budget, and find the highest point on this line in the convex hull. This point will always be the convex combination of at most two original landscape points which themselves lie on the convex hull. Thus, given the point on the convex hull, it is easy to compute a distribution on two bids which led to this point. Summarizing,
Lemma 1. If an advertiser is bidding on one keyword, subject to a budget U, then the optimal strategy is to pick a convex combination of (at most) two bids which are at the endpoints of the line on the convex hull at the highest point for cost U.
There is one subtlety in this formulation. Given any bidding strategy, randomized or otherwise, the resulting cost is itself a random variable representing the expected cost.
Thus if our budget constraint is a hard budget, we have to deal with the difficulties that arise if our strategy would be over budget. Therefore, we think of our budget constraint as soft, that is, we only require that our expected cost be less than the budget. In practice, the budget is often an average daily budget, and thus we don"t worry if we exceed it one day, as long as we are meeting the budget in expectation.
Further, either the advertiser or the search engine (possibly both), monitor the cost incurred over the day; hence, the advertiser"s bid can be changed to zero for part of the day, so that the budget is not overspent.6 Thus in the remain6 See https://adwords.google.com/support/bin/answer. py?answer=22183, for example. der of this paper, we will formulate a budget constraint that only needs to be respected in expectation.
As a warm-up, we will consider next the case when we have a set of queries, each which its own landscape. We want to bid on each query independently subject to our budget: the resulting optimization problem is a small generalization of the fractional knapsack problem, and was solved in [9].
The first step of the algorithm is to take the convex hull of each landscape, as in Figure 1, and remove any landscape points not on the convex hull. Each piecewise linear section of the curve represents the incremental number of clicks and cost incurred by moving one"s bid from one particular value to another. We regard these pieces as items in an instance of fractional knapsack with value equal to the incremental number of clicks and size equal to the incremental cost. More precisely, for each piece connecting two consecutive bids b and b on the convex hull, we create a knapsack item with value [clicks(b ) − clicks(b )] and size [cost(b ) − cost(b )]. We then emulate the greedy algorithm for knapsack, sorting by value/size (cost-per-click), and choosing greedily until the budget is exhausted.
In this reduction to knapsack we have ignored the fact that some of the pieces come from the same landscape and cannot be treated independently. However, since each curve is concave, the pieces that come from a particular query curve are in increasing order of cost-per-click; thus from each landscape we have chosen for our knapsack a set of pieces that form a prefix of the curve.
In reality, search advertisers can bid on a large set of keywords, each of them qualifying for a different (possibly overlapping) set of queries, but most search engines do not allow an advertiser to appear twice in the same search results page.7 Thus, if an advertiser has a bid on two different keywords that match the same query, this conflict must be resolved somehow. For example, if an advertiser has a bid out on the keywords shoes and high-heel, then if a user issues the query high-heel shoes, it will match on two different keywords. The search engine specifies, in advance, a rule for resolution based on the query the keyword and the bid. A natural rule is to take the keyword with the highest bid, which we adopt here, but our results apply to other resolution rules.
We model the keyword interaction problem using an undirected bipartite graph G = (K ∪ Q, E) where K is a set of keywords and Q is a set of queries. Each q ∈ Q has an associated landscape, as defined by costq(b) and clicksq(b). An edge (k, q) ∈ E means that keyword k matches query q.
The advertiser can control their individual keyword bid vector a ∈ R |K| + specifying a bid ak for each keyword k ∈ K. (For now, we do not consider randomized bids, but we will introduce that shortly.) Given a particular bid vector a on the keywords, we use the resolution rule of taking the maximum to define the effective bid on query q as bq(a) = max k:(k,q)∈E ak.
By submitting a bid vector a, the advertiser receives some 7 See https://adwords.google.com/support/bin/answer. py?answer=14179, for example. 43 number of clicks and pays some cost on each keyword. We use the term spend to denote the total cost; similarly, we use the term traffic to denote the total number of clicks: spend(a)= X q∈Q costq(bq(a)); traffic(a)= X q∈Q clicksq(bq(a)) We also allow randomized strategies, where an advertiser gives a distribution A over bid vectors a ∈ R |K| + . The resulting spend and traffic are given by spend(A)=Ea∼A[spend(a)]; traffic(A)=Ea∼A[traffic(a)] We can now state the problem in its full generality: Budget Optimization Input: a budget U, a keyword-query graph G = (K ∪ Q, E), and landscapes (costq(·), clicksq(·)) for each q ∈ Q.
Find: a distribution A over bid vectors a ∈ R |K| + such that spend(A) ≤ U and traffic(A) is maximized.
We conclude this section with a small example to illustrate some feature of the budget optimization problem. Suppose you have two keywords K = {u, v} and two queries Q = {x, y} and edges E = {(u, x), (u, y), (v, y)}. Suppose query x has one position with ctr αx [1] = 1.0, and there is one bid bx 1 = $1. Query y has two positions with ctrs αy [1] = αy [2] = 1.0, and bids by 1 = $ and by 2 = $1 To get any clicks from x, an advertiser must bid at least $1 on u. However, because of the structure of the graph, if the advertiser sets bu to $1, then his effective bid is $1 on both x and y. Thus he must trade-off between getting the clicks from x and getting the bargain of a click for $ that would be possible otherwise.
As we will show in Section 5, solving the Budget Optimization problem in its full generality is difficult. In addition, it may be difficult to reason about strategies that involve arbitrary distributions over arbitrary bid vectors.
Advertisers generally prefer strategies that are easy to understand, evaluate and use within their larger goals. With this motivation, we look at restricted classes of strategies that we can easily compute, explain and analyze.
We define a uniform bidding strategy to be a distribution A over bid vectors a ∈ R |K| + where each bid vector in the distribution is of the form (b, b, . . . , b) for some real-valued bid b. In other words, each vector in the distribution bids the same value on every keyword.
Uniform strategies have several advantages. First, they do not depend on the edges of the interaction graph, since all effective bids on queries are the same. Thus, they are effective in the face of limited or noisy information about the keyword interaction graph. Second, uniform strategies are also independent of the priority rule being used. Third, any algorithm that gives an approximation guarantee will then be valid for any interaction graph over those keywords and queries.
We now show that we can compute the best uniform strategy efficiently.
Suppose we have a set of queries Q, where the landscape Vq for each query q is defined by the set of points Vq = {(costq[1], αq[1]), . . . , (costq[p], αq[p])}. We define the set of interesting bids Iq = {costq[1]/αq [1], . . . , costq[p]/αq [p]}, let I = ∪q∈QIq, and let N = |I|. We can index the points in I as b1, . . . , bN in increasing order. The ith point in our aggregate landscape V is found by summing, over the queries, the cost and clicks associated with bid bi, that is,
V = ∪N i=1( P q∈Q costq(bi),
P q∈Q clicksq(bi)).
For any possible bid b, if we use the aggregate landscape just as we would a regular landscape, we exactly represent the cost and clicks associated with making that bid simultaneously on all queries associated with the aggregate landscape. Therefore, all the definitions and results of Section 2 about landscapes can be extended to aggregate landscapes, and we can apply Lemma 1 to compute the best uniform strategy (using the convex hull of the points in this aggregate landscape). The running time is dominated by the time to compute the convex hull, which is O(N log N)[12].
The resulting strategy is the convex combination of two points on the aggregate landscape. Define a two-bid strategy to be a uniform strategy which puts non-zero weight on at most two bid vectors. We have shown Lemma 2. Given an instance of Budget Optimization in which there are a total of N points in all the landscapes, we can find the best uniform strategy in O(N log N) time.
Furthermore, this strategy will always be a two-bid strategy.
Putting these ideas together, we get an O(N log N)-time algorithm for Budget Optimization, where N is the total number of landscape points (we later show that this is a (1 − 1 e )-approximation algorithm):
landscapes into a single aggregate landscape.
landscape.
budget, which is the convex combination of two points α and β.
combination of the uniform bid vectors corresponding to α and β.
We will also consider a special case of two-bid strategies.
A single-bid strategy is a uniform strategy which puts nonzero weight on at most one non-zero vector, i.e. advertiser randomizes between bidding a certain amount b∗ on all keywords, and not bidding at all. A single-bid strategy is even easier to implement in practice than a two-bid strategy. For example, the search engines often allow advertisers to set a maximum daily budget. In this case, the advertiser would simply bid b∗ until her budget runs out, and the ad serving system would remove her from all subsequent auctions until the end of the day. One could also use an ad scheduling tool offered by some search companies8 to implement this strategy. The best single-bid strategy can also be computed easily from the aggregate landscape. The optimal strategy for a budget U will either be the point x s.t. cost(x) is as large as possible without exceeding U, or a convex combination of zero and the point y, where cost(y) is as small as possible while larger than U. 8 See https://adwords.google.com/support/bin/answer. py?answer=33227, for example. 44 B D A C clicks cost cpc A 2 $1 $0.50 B 5 $0.50 $0.10 C 3 $2 $0.67 D 4 $1 $0.25 cpc $0.67 $0.50 $0.25 $0.10 Total clicks: 5 9 11 14 Figure 2: Four queries and their click-price curve.
In the previous section we proposed using uniform strategies and gave an efficient algorithm to compute the best such strategy. In section we prove that there is always a good uniform strategy: Theorem 3. There always exists a uniform bidding strategy that is (1 − 1 e )-optimal. Furthermore, for any > 0, there exists an instance for which all uniform strategies are at most (1 − 1 e + )-optimal.
We introduce the notion of a click-price curve, which is central to our analysis. This definition makes it simple to show that there is always a single-bid strategy that is a 1 2  approximation (and this is tight); we then build on this to prove Theorem 3.
Consider a set of queries Q, and for each query q ∈ Q, let (clicksq(·), costq(·)) be the corresponding bid landscape.
Consider an adversarial bidder Ω with the power to bid independently on each query. Note that this bidder is more powerful than an optimal bidder, which has to bid on the keywords. Suppose this strategy bids b∗ q for each query q.
Thus, Ω achieves traffic CΩ = P i clicks(b∗ i ), and incurs total spend UΩ = P i cost(b∗ i ).
Without loss of generality we can assume that Ω bids so that for each query q, the cost per click is equal to b∗ q , i.e. costq(b∗ q )/clicksq(b∗ q ) = b∗ q . We may assume this because for some query q, if costq(b∗ q)/clicksq(b∗ q) < b∗ q , we can always lower b∗ q and without changing the cost and clicks.
To aid our discussion, we introduce the notion of a clickprice curve (an example of which is shown in Figure 2), which describes the cpc distribution obtained by Ω.
Formally the curve is a non-decreasing function h : [0, CΩ] → R+ defined as h(r) = min{c | P q:b∗ q ≤c clicksq(b∗ q ) ≥ r}.
Another way to construct this curve is to sort the queries in increasing order by b∗ q = costq(b∗ q)/clicksq(b∗ q), then make a step function where the qth step has height b∗ q and width clicksq(b∗ q ) (see Figure 2). Note that the area of each step is costq(b∗ q ). The following claim follows immediately: Claim 1. UΩ = R CΩ 0 h(r)dr.
Suppose we wanted to buy some fraction r /CΩ of the traffic that Ω is getting. The click-price curve says that if we bid h(r ) on every keyword (and therefore every query), we get at least r traffic, since this bid would ensure that for all q such that b∗ q ≤ h(r ) we win as many clicks as Ω. Note that by bidding h(r ) on every keyword, we may actually get even more than r traffic, since for queries q where b∗ q is much less than h(r ) we may win more clicks than Ω. However, all of these extra clicks still cost at most h(r ) per click.
Thus we see that for any r ∈ [0, CΩ], if we bid h(r ) on every keyword, we receive at least r traffic at a total spend of at most h(r ) per click. Note that by randomizing between bidding zero and bidding h(r ), we can receive exactly r traffic at a total spend of at most r · h(r ). We summarize this discussion in the following lemma: Lemma 4. For any r ∈ [0, CΩ], there exists a single-bid strategy that randomizes between bidding h(r) and bidding zero, and this strategy receives exactly r traffic with total spend at most r · h(r).
Lemma 4 describes a landscape as a continuous function.
For our lower bounds, we will need to show that given any continuous function, there exists a discrete landscape that approximates it arbitrarily well.
Lemma 5. For any C, U > 0 and non-decreasing function f : [0, C] → R+ such that R C 0 f(r)dr = U, and any small > 0, there exists an instance of Budget Optimization with budget U + , where the optimal solution achieves C clicks at cost U + , and all uniform bidding strategies are convex combinations of single-bid strategies that achieve exactly r clicks at cost exactly rf(r) by bidding f(r) on all keywords.
Proof. Construct an instance as follows. Let > 0 be a small number that we will later define in terms of . Define r0 = 0, r1, r2, . . . , rm = C such that ri−1 < ri ≤ ri−1 + , f(ri−1) ≤ f(ri) ≤ f(ri−1)+ , and m ≤ (C +f(C))/ . (This is possible by choosing ri"s spaced by min( , f(ri)−f(ri−1))) Now make a query qi for all i ∈ [m] with bidders bidding h(ri), h(ri+1), . . . , h(rm), and ctrs α[1] = α[2] = · · · = α[m− i+1] = ri −ri−1. The graph is a matching with one keyword per query, and so we can imagine the optimal solution as bidding on queries. The optimal solution will always bid exactly h(ri) on query qi, and if it did so on all queries, it would spend U := Pm i=1(ri − ri−1)h(ri). Define small enough so that U = U + , which is always possible since U ≤ Z C 0 f(r)dr + mX i=1 (ri − ri−1)(h(ri) − h(ri−1)) ≤ U + 2 m ≤ U + (C + f(C)).
Note that the only possible bids (i.e., all others have the same results as one of these) are f(r0), . . . , f(rm), and bidding uniformly with f(ri) results in Pi j=1 ri − ri−1 = ri clicks at cost h(ri)ri.
2 -approximation algorithm Using Lemma 4 we can now show that there is a uniform single-bid strategy that is 1 2 -optimal. In addition to being an interesting result in its own right, it also serves as a warm-up for our main result.
Theorem 6. There always exists a uniform single-bid strategy that is 1 2 -optimal. Furthermore, for any > 0 there exists an instance for which all single-bid strategies are at most (1 2 + )-optimal. 45 Proof. Applying Lemma 4 with r = CΩ/2, we see that there is a strategy that achieves traffic CΩ/2 with spend CΩ/2·h(CΩ/2). Now, using the fact that h is a non-decreasing function combined with Claim 1, we have (CΩ/2)h(CΩ/2) ≤ Z CΩ CΩ/2 h(r)dr ≤ Z CΩ 0 h(r)dr = UΩ, (4) which shows that we spend at most UΩ. We conclude that there is a 1 2 -optimal single-bid strategy randomizing between bidding CΩ/2 and zero.
For the second part of the theorem, we construct a tight example using two queries Q = {x, y}, two keywords K = {u, v}, and edges E = {(u, x), (v, y)}.
Fix some α where 0 < α ≤ 1, and fix some very small > 0. Query x has two positions, with bids of bx 1 = 1/α and bx 2 = , and with identical click-through rates αx [1] = αx [2] = α. Query y has one position, with a bid of by 1 = 1/α and a click-through rate of αy [1] = α. The budget is U = 1 + α. The optimal solution is to bid on u (and therefore x) and bid 1/α on v (and therefore y), both with probability
exactly. The only useful bids are 0, and 1/α, since for both queries all other bids are identical in terms of cost and clicks to one of those three. Any single-bid solution that uses as its non-zero bid gets at most α clicks. Bidding 1/α on both keywords results in 2α clicks and total cost 2. Thus, since the budget is U = 1 + α < 2, a single-bid solution using 1/α can put weight at most (1+ α)/2 on the 1/α bid.
This results in at most α(1 + α) clicks. This can be made arbitrarily close to α by lowering .
e )-approximation algorithm The key to the proof of Theorem 3 is to show that there is a distribution over single-bid strategies from Lemma 4 that obtains at least (1 − 1 e )CΩ clicks. In order to figure out the best distribution, we wrote a linear program that models the behavior of a player who is trying to maximize clicks and an adversary who is trying to create an input that is hard for the player. Then using linear programming duality, we were able to derive both an optimal strategy and a tight instance. After solving the LP numerically, we were also able to see that there is a uniform strategy for the player that always obtains (1 − 1 e )CΩ clicks; and then from the solution were easily able to guess the optimal distribution. This methodology is similar to that used in work on factor-revealing LPs [8, 10].
Consider the adversary"s problem of finding a click-price curve for which no uniform bidding strategy can achieve αCΩ clicks. Recall that by Lemma 1 we can assume that a uniform strategy randomizes between two bids u and v. We also assume that the uniform strategy uses a convex combination of strategies from Lemma 4, which we can assume by Lemma 5. Thus, to achieve αCΩ clicks, a uniform strategy must randomize between bids h(u) and h(v) where u ≤ αCΩ and v ≥ αCΩ. Call the set of such strategies S. Given a (u, v) ∈ S, the necessary probabilities in order to achieve αCΩ clicks are easily determined, and we denote them by p1(u, v) and p2(u, v) respectively. Note further that the advertiser is trying to figure out which of these strategies to use, and ultimately wants to compute a distribution over uniform strategies. In the LP, she is actually going to compute a distribution over pairs of strategies in S, which we will then interpret as a distribution over strategies.
Using this set of uniform strategies as constraints, we can characterize a set of worst-case click-price curves by the constraints Z CΩ 0 h(r)dr ≤ U ∀(u, v) ∈ S p1(u, v)uh(u) + p2(u, v)vh(v) ≥ U A curve h that satisfies these constraints has the property that all uniform strategies that obtain αCΩ clicks spend more than U. Discretizing this set of inequalities, and pushing the first constraint into the objective function, we get the following LP over variables hr representing the curve: min X r∈{0, ,2 ,...,CΩ} · hr s.t. ∀(u, v) ∈ S, p1(u, v)uhu + p2(u, v)vhv ≥ U In this LP, S is defined in the discrete domain as S = {(u, v) ∈ {0, , 2 , . . . , CΩ}2 : 0 ≤ u ≤ αCΩ ≤ v ≤ CΩ}.
Solving this LP for a particular α, if we get an objective less than U, we know (up to some discretization) that an instance of Budget Optimization exists that cannot be approximated better than α. (The instance is constructed as in the proof of Lemma 5.) A binary search yields the smallest such α where the objective is exactly U.
To obtain a strategy for the advertiser, we look at the dual, constraining the objective to be equal to U in order to get the polytope of optimum solutions: X (u,v)∈S wu,v = 1 ∀(u, v) ∈ S,
X v :(u,v )∈S p1(u, v ) · u · wu,v ≤ and X u :(u ,v)∈S p2(u , v) · v · wu ,v ≤ .
It is straightforward to show that the second set of constraints is equivalent to the following: ∀h ∈ RCΩ/ : X r hr = U,
X (u,v)∈S wu,v(p1(u, v) · u · hu + p2(u, v) · v · hv) ≤ U.
Here the variables can be interpreted as weights on strategies in S. A point in this polytope represents a convex combination over strategies in S, with the property that for any click-price curve h, the cost of the mixed strategy is at most U. Since all strategies in S get at least αCΩ clicks, we have a strategy that achieves an α-approximation.
Interestingly, the equivalence between this polytope and the LP dual above shows that there is a mixture over values r ∈ [0, C] that achieves an α-approximation for any curve h.
After a search for the appropriate α (which turned out to be 1 − 1 e ), we solved these two LPs and came up with the plots in Figure 3, which reveal not only the right approximation ratio, but also a picture of the worst-case distribution and the approximation-achieving strategy.9 From the pic9 The parameters U and CΩ can be set arbitrarily using scaling arguments. 46 0 0 C/e C 0 0 C/e C Figure 3: The worst-case click-price curve and (1 − 1/e)-approximate uniform bidding strategy, as found by linear programming. tures, we were able to quickly guess the optimal strategy and worst case example.
By Lemma 4, we know that for each r ≤ UΩ, there is a strategy that can obtain traffic r at cost r · h(r). By mixing strategies for multiple values of r, we construct a uniform strategy that is guaranteed to achieve at least 1−e−1 = 0.63 fraction of Ω"s traffic and remain within budget. Note that the final resulting bid distribution will have some weight on the zero bid, since the single-bid strategies from Lemma 4 put some weight on bidding zero.
Consider the following probability density function over such strategies (also depicted in Figure 3): g(r) = j 0 for r < CΩ/e, 1/r for r ≥ CΩ/e.
Note that R CΩ 0 g(r)dr = R CΩ CΩ/e 1 r dr = 1, i.e. g is a probability density function. The traffic achieved by our strategy is equal to traffic = Z CΩ 0 g(r)·r dr = Z CΩ CΩ/e 1 r ·r dr = „ 1 − 1 e « CΩ.
The expected total spend of this strategy is at most spend = Z CΩ 0 g(r) · rh(r) dr = Z CΩ CΩ/e h(r) dr ≤ Z CΩ 0 h(r) dr = UΩ.
Thus we have shown that there exists a uniform bidding strategy that is (1 − 1 e )-optimal.
We now show that no uniform strategy can do better. We will prove that for all > 0 there exists an instance for which all uniform strategies are at most (1 − 1 e + )-optimal.
First we define the following click-price curve over the domain [0, 1]: h(r) = 8 < : 0 for r < e−1 1 e − 2 „ e − 1 r « for r ≥ e−1 Note that h is non-decreasing and non-negative. Since the curve is over the domain [0, 1] it corresponds to an instance where CΩ = 1. Note also that R 1 0 h(r) dr = 1 e−2 R 1 1/e e − 1 r dr = 1. Thus, this curve corresponds to an instance where UΩ = 1. Using Lemma 5, we construct an actual instance where the best uniform strategies are convex combinations of strategies that bid h(u) and achieve u clicks and u · h(u) cost.
Suppose for the sake of contradiction that there exists a uniform bidding strategy that achieves α > 1−e−1 traffic on this instance. By Lemma 1 there is always a two-bid optimal uniform bidding strategy and so we may assume that the strategy achieving α clicks randomizes over two bids. To achieve α clicks, the two bids must be on values h(u) and h(v) with probabilities pu and pv such that pu + pv = 1, 0 ≤ u ≤ α ≤ v and puu + pvv = α.
To calculate the spend of this strategy consider two cases: if u = 0 then we are bidding h(v) with probability pv = α/v.
The spend in this case is: spend = pv · v · h(v) = αh(v) = αe − α/v e − 2 .
Using v ≥ α and then α > 1 − 1 e we get spend ≥ αe − 1 e − 2 > (1 − 1/e)e − 1 e − 2 = 1, contradicting the assumption.
We turn to the case u > 0. Here we have pu = v−α v−u and pv = α−u v−u . Note that for r ∈ (0, 1] we have h(r) ≥ 1 e−2 (e − 1 r ). Thus spend ≥ pu · uh(u) + pv · vh(v) = (v − α)(ue − 1) + (α − u)(ve − 1) (v − u)(e − 2) = αe − 1 e − 2 > 1.
The final inequality follows from α > 1 − 1 e . Thus in both cases the spend of our strategy is over the budget of 1.
We ran simulations using the data available at Google which we briefly summarize here. We took a large advertising campaign, and, using the set of keywords in the campaign, computed three different curves (see Figure 4) for three different bidding strategies. The x-axis is the budget (units removed), and the y-axis is the number of clicks obtained (again without units) by the optimal bid(s) under each respective strategy. Query bidding represents our (unachievable) upper bound Ω, bidding on each query independently. The uniform bidding curves represent the results of applying our algorithm: deterministic uses a single bid level, while randomized uses a distribution. For reference, we include the lower bound of a (e − 1)/e fraction of the top curve.
The data clearly demonstrate that the best single uniform bid obtains almost all the possible clicks in practice.
Of course in a more realistic environment without full knowledge, it is not always possible to find the best such bid, so further investigation is required to make this approach useful. However, just knowing that there is such a bid available should make the on-line versions of the problem simpler.
By a reduction from vertex cover we can show the following (proof omitted): Theorem 7. Budget Optimization is strongly NP-hard. 47 Query Bidding Uniform Bidding (randomized) Uniform Bidding (deterministic) Lower bound 0 0
1 1 Budget Clicks Figure 4: An example with real data.
Now suppose we introduce weights on the queries that indicate the relative value of a click from the various search users. Formally, we have weights wq for all q ∈ Q and our goal is maximize the total weighted traffic given a budget.
Call this the Weighted Keyword Bidding problem.
With this additional generalization we can show hardness of approximation via a simple reduction from the Maximum Coverage problem, which is known to be (1−1/e)-hard [6] (proof omitted).
Theorem 8. The Weighted Keyword Bidding problem is hard to approximate to within (1 − 1/e).
GRAPHS If a graph has special structure, we can sometimes solve the budget optimization problem exactly. Note that the knapsack algorithm in Section 2 solves the problem for the case when the graph is a simple matching. Here we generalize this to the case when the graph has a laminar structure, which will allow us to impose a (partial) ordering on the possible bid values, and thereby give a pseudopolynomial algorithm via dynamic programming.
We first show that to solve the Budget Optimization problem (for general graphs) optimally in pseudopolynomial time, it suffices to provide an algorithm that solves the deterministic case. The proof (omitted) uses ideas similar to Observation 1 and Lemma 1.
Lemma 9. Let I be an input to the Budget Optimization problem and suppose that we find the optimal deterministic solution for every possible budget U ≤ U. Then we can find the optimal solution in time O(U log U).
A collection S of n sets S1, . . . , S2 is laminar if, for any two sets Si and Sj, if Si ∩ Sj = ∅ then either Si ⊆ Sj or Sj ⊆ Si.
Given a keyword interaction graph G, we associate a set of neighboring queries Qk = {q : (k, q) ∈ E} with each keyword k. If this collection of sets if laminar, we say that the graph has the laminar property. Note that a laminar interaction graph would naturally fall out as a consequence of designing a hierarchical keyword set (e.g., shoes, highheel shoes, athletic shoes).
We call a solution deterministic if it consists of one bid vector, rather than a general distribution over bid vectors.
The following lemma will be useful for giving a structure to the optimal solution, and will allow dynamic programming.
Lemma 10. For keywords i, j ∈ K, if Qi ⊆ Qj then there exists an optimal deterministic solution to the Budget Optimization problem with ai ≥ aj.
We can view the laminar order as a tree with keyword j as a parent of keyword i if Qj is the minimal set containing Qi.
In this case we say that j is a child of i. Given a keyword j with c children i1, . . . , ic, we now need to enumerate over all ways to allocate the budget among the children and also over all possible minimum bids for the children. A complication is that a node may have many children and thus a term of Uc would not even be pseudopolynomial. We can solve this problem by showing that given any laminar ordering, there is an equivalent one in which each keyword has at most 2 children.
Lemma 11. Let G be a graph with the laminar property.
There exists another graph G with the same optimal solution to the Budget Optimization problem, where each node has at most two children in the laminar ordering. Furthermore,
G has at most twice as many nodes as G.
Given a graph with at most two children per node, we define F[i, b, U] to be the maximum number of clicks achievable by bidding at least b on each of keywords j s.t. Qj ⊆ Qi (and exactly b on keyword i) while spending at most U. For this definition, we use Z(b, U) to denote set of allowable bids and budgets over children: Z(b, U) = {b, b , U , U : b ≥ b, U ≤ U, b ≥ b, U ≤ U, U + U ≤ U} Given a keyword i and a bid ai, compute an incremental spend and traffic associated with bidding ai on keyword i, that is ˆt(i, ai) = X q∈Qi\Qi−1 clicksq(ai), and ˆs(i, ai) = X q∈Qi\Qi−1 costq(ai).
Now we define F[i, b, U] as max b, b ,U ,U ∈Z(b,U) j F[j , b , U ] + F[j , b , U ] + ˆt(i, b) ff (5) if (ˆs(i, b) ≤ U − U − U and i > 0), and F[i, b, U] = 0 otherwise.
Lemma 12. If the graph G has the laminar property, then, after applying Lemma 11, the dynamic programming recurrence in (5) finds an optimal deterministic solution to the Budget Optimization problem exactly in O(B3 U3 n) time.
In addition, we can apply Lemma 9 to compute the optimal (randomized) solution. Observe that in the dynamic program, we have already solved the instance for every budget U ≤ U, so we can find the randomized solution with no additional asymptotic overhead. 48 Lemma 13. If the graph G has the laminar property, then, by applying Lemma 11, the dynamic programming recurrence in (5), and Lemma 9, we can find an optimal solution to the Budget Optimization problem in O(B3 U3 n) time.
The bounds in this section make pessimistic assumptions about having to try every budget and every level. For many problems, you only need to choose from a discrete set of bid levels (e.g., multiples of one cent). Doing so yields the obvious improvement in the bounds.
The GSP auction is not the only possible auction one could use for sponsored search. Indeed the VCG auction and variants [14, 4, 7, 1] offer alternatives with compelling game-theoretic properties. In this section we argue that the budget optimization problem is easy under the VCG auction.
For a full definition of VCG and its application to sponsored search we refer the reader to [1, 2, 5]. For the sake of the budget optimization problem we can define VCG by just redefining costq(b) (replacing Equation (2)): costq(b) = p−1 X j=i (α[j] − α[j + 1]) · b[j] where i = pos(b).
Observation 1 still holds, and we can construct a landscape as before, where each landscape point corresponds to a particular bid b[i].
We claim that in the VCG auction, the landscapes are convex. To see this, consider two consecutive positions i,i + 1.
The slope of line segment between the points corresponding to those two positions is cost(b[i]) − cost(b[i + 1]) clicks(b[i]) − clicks(b[i + 1]) = (α[i] − α[i + 1]) · b[i] α[i] − α[i + 1] = b[i].
Since b[i] ≥ b[i + 1], the slopes of the pieces of the landscape decrease, and we get that the curve is convex.
Now consider running the algorithm described in Section 2.1.4 for finding the optimal bids for a set of queries.
In this algorithm we took all the pieces from the landscape curves, sorted them by incremental cpc, then took a prefix of those pieces, giving us bids for each of the queries. But, the equation above shows that each piece has its incremental cpc equal to the bid that achieves it; thus in the case of VCG the pieces are also sorted by bid. Hence we can obtain any prefix of the pieces via a uniform bid on all the keywords. We conclude that the best uniform bid is an optimal solution to the budget optimization problem.
Our algorithmic result presents an intriguing heuristic in practice: bid a single value b on all keywords; at the end of the day, if the budget is under-spent, adjust b to be higher; if budget is overspent, adjust b to be lower; else, maintain b. If the scenario does not change from day to day, this simple strategy will have the same theoretical properties as our one-bid strategy, and in practice, is likely to be much better. Of course the scenario does change, however, and so coming up with a stochastic bidding strategy remains an important open direction, explored somewhat by [11, 13].
Another interesting generalization is to consider weights on the clicks, which is a way to model conversions. (A conversion corresponds to an action on the part of the user who clicked through to the advertiser site; e.g., a sale or an account sign-up.) Finally, we have looked at this system as a black box returning clicks as a function of bid, whereas in reality it is a complex repeated game involving multiple advertisers. In [3], it was shown that when a set of advertisers use a strategy similar to the one we suggest here, under a slightly modified first-price auction, the prices approach a well-understood market equilibrium.
Acknowledgments We thank Rohit Rao, Zoya Svitkina and Adam Wildavsky for helpful discussions.
[1] G. Aggarwal, A. Goel and R. Motwani. Truthful auctions for pricing search keywords. ACM Conference on Electronic Commerce, 1-7, 2006. [2] G. Aggarwal, J. Feldman and S. Muthukrishnan Bidding to the Top: VCG and Equilibria of Position-Based Auctions Proc. WAOA, 2006. [3] C. Borgs, J. Chayes, O. Etesami, N. Immorlica, K.
Jain, and M. Mahdian. Dynamics of bid optimization in online advertisement auctions. Proc. WWW 2007. [4] E. Clarke. Multipart pricing of public goods. Public Choice, 11(1):17-33, 1971. [5] B. Edelman, M. Ostrovsky and M. Schwarz. Internet Advertising and the Generalized Second Price Auction: Selling Billions of Dollars Worth of Keywords. Second workshop on sponsored search auctions, 2006. [6] U. Feige. A threshold of ln n for approximating set cover. 28th ACM Symposium on Theory of Computing, 1996, pp. 314-318. [7] T. Groves. Incentives in teams. Econometrica, 41(4): 617-631, 1973. [8] K. Jain, M. Mahdian, E. Markakis, A. Sabieri and V.
Vazirani. Greedy facility location algorithms analyzed using dual fitting with factor-revealing LP.
J. ACM, 50(6): 795-824, 2003. [9] W. Labio, M. Rose, S. Ramaswamy. Internal Document, Google, Inc. May, 2004. [10] A. Mehta, A. Saberi, U. Vazirani, and V. Vazirani,
Adwords and Generalized Online Matching. FOCS
[11] S. Muthukrishnan, M. P´al and Z. Svitkina.

Mechanism design is a sub-field of game theory that studies how to design rules of games resulting in desirable outcomes, when the players are rational. In a standard setting, players hold some private information - their types - and choose actions from their action spaces to maximize their utilities. The social planner wishes to implement a social-choice function, which maps each possible state of the world (i.e., a profile of the players" types) to a single alternative. For example, a government that wishes to undertake a public-good project (e.g., building a bridge) only if the total benefit for the players exceeds its cost.
Much of the literature on mechanism design restricts attention to direct revelation mechanisms, in which a player"s action space is identical to his type space. This focus is owing to the revelation principle that asserts that if some mechanism achieves a certain result in an equilibrium, the same result can be achieved in a truthful one - an equilibrium where each agent simply reports his private type [15].
Nonetheless, in many environments, direct-revelation mechanisms are not viable since the actions available for the players have a limited expressive power. Consider, for example, the well-studied screening model, where an insurance firm wishes to sell different types of policies to different drivers based on their caution levels, which is their private information. In this model, drivers may have a continuum of possible caution levels, but insurance companies offer only a few different policies since it might be either infeasible or illegal to advertise and sell more then few types of policies.
There are various reasons for such strict restrictions on the action spaces. In some situations, firms are not willing, or cannot, run a bidding process but prefer fixing a price for some product or service. The buyers in such environemnts face only two actions - to buy or not to buy - although they may have an infinite number of possible values for the item.
In many similar settings, players might be also reluctant to reveal their accurate types, but willing to disclose partial information about them. For example, agents will typically be unwilling to reveal their types, even if it is beneficial for them in the short run, since it might harm them in future transactions. Agents may also not trust the mechanism to keep their valuations private [16], or not even know their exact type while computing it may be expensive [12]. Limitations on the action space can also be caused by technical constraints, such as severe restrictions on the communication lines [5] or from the the need to perform quick transactions (e.g., discrete bidding in English auctions [9]). 62 Consider for example a public-good model: a social planner needs to decide whether to build a bridge. The two players in the game have some privately known benefits θ1, θ2 ∈ [0, 1] from using this bridge. The social planner aims to build the bridge only if the sum of these benfits exceeds the construction cost of the bridge. The social planner cannot access the private data of the players, and can only learn about it from the players" actions. When direct revelation is allowed, the social planner can run the well-known VCG mechanism, where the players have incentives to report their true data; hence, the planner can elicit the exact private information of the players and build the bridge only when it should be built. Assume now that the players cannot send their entire secret data, but can only choose an action out of two possible actions (e.g., 0 or 1). Now, the social planner will clearly no longer be able to always build the bridge according to her objective function, due to the limited expressivness of the players" messages. In this work we try to analyze what can be achieved in the presence of such restrictions.
Restrictions on the action space, for specific models, were studied in several earlier papers. The work of Blumrosen,
Nisan and Segal [4, 6, 5] is the closest in spirit to this paper.
They studied single-item auctions where bidders are allowed to send messages with severely bounded size. They characterized the optimal mechanisms under this restriction, and showed that nearly optimal results can be achieved even with very strict limitations on the action space. Other work studied similar models for the analysis of discrete-bid ascending auctions [9, 11, 8, 7], take-it-or-leave-it auctions [17], or for measuring the effect of discrete priority classes of buyers on the performance of electricity markets [19, 14]. Our work generalizes the main results of Blumrosen et al. to a general mechanism-design framework that can be applied to a multitude of models. We show that some main properties proved by Blumrosen et al. are preserved in more general frameworks (for example, that dominant-strategy equilibrium can be achieved with no additional cost, and that the loss diminishes with the number of possible actions in a similar rate), where some other properties do not always hold (for example, that asymmetric mechanisms are optimal and that players must always use all their action space).
A standard mechanism design setting is composed of agents with private information (their types), and a social planner, who wishes to implement a social choice function, c - a function that maps any profile of the agents" types into a chosen alternative. A classic result in this setting says that under some monotonicity assumption on the agents" preferences - the single-crossing assumption (see definition below) - a social-choice function is implementable in dominant strategies if and only if it is monotone in the players" types. However, in environments with restricted action spaces, the social planner cannot typically implement every social-choice function due to inherent informational constraints. That is, for some realizations of the players" types, the decision of the social planner will be incompatible with the social-choice function c. In order to quantitatively measure how well bounded-action mechanisms can approximate the original social-choice functions, we follow a standard assumption that the social choice function is derived from a social-value function, g, which assigns a real value for every alternative A and realization of the players" types.
The social-choice function c will therefore choose an alternative that maximizes the social value function, given the type vector −→ θ = (θ1, .., θn), i.e., c( −→ θ ) = argmaxA{g( −→ θ , A)}.
Observe that the social-value function is not necessarily the social welfare function - the social welfare function is a special case of g in which g is defined to be the sum of the players" valuations for the chosen alternative. Following are several simple examples of social-value functions: • Public goods. A government wishes to build a bridge only if the sum of the benefits that agents gain from it exceeds its construction cost C. The social value functions in a 2-player game will therefore be: g(θ1, θ2,build)=θ1+θ2-C and g(θ1, θ2,do not build)=0. • Routing in networks. Consider a network that is composed of two links in parallel. Each link has a secret probability pi of transferring a message successfully.
A sender wishes to send his message through the network only if the probability of success is greater than, say, 90 percent - the known probability in an alternate network. That is, g(p1, p2, send in network)=1-(1-p1)·(1-p2) and g(p1, p2,send in alternate network)=0.9. • Single-item auctions. Consider a 2-player auction, where the auctioneer wishes to allocate the item to the player who values it the most. The social choice function is given by: g(θ1, θ2, player 1 wins) = θ1 and for the second alternative is g(θ1, θ2, player 2 wins) = θ2.
In this paper, we present a general framework for the study of mechanism design in environments with a limited number of actions. We assume a Bayesian model where players have one-dimensional private types, independently distributed on some real interval.
The main question we ask is: when agents are only allowed to use k different actions, which mechanisms achieve the optimal expected social-value? Note that this question is actually composed of two separate questions. The first question is an information-theoretic question: what is the optimal result achievable when the players can only reveal information using these k actions (recall that their type space may be continuous). The other question involves gametheoretic considerations: what is the best result achievable with k actions, where this result should be achieved in a dominant-strategy equilibrium. These questions raise the question about the price of implementation: can the optimal information-theoretic result always be implemented in a dominant-strategy equilibrium? And if not, to what extent does the dominant-strategy requirement degrades the optimal result? What we call the price of implementation was also explored in other contexts in game theory where computational restrictions apply: for example, is it always true that the optimal polynomial-time approximation ratio (for example, in combinatorial auctions) can be achieved in equilibrium? (The answer for this interesting problem is still unclear, see, e.g., [3, 2, 13].) Our first contribution is the characterization of sufficient conditions for implementing the optimal informationtheoretic social-choice rule in dominant strategies. We show that for the family of multilinear social-value functions (that 63 is, polynomials where each variable has a degree of at most one in each monomial) the dominant-strategy implementation incurs no additional cost.
Theorem: Given any multilinear single-crossing socialvalue function, and for any number of alternatives and players, the social choice rule that is information-theoretically optimal is implementable in dominant strategies.
Multilinear social-value functions capture many important and well-studied models, and include, for instance, the routing example given above, and any social welfare function in which the players" valuations are linear in their types (such as public-goods and auctions).
The implementability of the information-theoretically optimal mechanisms enables us to use a standard routine in Mechanism Design and first determine the optimal socialchoice rule, and then calculate the appropriate payments that ensure incentive compatibility. To show this result, we prove a useful lemma that gives another characterization for social-choice functions whose price of implementation is zero. We show that for any social-choice function, incentive compatibility in action-bounded mechanisms is equivalent to the property that the optimal expected social value is achieved with non-decreasing strategies (or threshold strategies).1 In other words, this lemma implies that one can always implement, with dominant strategies, the best socialchoice rule that is achievable with non-decreasing strategies.
Our second contribution is in characterizing the optimal action-bounded mechanisms. We identify some necessary conditions for the optimality of mechanisms in general, and using these conditions, we fully characterize the optimal mechanisms in environments with two players and two alternatives. The optimal mechanisms turn out to be diagonal - that is, in their matrix representation, one alternative will be chosen in, and only in, entries below one of the main diagonals (this term extends the concept of Priority Games used in [5] for bounded-communication auctions). We complete the characterization of the optimal mechanisms with the depiction of the optimal strategies - strategies that are mutually maximizers. Since the payments in a dominantstrategy implementation are uniquely defined by a monotone allocation and a profile of strategies, this also defines the payments in the mechanism. We give an intuitive proof for the optimality of such strategies, generalizing the concept of optimal mutually-centered strategies from [4].
Surprisingly, as opposed to the optimal auctions in [4], for some non-trivial social-value functions, the optimal diagonal mechanism may not utilize all the k available actions.
Theorem: For any multilinear single-crossing social-value function over two alternatives, the informationally optimal 2-player k-action mechanism is diagonal, and the optimal dominant strategies are mutually-maximizers.
Achieving a full characterization of the optimal actionbounded mechanism for multi-player or multi-alternative environments seems to be harder. To support this claim, we observe that the number of mechanisms that satisfy the necessary conditions above is growing exponentially in the number of players. 1 The restriction to non-decreasing strategies is very common in the literature. One remarkable result by Athey [1] shows that when a non-decreasing strategy is a best response for any other profile of non-decreasing strategies, a pure Bayesian-Nash equilibrium must exist.
Our next result compares the expected social-value in k-action mechanisms to the optimal expected social value when the action space is unrestricted. For any number of players or alternatives, and for any profile of independent distribution functions, we construct mechanisms that are nearly optimal - up to an additive difference of O( 1 k2 ). This result is achieved in dominant strategies.
Theorem: For any multilinear social-value function, the optimal k-action mechanism incurs an expected social loss of O( 1 k2 ).
This is the same asymptotic rate proved for specific environments in [19, 9, 5]. Note that there are social-choice functions that can be implemented with k actions with no loss at all (for example, the rule always choose alternative A). However, we know that in some settings (e.g., auctions [5]) the optimal loss may be proportional to 1 k2 , thus a better general upper bound is impossible.
Finally, we present our results in the context of several natural applications. First, we give an explicit solution for a public-good game with k-actions. We show that the optimum is achieved in symmetric mechanisms (in contrast to action-bounded auctions [5]), and that the optimal allocation scheme depends on the value of the construction cost C. Then, we study the celebrated signaling model, in which potential employees send signals about their skills to potential employers by means of the education level they acquire.
This is a natural application in our context since education levels are often discrete (e.g., B.A, M.A and PhD). Lastly, we present our results in the context of routing in networks, where it is reasonable to assume that links report whether they have low or high loss rates, but less reasonable to require them to report their accurate loss rates. The latter example illustrates how our results apply to settings where the goal of the social planner is not welfare maximization (nor variants of it like affine maximizers).
The rest of the paper is organized as follows: our model and notations are described in Section 2. We then describe our general results regarding implementation in multi-player and multi-alternative environments in Section 3, including the asymptotic analysis of the social-value loss. In Section 4, we fully characterize the optimal mechanisms for 2player environments with two alternative. In Section 5, we conclude with applying our general results to several wellstudied models. Due to lack of space, some of the proofs are missing and can be found in the full version that can be found on the authors" web pages.
We first describe a standard mechanism-design model for players with one-dimensional types. Then, in Subsection
general model studies environments with n players and a set A = {A1, A2, ..., Am} of m alternatives. Each player has a privately known type θi ∈ [θi, θi] (where θi, θi ∈ R, θi < θi), and a type-dependent valuation function vi(θi, A) for each alternative A ∈ A. In other words, player i with type θi is willing to pay an amount of vi(θi, A) for alternative A to be chosen. Each type θi is independently distributed according to a publicly known distribution Fi, with an always positive density function fi. We denote the set of all possible type profiles by Θ = ×n i=1[θi, θi]. 64 The social planner has a social-choice function c : Θ → A, where the choice of alternatives is made in order to maximize a social-value function g( −→ θ ) : Θ × A → R. That is, c( −→ θ ) ∈ argmaxA∈A{g( −→ θ , A)} We assume that for every alternative A ∈ A, the function g(·, A) is continuous and differentiable in every type. Since the players" types are private information, in order to choose the optimal alternative, the social planner needs to get the players" types as an input. The players reveal information about their types by choosing an action, from an action set B.
Each player uses a strategy for determining the action he plays for any possible type. A strategy for player i is therefore a function si : [θi, θi] −→ B. We denote a profile of strategies by s = s1, ..., sn and the set of the strategies of all players except i by s−i. The utility of player i of type θi from alternative A under the payment pi is ui = vi(θi, A) − pi.
Following is a standard definition of a mechanism. The action space B is traditionally implicit, but we mention it explicitly since we later examine limitations on B.
Definition 1. A mechanism with an action set B is a pair (t, p) where: • t : Bn → A is the allocation rule.2 • p : Bn → Rn is the payment scheme (i.e., pi(b) is the payment to the ith player given a vector of actions b).
The main goal of this paper is to optimize the expected social value (in action-bounded mechanisms) while preserving a dominant-strategy equilibrium.
We say that a strategy si is dominant for player i in mechanism (t, p) if player i cannot increase his utility by reporting a different action than si(θi), regardless of the actions of the other players b−i.3 Definition 2. We say that a social-choice function h is implementable with a set of actions B if there exists a mechanism (t, p) with a dominant-strategy equilibrium s1, ..., sn (where for each i, si : [θi, θi] −→ B) that always chooses an alternative according to h, i.e., t(s1(θ1), ..., sn(θn)) = h( −→ θ ).
A fundamental result in the mechanism-design literature states that under the single-crossing condition, the monotonicity of the social-choice function is a sufficient and necessary condition for dominant-strategy implementability (in single-parameter environments). The single-crossing condition (also known as the Spence-Mirrlees condition) appears, very often implicitly, in almost every paper on mechanism design in one-dimensional domains. Without this assumption, general sufficient condition for implementability are unknown (for a survey on this topic see [10]).
Throughout this paper, we assume that the valuation functions of the players are single-crossing, as defined below. A player"s valuation function will be single-crossing if the effect of an increment in the player"s type on the player"s valuation for 2 We will show that, w.l.o.g., we can focus on deterministic allocation schemes. 3 That is, for every type θi and every action bi, we have that vi ( θi, t(si(θi), b−i) )-pi(si(θi), b−i)¿vi ( θi, t(bi, b−i) )pi(bi, b−i) two alternatives is always greater for one of these alternatives. The single-crossing condition on the players" preferences actually defines an order on the alternatives. For example, if the value of player i for alternative A increases more rapidly than his value for alternative B, we can denote it by A i B. Later on, we will use these orders for defining monotonicity of social-choice functions.
Definition 3. A function h : Θ × A → R is single crossing with respect to i if there is a weak order i on the alternatives, such that for any two alternatives Aj i Al we have that for every −→ θ ∈ Θ, ∂h( −→ θ , Aj ) ∂θi > ∂h( −→ θ , Al) ∂θi and if Aj ∼ Al (that is, Al i Aj and Aj i Al) then h(·, Aj) ≡ h(·, Al) (i.e., the functions are identical).
The definition of monotone social-choice functions also requires an order on the actions. This order is implicit in most of the standard settings where, for example, it is defined by the order on the real numbers (e.g., in direct revelation mechanisms where each type is drawn from a real interval). When the action space is discrete, the order can be determined by the names of the actions, for example, 0, 1,...,k-1 for k-action mechanisms. (We therefore describe this order with the standard relation on natural numbers <, >.) Definition 4. A deterministic mechanism is monotone if when player i raises his reported action, and fixing the actions of the other players, the mechanism never chooses an inferior alternative for i. That is, for any b−i ∈ {0, ..., k− 1}n−1 if bi > bi then t(bi, b−i) i t(bi, b−i).
Following is a classic result regarding the implementability of social-choice functions in single-parameter environments.
Note, however, that this characterization does not hold when the action space is bounded.
Proposition 1. Assume that the valuation functions vi(θi, A) are single crossing and that the action space is unrestricted. A social-choice function c is dominant-strategy implementable if and only if c is monotone.
The set of actions B is usually implicit in the literature, and it is assumed to be isomorphic to the type space. In this paper, we study environments where this assumption does not hold. We define a k-action game to be a game in which the number of possible actions for each player is k, i.e., |B| = k. In k-action games, the social planner typically cannot always choose an alternative according to the social choice function c due to the informational constraints. Instead, we are interested in implementing a social-choice function that, with k actions, maximizes the expected social value: E−→ θ g −→ θ , t (s1(θ1), ..., sn(θn))  .
Definition 5. We say that a social-choice function h : Θ → A is informationally achievable with a set of actions B if there exists a profile of strategies s1, ..., sn (where for each i, si : [θi, θi] −→ B), and an allocation rule t : Bn → A, such that t chooses the same alternative as h for any type profile, i.e., t(s1(θ1), ..., t(θn)) = h( −→ θ ). If |B| = k, we say that h is k-action informationally achievable. 65 Note that this definition does not take into account strategic considerations. For example, consider an environment with two alternatives A = {A, B}, and the following socialchoice function: ec(θ1, θ2) = A iff {θ1 > 1/2 and θ2 > 1/2}. ec is informationally achievable with two actions: if both players bid 0 when their value is greater than 1/2 and 1 otherwise, then the allocation rule choose alternative A iff both players report 1 derives exactly the same allocation for every profile of types. In contrast, it is easy to see that the function ˆc(θ1, θ2) = A iff θ1 + θ2 > 1/2 is not informationally achievable with two actions.
We now define a social-choice rule that maximizes the social value under the information-theoretic constraints that are implied by the limitations on the number of actions.
Definition 6. A social-choice function is k-action informationally optimal with respect to the social-value function g, if it achieves the maximal expected social value among all the k-action informationally achievable social-choice functions.4 Earlier in this section, we defined the single-crossing property for the players valuations. We now define a singlecrossing property on the social-value function g. This property clearly ensures the monotonicity of the corresponding social choice rule, and we will later show that it is also useful for action-bounded environments.
Definition 7. We say that the social-choice rule g( −→ θ , A) exhibits the single-crossing property if for every player i, g exhibits the single-crossing property with respect to i.
Note that the definition above requires that g will be single crossing with respect to every player i, given her individual order i on the alternatives. That is, the social value function g will be compatible in this sense with the singlecrossing conditions on the players" preferences.
Finally, we call attention to a natural set of strategies - non-decreasing strategies, where each player reports a higher action as her type increases. Equivalently, such strategies are threshold strategies - strategies where each player divides his type support into intervals, and simply reports the interval in which her type lies.
Definition 8. A real vector x = (x0, x1, ..., xk) is a vector of threshold values if x0 ≤ x1 ≤ ... ≤ xk.
Definition 9. A strategy si is a threshold strategy based on a vector of threshold values x = (x0, x1, ..., xk), if for any action j it holds that si(θi) = j iff θi ∈ [xj , xj+1]. A strategy si is called a threshold strategy, if there exists a vector x of threshold values such that si is a threshold strategy based on x.
NUMBER OF ACTIONS In this section, we study the general model of actionbounded mechanism design. Our first result is a sufficient and necessary condition for the implementability of the optimal solution achievable with k actions: this condition says that the optimal social-choice rule is achieved when all the 4 For simplicity, we assume that a maximum is attained and thus the optimal function is well defined. players use non-decreasing strategies. The basic idea is that with non-decreasing strategies (i.e., threshold strategies), we can apply the single-crossing property to show that when a player raises his reported action, the expected value for his high-priority alternatives increases faster; therefore, monotonicity must hold. The result holds for any number of players and alternatives, and for any profile of distribution functions on the players" types, as long as they are statistically independent. (It is easy to illustrate that this result does not hold if the players" types are dependent.) Lemma 1. Consider a single-crossing social-value function g. The informationally optimal k-action social-choice function c∗ (with respect to g) is implementable if and only if c∗ achieves its optimum when the players use non-decreasing strategies.
Next, we show that for a wide family of social-value functions - multilinear functions - the price of implementation is zero. That is, the information-theoretically optimal rule is dominant-strategy implementable. This family of functions captures many common settings from the literature.
In particular, it generalizes the auction setting studied by Blumrosen et al. [4, 6].
Definition 10. A multilinear function is a polynomial in which the degree of every variable in each monomial is at most 1.5 We say that a social-choice rule g is multilinear, if g(·, A) is multilinear for every alternative A ∈ A.
The basic idea behind the proof of the following theorem is as follows: for every player, we show that the expected social welfare when he chooses any action (fixing the strategies of the other players) is a linear function of his type. This is a result of the multilinearity of the social-value function and of the linearity of expectation. The maximum over a set of linear functions is a piecewise-linear function, hence the optimal social value is achieved when the player uses threshold strategies (the thresholds are the switching points). Since the optimum is achieved with threshold strategies, we can apply Lemma 1 to show the monotonicity of this socialchoice rule. Note that in this argument we characterize the players" strategies that maximize the social value, and not the players" utilities.
Theorem 1. If the social-value function is multilinear and single crossing, the informationally optimal k-action social-choice function is implementable.
Proof. We will show that for any k-action mechanism, the optimal expected social value is achieved when all players use threshold strategies. This will be shown by proving that for any player i and for any action bi of player i, the expected welfare when she chooses the action bi is a linear function in player i"s type θi. Then, it will follow from Lemma 1 that the social choice function is implementable.
For every action bi of player i, let qA denote the probability that alternative A is allocated, i.e., qA = Pr−→ θ h t(s( −→ θ )) = A|si(θi) = bi i 5 For example, f(x, y, z) = xyz + 5xy + 7. 66 Due to the linearity of expectation, the expected social value when player i with type θi reports bi is: X A∈A qA Eθ−i ( g(θi, θ−i, A) | t(bi, s−i(θ−i)) = A ) (1) = X A∈A qA Z θ−i g(θi, θ−i, A)fA −i(θ−i)d(θ−i) (2) where fA −i(θ−i) equals Q j=i fj (θj ) qA for types profiles θ−i such that t(bi, s−i(θ−i)) = A, and 0 otherwise.
Since g is multilinear, every function g(θi, θ−i, A) is a linear function in θi, where the coefficients depend on the values of θ−i. Denote this function by g(θi, θ−i, A) = λθ−i θi + βθ−i . Thus, we can write Equation 2 as: X A∈A qA Z θ−i ` λθ−i θi + βθ−i ´ fA −i(θ−i)d(θ−i) = X A∈A qA θi Z θ−i λθ−i fA −i(θ−i)d(θ−i) + Z θ−i βθ−i fA −i(θ−i)d(θ−i) ! In this expression, each integral is a constant independent of θi when the strategies of the other player are fixed.
Therefore, each summand, thus the whole function, is a linear function in θi. For achieving the optimal expected social value, the player must choose the action that maximizes the expected social value. A maximum of k linear functions is a piecewise-linear function with at most k−1 breaking points.
These breaking points are the thresholds to be used by the player. For all types between subsequent thresholds, the optimum is clearly achieved by a single action; Since linear functions are single-crossing, every action will be maximal in at most one interval.
The same argument applies to all the players, and therefore the optimal social value is obtained with threshold strategies.
Observe that the proof of Theorem 1 actually works for a more general setting. For proving that the informationtheoretically optimal result is achieved with threshold strategies, it is sufficient to show that the social-choice function exhibits a single-crossing condition on expectation: given any allocation scheme, and fixing the behavior of the other players, the expected social value in any two actions (as a function of θi) is single crossing. Theorem 1 shows that this requirement holds for multilinear functions, but we were not able to give an exact characterization of this general class of functions.
The implementability of the information-theoretically optimal solution makes the characterization of the optimal incentive-compatible mechanisms significantly easier: we can apply the standard mechanism-design technique and first calculate the optimal allocation scheme and then find the right payments.
Observe that if the valuation functions of the players are linear and single crossing, then the social-welfare function (i.e., the sum of the players" valuations) is multilinear and single-crossing. This holds since the single-crossing conditions on the valuations are defined with a similar order on the alternatives as in the social-value function. Therefore, an immediate conclusion from Theorem 1 is that the optimal social welfare, which is achievable with k actions, is implementable when the valuations are linear.
Corollary 1. If the valuation functions vi(·, A) are single crossing and linear in θi for every player i and for every alternative, then the informationally optimal k-action social welfare function is implementable.
In this section we show that the social value loss of multilinear social-value rules diminishes quadratically with the number of possible actions, k. This is the same asymptotic ratio presented in the study of specific models in the same spirit [19, 5, 18, 9]. The main challenge here, compared to earlier results, is in dealing with the general mechanismdesign framework, that allows a large family of social-value functions for any number of players and alternatives. As opposed to the specific models, the social-value function may be asymmetric with respect to the players" types; for instance, the social-value loss may a-priori occur in any entry (i.e., profile of actions).
The basic intuition for the proof is that even for this general framework, we can construct mechanisms where the probability of having an allocation that is incompatible with the original social-choice function is O( 1 k ). (This fact holds for all single-crossing social-choice functions, not only for multilinear functions.) Then, we can use the multilinearity to show that the social-value loss will always be O( 1 k ) in the mechanisms we construct. Taken together, the expected loss becomes O( 1 k2 ). Our proof is constructive - we present an explicit construction for a mechanism that exhibits the desired loss in dominant strategies. The additive expected social-value depends on the length of the support of the type space. Hence, we assume that the type space is normalized to [0, 1], that is, for every player i, θi = 0 and θi = 1.
Theorem 2. Assume that the type spaces are normalized to [0, 1]. For any number of players and alternatives, and for any set of distribution functions of the players" types, if the social-value function g is single crossing and multilinear, then the informationally optimal k-action social-choice function (with respect to g) incurs an expected social-value loss of O( 1 k2 ).
Moreover, as discussed in [4], this bound is asymptotically tight. That is, there exists a set of distribution functions for the players (the uniform distribution in particular) and there are social-value functions (e.g., auctions) for which any mechanism incurs a social-value loss of at least Ω( 1 k2 ). In that sense, auctions are the hardest problems with respect to the incurred loss. Yet, note that this claim does not imply that the loss of any social-choice function will be proportional to 1 k2 . For example, in the social choice function that chooses the same alternative for any type profile, no loss will be incurred (even with 0 actions). 67
PLAYERS AND TWO ALTERNATIVES In this section, we present a full characterization of the optimal mechanisms in action-bounded environments with two players and two alternatives, where the social-choice functions are multilinear and single crossing.
Note that in this section, as in most parts of this paper, we characterize monotone mechanisms by their allocation scheme and by a profile of strategies for the players. Doing this, we completely describe which alternative is chosen for every profile of types of the players. It is well known that in monotone mechanisms for one dimensional environments, the allocation scheme uniquely defines the payments in the dominant-strategy implementation. We find this description, which does not explicitly mention the payments, easier for the presentation.
A key notion in our characterization of the optimal actionbounded mechanism, is the notion of non-degenerate mechanisms. In a degenerate mechanism, there are two actions for one of the players that are identical in their allocation.
Intuitively, a degenrate mechanism does not utilize all the action space it is allowed to use, and therefore it cannot be optimal. Using this propery, we then define diagonal mechanisms that turns out to exactly characterize the set of optimal mechanisms.
Definition 11. A mechanism is degenerate with respect to player i if there exist two actions bi, bi for player i such that for all profiles b−i of actions of the other players, the allocation scheme is identical whether player i reports bi or bi (i.e., ∀b−i, t(bi, b−i) = t(bi, b−i)).
For example, a 2-player mechanism is degenerate with respect to the rows player, if there are two rows with identical allocation in the matrix representation of the game.
Definition 12. A 2-player 2-alternative mechanism with k-possible actions is called diagonal if it is monotone, and non-degenerate with respect to at least one of the players.
The term diagonal originates from the matrix representation of these mechanisms, in which one of the diagonals determines the boundary between the choice of the two alternatives (see Figure 1). Simple combinatorial considerations show that diagonal mechanisms may come in very few forms. Interestingly, one of these forms is degenerate with respect to one of the players; that is, it can be described as a mechanism with k − 1 actions for this player.
Proposition 2. Any diagonal 2-player mechanism has one of the following forms:
B i A for i = 1, 2) then either (a). t(b1, b2) = B iff b1 + b2 ≥ k − 1 (b). t(b1, b2) = B iff b1 + b2 ≥ k.
A 1 B and B 2 A) then either (a). t(b1, b2) = B iff b1 ≥ b2 (b). t(b1, b2) = B iff b1 > b2.
In both cases, the optimal mechanism can also take the form of one of the possibilities described, except one of the players is not allowed to choose the fixed allocation action.
To complete the description of the optimal allocation scheme, we now move to determine the optimal strategies in diagonal mechanisms. We define the notion of mutuallymaximizer thresholds, and show that threshold strategies based on such thresholds are optimal. The reason why mutually-maximizer strategies maximize the expected social value in monotone mechanisms is intuitive: Consider some action i (row in the matrix representation) for player 1. In a monotone mechanism, the allocation in such a row will be of the form [A, A, ..., B, B] (assuming that B 2 A). That is, the alternative A will be chosen for low actions of player 2, and the alternative B will be chosen for higher actions of player 2. By determining a threshold for player 2, the social planner actually determines the minimal type of player 2 from which the alternative B will be chosen. For optimizing the expected social value, this type for player 2 should clearly be the type for which the expected social value from A equals the expected social value from B (given that player 1 plays i); for greater values of player 2, the single-crossing condition ensures that B will be preferred.
Definition 13. Consider a monotone 2-player mechanism g that is non-degenerate with respect to the two players, where the players use threshold strategies based on the threshold vectors x, y. We say that the threshold xi of one player (w.l.o.g. player 1) is a maximizer if Eθ2 ( g(xi, θ2, A) | θ2 ∈ [yj , yj+1] ) = Eθ2 ( g(xi, θ2, B) | θ2 ∈ [yj , yj+1] ) where j is the action of player 2 for which the mechanism swaps the chosen alternative exactly when player 1 plays i, i.e., t(i, j) = t(i − 1, j) (we denote, w.l.o.g., t(i, j) = A, t(i − 1, j) = B).
The threshold vectors x, y are called mutually maximizers if all their thresholds are maximizers (except the first and the last).
It turns out that in 2-player, 2-alternative environments, where the social-choice rule is multilinear and single crossing, the optimal expected social value is achieved in diagonal mechanisms with mutually-maximizer strategies. In the proof, we start with a k × k allocation matrix, and show that the mechanism cannot be degenerate with respect to one of the players (we show how to choose this player). If the player, w.l.o.g., the columns player, is degenerate, then there are two columns with an identical allocation. These two columns can be unified to a single action, and the mechanism can therefore be described as a k × k − 1 matrix. We then show that we can insert a new missing column, and an appropriately chosen threshold, and strictly increase the expected social value in the mechanism. Therefore, the original mechanism was not the optimal k-action mechanism.
Theorem 3. In environments with two alternatives and two players, if the social-value function is multilinear and single crossing, then the optimal k-action mechanism is diagonal, and the optimum is achieved with threshold strategies that are mutually maximizers.
A corollary from the proof of Theorem 1 is that the optimal 2-player k-action mechanism may be degenerate for one of the players (that is, equivalent to a game where one of the players has only k − 1 different actions). However, the proof identifies the following sufficient condition under which the 68 0 1 2 3 0 A A A B 1 A A B B 2 A B B B 3 B B B B 0 1 2 3 0 A A A A 1 A A A B 2 A A B B 3 A B B B 0 1 2 3 0 B B B B 1 A B B B 2 A A B B 3 A A A B 0 1 2 3 0 A A A B 1 A A B B 2 A B B B Figure 1: The three left tables show all possible diagonal allocation scheme with 4 possible actions for each player.
The rightmost table show an example for a diagonal allocation scheme where one of the player has only k − 1 possible actions. optimal mechanism will be non-degenerate with respect to both players: if the players" preferences are correlated (e.g.,
A 1 B and A 2 B), then the optimal alternative must be the same under the profiles (θ1, θ2) and (θ1, θ2). Similarly, if the players" preferences are conflicting (e.g., A 1 B and B 2 A), then the optimal alternative must be the same under the profiles (θ1, θ2) and (θ1, θ2). Examples in which this condition holds are the public good model presented in section 5 and auctions [5].
We do not know how to give an exact characterization of the optimal mechanisms in multi-player and multi-alternati ve environments. The hardness stems from the fact that the necessary conditions we specified before for the optimality of the mechanisms (i.e., non-degenrate and monotone allocations) are not restrictive enough for the general model. In other words, for n > 2 players, the number of monotone and non-degenerate mechanisms becomes exponential in n.
Proposition 3. The number of monotone non- degenerate k-action mechanisms in an n-player game is exponential in n, even if |A| = 2.
Our results apply to a variety of economic, computational and networked settings. In this section, we demonstrate the applicability of our results to public-good models, signaling games and routing applications.
The public-good model deals with a social planner (e.g., government) that needs to decide whether to supply a public good, such as building a bridge. Let Y es and No denote the respective alternatives of building and not building the bridge. v = v1, . . . , vn is the vector of the players" typesthe values they gain from using the bridge. The decision that maximizes the social welfare is to build the bridge if and only if P i vi is greater than its cost, denoted by C. If the bridge is built, the social welfare is P i vi − C, and zero otherwise; thus, g(v, Y es) = P i vi − C, and g(v, No) = 0. The utility of player i under payment pi is ui = vi − pi if the bridge is built, and 0 otherwise. It is well-known that under no restriction on the action space, it is possible to induce truthful revelation by VCG mechanisms, therefore full efficiency can be achieved. Obviously, when the action set is limited to k actions, we cannot achieve full efficiency due to the informational constraints. Yet, since g(v, Y es) and g(v, No) are multilinear and single crossing, we can directly apply Theorem 1. Hence, the information-theoretically optimal kaction mechanism is implementable in dominant strategies.
Corollary 2. The k-action informationally optimal social welfare in the n-player public-good game is implementable in dominant strategies.
Moreover, as Theorem 3 suggests, in the k-action 2-player public-good game, we can fully characterize the optimal mechanisms. In the proof of Theorem 3, we saw that when for both players g(θi, θi, A) = g(θi, θi, B), the mechanism is non-degenerate with respect to both players.6 This condition clearly holds here (1+ 0− C = 0+ 1− C), therefore the optimal mechanisms will use all k actions.
Corollary 3. The optimal expected welfare in a 2-player k-action public-good game is achieved with one of the following mechanisms:7
Strategies: Threshold strategies based on the vectors −→x ,−→y where for every 1 ≤ i ≤ k-1, xi = C − E[v2|v2 ∈ [yk−i, yk−i+1]] yi = C − E[v1|v1 ∈ [xk−i, xk−i+1]]
Strategies: Threshold strategies based on the vectors −→x ,−→y where for every 1 ≤ i ≤ k-1: xi = C − E[v2|v2 ∈ [yk−i−1, yk−i]] yi = C − E[v1|v1 ∈ [xk−i−1, xk−i]] Recall that we define the optimal mechanisms by their allocation scheme and by the optimal strategies for the players. It is well known, that the allocation scheme in monotone mechanisms uniquely defines the payments that ensure incentive-compatibility. In public-good games, these payments satisfy the rule that a player pays his lowest value for which the bridge is built, when the action of the other player is fixed. Therefore, the payments for the players 1 and 2 reporting the actions b1 and b2 are as follows: in mechanism 1 from Proposition 3, p1 = xb2 and p2 = yb1 ; in mechanism 2 from Proposition 3, p1 = xb2−1 and p2 = yb1−1.
We now show a more specific example that assumes uniform distributions. The example shows how the optimal mechanism is determined by the cost C: for low costs, mechanism of type 1 is optimal, and for high costs the optimal mechanism is of type 2. An additional interesting feature of the optimal mechanisms in the example is that they are symmetric with respect to the players. This come as opposed to the optimal mechanisms in the auction model [5] that are asymmetric (even when the players" values are drawn from identical distributions). 6 More precisely, the condition for non-degeneracy when B 1 A and B 2 A is that sign(g(θi, θi, A)−g(θi, θi, B)) = sign(g(θi, θi, A) − g(θi, θi, B)) (when sign(0) is considered both negative and positive). 7 We denote x0 = y0 = 0 and xk = yk = 1. 69 C ≤ 1 0 1 0 No p1 = p2 = 0 No p1 = p2 = 0 1 No p1 = p2 = 0 Yes p1 = p2 = 2 3 C − 1 3 C ≥ 1 0 1 0 No p1 = p2 = 0 Yes p1 = 0; p2 = 2C 3 1 Yes p1 = 2C 3 ; p2 = 0 Yes p1 = p2 = 0 Figure 2: Optimal mechanisms in a 2-player, 2-alternative, 2-action public-goods game, when the types are uniformly distributed in [0, 1]. The mechanism on the left is optimal when C ≤ 1 and the other is optimal when C ≥ 1.
Example 1. Suppose that the types of both players are uniformly distributed on [0, 1]. Figure 2 illustrates the optimal mechanisms for k = 2, and shows how both the allocation scheme and the payments depend on the construction cost C. Then, the welfare-maximizing mechanisms are: • If the cost of building is at least 1: Allocation: Build iff b1 + b2 ≥ k Strategies: The thresholds of both players are (for i = {1, . . . , k − 1}), xi = 2(k−i)·C 2k−1 − 2k−4i+1 2k−1 • If the cost of building is smaller than 1: Allocation: Build iff b1 + b2 ≥ k − 1 Strategies: The thresholds of both players are (for i = {1, . . . , k − 1}), xi = 2iC 2k−1
We now study a signaling model in labor markets. In this model, the type of each worker, θi ∈ [θ, θ], describes the worker"s productivity level. The firm wants to make her hiring decisions according to a decision function f( −→ θ ). For example, the firm may want to hire the most productive worker (like the auction model), or hire a group of workers only if their sum of productivities is greater than some threshold (similar to the public-good model). However, the worker"s productivity is invisible to the firm; the firm only observes the worker"s education level e that should convey signals about her productivity level. Note that the assumption here is that acquiring education, at any level, does not affect the productivity of the worker, but only signals about the worker"s skills.
A main component in this model, is the fact that as the worker is more productive, it is easier for him to acquire high-level education. In addition, the cost of acquiring education increases with the education level. More formally, a continuous function C(e, θ) describes the cost to a worker from acquiring each education level as a function of his productivity. The standard assumptions about the cost function are: ∂C ∂e > 0, ∂C ∂θ < 0, ∂C ∂e∂θ < 0, where the latter requirement is exactly equivalent to the single-crossing property (when C is differentiable in both variables). The utility of a worker is determined according to the education level he chooses and the wage w(e) attached to this education level, that is, ui(e, θi) = −C(θi, e) + w(e).
An action for a worker in this game is the education level he chooses to acquire. In standard models, this action space is continuous, and then a fully separating equilibrium exists (under the single-crossing conditions on the cost function). That is, there exists an equilibrium in which every type is mapped into a different education level; thus, the firm can induce the exact productivity levels of the workers by this signaling mechanism. However, it is hard to imagine a world with a continuum of education levels. It is usually the case that there are only several discrete education levels (e.g., BSc, MSc, PhD).
With k education levels, the firm may not be able to exactly follow the decision function f. For achieving the best result in k actions, the firm may want the workers to play according to specific threshold strategies. It turns out that the standard condition, the single-crossing condition on the cost function, suffices for ensuring that these threshold strategies will be dominant for the players. We can now apply Theorem 2, and show that if the decision function f of the firm is multilinear (i.e., the decisions are made to maximize a set of multilinear functions), then the firm can design the education system such that the expected loss will be O( 1 k2 ), with a dominant-strategy equilibrium. Note that while in the classic example of the job market it is not reasonable for each firm to select the education level, in other reasonable applications the social planners may be able to determine the thresholds, e.g., by fixing the levels of qualifying exams or other means for the players to demonstrate their skills.
Corollary 4. Consider a multilinear decision function f, and a single-crossing cost function for the players. With k education levels, the firm can implement in dominant strategies a decision function that incurs a loss of O( 1 k2 ) compared with the decision function f.
In our last example, we show the applicability of our results to routing in lossy networks. In such systems, a sender needs to decide through which network to transmit his message. It is natural to assume that the agents (i.e., links) may not be able to report their accurate probabilities of success, but only, e.g., whether these are low, intermediate, or high. In this example, we focus on parallel-path networks.
Let N1, N2 denote two networks, where each network is composed of multiple parallel paths with variable lengths from a given source to a given sink (an example for such a network appears in Figure 3). The edges in these networks are controlled by different selfish agents, and each edge appears only in one of the networks. Suppose that the sender, who wishes to send a message from the source to the sink, knows the topology of each network, but the probability of success on each link, pi, is the link"s private information.
The problem of the sender is to decide whether to send a message through the network N1 or through an alternate network N2. Obviously, the sender wishes to send the message through N1 only if the total probability of success in N1 is greater than the success probability in N2. Let fN (−→p ) denote the probability of success in network N with a successprobability vector −→p . The social choice function in this example is thus: c(−→p ) ∈ argmax{N1,N2}{fN1 (−→p ), fN2 (−→p )}. 70 p5p4 p3 p2p1 s t Figure 3: An example for a parallel-path network, where each link has a probability pi for transmission success. We show that the overall probability of success in such networks is multilinear in pi, and thus the optimal k-action social-choice function is dominant-strategy implementable.
In this example, we assume that every agent has a singlecrossing valuation function over the alternatives. That is, each player wishes that the message will be sent through his network, and his benefit is positively correlated with his secret data (e.g., the valuation of player i may be exactly pi). We would like to emphasize that the social planner in this example (the sender) does not aim to maximize the social welfare. That is, the social value is not the sum of the players" types nor any weighted sum of the types (affine maximizer).
The success probability of sending a message through a parallel-path network is multilinear, since it can be expressed by the following multilinear formula (where P denotes the set of all paths between the source and the sink): 1 − Y P ∈P (1 − Y j∈P pj) (3) For example, in the network presented in figure 3, the probability of success is given by f(−→p ) = 1 − (1 − p1p2) · (1 − p3) · (1 − p4p5) Thus, if all the candidate networks are parallel-path networks, the social-value function is multilinear, and we can apply Theorem 1 and get the following corollary. Note that for every link i, the partial derivative in pi of the success probability written in Equation 3 is positive. In all the other networks, that do not contain link i, the partial derivative is clearly zero. Therefore, the social-value function is single crossing and our general results can be applied.
Corollary 5. For any social-choice function that maximizes the success probability over parallel-path networks, the informationally optimal k-action social-choice function is implementable (for any k).
Acknowledgment. We thank Noam Nisan for helpful discussions and anonymous referees for helpful comments. This work is supported by grants from the Israeli Academy of Sciences and the USA-Israel Binational Science Foundation.
The work of the second author is also supported by the Lady Davis Trust Fellowship.
[1] S. Athey. Single crossing properties and the existence of pure strategy equilibria in games of incomplete information. Econometrica, 69(4):861-89, 2001. [2] M. Babaioff and L. Blumrosen.
Computationally-feasible auctions for convex bundles.
In APPROX 04, pages 27-38, 2004. [3] M. Babaioff, R. Lavi, and E. Pavlov. Mechanism design for single-value domains. In AAAI"05, pages 241-247, 2005. [4] L. Blumrosen and N. Nisan. Auctions with severely bounded communications. 43th Annual Symposium on Foundations of Computer Science (FOCS 2002), 2002. [5] L. Blumrosen, N. Nisan, and I. Segal. Auctions with severely bounded communications. Working paper,
The Hebrew University. Preliminary versions appeared in FOCS 2002 and ESA 03., 2003. [6] L. Blumrosen, N. Nisan, and I. Segal. Multi-player and multi-round auctions with severely bounded communication. ESA 2003, 2003. [7] M. Chwe. The discrete bid first price auction. In Economics Letters, volume 31, pages 303-306, 1989. [8] E. David, A. Rogers, J. Schiff, S. Kraus, and N. Jennings. Optimal design of english auctions with discrete bid levels. In EC 05. [9] R. M. Harstad and M. H. Rothkopf. On the role of discrete bid levels in oral auctions. Europian Journal of Operations Research, pages 572-581, 1994. [10] B. E. Hermalin. Lecture notes in economics, 2005. [11] A. Kress and C. Boutilier. A study of limited precision, incremental elicitation in auctions. In AAMAS 04. [12] K. Larson and T. Sandholm. Costly valuation computation in auctions. In 8th conference of theoretical aspects of knowledge and rationality, 2001. [13] R. Lavi, A. Mu"alem, and N. Nisan. Towards a characterization of truthful combinatorial auctions. In In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2003.,
[14] P. McAfee. Coarse matching. Econometrica, 70(5):2025-2034, 2002. [15] R. B. Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):58-73, 1981. [16] M. Naor, B. Pinkas, and R. Summer. Privacy preserving auctions and mechanism design. In ACM Conference on Electronic Commerce, 1999. [17] T. Sandholm and A. Gilpin. Sequences of take-it-or-leave-it offers: Near-optimal auctions without full-valuation revelation. In AMEC-V, 2003. [18] M. A. Satterthwaite and S. R. Williams. The optimality of a simple market mechanism.
Econometrica, 70(5):1841-1863, 2002. [19] R. Wilson. Efficient and competitive rationing.

In multiagent systems with self-interested agents (including most economic settings), the optimal action for one agent to take depends on the actions that the other agents take.
To analyze how an agent should behave in such settings, the tools of game theory need to be applied. Typically, when a strategic setting is modeled in the framework of game theory, it is assumed that players choose their strategies simultaneously. This is especially true when the setting is modeled as a normal-form game, which only specifies each agent"s utility as a function of the vector of strategies that the agents choose, and does not provide any information on the order in which agents make their decisions and what the agents observe about earlier decisions by other agents.
Given that the game is modeled in normal form, it is typically analyzed using the concept of Nash equilibrium. A Nash equilibrium specifies a strategy for each player, such that no player has an incentive to individually deviate from this profile of strategies. (Typically, the strategies are allowed to be mixed, that is, probability distributions over the original (pure) strategies.) A (mixed-strategy) Nash equilibrium is guaranteed to exist in finite games [18], but one problem is that there may be multiple Nash equilibria. This leads to the equilibrium selection problem of how an agent can know which strategy to play if it does not know which equilibrium is to be played.
When the setting is modeled as an extensive-form game, it is possible to specify that some players receive some information about actions taken by others earlier in the game before deciding on their action. Nevertheless, in general, the players do not know everything that happened earlier in the game. Because of this, these games are typically still analyzed using an equilibrium concept, where one specifies a mixed strategy for each player, and requires that each player"s strategy is a best response to the others" strategies. (Typically an additional constraint on the strategies is now imposed to ensure that players do not play in a way that is irrational with respect to the information that they have received so far. This leads to refinements of Nash equilibrium such as subgame perfect and sequential equilibrium.) However, in many real-world settings, strategies are not selected in such a simultaneous manner. Oftentimes, one player (the leader) is able to commit to a strategy before another player (the follower). This can be due to a variety of reasons. For example, one of the players may arrive at the site at which the game is to be played before another agent (e.g., in economic settings, one player may enter a market earlier and commit to a way of doing busi82 ness). Such commitment power has a profound impact on how the game should be played. For example, the leader may be best off playing a strategy that is dominated in the normal-form representation of the game. Perhaps the earliest and best-known example of the effect of commitment is that by von Stackelberg [25], who showed that, in Cournot"s duopoly model [5], if one firm is able to commit to a production quantity first, that firm will do much better than in the simultaneous-move (Nash) solution. In general, if commitment to mixed strategies is possible, then (under minor assumptions) it never hurts, and often helps, to commit to a strategy [26]. Being forced to commit to a pure strategy sometimes helps, and sometimes hurts (for example, committing to a pure strategy in rock-paper-scissors before the other player"s decision will naturally result in a loss). In this paper, we will assume commitment is always forced; if it is not, the player who has the choice of whether to commit can simply compare the commitment outcome to the non-commitment (simultaneous-move) outcome.
Models of leadership are especially important in settings with multiple self-interested software agents. Once the code for an agent (or for a team of agents) is finalized and the agent is deployed, the agent is committed to playing the (possibly randomized) strategy that the code prescribes. Thus, as long as one can credibly show that one cannot change the code later, the code serves as a commitment device. This holds true for recreational tournaments among agents (e.g., poker tournaments, RoboSoccer), and for industrial applications such as sensor webs.
Finally, there is also an implicit leadership situation in the field of mechanism design, in which one player (the designer) gets to choose the rules of the game that the remaining players then play. Mechanism design is an extremely important topic to the EC community: the papers published on mechanism design in recent EC conferences are too numerous to cite. Indeed, the mechanism designer may benefit from committing to a choice that, if the (remaining) agents" actions were fixed, would be suboptimal. For example, in a (first-price) auction, the seller may wish to set a positive (artificial) reserve price for the item, below which the item will not be sold-even if the seller values the item at 0. In hindsight (after the bids have come in), this (na¨ıvely) appears suboptimal: if a bid exceeding the reserve price came in, the reserve price had no effect, and if no such bid came in, the seller would have been better off accepting a lower bid. Of course, the reason for setting the reserve price is that it incentivizes the bidders to bid higher, and because of this, setting artificial reserve prices can actually increase expected revenue to the seller.
A significant amount of research has recently been devoted to the computation of solutions according to various solution concepts for settings in which the agents choose their strategies simultaneously, such as dominance [7, 11, 3] and (especially) Nash equilibrium [8, 21, 16, 15, 2, 22, 23, 4]. However, the computation of the optimal strategy to commit to in a leadership situation has gone ignored.
Theoretically, leadership situations can simply be thought of as an extensive-form game in which one player chooses a strategy (for the original game) first. The number of strategies in this extensive-form game, however, can be exceedingly large. For example, if the leader is able to commit to a mixed strategy in the original game, then every one of the (continuum of) mixed strategies constitutes a pure strategy in the extensive-form representation of the leadership situation. (We note that a commitment to a distribution is not the same as a distribution over commitments.) Moreover, if the original game is itself an extensive-form game, the number of strategies in the extensive-form representation of the leadership situation (which is a different extensive-form game) becomes even larger. Because of this, it is usually not computationally feasible to simply transform the original game into the extensive-form representation of the leadership situation; instead, we have to analyze the game in its original representation.
In this paper, we study how to compute the optimal strategy to commit to, both in normal-form games (Section 2) and in Bayesian games, which are a special case of extensiveform games (Section 3).
In this section, we study how to compute the optimal strategy to commit to for games represented in normal form.
In a normal-form game, every player i ∈ {1, . . . , n} has a set of pure strategies (or actions) Si, and a utility function ui : S1×S2×. . .×Sn → R that maps every outcome (a vector consisting of a pure strategy for every player, also known as a profile of pure strategies) to a real number. To ease notation, in the case of two players, we will refer to player 1"s pure strategy set as S, and player 2"s pure strategy set as T. Such games can be represented in (bi-)matrix form, in which the rows correspond to player 1"s pure strategies, the columns correspond to player 2"s pure strategies, and the entries of the matrix give the row and column player"s utilities (in that order) for the corresponding outcome of the game. In the case of three players, we will use R, S, and T, for player 1, 2, and 3"s pure strategies, respectively. A mixed strategy for a player is a probability distribution over that player"s pure strategies. In the case of two-player games, we will refer to player 1 as the leader and player 2 as the follower.
Before defining optimal leadership strategies, consider the following game which illustrates the effect of the leader"s ability to commit. 2, 1 4, 0 1, 0 3, 1 In this normal-form representation, the bottom strategy for the row player is strictly dominated by the top strategy.
Nevertheless, if the row player has the ability to commit to a pure strategy before the column player chooses his strategy, the row player should commit to the bottom strategy: doing so will make the column player prefer to play the right strategy, leading to a utility of 3 for the row player.
By contrast, if the row player were to commit to the top strategy, the column player would prefer to play the left strategy, leading to a utility of only 2 for the row player. If the row player is able to commit to a mixed strategy, then she can get an even greater (expected) utility: if the row player commits to placing probability p > 1/2 on the bottom strategy, then the column player will still prefer to play the right strategy, and the row player"s expected utility will be 3p + 4(1 − p) = 4 − p ≥ 3. If the row player plays each strategy with probability exactly 1/2, the column player is 83 indifferent between the strategies. In such cases, we will assume that the column player will choose the strategy that maximizes the row player"s utility (in this case, the right strategy). Hence, the optimal mixed strategy to commit to for the row player is p = 1/2. There are a few good reasons for this assumption. If we were to assume the opposite, then there would not exist an optimal strategy for the row player in the example game: the row player would play the bottom strategy with probability p = 1/2 + with > 0, and the smaller , the better the utility for the row player.
By contrast, if we assume that the follower always breaks ties in the leader"s favor, then an optimal mixed strategy for the leader always exists, and this corresponds to a subgame perfect equilibrium of the extensive-form representation of the leadership situation. In any case, this is a standard assumption for such models (e.g. [20]), although some work has investigated what can happen in the other subgame perfect equilibria [26]. (For generic two-player games, the leader"s subgame-perfect equilibrium payoff is unique.) Also, the same assumption is typically used in mechanism design, in that it is assumed that if an agent is indifferent between revealing his preferences truthfully and revealing them falsely, he will report them truthfully. Given this assumption, we can safely refer to optimal leadership strategies rather than having to use some equilibrium notion.
Hence, for the purposes of this paper, an optimal strategy to commit to in a 2-player game is a strategy s ∈ S that maximizes maxt∈BR(s) ul(s, t), where BR(s) = arg maxt∈T uf (s, t). (ul and uf are the leader and follower"s utility functions, respectively.) We can have S = S for the case of commitment to pure strategies, or S = ∆(S), the set of probability distributions over S, for the case of commitment to mixed strategies. (We note that replacing T by ∆(T) makes no difference in this definition.) For games with more than two players, in which the players commit to their strategies in sequence, we define optimal strategies to commit to recursively. After the leader commits to a strategy, the game to be played by the remaining agents is itself a (smaller) leadership game. Thus, we define an optimal strategy to commit to as a strategy that maximizes the leader"s utility, assuming that the play of the remaining agents is itself optimal under this definition, and maximizes the leader"s utility among all optimal ways to play the remaining game. Again, commitment to mixed strategies may or may not be a possibility for every player (although for the last player it does not matter if we allow for commitment to mixed strategies).
We first study how to compute the optimal pure strategy to commit to. This is relatively simple, because the number of strategies to commit to is not very large. (In the following, #outcomes is the number of complete strategy profiles.) Theorem 1. Under commitment to pure strategies, the set of all optimal strategy profiles in a normal-form game can be found in O(#players · #outcomes) time.
Proof. Each pure strategy that the first player may commit to will induce a subgame for the remaining players. We can solve each such subgame recursively to find all of its optimal strategy profiles; each of these will give the original leader some utility. Those that give the leader maximal utility correspond exactly to the optimal strategy profiles of the original game.
We now present the algorithm formally. Let Su(G, s1) be the subgame that results after the first (remaining) player in G plays s1 ∈ SG 1 . A game with 0 players is simply an outcome of the game. The function Append(s, O) appends the strategy s to each of the vectors of strategies in the set O. Let e be the empty vector with no elements. In a slight abuse of notation, we will write uG 1 (C) when all strategy profiles in the set C give player 1 the same utility in the game G. (Here, player 1 is the first remaining player in the subgame G, not necessarily player 1 in the original game.) We note that arg max is set-valued. Then, the following algorithm computes all optimal strategy profiles: Algorithm Solve(G) if G has 0 players return {e} C ← ∅ for all s1 ∈ SG 1 { O ← Solve(Su(G, s1)) O ← arg maxo∈O uG 1 (s1, o) if C = ∅ or uG 1 (s1, O ) = uG 1 (C) C ← C∪Append(s1, O ) if uG 1 (s1, O ) > uG 1 (C) C ←Append(s1, O ) } return C Every outcome is (potentially) examined by every player, which leads to the given runtime bound.
As an example of how the algorithm works, consider the following 3-player game, in which the first player chooses the left or right matrix, the second player chooses a row, and the third player chooses a column. 0,1,1 1,1,0 1,0,1 2,1,1 3,0,1 1,1,1 0,0,1 0,0,0 3,3,0 3,3,0 0,2,0 3,0,1 4,4,2 0,0,2 0,0,0 0,5,1 0,0,0 3,0,0 First we eliminate the outcomes that do not correspond to best responses for the third player (removing them from the matrix): 0,1,1 1,0,1 2,1,1 3,0,1 1,1,1 0,0,1 3,0,1 4,4,2 0,0,2 0,5,1 Next, we remove the entries in which the third player does not break ties in favor of the second player, as well as entries that do not correspond to best responses for the second player. 0,1,1 2,1,1 1,1,1 0,5,1 Finally, we remove the entries in which the second and third players do not break ties in favor of the first player, as well as entries that do not correspond to best responses for the first player. 2,1,1 84 Hence, in optimal play, the first player chooses the left matrix, the second player chooses the middle row, and the third player chooses the left column. (We note that this outcome is Pareto-dominated by (Right, Middle, Left).) For general normal-form games, each player"s utility for each of the outcomes has to be explicitly represented in the input, so that the input size is itself Ω(#players · #outcomes).
Therefore, the algorithm is in fact a linear-time algorithm.
In the special case of two-player zero-sum games, computing an optimal mixed strategy for the leader to commit to is equivalent to computing a minimax strategy, which minimizes the maximum expected utility that the opponent can obtain. Minimax strategies constitute the only natural solution concept for two-player zero-sum games: von Neumann"s Minimax Theorem [24] states that in two-player zero-sum games, it does not matter (in terms of the players" utilities) which player gets to commit to a mixed strategy first, and a profile of mixed strategies is a Nash equilibrium if and only if both strategies are minimax strategies. It is well-known that a minimax strategy can be found in polynomial time, using linear programming [17]. Our first result in this section generalizes this result, showing that an optimal mixed strategy for the leader to commit to can be efficiently computed in general-sum two-player games, again using linear programming.
Theorem 2. In 2-player normal-form games, an optimal mixed strategy to commit to can be found in polynomial time using linear programming.
Proof. For every pure follower strategy t, we compute a mixed strategy for the leader such that 1) playing t is a best response for the follower, and 2) under this constraint, the mixed strategy maximizes the leader"s utility. Such a mixed strategy can be computed using the following simple linear program: maximize s∈S psul(s, t) subject to for all t ∈ T, s∈S psuf (s, t) ≥ s∈S psuf (s, t ) s∈S ps = 1 We note that this program may be infeasible for some follower strategies t, for example, if t is a strictly dominated strategy. Nevertheless, the program must be feasible for at least some follower strategies; among these follower strategies, choose a strategy t∗ that maximizes the linear program"s solution value. Then, if the leader chooses as her mixed strategy the optimal settings of the variables ps for the linear program for t∗ , and the follower plays t∗ , this constitutes an optimal strategy profile.
In the following result, we show that we cannot expect to solve the problem more efficiently than linear programming, because we can reduce any linear program with a probability constraint on its variables to a problem of computing the optimal mixed strategy to commit to in a 2-player normalform game.
Theorem 3. Any linear program whose variables xi (with xi ∈ R≥0 ) must satsify i xi = 1 can be modeled as a problem of computing the optimal mixed strategy to commit to in a 2-player normal-form game.
Proof. Let the leader have a pure strategy i for every variable xi. Let the column player have one pure strategy j for every constraint in the linear program (other than i xi = 1), and a single additional pure strategy 0. Let the utility functions be as follows. Writing the objective of the linear program as maximize i cixi, for any i, let ul(i, 0) = ci and uf (i, 0) = 0. Writing the jth constraint of the linear program (not including i xi = 1) as i aijxi ≤ bj, for any i, j > 0, let ul(i, j) = mini ci − 1 and uf (i, j) = aij − bj.
For example, consider the following linear program. maximize 2x1 + x2 subject to x1 + x2 = 1 5x1 + 2x2 ≤ 3 7x1 − 2x2 ≤ 2 The optimal solution to this program is x1 = 1/3, x2 = 2/3. Our reduction transforms this program into the following leader-follower game (where the leader is the row player). 2, 0 0, 2 0, 5 1, 0 0, -1 0, -4 Indeed, the optimal strategy for the leader is to play the top strategy with probability 1/3 and the bottom strategy with probability 2/3. We now show that the reduction works in general.
Clearly, the leader wants to incentivize the follower to play 0, because the utility that the leader gets when the follower plays 0 is always greater than when the follower does not play 0. In order for the follower not to prefer playing j > 0 rather than 0, it must be the case that i pl(i)(aij − bj) ≤ 0, or equivalently i pl(i)aij ≤ bj. Hence the leader will get a utility of at least mini ci if and only if there is a feasible solution to the constraints. Given that the pl(i) incentivize the follower to play 0, the leader attempts to maximize i pl(i)ci. Thus the leader must solve the original linear program.
As an alternative proof of Theorem 3, one may observe that it is known that finding a minimax strategy in a zerosum game is as hard as the linear programming problem [6], and as we pointed out at the beginning of this section, computing a minimax strategy in a zero-sum game is a special case of the problem of computing an optimal mixed strategy to commit to.
This polynomial-time solvability of the problem of computing an optimal mixed strategy to commit to in two-player normal-form games contrasts with the unknown complexity of computing a Nash equilibrium in such games [21], as well as with the NP-hardness of finding a Nash equilibrium with maximum utility for a given player in such games [8, 2].
Unfortunately, this result does not generalize to more than two players-here, the problem becomes NP-hard. To show this, we reduce from the VERTEX-COVER problem.
Definition 1. In VERTEX-COVER, we are given a graph G = (V, E) and an integer K. We are asked whether there 85 exists a subset of the vertices S ⊆ V , with |S| = K, such that every edge e ∈ E has at least one of its endpoints in S.
BALANCED-VERTEX-COVER is the special case of VERTEX-COVER in which K = |V |/2.
VERTEX-COVER is NP-complete [9]. The following lemma shows that the hardness remains if we require K = |V |/2. (Similar results have been shown for other NP-complete problems.) Lemma 1. BALANCED-VERTEX-COVER is NP-complete.
Proof. Membership in NP follows from the fact that the problem is a special case of VERTEX-COVER, which is in NP. To show NP-hardness, we reduce an arbitrary VERTEX-COVER instance to a BALANCED-VERTEXCOVER instance, as follows. If, for the VERTEX-COVER instance, K > |V |/2, then we simply add isolated vertices that are disjoint from the rest of the graph, until K = |V |/2.
If K < |V |/2, we add isolated triangles (that is, the complete graph on three vertices) to the graph, increasing K by 2 every time, until K = |V |/2.
Theorem 4. In 3-player normal-form games, finding an optimal mixed strategy to commit to is NP-hard.
Proof. We reduce an arbitrary BALANCED-VERTEXCOVER instance to the following 3-player normal-form game.
For every vertex v, each of the three players has a pure strategy corresponding to that vertex (rv, sv, tv, respectively). In addition, for every edge e, the third player has a pure strategy te; and finally, the third player has one additional pure strategy t0. The utilities are as follows: • for all r ∈ R, s ∈ S, u1(r, s, t0) = u2(r, s, t0) = 1; • for all r ∈ R, s ∈ S, t ∈ T−{t0}, u1(r, s, t) = u2(r, s, t) = 0; • for all v ∈ V, s ∈ S, u3(rv, s, tv) = 0; • for all v ∈ V, r ∈ R, u3(r, sv, tv) = 0; • for all v ∈ V , for all r ∈ R − {rv}, s ∈ S − {sv}, u3(r, s, tv) = |V | |V |−2 ; • for all e ∈ E, s ∈ S, for both v ∈ e, u3(rv, s, te) = 0; • for all e ∈ E, s ∈ S, for all v /∈ e, u3(rv, s, te) = |V | |V |−2 . • for all r ∈ R, s ∈ S, u3(r, s, t0) = 1.
We note that players 1 and 2 have the same utility function.
We claim that there is an optimal strategy profile in which players 1 and 2 both obtain 1 (their maximum utility) if and only if there is a solution to the BALANCED-VERTEXCOVER problem. (Otherwise, these players will both obtain
First, suppose there exists a solution to the BALANCEDVERTEX-COVER problem. Then, let player 1 play every rv such that v is in the cover with probability 2 |V | , and let player 2 play every sv such that v is not in the cover with probability 2 |V | . Then, for player 3, the expected utility of playing tv (for any v) is (1 − 2 |V | ) |V | |V |−2 = 1, because there is a chance of 2 |V | that rv or sv is played.
Additionally, the expected utility of playing te (for any e) is at most (1 − 2 |V | ) |V | |V |−2 = 1, because there is a chance of at least 2 |V | that some rv with v ∈ e is played (because player 1 is randomizing over the pure strategies corresponding to the cover). It follows that playing t0 is a best response for player 3, giving players 1 and 2 a utility of 1.
Now, suppose that players 1 and 2 obtain 1 in optimal play. Then, it must be the case that player 3 plays t0. Hence, for every v ∈ V , there must be a probability of at least 2 |V | that either rv or sv is played, for otherwise player 3 would be better off playing tv. Because players 1 and 2 have only a total probability of 2 to distribute, it must be the case that for each v, either rv or sv is played with probability 2 |V | , and the other is played with probability 0. (It is not possible for both to have nonzero probability, because then there would be some probability that both are played simultaneously (correlation is not possible), hence the total probability of at least one being played could not be high enough for all vertices.) Thus, for exactly half the v ∈ V , player 1 places probability 2 |V | on rv. Moreover, for every e ∈ E, there must be a probability of at least 2 |V | that some rv with v ∈ e is played, for otherwise player 3 would be better off playing te. Thus, the v ∈ V such that player 1 places probability 2 |V | on rv constitute a balanced vertex cover.
So far, we have restricted our attention to normal-form games. In a normal-form game, it is assumed that every agent knows every other agent"s preferences over the outcomes of the game. In general, however, agents may have some private information about their preferences that is not known to the other agents. Moreover, at the time of commitment to a strategy, the agents may not even know their own (final) preferences over the outcomes of the game yet, because these preferences may be dependent on a context that has yet to materialize. For example, when the code for a trading agent is written, it may not yet be clear how that agent will value resources that it will negotiate over later, because this depends on information that is not yet available at the time at which the code is written (such as orders that will have been placed to the agent before the negotiation). In this section, we will study commitment in Bayesian games, which can model such uncertainty over preferences.
In a Bayesian game, every player i has a set of actions Si, a set of types Θi with an associated probability distribution πi : Θi → [0, 1], and, for each type θi, a utility function uθi i : S1 × S2 × . . . × Sn → R. A pure strategy in a Bayesian game is a mapping from the player"s types to actions, σi : Θi → Si. (Bayesian games can be rewritten in normal form by enumerating every pure strategy σi, but this will cause an exponential blowup in the size of the representation of the game and therefore cannot lead to efficient algorithms.) The strategy that the leader should commit to depends on whether, at the time of commitment, the leader knows her own type. If the leader does know her own type, the other types that the leader might have had become irrelevant and the leader should simply commit to the strategy that is optimal for the type. However, as argued above, the leader does not necessarily know her own type at the time of commitment (e.g., the time at which the code is submitted).
In this case, the leader must commit to a strategy that is 86 dependent upon the leader"s eventual type. We will study this latter model, although we will pay specific attention to the case where the leader has only a single type, which is effectively the same as the former model.
It turns out that computing an optimal pure strategy to commit to is hard in Bayesian games, even with two players.
Theorem 5. Finding an optimal pure strategy to commit to in 2-player Bayesian games is NP-hard, even when the follower has only a single type.
Proof. We reduce an arbitrary VERTEX-COVER instance to the following Bayesian game between the leader and the follower. The leader has K types θ1, θ2, . . . , θK , each occurring with probability 1/K, and for every vertex v ∈ V , the leader has an action sv. The follower has only a single type; for each edge e ∈ E, the follower has an action te, and the follower has a single additional action t0. The utility function for the leader is given by, for all θl ∈ Θl and all s ∈ S, u θl l (s, t0) = 1, and for all e ∈ E, u θl l (s, te) = 0.
The follower"s utility is given by: • For all v ∈ V , for all e ∈ E with v /∈ e, uf (sv, te) = 1; • For all v ∈ V , for all e ∈ E with v ∈ e, uf (sv, te) = −K; • For all v ∈ V , uf (sv, t0) = 0.
We claim that the leader can get a utility of 1 if and only if there is a solution to the VERTEX-COVER instance.
First, suppose that there is a solution to the VERTEXCOVER instance. Then, the leader can commit to a pure strategy such that for each vertex v in the cover, the leader plays sv for some type. Then, the follower"s utility for playing te (for any e ∈ E) is at most K−1 K + 1 K (−K) = − 1 K , so that the follower will prefer to play t0, which gives the leader a utility of 1, as required.
Now, suppose that there is a pure strategy for the leader that will give the leader a utility of 1. Then, the follower must play t0. In order for the follower not to prefer playing te (for any e ∈ E) instead, for at least one v ∈ e the leader must play sv for some type θl. Hence, the set of vertices v that the leader plays for some type must constitute a vertex cover; and this set can have size at most K, because the leader has only K types. So there is a solution to the VERTEXCOVER instance.
However, if the leader has only a single type, then the problem becomes easy again (#types is the number of types for the follower): Theorem 6. In 2-player Bayesian games in which the leader has only a single type, an optimal pure strategy to commit to can be found in O(#outcomes · #types) time.
Proof. For every leader action s, we can compute, for every follower type θf ∈ Θf , which actions t maximize the follower"s utility; call this set of actions BRθf (s). Then, the utility that the leader receives for committing to action s can be computed as θf ∈Θf π(θf ) maxt∈BRθf (s) ul(s, t), and the leader can choose the best action to commit to.
In two-player zero-sum imperfect information games with perfect recall (no player ever forgets something that it once knew), a minimax strategy can be constructed in polynomial time [12, 13]. Unfortunately, this result does not extend to computing optimal mixed strategies to commit to in the general-sum case-not even in Bayesian games. We will exhibit NP-hardness by reducing from the INDEPENDENTSET problem.
Definition 2. In INDEPENDENT-SET, we are given a graph G = (V, E) and an integer K. We are asked whether there exists a subset of the vertices S ⊆ V , with |S| = K, such that no edge e ∈ E has both of its endpoints in S.
Again, this problem is NP-complete [9].
Theorem 7. Finding an optimal mixed strategy to commit to in 2-player Bayesian games is NP-hard, even when the leader has only a single type and the follower has only two actions.
Proof. We reduce an arbitrary INDEPENDENT-SET instance to the following Bayesian game between the leader and the follower. The leader has only a single type, and for every vertex v ∈ V , the leader has an action sv. The follower has a type θv for every v ∈ V , occurring with probability 1 (|E|+1)|V | , and a type θe for every e ∈ E, occurring with probability 1 |E|+1 . The follower has two actions: t0 and t1.
The leader"s utility is given by, for all s ∈ S, ul(s, t0) = 1 and ul(s, t1) = 0. The follower"s utility is given by: • For all v ∈ V , uθv f (sv, t1) = 0; • For all v ∈ V and s ∈ S − {sv}, uθv f (s, t1) = K K−1 ; • For all v ∈ V and s ∈ S, uθv f (s, t0) = 1; • For all e ∈ E, s ∈ S, uθe f (s, t0) = 1; • For all e ∈ E, for both v ∈ e, uθe f (sv, t1) = 2K 3 ; • For all e ∈ E, for all v /∈ e, uθe f (sv, t1) = 0.
We claim that an optimal strategy to commit to gives the leader an expected utility of at least |E| |E|+1 + K (|E|+1)|V | if and only if there is a solution to the INDEPENDENT-SET instance.
First, suppose that there is a solution to the INDEPENDENT-SET instance. Then, the leader could commit to the following strategy: for every vertex v in the independent set, play the corresponding sv with probability 1/K. If the follower has type θe for some e ∈ E, the expected utility for the follower of playing t1 is at most 1 K 2K 3 = 2/3, because there is at most one vertex v ∈ e such that sv is played with nonzero probability. Hence, the follower will play t0 and obtain a utility of 1. If the follower has type θv for some vertex v in the independent set, the expected utility for the follower of playing t1 is K−1 K K K−1 = 1, because the leader plays sv with probability 1/K. It follows that the follower (who breaks ties to maximize the leader"s utility) will play t0, which also gives a utility of 1 and gives the leader a higher utility. Hence the leader"s expected utility for this strategy is at least |E| |E|+1 + K (|E|+1)|V | , as required. 87 Now, suppose that there is a strategy that gives the leader an expected utility of at least |E| |E|+1 + K (|E|+1)|V | . Then, this strategy must induce the follower to play t0 whenever it has a type of the form θe (because otherwise, the utility could be at most |E|−1 |E|+1 + |V | (|E|+1)|V | = |E| |E|+1 < |E| |E|+1 + K (|E|+1)|V | ). Thus, it cannot be the case that for some edge e = (v1, v2) ∈ E, the probability that the leader plays one of sv1 and sv2 is at least 2/K, because then the expected utility for the follower of playing t1 when it has type θe would be at least 2 K 2K 3 = 4/3 > 1. Moreover, the strategy must induce the follower to play t0 for at least K types of the form θv.
Inducing the follower to play t0 when it has type θv can be done only by playing sv with probability at least 1/K, which will give the follower a utility of at most K−1 K K K−1 = 1 for playing t1. But then, the set of vertices v such that sv is played with probability at least 1/K must constitute an independent set of size K (because if there were an edge e between two such vertices, it would induce the follower to play t1 for type θe by the above).
By contrast, if the follower has only a single type, then we can generalize the linear programming approach for normalform games: Theorem 8. In 2-player Bayesian games in which the follower has only a single type, an optimal mixed strategy to commit to can be found in polynomial time using linear programming.
Proof. We generalize the approach in Theorem 2 as follows. For every pure follower strategy t, we compute a mixed strategy for the leader for every one of the leader"s types such that 1) playing t is a best response for the follower, and 2) under this constraint, the mixed strategy maximizes the leader"s ex ante expected utility. To do so, we generalize the linear program as follows: maximize θl∈Θl π(θl) s∈S pθl s uθl l (s, t) subject to for all t ∈ T, θl∈Θl π(θl) s∈S p θl s uf (s, t) ≥ θl∈Θl π(θl) s∈S p θl s uf (s, t ) for all θl ∈ Θl, s∈S p θl s = 1 As in Theorem 2, the solution for the linear program that maximizes the solution value is an optimal strategy to commit to.
This shows an interesting contrast between commitment to pure strategies and commitment to mixed strategies in Bayesian games: for pure strategies, the problem becomes easy if the leader has only a single type (but not if the follower has only a single type), whereas for mixed strategies, the problem becomes easy if the follower has only a single type (but not if the leader has only a single type).
RESEARCH In multiagent systems, strategic settings are often analyzed under the assumption that the players choose their strategies simultaneously. This requires some equilibrium notion (Nash equilibrium and its refinements), and often leads to the equilibrium selection problem: it is unclear to each individual player according to which equilibrium she should play. However, this model is not always realistic. In many settings, one player is able to commit to a strategy before the other player makes a decision. For example, one agent may arrive at the (real or virtual) site of the game before the other, or, in the specific case of software agents, the code for one agent may be completed and committed before that of another agent. Such models are synonymously referred to as leadership, commitment, or Stackelberg models, and optimal play in such models is often significantly different from optimal play in the model where strategies are selected simultaneously. Specifically, if commitment to mixed strategies is possible, then (optimal) commitment never hurts the leader, and often helps.
The recent surge in interest in computing game-theoretic solutions has so far ignored leadership models (with the exception of the interest in mechanism design, where the designer is implicitly in a leadership position). In this paper, we studied how to compute optimal strategies to commit to under both commitment to pure strategies and commitment to mixed strategies, in both normal-form and Bayesian games. For normal-form games, we showed that the optimal pure strategy to commit to can be found efficiently for any number of players. An optimal mixed strategy to commit to in a normal-form game can be found efficiently for two players using linear programming (and no more efficiently than that, in the sense that any linear program with a probability constraint can be encoded as such a problem). (This is a generalization of the polynomial-time computability of minimax strategies in normal-form games.) The problem becomes NP-hard for three (or more) players. In Bayesian games, the problem of finding an optimal pure strategy to commit to is NP-hard even in two-player games in which the follower has only a single type, although two-player games in which the leader has only a single type can be solved efficiently. The problem of finding an optimal mixed strategy to commit to in a Bayesian game is NP-hard even in two-player games in which the leader has only a single type, although two-player games in which the follower has only a single type can be solved efficiently using a generalization of the linear progamming approach for normal-form games.
The following two tables summarize these results. 2 players ≥ 3 players normal-form O(#outcomes) O(#outcomes· #players) Bayesian, O(#outcomes· NP-hard 1-type leader #types) Bayesian, NP-hard NP-hard 1-type follower Bayesian (general) NP-hard NP-hard Results for commitment to pure strategies. (With more than 2 players, the follower is the last player to commit, the leader is the first.) 88 2 players ≥ 3 players normal-form one LP-solve per NP-hard follower action Bayesian, NP-hard NP-hard 1-type leader Bayesian, one LP-solve per NP-hard 1-type follower follower action Bayesian (general) NP-hard NP-hard Results for commitment to mixed strategies. (With more than 2 players, the follower is the last player to commit, the leader is the first.) Future research can take a number of directions. First, we can empirically evaluate the techniques presented here on test suites such as GAMUT [19]. We can also study the computation of optimal strategies to commit to in other1  concise representations of normal-form games-for example, in graphical games [10] or local-effect/action graph games [14, 1]. For the cases where computing an optimal strategy to commit to is NP-hard, we can also study the computation of approximately optimal strategies to commit to. While the correct definition of an approximately optimal strategy is in this setting may appear simple at first-it should be a strategy that, if the following players play optimally, performs almost as well as the optimal strategy in expectation-this definition becomes problematic when we consider that the other players may also be playing only approximately optimally. One may also study models in which multiple (but not all) players commit at the same time.
Another interesting direction to pursue is to see if computing optimal mixed strategies to commit to can help us in, or otherwise shed light on, computing Nash equilibria.
Often, optimal mixed strategies to commit to are also Nash equilibrium strategies (for example, in two-player zero-sum games this is always true), although this is not always the case (for example, as we already pointed out, sometimes the optimal strategy to commit to is a strictly dominated strategy, which can never be a Nash equilibrium strategy).
[1] N. A. R. Bhat and K. Leyton-Brown. Computing Nash equilibria of action-graph games. In Proceedings of the 20th Annual Conference on Uncertainty in Artificial Intelligence (UAI), Banff, Canada, 2004. [2] V. Conitzer and T. Sandholm. Complexity results about Nash equilibria. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI), pages 765-771,
Acapulco, Mexico, 2003. [3] V. Conitzer and T. Sandholm. Complexity of (iterated) dominance. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 88-97, Vancouver, Canada, 2005. [4] V. Conitzer and T. Sandholm. A generalized strategy eliminability criterion and computational methods for applying it. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 483-488,
Pittsburgh, PA, USA, 2005. [5] A. A. Cournot. Recherches sur les principes math´ematiques de la th´eorie des richesses (Researches 1 Bayesian games are one potentially concise representation of normal-form games. into the Mathematical Principles of the Theory of Wealth). Hachette, Paris, 1838. [6] G. Dantzig. A proof of the equivalence of the programming problem and the game problem. In T. Koopmans, editor, Activity Analysis of Production and Allocation, pages 330-335. John Wiley & Sons,
[7] I. Gilboa, E. Kalai, and E. Zemel. The complexity of eliminating dominated strategies. Mathematics of Operation Research, 18:553-565, 1993. [8] I. Gilboa and E. Zemel. Nash and correlated equilibria: Some complexity considerations. Games and Economic Behavior, 1:80-93, 1989. [9] R. Karp. Reducibility among combinatorial problems.
In R. E. Miller and J. W. Thatcher, editors,
Complexity of Computer Computations, pages 85-103.
Plenum Press, NY, 1972. [10] M. Kearns, M. Littman, and S. Singh. Graphical models for game theory. In Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI), 2001. [11] D. E. Knuth, C. H. Papadimitriou, and J. N.
Tsitsiklis. A note on strategy elimination in bimatrix games. Operations Research Letters, 7(3):103-107,
[12] D. Koller and N. Megiddo. The complexity of two-person zero-sum games in extensive form. Games and Economic Behavior, 4(4):528-552, Oct. 1992. [13] D. Koller, N. Megiddo, and B. von Stengel. Efficient computation of equilibria for extensive two-person games. Games and Economic Behavior, 14(2):247-259,
[14] K. Leyton-Brown and M. Tennenholtz. Local-effect games. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence (IJCAI),
Acapulco, Mexico, 2003. [15] R. Lipton, E. Markakis, and A. Mehta. Playing large games using simple strategies. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 36-41, San Diego, CA, 2003. [16] M. Littman and P. Stone. A polynomial-time Nash equilibrium algorithm for repeated games. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 48-54, San Diego, CA,
[17] R. D. Luce and H. Raiffa. Games and Decisions. John Wiley and Sons, New York, 1957. Dover republication
[18] J. Nash. Equilibrium points in n-person games. Proc. of the National Academy of Sciences, 36:48-49, 1950. [19] E. Nudelman, J. Wortman, K. Leyton-Brown, and Y. Shoham. Run the GAMUT: A comprehensive approach to evaluating game-theoretic algorithms. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), New York, NY, USA,
[20] M. J. Osborne and A. Rubinstein. A Course in Game Theory. MIT Press, 1994. [21] C. Papadimitriou. Algorithms, games and the Internet. In Proceedings of the Annual Symposium on Theory of Computing (STOC), pages 749-753, 2001. 89 [22] R. Porter, E. Nudelman, and Y. Shoham. Simple search methods for finding a Nash equilibrium. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 664-669, San Jose, CA,
USA, 2004. [23] T. Sandholm, A. Gilpin, and V. Conitzer.
Mixed-integer programming methods for finding Nash equilibria. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 495-501,
Pittsburgh, PA, USA, 2005. [24] J. von Neumann. Zur Theorie der Gesellschaftsspiele.
Mathematische Annalen, 100:295-320, 1927. [25] H. von Stackelberg. Marktform und Gleichgewicht.

Graphical games were introduced in the papers of Kearns et al. [8] and Littman et al. [9] as a succinct representation of games with a large number of players. The classical normal form (or matrix form) representation has a size that is exponential in the number of players, making it unsuitable for large-scale distributed games.
A graphical game associates each player with a vertex of an underlying graph G, and the payoff to that player is a function of the actions chosen by himself and his neighbours in G; if G has low degree, this is a concise way to represent a game with many players.
The papers [8, 9] give a dynamic-programming algorithm for finding Nash equilibria in graphical games where there are two actions per player and G is a tree. The first of these papers describes a generic algorithm for this problem that can be specialized in two ways: as an algorithm that computes approximations to all Nash equilibria in time polynomial in the input size and the approximation quality, or as an exponential-time algorithm that allows the exact computation of all Nash equilibria in G. In [9], the authors propose a modification to the latter algorithm that aims to find a single Nash equilibrium in polynomial time. This does not quite work, as we show in Section 3, though it introduces a useful idea.
The generic algorithm of [8] consists of two phases which we will refer to as the upstream pass and the downstream pass; 1 the former starts at the leaves of the tree and ends at the root, while the latter starts at the root and ends at the leaves. It is assumed that each player has two pure strategies (actions), which are denoted by 0 and 1; it follows that any mixed strategy can be represented as a single number x ∈ [0, 1], where x is the probability that the player selects 1. During the upstream pass, each vertex V computes the set of its potential best responses to every mixed strategy w of its parent W ; a strategy v is a potential best response to w if 1 Note that the terminology upstream and downstream are reversed in [8, 9] - our trees are rooted at the top. 100 there is a Nash equilibrium in the graphical game downstream of V (inclusive) given that W plays w (for a more technical definition, the reader is referred to Section 2). The output of this stage can be viewed as a (continuous) table T(w, v), where T(w, v) = 1 if and only if v is a potential best response to w; we refer to this table as the best response policy for V . The generic algorithm does not address the problem of representing the best response policy; in fact, the most important difference between the two instantiations of the generic algorithm described in [8] is in their approach to this issue. The computation is performed inductively: the best response policy for V is computed based on the best response policies of V "s children U1, . . . , Uk. By the end of the upstream pass, all children of the root have computed their best response policies.
In the beginning of the downstream pass, the root selects its strategy and informs its children about its choice. It also selects a strategy for each child. A necessary and sufficient condition for the algorithm to proceed is that the strategy of the root is a best response to the strategies of its children and, for each child, the chosen strategy is one of the pre-computed potential best responses to the chosen strategy of the root. The equilibrium then propagates downstream, with each vertex selecting its children"s actions. The action of the child is chosen to be any strategy from the pre-computed potential best responses to the chosen strategy of the parent.
To bound the running time of this algorithm, the paper [8] shows that any best response policy can be represented as a union of an exponential number of rectangles; the polynomial time approximation algorithm is obtained by combining this representation with a polynomial-sized grid. The main idea of [9] is that it is not necessary to keep track of all rectangles in the best response policies; rather, at each step of the upstream pass, it is possible to select a polynomial-size subset of the corresponding policy (in [9], this subset is called a breakpoint policy), and still ensure that the downstream pass can proceed successfully (a sufficient condition for this is that the subset of the best response policy for V stored by the algorithm contains a continuous path from w = 0 to w = 1).
One of the main contributions of our paper is to show that the algorithm proposed by [9] is incorrect. In Section 3 we describe a simple example for which the algorithm of [9] outputs a vector of strategies that does not constitute a Nash equilibrium of the underlying game.
In Sections 4, 5 and 6 we show how to fix the algorithm of [9] so that it always produces correct output.
Section 4 considers the case in which the underlying graph is a path of length n. For this case, we show that the number of rectangles in each of the best response policies is O(n2 ). This gives us an O(n3 ) algorithm for finding a Nash equilibrium, and for computing a representation of all Nash equilibria. (This algorithm is a special case of the generic algorithm of [8] - we show that it runs in polynomial time when the underlying graph is a path.) We can improve the running time of the generic algorithm using the ideas of [9]. In particular, we give an O(n2 ) algorithm for finding a Nash equilibrium of a graphical game on a path of length n. Instead of storing best response policies, this algorithm stores appropriately-defined subsets, which, following [9], we call breakpoint policies (modifying the definition as necessary). We obtain the following theorem THEOREM 1. There is an O(n2 ) algorithm that finds a Nash equilibrium of a graphical game with two actions per player on an n-vertex path. There is an O(n3 ) algorithm that computes a representation of all Nash equilibria of such a game.
In Section 5 we extend the results of Section 4 to general degree2 graphs, obtaining the following theorem.
THEOREM 2. There is a polynomial-time algorithm that finds a Nash equilibrium of a graphical game with two actions per player on a graph with maximum degree 2.
In Section 6 we extend our algorithm so that it can be used to find a Nash equilibrium of a graphical game on an arbitrary tree. Even when the tree has bounded degree, the running time can be exponential. We show that this is inevitable by constructing a family of graphical games on bounded-degree trees for which best response policies of some of the vertices have exponential size, and any twopass algorithm (i.e., an algorithm that is similar in spirit to that of [8]) has to store almost all points of the best response policies.
In particular, we show the following.
THEOREM 3. There is an infinite family of graphical games on bounded-degree trees with pathwidth 2 such that any two-pass algorithm for finding Nash equilibria on these trees requires exponential time and space.
It is interesting to note that the trees used in the proof of Theorem 3 have pathwidth 2, that is, they are very close to being paths. It is an open question whether our algorithm runs in polynomial time for graphs of pathwidth 1. This question can be viewed as a generalization of a very natural computational geometry problem - we describe it in more detail in Section 8.
In Section 7, we give a complexity-theoretic intractability result for the problem of finding a Nash equilibrium of a graphical game on a graph with small pathwidth. We prove the following theorem.
THEOREM 4. Consider the problem of finding a Nash equilibrium for a graphical game in which the underlying graph has maximum degree 3 and pathwidth k. There is a constant k such that this problem is PPAD-complete.
Theorem 4 limits the extent to which we can exploit path-like properties of the underlying graph, in order to find Nash equilibria.
To prove Theorem 4, we use recent PPAD-completeness results for games, in particular the papers [7, 4] which show that the problem of finding Nash equilibria in graphical games of degree d (for d ≥ 3) is computationally equivalent to the problem of solving r-player normal-form games (for r ≥ 4), both of which are PPAD-complete.
We consider graphical games in which the underlying graph G is an n-vertex tree. Each vertex has two actions, which are denoted by 0 and 1. A mixed strategy is given by a single number x ∈ [0, 1], which denotes the probability that the player selects action 1.
Fur the purposes of the algorithm, the tree is rooted arbitrarily.
For convenience, we assume without loss of generality that the root has a single child, and that its payoff is independent of the action chosen by the child. This can be achieved by first choosing an arbitrary root of the tree, and then adding a dummy parent of this root, giving the new parent a constant payoff function.
Given an edge (V, W ) of the tree G, and a mixed strategy w for W , let G(V,W ),W =w be the instance obtained from G by (1) deleting all nodes Z which are separated from V by W (i.e., all nodes Z such that the path from Z to V passes through W ), and (2) restricting the instance so that W is required to play mixed strategy w.
Definition 1. Suppose that (V, W ) is an edge of the tree, that v is a mixed strategy for V and that w is a mixed strategy for W . 101 We say that v is a potential best response to w (denoted by v ∈ pbrV (w)) if there is an equilibrium in the instance G(V,W ),W =w in which V has mixed strategy v. We define the best response policy for V , given W , as B(W, V ) = {(w, v) | v ∈ pbrV (w), w ∈ [0, 1]}. Typically, W is the parent of V , and this is just referred to as the best response policy for V . The expression B(W, V )|V =v is used to denote the set B(W, V ) ∩ [0, 1]×{v}.
The upstream pass of the generic algorithm of [8] computes the best response policy for V for every node V other than the root.
With the above assumptions about the root, the downstream pass is straightforward: Let W denote the root and V denote its child.
The root selects any pair (w, v) from B(W, V ). It decides to play mixed strategy w and it instructs V to play mixed strategy v. The remainder of the downward pass is recursive. When a node V is instructed by its parent to adopt mixed strategy v, it does the following for each child U - It finds a pair (v, u) ∈ B(V, U) (with the same v value that it was given by its parent) and instructs U to play u.
The algorithm of [9] is based on the following observation: to compute a single Nash equilibrium by a two-pass algorithm, it is not necessary to construct the entire best response policy for each vertex. As long as, at each step of the downstream pass, the vertex under consideration can select a vector of strategies for all its children so that each child"s strategy is a potential best response to the parent"s strategy, the algorithm succeeds in producing a Nash equilibrium. This can be achieved if, at the beginning of the downstream pass, we have a data structure in which each vertex V with parent W stores a set ˆB(W, V ) ⊆ B(W, V ) (called a breakpoint policy) which covers every possible w ∈ [0, 1]. We will show later that a sufficient condition for the construction of such a data structure is the invariant that, at every level of the upstream pass, ˆB(W, V ) contains a continuous path from w = 0 to w = 1.
In [9], it is suggested that we can select the breakpoint policy in a particular way. Namely, the paper uses the following definition: Definition 2. (cf. [9]) A breakpoint policy for a node V with parent W consists of an ordered set of W -breakpoints w0 = 0 < w1 < w2 < · · · < wt−1 < wt = 1 and an associated set of V -values v1, . . . , vt. The interpretation is that for any w ∈ [0, 1], if wi−1 < w < wi for some index i and W plays w, then V shall play vi; and if w = wi for some index i, then V shall play any value between vi and vi+1. We say such a breakpoint policy has t − 1 breakpoints.
The paper then claims that any vertex V can compute its breakpoint policy with respect to its parent W given the breakpoint policies of its children U1, . . . , Uk. The proof proceeds by ordering the children"s breakpoints (i.e., the respective values of v) from left to right (it can be assumed without loss of generality that all these breakpoints are distinct) and considering them in turn; each such point vl ∈ {v1, . . . , vL} corresponds to a fixed choice of strategies for k − 1 children and an interval of admissible strategies for one child. Assume for convenience that this child is U1 and its interval of admissible strategies at vl is [a, b]; assume also that for Uj , j = 2, . . . , k, their respective breakpoint policies prescribe them to play uj in response to vl. Let P i (u, w), i = 0, 1, be the expected payoff for V when V plays i, U1 plays u, each Uj , j = 2, . . . , k, plays uj, and W plays w, and consider the set Wl = {w ∈ [0, 1] | ∃u ∈ [a, b] s.t. P 0 (u, w) = P1 (u, w)}; note that for any w ∈ Wl we have vl ∈ pbrV (w). v1 v2 v3 v4 v5 v6 v7 V W Figure 1: LKS: Trimming to find breakpoint policies.
The authors show that for any breakpoint vl, the set Wl is either empty, a single interval, or a union of two non-floating intervals (an interval is non-floating if one of its endpoints is 0 or 1); moreover, the union of all sets Wl, l = 1, . . . , L, covers the interval [0, 1]. It follows easily that one can cover [0, 1] with at most L+2 intervals, each of which is a subset of some Wl. The authors then claim that any such cover can be transformed into a breakpoint policy for V .
Namely, they say that for any two intervals Wl1 and Wl2 in the cover, Any overlap between Wl1 and Wl2 can be arbitrarily assigned coverage by Wl1 and Wl2 trimmed accordingly (cf. [9], p. 5). They illustrate their approach in a figure, which is reproduced as Figure 1 here. In the figure, the dashed horizontal lines represent the breakpoints v1, v2, . . . , v7 and the solid intervals along these breakpoints are the sets W1, W2, . . . , W7. The thick connected path is the corresponding breakpoint policy. It is chosen as follows: begin on the left, and always jump to the interval allowing greatest progress to the right.
To see why this approach does not work in general, consider a path of length 4 consisting of an indifferent root R, its child W ,
W "s child V , and V "s child U. Suppose that U receives a payoff of 1 if it plays differently to V and 0 otherwise. Thus, if v denotes the mixed strategy of V (i.e., V plays 1 with probability v), then the expected payoff that U derives from playing 0 is given by P0 (U) = v and the expected payoff that U derives from playing 1 is given by P1 (U) = 1 − v. Suppose that V derives no payoff from playing 1 (so P1 (V ) = 0) and that its payoff matrix for playing 0 is 1 −9 9 −1 , so if u denotes the mixed strategy of U and w denotes the mixed strategy of W , the expected payoff that V derives from playing 0 is given by P0 (V ) = (1 − u)(1 − w) + (1 − u)w(−9) + u(1 − w)9 + uw(−1).
Using the techniques of [8] (or, alternatively, those of Section 4), it is not hard to verify that the best response policies for U and V (as in Definition 1) are given by the graphs in Figure 2. The best response policy for U is a breakpoint policy for U (as in Definition 2) with V -breakpoints v0 = 0, v1 = 1/2 and v2 = 1 with associated values u1 = 1 and u2 = 0. The best response policy for V is not a breakpoint policy (because of how the curve from w = 0 to w = 1 doubles back).
The LKS algorithm would trim to get a breakpoint policy such as the one in Figure 3. Note that this breakpoint policy ˆB(W, V ) is invalid in the sense that it does not satisfy ˆB(W, V ) ⊆ B(W, V ). 102 1
1
u v v w Figure 2: Best response policies for U and V .
1 v w Figure 3: A trimmed policy for V The point is that the payoff matrix of W can now be chosen to prevent the LKS algorithm from finding a Nash equilibrium. For example, suppose the payoffs are given so that P0 (W ) = v and P1 (W ) = (1−v)2. The best response policy for W is a horizontal line at w = .1 (This is the value of w that allows v = 2/3 - see Figure 2, which makes P0 (W ) = P1 (W ).) In the downward pass, the chosen values are w = .1, then, from the trimming, v = 0 and u = 1, which is not a Nash equilibrium since W prefers action 1.
The failure of the algorithm is not caused by the fact that the trimming policy goes as far to the right as possible. Any other trimming would be just as bad. For example, suppose the breakpoint policy for V has v = 0 until some point w∗ < .9 and then jumps to v = 1. The algorithm is then defeated by the payoff matrix with P 0 (W ) = 2v and P1 (W ) = (1 − v) in which the best response policy for W is a horizontal line at w = .9. The algorithm then gives w = .9, v = 1, and u = 0, which is not a Nash equilibrium since W prefers action 0.
We conclude that the LKS algorithm does not always find a Nash equilibrium. In Sections 4 and 6 we show how to modify the algorithm so that it always finds a Nash equilibrium. For the modified algorithm, we have to extend the definition of breakpoint policy (see Definition 3) so that it includes breakpoint policies such as the best response policy for V in Figure 2. Unfortunately, such a breakpoint policy may be exponential in size (see Figure 7) so the corrected algorithm does not run in polynomial time on all trees. In the next section, we show that it runs in polynomial time on a path.
In this section, we focus on the case when the underlying graph is a path, i.e., its vertex set is {V1, . . . , Vn}, and its edge set is {(Vj , Vj+1) | j = 1, . . . , n − 1}. We show that in this case the best response policy for each vertex can be represented as a union of a polynomial number of rectangles, where a rectangle is defined by a pair of closed intervals (IV , IU ) and consists of all points in IV × IU ; it may be the case that one or both of the intervals IV and IU consists of a single point.
THEOREM 5. For any j = 1, . . . , n, the set B(Vj , Vj−1) can be represented as a disjoint union of at most (j + 4)2 rectangles.
Moreover, given such representation of B(Vj , Vj−1), one can compute a representation of B(Vj+1, Vj) in time O(j2 ).
PROOF. For any set A ⊆ [0, 1]2 that is represented as a union of a finite number of rectangles, we say that a point u ∈ [0, 1] on the U-axis is a U-event point of A if u = 0 or u = 1 or A contains a rectangle of the form IV × IU and u is an endpoint of IU ; V -event points are defined similarly. Observe that for any u ∈ [0, 1], the number of connected components of [0, 1]×{u} ∩ A is at most the number of V -event points of A.
We use induction on j to show that for each Vj the statement of the theorem holds and, additionally, each B(Vj , Vj−1) has at most 2j + 4 event points.
To simplify the base case, we modify the graphical game by appending a dummy vertex V0 to the beginning of the path: the only neighbour of V0 is V1, the payoffs of V0 are always equal to 0, and the payoffs of all other vertices (including V1) are the same as in the original game.
For j = 0, we have B(V1, V0) = [0, 1]2 , so the statement of the theorem is trivially true.
Now, suppose that j > 0, set V = Vj and let U = Vj−1 and W = Vj+1 be the vertices that precede and follow V , respectively.
The payoffs to V are described by a 2×2×2 matrix P: Pxyz is the payoff that V receives when U plays x, V plays y, and W plays z, where x, y, z ∈ {0, 1}. Suppose that U plays 1 with probability u and W plays 1 with probability w. Then V "s expected payoff from playing 0 is P0 =(1−u)(1−w)P000+(1−u)wP001+u(1−w)P100+uwP101, while its expected payoff from playing 1 is P1 =(1−u)(1−w)P010+(1−u)wP011+u(1−w)P110+uwP111.
If P 0 > P1 , V strictly prefers to play 0, if P0 < P1 , V strictly prefers to play 1, and if P0 = P1 , V is indifferent, i.e., can play any (mixed) strategy. Since P0 and P1 are linear in w and u, there exist some constants A1, A0, B1, and B0 that depend on the matrix P, but not on u and w, such that P0 − P1 = w(B1u + B0) − (A1u + A0). (1) Depending on the values of A1, A0, B1, and B0, we subdivide the rest of the proof into the following cases. • B1 = 0, B0 = 0.
In this case, P0 > P1 if and only if A1u + A0 < 0.
If also A1 = 0, A0 = 0, clearly, B(W, V ) = [0, 1]2 , and the statement of the theorem is trivially true.
Otherwise, the vertex V is indifferent between 0 and 1 if and only if A1 = 0 and u = −A0/A1. Let V = {v | v ∈ (0, 1), −A0/A1 ∈ pbrU (v)}. By the inductive hypothesis, V consists of at most 2(j − 1) + 4 segments and isolated points.
For any v ∈ V, we have B(W, V )|V =v = [0, 1]: no matter what W plays, as long as U is playing −A0/A1, V is content to play v. On the other hand, for any v ∈ (0, 1) \ V we have B(W, V )|V =v = ∅: when V plays v, U can only respond with u = −A0/A1, in which case V can benefit from switching to one of the pure strategies.
To complete the description of B(W, V ), it remains to analyze the cases v = 0 and v = 1. The vertex V prefers to play 0 if A1 > 0 and u ≤ −A0/A1, or A1 < 0 and u ≥ −A0/A1, or 103 A1 = 0 and A0 < 0. Assume for now that A1 > 0; the other two cases can be treated similarly. In this case 0 ∈ pbrV (w) for some w ∈ [0, 1] if and only if there exists a u ∈ pbrU (0) such that u ≤ −A0/A1: if no such u exists, whenever V plays 0 either U"s response is not in pbrU (0) or V can improve its payoff by playing
Similarly, B(W, V )|V =1 is equal to either [0, 1] or ∅, depending on pbrU (1).
Therefore, the set B(W, V ) consists of at most 2j + 4 ≤ (j + 4)2 rectangles: B(W, V ) ∩ [0, 1]×(0, 1) = [0, 1]×V contributes at most 2j + 2 rectangles, and each of the sets B(W, V )|V =0 and B(W, V )|V =1 contributes at most one rectangle. Similarly, its total number of event points is at most 2j + 4: the only W -event points are 0 and 1, each V -event point of B(W, V ) is a V -event point of B(V, U), and there are at most 2j + 2 of them. • B1u + B0 ≡ 0, A1 = αB1, A0 = αB0 for some α ∈ R.
In this case, V is indifferent between 0 and 1 if and only if w = α, or B1 = 0 and u = −B0/B1 = −A0/A1. Similarly to the previous case, we can show that B(W, V )∩[0, 1]×(0, 1) consists of the rectangle {α}×[0, 1] and at most 2j + 2 rectangles of the form [0, 1]×IV , where each IV corresponds to a connected component of B(V, U)|U=−B0/B1 .
Furthermore, V prefers to play 0 if B1u + B0 > 0 and w ≥ α or B1u + B0 < 0 and w ≤ α. Therefore, if B1u∗ + B0 > 0 for some u∗ ∈ pbrU (0), then B(W, V )|V =0 contains [α, +∞) ∩ [0, 1] and if B1u∗∗ + B0 < 0 for some u∗∗ ∈ pbrU (0), then B(W, V )|V =0 contains [−∞, α] ∩ [0, 1]; if both u∗ and u∗∗ exist,
B(W, V )|V =0 = [0, 1]. The set B(W, V )|V =1 can be described in a similar manner.
By the inductive hypothesis, B(V, U) has at most 2j + 2 event points; as at least two of these are U-event points, it has at most 2j V -event points. Since each V -event point of B(W, V ) is a Vevent point of B(V, U) and B(W, V ) has at most 3 W -event points (0, 1, and α), its total number of event points is at most 2j + 3 < 2j +4. Also, similarly to the previous case it follows that B(W, V ) consists of at most 2j + 4 < (j + 4)2 rectangles. • B1u + B0 ≡ 0, α(B1u + B0) ≡ A1u + A0.
In this case, one can define the indifference function f(·) as f(u) = A(u) B(u) = A1u+A0 B1u+B0 , where A(u) and B(u) never turn into zero simultaneously. Observe that whenever w = f(u) and u, w ∈ [0, 1], V is indifferent between playing 0 and 1. For any A ⊆ [0, 1]2 , we define a function ˆfV by ˆfV (A) = {(f(u), v) | (v, u) ∈ A}; note that ˆfV maps subsets of [0, 1]2 to subsets of R×[0, 1]. Sometimes we drop the subscript V when it is clear from the context.
LEMMA 1. For any (w, v) ∈ [0, 1]×(0, 1) we have (w, v) ∈ B(W, V ) if and only if there exists a u ∈ [0, 1] such that (v, u) ∈ B(V, U) and w = f(u).
PROOF. Fix an arbitrary v ∈ (0, 1). Suppose that U plays some u ∈ pbrU (v), w = f(u) satisfies w ∈ [0, 1], and W plays w.
There exists a vector of strategies v1, . . . , vj−1 = u, vj = v such that for each Vk, k < j, its strategy is a best response to its neighbours" strategies. Since w = f(u), V is indifferent between playing 0 and 1; in particular, it can play v. Therefore, if we define vj+1 = w, the vector of strategies (v1, . . . , vj+1) will satisfy the conditions in the definition of potential best response, i.e., we have v ∈ pbrV (w).
Conversely, suppose v ∈ pbrV (w) for some w ∈ [0, 1], v = 0, 1. Then there exists a vector of strategies v1, . . . , vj−1, vj = v, vj+1 = w such that for each Vk, k ≤ j, its strategy is a best response to its neighbours" strategies. As v = 0, 1, V is, in fact, indifferent between playing 0 and 1, which is only possible if w = f(vj−1). Choose u = vj−1; by construction, u ∈ pbrU (v).
Lemma 1 describes the situations when V is indifferent between playing 0 and playing 1. However, to fully characterize B(W, V ), we also need to know when V prefers a pure strategy.
Define ˆf(0) = ∪u∈pbrU (0)Ru, where Ru = ´ [f(u), +∞)×{0} if B(u) > 0, (−∞, f(u)]×{0} if B(u) < 0. and ˆf(1) = ∪u∈pbrU (1)Ru, where Ru = ´ [f(u), +∞)×{1} if B(u) < 0, (−∞, f(u)]×{1} if B(u) > 0.
LEMMA 2. For any w ∈ [0, 1], we have (w, 0) ∈ ˆf(0) if and only if 0 ∈ pbrV (w) and (w, 1) ∈ ˆf(1) if and only if 1 ∈ pbrV (w).
PROOF. Consider an arbitrary u0 ∈ pbrU (0). If B(u0) > 0, for u = u0 the inequality P0 ≥ P1 is equivalent to w ≥ f(u0).
Therefore, when U plays u0 and W plays w, w ≥ f(u0), V prefers to play 0; as u0 ∈ pbrU (u), it follows that 0 ∈ pbrV (w). The argument for the case B(u0) < 0 is similar.
Conversely, if 0 ∈ pbrV (w) for some w ∈ [0, 1], there exists a vector (v1, . . . , vj−1, vj = 0, vj+1 = w) such that for each Vk, k ≤ j, Vk plays vk, and this strategy is a best response to the strategies of Vk"s neighbours. Note that for any such vector we have vj−1 ∈ pbrU (0). By way of contradiction, assume (w, 0) ∈ Ë u∈pbrU (0) Ru. Then it must be the case that for any u0 ∈ pbrU (0) either f(u0) < w and Ru0 = (−∞, f(u0)]×{0} or f(u0) > w and Ru0 = [f(u0), +∞)×{0}. In both cases, when V plays 0, U plays u0, and V plays w, the inequality between f(u0) and w is equivalent to P0 < P1 , i.e., V would benefit from switching to 1.
The argument for ˆf(1) is similar.
Together, Lemma 1 and Lemma 2 completely describe the set B(W, V ): we have B(W, V ) = ˆf(0) ∪ ˆf(B(V, U)) ∪ ˆf(1) [0, 1]2 .
It remains to show that B(W, V ) can be represented as a union of at most (j + 4)2 rectangles, has at most 2j + 4 event points, and can be computed in O(j2 ) time.
Set u∗ = −B0/B1. 2 Consider an arbitrary rectangle R = [v1, v2]×[u1, u2] ⊆ B(V, U). If u∗ ∈ [u1, u2], the function f(·) is continuous on [u1, u2] and hence ˆf(R) = [fmin, fmax]×[v1, v2], where fmin = min{f(u1), f(u2)}, fmax = max{f(u1), f(u2)}, i.e., in this case ˆf(R) ∩ [0, 1]2 consists of a single rectangle.
Now, suppose that R is intersected by the line [0, 1]×{u∗ }; as was noted earlier, there are at most 2j+2 such rectangles. Suppose that limu→u∗− f(u) = +∞; as f(·) is a fractional linear function, this implies that limu→u∗+ f(u) = −∞ and also f(u1) > f(u2).
Since f(·) is continuous on [u1, u∗ ) and (u∗ , u2], it is easy to see that ˆf([v1, v2]×[u1, u∗ )) = [f(u1), +∞)×[v1, v2] 2 The case B1 = 0 causes no special problems. For completeness, set u∗ to be any value outside of [0, 1] in this case. 104 v u v u* 1 f(0) f(a)f(b) f(1) a b (0, 0) w v 2 v (0, 0) 1 1 1 v 2 v 1 1 Figure 4: f is increasing on (−∞, u∗ ) and (u∗ , +∞). and ˆf([v1, v2]×(u∗ , u2]) = (−∞, f(u2)]×[v1, v2], i.e., in this case ˆf(R) ∩ [0, 1]2 consists of at most two rectangles.
The case limu→u∗− f(u) = −∞ is similar.
As ˆf(B(V, U)) = Ë R⊂B(V,U) ˆf(R), it follows that ˆf(B(V, U)) consists of at most (j + 3)2 + 2j + 2 rectangles. Also, it is easy to see that both ˆf(0) and ˆf(1) consist of at most 2 line segments each. We conclude that B(W, V ) can be represented as a union of at most (j + 3)2 + 2j + 6 < (j + 4)2 rectangles.
Moreover, if v is a V -event point of B(W, V ), then v is a Vevent point of B(V, U) (this includes the cases v = 0 and v = 1, as 0 and 1 are V -event points of B(V, U)) and if w is a W -event point of B(W, V ), then either w = 0 or w = 1 or there exists some u ∈ [0, 1] such that w = f(u) and u is a U-event point of B(V, U).
Hence, B(W, V ) has at most 2j + 4 event points.
The O(j2 ) bound on the running time in Theorem 5 follows from our description of the algorithm. The O(n3 ) bound on the overall running time for finding a Nash equilibrium (and a representation of all Nash equilibria) follows.
) Time The upper bound on the running time of our algorithm is tight, at least assuming the straightforward implementation, in which each B(Vj+1, Vj) is stored as a union of rectangles: it is not hard to construct an example in which the size of B(Vj+1, Vj) is Ω(j2 ).
However, in some cases it is not necessary to represent all Nash equilibria; rather, the goal is to find an arbitrary equilibrium of the game. In this section, we show that this problem can be solved in quadratic time, thus obtaining a proof of Theorem 1. Our solution is based on the idea of [9], i.e., working with subsets of the best response policies rather than the best response policies themselves; following [9], we will refer to such subsets as breakpoint policies.
While it is not always possible to construct a breakpoint policy as defined in [9], we show how to modify this definition so as to ensure that a breakpoint policy always exists; moreover, we prove that for a path graph, the breakpoint policy of any vertex can be stored in a data structure whose size is linear in the number of descendants this vertex has.
Definition 3. A breakpoint policy ˆB(V, U) for a vertex U whose parent is V is a non-self-intersecting curve of the form X1 ∪ Y1 ∪ · · · ∪ Ym−1 ∪ Xm, where Xi = [vi−1, vi]×{ui}, Yi = {vi}×[ui, ui+1] and ui, vi ∈ [0, 1] for i = 0, . . . , m. We say that a breakpoint policy is valid if v0 = 0, vm = 1, and ˆB(V, U) ⊆ B(V, U).
We will sometimes abuse notation by referring to ˆB(V, U) as a collection of segments Xi, Yi rather than their union. Note that we do not require that vi ≤ vi+1 or ui ≤ ui+1; consequently, in any argument involving breakpoint policies, all segments are to be treated as directed segments. Observe that any valid breakpoint policy ˆB(V, U) can be viewed as a continuous 1-1 mapping γ(t) = (γv(t), γu(t)), γ : [0, 1] → [0, 1]2 , where γ(0) = (0, u1), γ(1) = (1, um) and there exist some t0 = 0, t1, . . . , t2m−2 = 1 such that {γ(t) | t2k ≤ t ≤ t2k+1} = Xk+1, {γ(t) | t2k+1 ≤ t ≤ t2k+2} = Yk+1.
As explained in Section 3, we can use a valid breakpoint policy instead of the best response policy during the downstream pass, and still guarantee that in the end, we will output a Nash equilibrium.
Theorem 6 shows that one can inductively compute valid breakpoint policies for all vertices on the path; the proof of this theorem can be found in the full version of this paper [6].
THEOREM 6. For any V = Vj, one can find in polynomial time a valid breakpoint policy ˆB(W, V ) that consists of at most 2j + 1 segments.
MAXIMUM DEGREE 2 In this section we show how the algorithm for paths can be applied to solve a game on any graph whose vertices have degree at most 2. A graph having maximum degree 2 is, of course, a union of paths and cycles. Since each connected component can be handled independently, to obtain a proof of Theorem 2, we only need to show how to deal with cycles.
Given a cycle with vertices V1, . . . , Vk (in cyclic order), we make two separate searches for a Nash equilibrium: first we search for a Nash equilibrium where some vertex plays a pure strategy, then we search for a fully mixed Nash equilibrium, where all vertices play mixed strategies. For i ≤ k let vi denote the probability that Vi plays 1.
The first search can be done as follows. For each i ∈ {1, . . . , k} and each b ∈ {0, 1}, do the following.
only on vi+1 and vi+2.)
keep track of all possible mixed strategies vj
with vi = b; if so we have a Nash equilibrium. (Otherwise, there is no Nash equilibrium of the desired form.) For the second search, note that if Vi plays a mixed strategy, then vi+1 and vi−1 satisfy an equation of the form vi+1 = (A0 + A1vi−1)/(B0 + B1vi−1). Since all vertices in the cycle play mixed strategies, we have vi+3 = (A0 +A1vi+1)/(B0 +B1vi+1).
Composing the two linear fractional transforms, we obtain vi+3 = (A0 +A1 vi−1)/(B0 +B1 vi−1). for some new constants A0 , A1 ,
B0 , B1 .
Choose any vertex Vi. We can express vi in terms of vi+2, then vi+4, vi+6 etc. and ultimately vi itself to obtain a quadratic equation (for vi) that is simple to derive from the payoffs in the game. If the equation is non-trivial it has at most 2 solutions in (0, 1). For an odd-length cycle all other vj "s are derivable from those solutions, and if a fully mixed Nash equilibrium exists, all the vj should turn out to be real numbers in the range (0, 1). For an even-length cycle, we obtain two quadratic equations, one for vi and another for 105 vi+1, and we can in the same way test whether any solutions to these yield values for the other vj , all of which lie in (0, 1).
If the quadratic equation is trivial, there is potentially a continuum of fully-mixed equilibria. The values for vi that may occur in a Nash equilibrium are those for which all dependent vj values lie in (0, 1); the latter condition is easy to check by computing the image of the interval (0, 1) under respective fractional linear transforms.
(ARBITRARY) TREE For arbitrary trees, the general structure of the algorithm remains the same, i.e., one can construct a best response policy (or, alternatively, a breakpoint policy) for any vertex based on the best response policies of its children. We assume that the degree of each vertex is bounded by a constant K, i.e., the payoff matrix for each vertex is of size O(2K ).
Consider a vertex V whose children are U1, . . . , Uk and whose parent is W ; the best response policy of each Uj is B(V, Uj).
Similarly to the previous section, we can compute V "s expected payoffs P0 and P1 from playing 0 or 1, respectively. Namely, when each of the Uj plays uj and W plays w, we have P0 = L0 (u1, . . . , uk, w), P 1 = L1 (u1, . . . , uk, w), where the functions L0 (·, . . . , ·), L1 (·, . . . , ·) are linear in all of their arguments.
Hence, the inequality P0 > P1 can be rewritten as wB(u1, . . . , uk) > A(u1, . . . , uk), where both A(·, . . . , ·) and B(·, . . . , ·) are linear in all of their arguments. Set u = (u1, . . . , uk) and define the indifference function f : [0, 1]k → [0, 1] as f(u) = A(u)/B(u); clearly, if each Uj plays uj, W plays w and w = f(u), V is indifferent between playing 0 and 1. For any X = X1 × · · · × Xk, where Xi ⊆ [0, 1]2 define ˆf(X) = {(f(u), v) | (v, ui) ∈ Xi, i = 1, . . . , k} Also, set ˆf(0) = {(w, 0) | ∃u s.t. ui ∈ pbrUi (0) and wB(u) ≥ A(u)} and ˆf(1) = {(w, 1) | ∃u s.t. ui ∈ pbrUi (1) and wB(u) ≤ A(u)}.
As in previous section, we can show that B(W, V ) is equal to ˆf(0) ∪ ˆf(B(V, U1) × · · · × B(V, Uk)) ∪ ˆf(1) [0, 1]2 ; also, any path from w = 0 to w = 1 that is a subset of B(W, V ) constitutes a valid breakpoint policy.
While the algorithm of Section 4 can be generalized for boundeddegree trees, its running time is no longer polynomial. In fact, the converse is true: we can construct a family of trees and payoff matrices for all players so that the best response policies for some of the players consist of an exponential number of segments.
Moreover, in our example the breakpoint policies coincide with the best response policies, which means that even finding a single Nash equilibrium using the approach of [8, 9] is going to take exponentially long time. In fact, a stronger statement is true: for any polynomial-time two-pass algorithm (defined later) that works with subsets of best response policies for this graph, we can choose the payoffs of the vertices so that the downstream pass of this algorithm will fail.
S 1 1 T S n−1 00 0000 11 1111 00 0000 11 1111 00 0000 11 1111 0 00 1 11 0 00 1 11 0 00 1 11 0 00 1 11 0 00 1 11 0 00 1 11 0 00 1 11 00 0000 11 1111 00 0000 11 1111 0 00 1 11 0000 00000000 00000000 0000 1111 11111111 11111111 1111 000 000000 000000 000 111 111111 111111 111 S S T T T 2 n−1 n 2 n 1 n−12 n VVVVV 0 Figure 5: The tree Tn that corresponds to exponential-size breakpoint policy.
In the rest of this subsection, we describe this construction.
Consider the tree Tn given by Figure 5; let Vn be the root of this tree.
For every k = 1, . . . , n, let the payoffs of Sk and Tk be the same as those for the U and V described in Section 3; recall that the breakpoint policies for U and V are shown in Figure 2. It is not hard to see that the indifference function for Tk is given by f(s) = .8s+.1.
The payoff of V0 is 1 if V1 selects the same action as V0 and 0 otherwise; V0"s best response policy is given by Figure 6.
LEMMA 3. Fix k < n, and let u, t, v, and w denote the strategies of Vk−1, Tk, Vk, and Vk+1, respectively. Suppose that Vk prefers playing 0 to playing 1 if and only if .5t + .1u + .2 > w.
Then B(Vk+1, Vk) consists of at least 3k segments. Moreover, {(v, w) | (v, w) ∈ B(Vk+1, Vk), 0 ≤ w ≤ .2} = [0, .2]×{0} and {(v, w) | (v, w) ∈ B(Vk+1, Vk), .8 ≤ w ≤ 1} = [.8, 1]×{1}.
PROOF. The proof proceeds by induction on k. For k = 0, the statement is obvious. Now, suppose it is true for B(Vk, Vk−1).
One can view B(Vk+1, Vk) as a union of seven components: ˆf(0) ∩ [0, 1]×{0}, ˆf(1) ∩ [0, 1]×{1}, and five components that correspond to the segments of B(Vk, Tk). Let us examine them in turn.
To describe ˆf(0)∩[0, 1]×{0}, note that f(u, t) = .5t+.1u+.2 is monotone in t and u and satisfies f(0, 0) = .2. Also, we have pbrVk−1 (0) = {0} and pbrTk (0) = {0}. For any w ∈ [0, 1] we have f(0, 0) ≥ w if and only if w ∈ [0, .2]. We conclude that ˆf(0) ∩ [0, 1]×{0} = [0, .2]×{0}. Similarly, it follows that ˆf(1) ∩ [0, 1]×{1} = [.8, 1]×{1}.
Define S1 = {(f(u, 0), v) | (v, u) ∈ B(Vk, Vk−1) ∩ [0, .9]×[0, 1]},
S2 = {(f(u, .5), v) | (v, u) ∈ B(Vk, Vk−1) ∩ [.1, .9]×[0, 1]},
S3 = {(f(u, 1), v) | (v, u) ∈ B(Vk, Vk−1) ∩ [.1, 1]×[0, 1]}; these sets correspond to horizontal segments of B(Vk, Tk).
It is easy to see that S1, S2, S3 ⊂ B(Vk+1, Vk). Since f is a continuous function, the number of segments in each Si is at least the number of segments in B(Vk, Vk−1)∩[.1, .9]×[0, 1], which is at least 3k−1 by induction hypothesis. Moreover, as f is monotone in u and f(1, 0) < f(0, .5) < f(1, .5) < f(0, 1), all Si, i = 1, 2, 3, are disjoint.
Finally, the set B(Vk+1, Vk) contains two segments that correspond to the vertical segments of B(Vk, Tk), i.e.,
S4 = {(f(0, t), .1) | t ∈ [.5, 1]) = [.45, .7]×{.1} and S5 = {(f(1, t), .9) | t ∈ [0, .5]) = [.3, .55]×{.9}.
Clearly, S4 connects S2 and S3, S5 connects S1 and S2, and S4 and S5 do not intersect each other. We conclude that B(Vk+1, Vk) 106 0 00 00 00 00 00 00 00 00 00 0 1 11 11 11 11 11 11 11 11 11 1 00000000001111111111 1 1
1 1
V V0.5 0.2 V V 21 10 Figure 6: Breakpoint policies for V0 and V1. is a continuous line that consist of at least 3k segments and satisfies the condition of the lemma.
To complete the construction, we need to show that we can design the payoff matrix for Vk so that it prefers playing 0 to playing 1 if and only if .5t + .1u + .2 > w. To this end, we prove a more general statement, namely, that the indifference function of a vertex can be an arbitrary fractional multilinear function of its descendants" strategies.
We say that a function of k variables is multilinear if it can be represented as a sum of monomials and each of these monomials is linear in all of its variables. Note that this definition is different from a more standard one in that we do not require that all of the monomials have the same degree. Recall that the payoffs of a vertex with k + 1 neighbours are described by matrices P0 and P1 , where Pj i0i1...ik is the payoff that V gets when it plays j, and its neighbours play i0, . . . , ik, and j, i0, . . . , ik ∈ {0, 1}. Let P[j] = P[j](w, u1, . . . , uk) be the expected payoff obtained by this vertex when it plays j and the (mixed) strategies of its neighbours are given by a vector (w, u1, . . . , uk), i.e., P[j] = E[P j i0i1...ik ] where i0, . . . , ik are independent Bernoulli random variables, each of which is 1 with the respective probabilities w, u1, . . . , uk.
LEMMA 4. Given a tree vertex V whose parent is W and whose children are U1, . . . , Uk, for any function f = f(u1, . . . , uk) that can be represented as a ratio of two multilinear functions f1, f2, i.e., f = f1(u1,...,uk) f2(u1,...,uk) , there exist payoff matrices P0 and P1 for V such that P[0] − P[1] = wf2(u1, . . . , uk) − f1(u1, . . . , uk).
The proof of this lemma is based on the fact that every monomial of the form as(u0)s0 . . . (uk)sk , s1, . . . , sk ∈ {0, 1}, can be represented as t=t0...tk∈Σk+1 Ct(u0)t0 (1 − u0)1−t0 . . . (uk)tk (1 − uk)1−tk for some Ct, t ∈ {0, 1}k+1 . The details can be found in the full version of this paper [6].
for Tn While the best response policy constructed in the previous subsection has exponential size, it is not clear `a priori that it is necessary to keep track of all of its line segments rather than to focus on a small subset of these segments. However, it turns out that for two-pass algorithms such as the algorithm of [8], the best response policy cannot be simplified. More precisely, we say that an algorithm A is a two-pass algorithm if 0 0 0 00 0 0 0 0 00 0 0 0 0 00 0 0 0 0 1 1 1 11 1 1 1 1 11 1 1 1 1 11 1 1 1 1 0000000000000000000000000000011111111111111111111111111111
1
1 V V 2 3 S 1 S S S S 1 T 0 T 2 3 4 5 Figure 7: Breakpoint policy for V2. • A consists of an upstream pass and a downstream pass. • During the upstream pass, for each vertex V with parent W ,
A constructs a set BB(W, V ) ⊆ B(W, V ). This set is produced from the sets {BB(V, U) | U is a child of V } by applying the procedure from the beginning of Section 6 (substituting BB(V, Uj ) for B(V, Uj) for all children Uj of V ) , and then possibly omitting some of the points of the resulting set (which is then stored explicitly). • The downstream pass is identical to the downstream pass of [8] as described in Section 2 except that it operates on the sets BB(W, V ) rather than on the sets B(W, V ).
Theorem 7 demonstrates that any two-pass algorithm will fail during the downstream pass on Tn if there is an index j such that the set BB(Vj+1, Vj) omits any interior point of any of the (at least 3j ) segments of B(Vj+1, Vj). This implies Theorem 3.
THEOREM 7. For any two-pass algorithm A for which there exists an index j, j ∈ [1, n/4], a segment S of B(Vj , Vj−1), and an interior point (x, y) of S such that BB(Vj, Vj−1) does not contain (x, y), we can choose payoff matrices of the vertices Vj, . . . , Vn so that the downstream pass of A will fail, and, additionally, payoffs to V4j , . . . , Vn are identically 0.
We sketch the proof of Theorem 7; the details can be found in the full version of this paper [6]. We proceed by induction. For j = 1, the argument is similar to that in Section 3. For the inductive step, the main idea is that we can zoom in on any part of a best response policy (including the part that was omitted!) by using an appropriate indifference function; this allows us to reduce the case j = j0 to j = j0 − 1.
PATHWIDTH GRAPHICAL GAMES In the previous section, we showed that for graphical games on trees that are almost but not quite paths, two-pass algorithms fail to find Nash equilibria in polynomial time. We next show that a milder path-like graph property allows us to construct graphical games for which it is unlikely that any polynomial-time algorithm will find Nash equilibria.
A path decomposition of a graph G = (V, E) is a sequence of subset Si(V ) ⊆ V such that for each edge (v, v ) ∈ E, v, v ∈ Si(V ) for some i, and furthermore, for each v ∈ V , if v ∈ Si(V ) and v ∈ Sj(V ) for j > i, then v ∈ Sk(V ) for all i ≤ k ≤ j. The path decomposition has width k if all sets Si(V ) have cardinality at most k + 1. The pathwidth of G is the minimum width of any path decomposition of G. 107 Pathwidth is a restriction of treewidth (in which one would seek a tree whose vertices were the sets Si(V ), and the sets containing some vertex would have to form a subtree). For any constant k it can be decided in polynomial time whether a graph has pathwidth (or treewidth) k. Furthermore many graph-theoretic problems seem easier to solve in polynomial time, when restricted to fixed treewidth, or pathwidth, graphs, see [1] for an overview. Note that a path has pathwidth 1 and a cycle has pathwidth 2.
We review some basic definitions from the computational complexity theory of search problems. A search problem associates any input (here, a graphical game) with a set of solutions (here, the Nash equilibria of the input game), where the description length of any solution should be polynomially bounded as a function of the description length of its input. In a total search problem, there is a guarantee that at least one solution exists for any input. Nash"s theorem assures us that the problem of finding Nash equilibria is total.
A reduction from search problem S to problem S is a mechanism that shows that any polynomial-time algorithm for S implies a polynomial-time algorithm for S. It consists of functions f and g, computable in polynomial time, where f maps inputs of S to inputs of S , and g maps solutions of S to solutions of S, in such a way that if IS is an input to S, and SS is a solution to f(IS), then g(SS ) is a solution to IS.
Observe that total search problems do not allow the above reductions from problems such as CIRCUIT SAT (where the input is a boolean circuit, and solutions are input vectors that make the output true) due to the fact that CIRCUIT SAT and other NP-complete problems have inputs with empty solution sets. Instead, recent work on the computational complexity of finding a Nash equilibrium [7, 4, 5, 2, 3] has related it to the following problem.
Definition 4. END OF THE LINE. Input: boolean circuits S and P, each having n input and n output bits, where P(0n ) = 0n and S(0n ) = 0n . Solution: x ∈ {0, 1}n such that S(x) = x, or alternatively x ∈ {0, 1}n such that P(S(x)) = x.
S and P can be thought of as standing for successor and predecessor. Observe that by computing Si (0n ) (for i = 0, 1, 2, . . .) and comparing with P(Si+1 (0n )), we must eventually find a solution to END OF THE LINE. END OF THE LINE characterizes the complexity class PPAD (standing for parity argument on a graph, directed version), introduced in Papadimitriou [11], and any search problem S is PPAD-complete if END OF THE LINE reduces to S. Other PPAD-complete problems include the search for a ham sandwich hyperplane, and finding market equilibria in an exchange economy (see [11] for more detailed descriptions of these problems). 3-GRAPHICAL NASH is the problem of finding a Nash equilibrium for a graphical game whose graph has degree 3. Daskalakis et al. [4] show PPAD-completeness of 3-GRAPHICAL NASH by a reduction from 3-DIMENSIONAL BROUWER, introduced in [4] and defined as follows.
Definition 5. 3-DIMENSIONAL BROUWER. Input: a circuit C having 3n input bits and 2 output bits. The input bits define a cubelet of the unit cube, consisting of the 3 coordinates of its points, given to n bits of precision. The output represents one of four colours assigned by C to a cubelet. C is restricted so as to assign colour 1 to cubelets adjacent to the (y, z)-plane, colour 2 to remaining cubelets adjacent to the (x, z)-plane, colour 3 to remaining cubelets on the (x, y)-plane, and colour 0 to all other cubelets on the surface of the unit cube.
A solution is a panchromatic vertex, a vertex adjacent to cubelets that have 4 distinct colours.
The reason why a solution is guaranteed to exist, is that an associated Brouwer function φ can be constructed, i.e. a continuous function from the unit cube to itself, such that panchromatic vertices correspond to fixpoints of φ. Brouwer"s Fixpoint Theorem promises the existence of a fixpoint.
The proof of Theorem 4 uses a modification of the reduction of [4] from 3-DIMENSIONAL BROUWER to 3-GRAPHICAL NASH.
To prove the theorem, we begin with some preliminary results as follows. Each player has 2 actions, denoted 0 and 1. For a player at vertex V let p[V ] denote the probability that the player plays 1.
LEMMA 5. [7] There exists a graphical game Gshift of fixed size having vertices V , V where p[V ] is the fractional part of 2p[V ].
COROLLARY 1. There exists a graphical game Gn−shift of size Θ(n) of constant pathwidth, having vertices V , Vn where p[Vn] is the fractional part of 2n .p[V ].
PROOF. Make a chain of n copies of Gshift in Lemma 5. Each subset of vertices in the path decomposition is the vertices in a copy of Gshift.
Let In(x) denote the n-th bit of the binary expansion of x, where we interpret 1 as true and 0 as false. The following uses gadgets from [7, 4].
COROLLARY 2. There exists k such that for all n, and for all n1, n2, n3 ≤ n, there exists a graphical game of size O(n) with pathwidth k, having vertices V1, V2, V3 where p[V3] = p[V1] + 2−n3 (In1 p[V1] ∧ In2 p[V2]).
PROOF OF THEOREM 4. Let C be the boolean circuit describing an instance of 3-DIMENSIONAL BROUWER. Let g1, . . . , gp(n) be the gates of C indexed in such a way that the input(s) to any gate are the output(s) of lower-indexed gates. g1, . . . , g3n will be the 3n inputs to C.
All players in the graphical game G constructed in [4] have 2 actions denoted 0 and 1. The probability that V plays 1 is denoted p[V ]. G has 3 players Vx, Vy and Vz for which p[Vx], p[Vy] and p[Vz] represent the coordinates of a point in the unit cube. G is designed to incentivize Vx, Vy and Vz to adjust their probabilities in directions given by a Brouwer function which is itself specified by the circuit C. In a Nash equilibrium, p[Vx], p[Vy] and p[Vz] represent coordinates of a fixpoint of a function that belongs to the class of functions represented by 3-DIMENSIONAL BROUWER.
For 1 ≤ i ≤ p(n) we introduce a vertex V (i) C such that for 1 ≤ j ≤ i, Ij(p[V (i) C ]) is the output of gate gj; for i < j ≤ p(n),
Ij(p[V (i) C ]) is 0.
Construct V (i) C from V (i−1) C using Corollary 2. Let G(i) be the graphical game that does this. Let S1(G(i) ), . . . , Sn(G(i) ) be a length n path decomposition of G(i) , where V (i−1) C ∈ S1(G(i) ) and V (i) C ∈ Sn(G(i) ).
Then, a path decomposition of ∪1≤i≤p(n)G(i) is obtained by taking the union of the separate path decompositions, together with Sn(G(i−1) ) ∪ S1(G(i) ) for 2 ≤ i ≤ p(n).
Let GC be the above graphical game that simulates C. GC has 3n inputs, consisting of the first n bits of the binary expansions of p[Vx], p[Vy] and p[Vz]. Similarly to [4], the output of GC affects Vx, Vy and Vz as follows. Colour 0 incentivizes Vx, Vy and Vz 108 to adjust their probabilities p[Vx], p[Vy] and p[Vz] in the direction (−1, −1, −1); colour 2 incentivizes them to move in direction (1, 0, 0); colour 2, direction (0, 1, 0); colour 3, direction (0, 0, 1).
We need to ensure that at points at the boundaries of adjacent cubelets, the change of direction will be approximately the average of directions of surrounding points. That way, all four colors/directions must be nearby so that they can cancel each other out (and we are at a panchromatic vertex). This is achieved using the same trick as [4], in which we make a constant number M of copies of GC, which differ in that each copy adds a tiny displacement vector to its copies of p[Vx], p[Vy] and p[Vz] (which are derived from the original using the addition gadget of [7]). Using the addition and multiplication gadgets of [7] we average the directions and add a small multiple of this average to (p[Vx], p[Vy], p[Vz]).
At a Nash equilibrium the outputs of each copy will cancel each other out. The pathwidth of the whole game is at most M times the pathwidth GC.
The most important problem left open by this paper is whether it is possible to find a Nash equilibrium of a graphical game on a bounded-degree tree in polynomial time. Our construction shows that any two-pass algorithm that explicitly stores breakpoint policies needs exponential time and space. However, it does not preclude the existence of an algorithm that is based on a similar idea, but, instead of computing the entire breakpoint policy for each vertex, uses a small number of additional passes through the graph to decide which (polynomial-sized) parts of each breakpoint policy should be computed. In particular, such an algorithm may be based on the approximation algorithm of [8], where the value of is chosen adaptively.
Another intriguing question is related to the fact that the graph for which we constructed an exponential-sized breakpoint policy has pathwidth 2, while our positive results are for a path, i.e., a graph of pathwidth 1. It is not clear if for any bounded-degree graph of pathwidth 1 the running time of (the breakpoint policybased version of) our algorithm will be polynomial. In particular, it is instructive to consider a caterpillar graph, i.e., the graph that can be obtained from Tn by deleting the vertices S1, . . . , Sn. For this graph, the best response policy of a vertex Vk in the spine of the caterpillar is obtained by combining the best response policy of its predecessor on the spine Vk−1 and its other child Tk; since the latter is a leaf, its best response policy is either trivial (i.e., [0, 1]2 , [0, 1]×{0}, or [0, 1]×{1}) or consists of two horizontal segments and one vertical segment of the form {α}×[0, 1] that connects them. Assuming for convenience that B(Vk, Tk) = [0, α]×{0} ∪ {α}×[0, 1] ∪ [α, 1]×{1}, and f is the indifference function for Vk, we observe that the best response policy for Vk consists of 5 components: ˆf(0), ˆf(1), and three components that correspond to [0, α]×{0}, {α}×[0, 1], and [α, 1]×{1}.
Hence, one can think of constructing B(Vk+1, Vk) as the following process: turn B(Vk, Vk−1) by π/2, cut it along the (now horizontal) line vk = α, apply a fractional linear transform to the horizontal coordinate of both parts, and reconnect them using the image of the segment {α}×[0, 1] under f. This implies that the problem of bounding the size of the best response policy (or, alternatively, the breakpoint policy), can be viewed as a generalization of the following computational geometry problem, which we believe may be of independent interest: PROBLEM 1. Given a collection of axis-parallel segments in R2 , consider the following operation: pick an axis-parallel line li (either vertical or horizontal), cut the plane along this line, and shift one of the resulting two parts by an arbitrary amount δi; as a result, some segments will be split into two parts. Reconnect these parts, i.e., for each segment of the form [a, b] × {c} that was transformed into [a, t] × {c + δi} and [t, b] × {c}, introduce a segment {t}×[c, c+δi]. Is it possible to start with the segment [0, 1] and after n operations obtain a set that cannot be represented as a union of poly(n) line segments? If yes, can it be the case that in this set, there is no path with a polynomial number of turns that connects the endpoints of the original segment?
It turns out that in general, the answer to the first question is positive, i.e., after n steps, it is possible to obtain a set that consists of Θ(cn ) segments for some c > 0. This implies that even for a caterpillar, the best response policy can be exponentially large.
However, in our example (which is omitted from this version of the paper due to space constraints), there exists a polynomial-size path through the best response policy, i.e., it does not prove that the breakpoint policy is necessarily exponential in size. If one can prove that this is always the case, it may be possible to adapt this proof to show that there can be an exponential gap between the sizes of best response policies and breakpoint policies.

A multiattribute auction is a market-based mechanism where goods are described by vectors of features, or attributes [3, 5, 8, 19]. Such mechanisms provide traders with the ability to negotiate over a multidimensional space of potential deals, delaying commitment to specific configurations until the most promising candidates are identified. For example, in a multiattribute auction for computers, the good may be defined by attributes such as processor speed, memory, and hard disk capacity. Agents have varying preferences (or costs) associated with the possible configurations. For example, a buyer may be willing to purchase a computer with a 2 GHz processor, 500 MB of memory, and a 50 GB hard disk for a price no greater than $500, or the same computer with 1GB of memory for a price no greater than $600.
Existing research in multiattribute auctions has focused primarily on one-sided mechanisms, which automate the process whereby a single agent negotiates with multiple potential trading partners [8, 7, 19, 5, 23, 22]. Models of procurement typically assume the buyer has a value function, v, ranging over the possible configurations, X, and that each seller i can similarly be associated with a cost function ci over this domain. The role of the auction is to elicit these functions (possibly approximate or partial versions), and identify the surplus-maximizing deal. In this case, such an outcome would be arg maxi,x v(x) − ci(x). This problem can be translated into the more familiar auction for a single good without attributes by computing a score for each attribute vector based on the seller valuation function, and have buyers bid scores. Analogs of the classic first- and second-price auctions correspond to firstand second-score auctions [8, 7].
In the absence of a published buyer scoring function, agents on both sides may provide partial specifications of the deals they are willing to engage. Research on such auctions has, for example, produced iterative mechanisms for eliciting cost functions incrementally [19]. Other efforts focus on the optimization problem facing the bid taker, for example considering side constraints on the combination of trades comprising an overall deal [4]. Side constraints have also been analyzed in the context of combinatorial auctions [6, 20].
Our emphasis is on two-sided multiattribute auctions, where multiple buyers and sellers submit bids, and the objective is to construct a set of deals maximizing overall surplus. Previous research on such auctions includes works by Fink et al. [12] and Gong [14], both of which consider a matching problem for continuous double auctions (CDAs), where deals are struck whenever a pair of compatible bids is identified.
In a call market, in contrast, bids accumulate until designated times (e.g., on a periodic or scheduled basis) at which the auction clears by determining a comprehensive match over the entire set of bids. Because the optimization is performed over an aggregated scope, call markets often enjoy liquidity and efficiency advantages over CDAs [10].1 Clearing a multiattribute CDA is much like clearing a one-sided multiattribute auction. Because nothing happens between bids, the problem is to match a given new bid (say, an offer to buy) with the existing bids on the other (sell) side. Multiattribute call markets are potentially much more complex. Constructing an optimal overall matching may require consideration of many different combina1 In the interim between clears, call markets may also disseminate price quotes providing summary information about the state of the auction [24]. Such price quotes are often computed based on hypothetical clears, and so the clearing algorithm may be invoked more frequently than actual market clearing operations. 110 tions of trades, among the various potential trading-partner pairings. The problem can be complicated by restrictions on overall assignments, as expressed in side constraints [16].
The goal of the present work is to develop a general framework for multiattribute call markets, to enable investigation of design issues and possibilities. In particular, we use the framework to explore tradeoffs between expressive power of agent bids and computational properties of auction clearing. We conduct our exploration independent of any consideration of strategic issues bearing on mechanism design. As with analogous studies of combinatorial auctions [18], we intend that tradeoffs quantified in this work can be combined with incentive factors within a comprehensive overall approach to multiattribute auction design.
We provide the formal semantics of multiattribute offers in our framework in the next section. We abstract, where appropriate, from the specific language used to express offers, characterizing expressiveness semantically in terms of what deals may be offered.
This enables us to identify some general conditions under which the problem of multilateral matching can be decomposed into bilateral matching problems. We then develop a family of network flow problems that capture corresponding classes of multiattribute call market optimizations. Experimental trials provide preliminary confirmation that the network formulations provide useful structure for implementing clearing algorithms.
The distinguishing feature of a multiattribute auction is that the goods are defined by vectors of attributes, x = (x1, . . . , xm), xj ∈ Xj . A configuration is a particular attribute vector, x ∈ X = Qm j=1 Xj . The outcome of the auction is a set of bilateral trades. Trade t takes the form t = (x, q, b, s, π), signifying that agent b buys q > 0 units of configuration x from seller s, for payment π > 0. For convenience, we use the notation xt to denote the configuration associated with trade t (and similarly for other elements of t). For a set of trades T, we denote by Ti that subset of T involving agent i (i.e., b = i or s = i). Let T denote the set of all possible trades.
A bid expresses an agent"s willingness to participate in trades.
We specify the semantics of a bid in terms of offer sets. Let OT i ⊆ Ti denote agent i"s trade offer set. Intuitively, this represents the trades in which i is willing to participate. However, since the outcome of the auction is a set of trades, several of which may involve agent i, we must in general consider willingness to engage in trade combinations. Accordingly, we introduce the combination offer set of agent i, OC i ⊆ 2Ti .
A fully expressive bid language would allow specification of arbitrary combination offer sets. We instead consider a more limited class which, while restrictive, still captures most forms of multiattribute bidding proposed in the literature. Our bids directly specify part of the agent"s trade offer set, and include further directives controlling how this can be extended to the full trade and combination offer sets.
For example, one way to specify a trade (buy) offer set would be to describe a set of configurations and quantities, along with the maximal payment one would exchange for each (x, q) specified.
This description could be by enumeration, or any available means of defining such a mapping.
An explicit set of trades in the offer set generally entails inclusion of many more implicit trades. We assume payment monotonicity, which means that agents always prefer more money. That is, for π > π > 0, (x, q, i, s, π) ∈ OT i ⇒ (x, q, i, s, π ) ∈ OT i , (x, q, b, i, π ) ∈ OT i ⇒ (x, q, b, i, π) ∈ OT i .
We also assume free disposal, which dictates that for all i, q > q > 0, (x, q , i, s, π) ∈ OT i ⇒ (x, q, i, s, π) ∈ OT i , (x, q, b, i, π) ∈ OT i ⇒ (x, q , b, i, π) ∈ OT i .
Note that the conditions for agents in the role of buyers and sellers are analogous. Henceforth, for expository simplicity, we present all definitions with respect to buyers only, leaving the definition for sellers as understood. Allowing agents" bids to comprise offers from both buyer and seller perspectives is also straightforward.
An assertion that offers are divisible entails further implicit members in the trade offer set.
DEFINITION 1 (DIVISIBLE OFFER). Agent i"s offer is divisible down to q iff ∀q < q < q. (x, q, i, s, π) ∈ OT i ⇒ (x, q , i, s, q q π) ∈ OT i .
We employ the shorthand divisible to mean divisible down to 0.
The definition above specifies arbitrary divisibility. It would likewise be possible to define divisibility with respect to integers, or to any given finite granularity. Note that when offers are divisible, it suffices to specify one offer corresponding to the maximal quantity one is willing to trade for any given configuration, trading partner, and per-unit payment (called the price).
At the extreme of indivisibility are all-or-none offers.
DEFINITION 2 (AON OFFER). Agent i"s offer is all-or-none (AON) iff (x, q, i, s, π) ∈ OT i ∧ (x, q , i, s, π ) ∈ OT i ⇒ [q = q ∨ π = π ].
In many cases, the agent will be indifferent with respect to different trading partners. In that event, it may omit the partner element from trades directly specified in its offer set, and simply assert that its offer is anonymous.
DEFINITION 3 (ANONYMITY). Agent i"s offer is anonymous iff ∀s, s , b, b . (x, q, i, s, π) ∈ OT i ⇔ (x, q, i, s , π) ∈ OT i ∧ (x, q, b, i, π) ∈ OT i ⇔ (x, q, b , i, π) ∈ OT i .
Because omitting trading partner qualifications simplifies the exposition, we generally assume in the following that all offers are anonymous unless explicitly specified otherwise. Extending to the non-anonymous case is conceptually straightforward. We employ the wild-card symbol ∗ in place of an agent identifier to indicate that any agent is acceptable.
To specify a trade offer set, a bidder directly specifies a set of willing trades, along with any regularity conditions (e.g., divisibility, anonymity) that implicitly extend the set. The full trade offer set is then defined by the closure of this direct set with respect to payment monotonicity, free disposal, and any applicable divisibility assumptions.
We next consider the specification of combination offer sets.
Without loss of generality, we restrict each trade set T ∈ OC i to include at most one trade for any combination of configuration and trading partner (multiple such trades are equivalent to one net trade aggregating the quantities and payments). The key question is to what extent the agent is willing to aggregate deals across configurations or trading partners. One possibility is disallowing any aggregation. 111 DEFINITION 4 (NO AGGREGATION). The no-aggregation combinations are given by ONA i = {∅} ∪ {{t} | t ∈ OT i }. Agent i"s offer exhibits non-aggregation iff OC i = ONA i .
We require in general that OC i ⊇ ONA i .
A more flexible policy is to allow aggregation across trading partners, keeping configuration constant.
DEFINITION 5 (PARTNER AGGREGATION). Suppose a particular trade is offered in the same context (set of additional trades,
T) with two different sellers, s and s . That is, {(x, q, i, s, π)} ∪ T ∈ OC i ∧ {(x, q, i, s , π)} ∪ T ∈ OC i .
Agent i"s offer allows seller aggregation iff in all such cases, {(x, q , i, s, π ), (x, q − q , i, s , π − π )} ∪ T ∈ OC i .
In other words, we may create new trade offer combinations by splitting the common trade (quantity and payment, not necessarily proportionately) between the two sellers.
In some cases, it might be reasonable to form combinations by aggregating different configurations.
DEFINITION 6 (CONFIGURATION AGGREGATION). Suppose agent i offers, in the same context, the same quantity of two (not necessarily different) configurations, x and x . That is, {(x, q, i, ∗, π)} ∪ T ∈ OC i ∧ {(x , q, i, ∗, π )} ∪ T ∈ OC i .
Agent i"s offer allows configuration aggregation iff in all such cases (and analogously when it is a seller), {(x, q , i, ∗, q q π), (x , q − q , i, ∗, q − q q π )} ∪ T ∈ OC i .
Note that combination offer sets can accommodate offerings of configuration bundles. However, classes of bundles formed by partner or configuration aggregation are highly regular, covering only a specific type of bundle formed by splitting a desired quantity across configurations. This is quite restrictive compared to the general combinatorial case.
An agent"s offer trade set implicitly defines the agent"s willingness to pay for any given configuration and quantity. We assume anonymity to avoid conditioning our definitions on trading partner.
DEFINITION 7 (WILLINGNESS TO PAY). Agent i"s willingness to pay for quantity q of configuration x is given by ˆuB i (x, q) = max π s.t. (x, q, i, ∗, π) ∈ OT i .
We use the symbol ˆu to recognize that willingness to pay can be viewed as a proxy for the agent"s utility function, measured in monetary units. The superscript B distinguishes the buyer"s willingnessto-pay function from, a seller"s willingness to accept, ˆuS i (x, q), defined as the minimum payment seller i will accept for q units of configuration x. We omit the superscript where the distinction is inessential or clear from context.
DEFINITION 8 (TRADE QUANTITY BOUNDS). Agent i"s minimum trade quantity for configuration x is given by qi(x) = min q s.t. ∃π. (x, q, i, ∗, π) ∈ OT i .
The agent"s maximum trade quantity for x is ¯qi(x) = max q s.t. ∃π. (x, q, i, ∗, π) ∈ OT i ∧ ¬∃q < q. (x, q , i, ∗, π) ∈ OT i .
When the agent has no offers involving x, we take qi(x) = ¯qi(x) =
It is useful to define a special case where all configurations are offered in the same quantity range.
DEFINITION 9 (CONFIGURATION PARITY). Agent i"s offers exhibit configuration parity iff qi(x) > 0 ∧ qi(x ) > 0 ⇒ qi(x) = qi(x ) ∧ ¯qi(x) = ¯qi(x ).
Under configuration parity we drop the arguments from trade quantity bounds, yielding the constants ¯q and q which apply to all offers.
DEFINITION 10 (LINEAR PRICING). Agent i"s offers exhibit linear pricing iff for all qi(x) ≤ q ≤ ¯qi(x), ˆui(x, q) = q ¯qi(x) ˆui(x, ¯qi(x)).
Note that linear pricing assumes divisibility down to qi(x). Given linear pricing, we can define the unit willingness to pay, ˆui(x) = ˆui(x, ¯qi(x))/¯qi(x), and take ˆui(x, q) = qˆui(x) for all qi(x) ≤ q ≤ ¯qi(x).
In general, an agent"s willingness to pay may depend on a context of other trades the agent is engaging in.
DEFINITION 11 (WILLINGNESS TO PAY IN CONTEXT). Agent i"s willingness to pay for quantity q of configuration x in the context of other trades T is given by ˆuB i (x, q; T) = max π s.t. {(x, q, i, s, π)} ∪ Ti ∈ OC i .
LEMMA 1. If OC i is either non aggregating, or exhibits linear pricing, then ˆuB i (x, q; T) = ˆuB i (x, q).
DEFINITION 12 (TRADE SURPLUS). The surplus of trade t = (x, q, b, s, π) is given by σ(t) = ˆuB b (x, q) − ˆuS s (x, q).
Note that the trade surplus does not depend on the payment, which is simply a transfer from buyer to seller.
DEFINITION 13 (TRADE UNIT SURPLUS). The unit surplus of trade t = (x, q, b, s, π) is given by σ1 (t) = σ(t)/q.
Under linear pricing, we can equivalently write σ1 (t) = ˆuB b (x) − ˆuS s (x).
DEFINITION 14 (SURPLUS OF A TRADE IN CONTEXT). The surplus of trade t = (x, q, b, s, π) in the context of other trades T, σ(t; T), is given by ˆuB b (x, q; T) − ˆuS s (x, q; T).
DEFINITION 15 (GMAP). The Global Multiattribute Allocation Problem (GMAP) is to find the set of acceptable trades maximizing total surplus, max T ∈2T X t∈T σ(t; T \ {t}) s.t. ∀i. Ti ∈ OC i .
DEFINITION 16 (MMP). The Multiattribute Matching Problem (MMP) is to find a best trade for a given pair of traders,
MMP(b, s) = arg max t∈OT b ∩OT s σ(t).
If OT b ∩ OT s is empty, we say that MMP has no solution. 112 Proofs of all the following results are provided in an extended version of this paper available from the authors.
THEOREM 2. Suppose all agents" offers exhibit no aggregation (Definition 4). Then the solution to GMAP consists of a set of trades, each of which is a solution to MMP for its specified pair of traders.
THEOREM 3. Suppose that each agent"s offer set satisfies one of the following (not necessarily the same) sets of conditions.
(Definitions 1, 10, and 9), with combination offer set defined as the minimal set consistent with configuration aggregation (Definition 6).2 Then the solution to GMAP consists of a set of trades, each of which employs a configuration that solves MMP for its specified pair of traders.
Let MMPd (b, s) denote a modified version of MMP, where OT b and OT s are extended to assume divisibility (i.e., the offer sets are taken to be their closures under Definition 1). Then we can extend Theorem 3 to allow aggregating agents to maintain AON or minquantity offers as follows.
THEOREM 4. Suppose offer sets as in Theorem 3, except that agents i satisfying configuration aggregation need be divisible only down to qi, rather than down to 0. Then the solution to GMAP consists of a set of trades, each of which employs the same configuration as a solution to MMPd for its specified pair of traders.
THEOREM 5. Suppose agents b and s exhibit configuration parity, divisibility, and linear pricing, and there exists configuration x such that ˆub(x) − ˆus(x) > 0. Then t ∈ MMPd (b, s) iff xt = arg max x {ˆub(x) − ˆus(x)} qt = min(¯qb, ¯qs). (1) The preceding results signify that under certain conditions, we can divide the global optimization problem into two parts: first find a bilateral trade that maximizes unit surplus for each pair of traders (or total surplus in the non-aggregation case), and then use the results to find a globally optimal set of trades. In the following two sections we investigate each of these subproblems.
We turn next to consider the problem of finding a best deal between pairs of traders. The complexity of MMP depends pivotally on the representation by bids of offer sets, an issue we have postponed to this point.
Note that issues of utility representation and MMP apply to a broad class of multiattribute mechanisms, beyond the multiattribute call markets we emphasize. For example, the complexity results contained in this section apply equally to the bidding problem faced by sellers in reverse auctions, given a published buyer scoring function.
The simplest representation of an offer set is a direct enumeration of configurations and associated quantities and payments. This approach treats the configurations as atomic entities, making no use 2 That is, for such an agent i, OC i is the closure under configuration aggregation of ONA i . of attribute structure. A common and inexpensive enhancement is to enable a trader to express sets of configurations, by specifying subsets of the domains of component attributes. Associating a single quantity and payment with a set of configurations expresses indifference among them; hence we refer to such a set as an indifference range.3 Indifference ranges include the case of attributes with a natural ordering, in which a bid specifies a minimum or maximum acceptable attribute level. The use of indifference ranges can be convenient for MMP. The compatibility of two indifference ranges is simply found by testing set intersection for each attribute, as demonstrated by the decision-tree algorithm of Fink et al. [12].
Alternatively, bidders may specify willingness-to-pay functions ˆu in terms of compact functional forms. Enumeration based representations, even when enhanced with indifference ranges, are ultimately limited by the exponential size of attribute space.
Functional forms may avoid this explosion, but only if ˆu reflects structure among the attributes. Moreover, even given a compact specification of ˆu, we gain computational benefits only if we can perform the matching without expanding the ˆu values of an exponential number of configuration points.
One particularly useful multiattribute representation is known as the additive scoring function. Though this form is widely used in practice and in the academic literature, it is important to stress the assumptions behind it. The theory of multiattribute representation is best developed in the context where ˆu is interpreted as a utility function representing an underlying preference order [17]. We present the premises of additive utility theory in this section, and discuss some generalizations in the next.
DEFINITION 17. A set of attributes Y ⊂ X is preferentially independent (PI) of its complement Z = X \ Y if the conditional preference order over Y given a fixed level Z0 of Z is the same regardless of the choice of Z0 .
In other words, the preference order over the projection of X on the attributes in Y is the same for any instantiation of the attributes in Z.
DEFINITION 18. X = {x1, . . . , xm} is mutually preferentially independent (MPI) if any subset of X is preferentially independent of its complement.
THEOREM 6 ([9]). A preference order over set of attributes X has an additive utility function representation u(x1, . . . , xm) = mX i=1 ui(xi) iff X is mutually preferential independent.
A utility function over outcomes including money is quasi-linear if the function can be represented as a function over non-monetary attributes plus payments, π. Interpreting ˆu as a utility function over non-monetary attributes is tantamount to assuming quasi-linearity.
Even when quasi-linearity is assumed, however, MPI over nonmonetary attributes is not sufficient for the quasi-linear utility function to be additive. For this, we also need that each of the pairs (π, Xi) for any attribute Xi would be PI of the rest of the attributes. 3 These should not be mistaken with indifference curves, which express dependency between the attributes. Indifference curves can be expressed by the more elaborate utility representations discussed below. 113 This (by MAUT) in turn implies that the set of attributes including money is MPI and the utility function can be represented as u(x1, . . . , xm, π) = mX i=1 ui(xi) + π.
Given that form, a willingness-to-pay function reflecting u can be represented additively, as ˆu(x) = mX i=1 ui(xi) In many cases the additivity assumption provides practically crucial simplification of offer set elicitation. In addition to compactness, additivity dramatically simplifies MMP. If both sides provide additive ˆu representations, the globally optimal match reduces to finding the optimal match separately for each attribute.
A common scenario in procurement has the buyer define an additive scoring function, while suppliers submit enumerated offer points or indifference ranges. This model is still very amenable to MMP: for each element in a supplier"s enumerated set, we optimize each attribute by finding the point in the supplier"s allowable range that is most preferred by the buyer.
A special type of scoring (more particularly, cost) function was defined by Bichler and Kalagnanam [4] and called a configurable offer. This idea is geared towards procurement auctions: assuming suppliers are usually comfortable with expressing their preferences in terms of cost that is quasi-linear in every attribute, they can specify a price for a base offer, and additional cost for every change in a specific attribute level. This model is essentially a pricing out approach [17]. For this case, MMP can still be optimized on a per-attribute basis. A similar idea has been applied to one-sided iterative mechanisms [19], in which sellers refine prices on a perattribute basis at each iteration.
Under MPI, the tradeoffs between the attributes in each subset cannot be affected by the value of other attributes. For example, when buying a PC, a weaker CPU may increase the importance of the RAM compared to, say, the type of keyboard. Such relationships cannot be expressed under an additive model.
Multiattribute utility theory (MAUT) develops various compact representations of utility functions that are based on weaker structural assumptions [17, 2]. There are several challenges in adapting these techniques to multiattribute bidding. First, as noted above, the theory is developed for utility functions, which may behave differently from willingness-to-pay functions. Second, computational efficiency of matching has not been an explicit goal of most work in the area. Third, adapting such representations to iterative mechanisms may be more challenging.
One representation that employs somewhat weaker assumptions than additivity, yet retains the summation structure is the generalized additive (GA) decomposition: u(x) = JX j=1 fj(xj ), xj ∈ Xj , (2) where the Xj are potentially overlapping sets of attributes, together exhausting the space X.
A key point from our perspective is that the complexity of the matching is similar to the complexity of optimizing a single function, since the sum function is in the form (2) as well. Recent work by Gonzales and Perny [15] provides an elicitation process for GA decomposable preferences under certainty, as well as an optimization algorithm for the GA decomposed function. The complexity of exact optimization is exponential in the induced width of the graph.
However, to become operational for multiattribute bidding this decomposition must be detectable and verifiable by statements over preferences with respect to price outcomes. We are exploring this topic in ongoing work [11].
CONSTRAINTS Theorems 2, 3, and 4 establish conditions under which GMAP solutions must comprise elements from constituent MMP solutions.
In Sections 5.1 and 5.2, we show how to compute these GMAP solutions, given the MMP solutions, under these conditions. In these settings, traders that aggregate partners also aggregate configurations; hence we refer to them simply as aggregating or nonaggregating. Section 5.3 suggests a means to relax the linear pricing restriction employed in these constructions. Section 5.4 provides strategies for allowing traders to aggregate partners and restrict configuration aggregation at the same time.
Our clearing algorithms are based on network flow formulations of the underlying optimization problem [1]. The network model is based on a bipartite graph, in which nodes on the left side represent buyers, and nodes on the right represent sellers. We denote the sets of buyers and sellers by B and S, respectively.
We define two graph families, one for the case of non-aggregating traders (called single-unit), and the other for the case of aggregating traders (called multi-unit).4 For both types, a single directed arc is placed from a buyer i ∈ B to a seller j ∈ S if and only if MMP(i, j) is nonempty. We denote by T(i) the set of potential trading partners of trader i (i.e., the nodes connected to buyer or seller i in the bipartite graph.
In the single-unit case, we define the weight of an arc (i, j) as wij = σ(MMP(i, j)). Note that free disposal lets a buy offer receive a larger quantity than desired (and similarly for sell offers).
For the multi-unit case, the weights are wij = σ1 (MMP(i, j)), and we associate the quantity ¯qi with the node for trader i. We also use the notation qij for the mathematical formulations to denote partial fulfillment of qt for t = MMP(i, j).
Constraints Under the restrictions of Theorems 2, 3, or 4, and when the solution to MMP is given, GMAP exhibits strong similarity to the problem of clearing double auctions with assignment constraints [16]. A match in our bipartite representation corresponds to a potential trade in which assignment constraints are satisfied. Network flow formulations have been shown to model this problem under the assumption of indivisibility and aggregation for all traders. The novelty in this part of our work is the use of generalized network flow formulations for more complex cases where aggregation and divisibility may be controlled by traders.
Initially we examine the simple case of no aggregation (Theorem 2). Observe that the optimal allocation is simply the solution to the well known weighted assignment problem [1] on the singleunit bipartite graph described above. The set of matches that maximizes the total weight of arcs corresponds to the set of trades that maximizes total surplus. Note that any form of (in)divisibility can 4 In the next section, we introduce a hybrid form of graph accommodating mixes of the two trader categories. 114 also be accommodated in this model via the constituent MMP subproblems.
The next formulation solves the case in which all traders fall under case 2 of Theorem 3-that is, all traders are aggregating and divisible, and exhibit linear pricing. This case can be represented using the following linear program, corresponding to our multi-unit graph: max X i∈B,j∈S wij qij s.t.
X i∈T (j) qij ≤ ¯qj j ∈ S X j∈T (i) qij ≤ ¯qi i ∈ B qij ≥ 0 j ∈ S, i ∈ B Recall that the qij variables in the solution represent the number of units that buyer i procures from seller j. This formulation is known as the network transportation problem with inequality constraints, for which efficient algorithms are available [1]. It is a well known property of the transportation problem (and flow problems on pure networks in general) that given integer input values, the optimal solution is guaranteed to be integer as well. Figure 1 demonstrates the transformation of a set of bids to a transportation problem instance.
Figure 1: Multi-unit matching with two boolean attributes. (a) Bids, with offers to buy in the left column and offers to sell at right. q@p indicates an offer to trade q units at price p per unit. Configurations are described in terms of constraints on attribute values. (b) Corresponding multi-unit assignment model. W represents arc weights (unit surplus), s represents source (exogenous) flow, and t represents sink quantity.
The problem becomes significantly harder when aggregation is given as an option to bidders, requiring various enhancements to the basic multi-unit bipartite graph described above. In general, we consider traders that are either aggregating or not, with either divisible or AON offers.
Initially we examine a special case, which at the same time demonstrates the hardness of the problem but still carries computational advantages. We designate one side (e.g., buyers) as restrictive (AON and non-aggregating), and the other side (sellers) as unrestrictive (divisible and aggregating). This problem can be represented using the following integer programming formulation: max X i∈B,j∈S wij qij s.t.
X i∈T (j) ¯qiqij ≤ ¯qj j ∈ S X j∈T (i) qij ≤ 1 i ∈ B qij ∈ {0, 1} j ∈ S, i ∈ B (3) This formulation is a restriction of the generalized assignment problem (GAP) [13]. Although GAP is known to be NP-hard, it can be solved relatively efficiently by exact or approximate algorithms.
GAP is more general than the formulation above as it allows buyside quantities (¯qi above) to be different for each potential seller.
That this formulation is NP-hard as well (even the case of a single seller corresponds to the knapsack problem), illustrates the drastic increase in complexity when traders with different constraints are admitted to the same problem instance.
Other than the special case above, we found no advantage in limiting AON constraints when traders may specify aggregation constraints. Therefore, the next generalization allows any combination of the two boolean constraints, that is, any trader chooses among four bid types: NI Bid AON and not aggregating.
AD Bid allows aggregation and divisibility.
AI Bid AON, allows aggregation (quantity can be aggregated across configurations, as long as it sums to the whole amount).
ND No aggregation, divisibility (one trade, but smaller quantities are acceptable).
To formulate an integer programming representation for the problem, we introduce the following variables. Boolean (0/1) variables ri and rj indicate whether buyer i and seller j participate in the solution (used for AON traders). Another indicator variable, yij , applied to non-aggregating buyer i and seller j, is one iff i trades with j. For aggregating traders, yij is not constrained. max X i∈B,j∈S Wij qij (4a) s.t.
X j∈T (i) qij = ¯qiri i ∈ AIb (4b) X j∈T (i) qij ≤ ¯qiri i ∈ ADb (4c) X i∈T (j) qij = ¯qirj j ∈ AIs (4d) X i∈T (j) qij ≤ qj rj j ∈ ADs (4e) xij ≤ ¯qiyij i ∈ NDb , j ∈ T(i) (4f) xij ≤ ¯qj yij j ∈ NIs , i ∈ T(j) (4g) X j∈T (i) yij ≤ ri i ∈ NIb ∪ NDb (4h) X i∈T (j) yij ≤ rj j ∈ NIs ∪ NDs (4i) int qij (4j) yij , rj, ri ∈ {0, 1} (4k) 115 Figure 2: Generalized network flow model. B1 is a buyer in AD, B2 ∈ NI, B3 ∈ AI, B4 ∈ ND. V 1 is a seller in ND,
V 2 ∈ AI, V 4 ∈ AD. The g values represent arc gains.
Problem (4) has additional structure as a generalized min-cost flow problem with integral flow.5 A generalized flow network is a network in which each arc may have a gain factor, in addition to the pure network parameters (which are flow limits and costs).
Flow in an arc is then multiplied by its gain factor, so that the flow that enters the end node of an arc equals the flow that entered from its start node, multiplied by the gain factor of the arc. The network model can in turn be translated into an IP formulation that captures such structure.
The generalized min-cost flow problem is well-studied and has a multitude of efficient algorithms [1]. The faster algorithms are polynomial in the number of arcs and the logarithm of the maximal gain, that is, performance is not strongly polynomial but is polynomial in the size of the input. The main benefit of this graphical formulation to our matching problem is that it provides a very efficient linear relaxation. Integer programming algorithms such as branch-and-bound use solutions to the linear relaxation instance to bound the optimal integer solution. Since network flow algorithms are much faster than arbitrary linear programs (generalized network flow simplex algorithms have been shown to run in practice only 2 or 3 times slower than pure network min-cost flow [1]), we expect a branch-and-bound solver for the matching problem to show improved performance when taking advantage of network flow modeling.
The network flow formulation is depicted in Figure 2.
Nonrestrictive traders are treated as in Figure 1. For a non-aggregating buyer, a single unit from the source will saturate up to one of the yij for all j, and be multiplied by ¯qi. If i ∈ ND, the end node of yij will function as a sink that may drain up to ¯qi of the entering flow. For i ∈ NI we use an indicator (0/1) arc ri, on which the flow is multiplied by ¯qi. Trader i trades the full quantity iff ri = 1.
At the seller side, the end node of a qij arc functions as a source for sellers j ∈ ND, in order to let the flow through yij arcs be 0 or ¯qj. The flow is then multiplied by 1 ¯qj so 0/1 flows enter an end node which can drain either 1 or 0 units. for sellers j ∈ NI arcs rj ensure AON similarly to arcs rj for buyers.
Having established this framework, we are ready to accommo5 Constraint (4j) could be omitted (yielding computational savings) if non-integer quantities are allowed. Here and henceforth we assume the harder problem, where divisibility is with respect to integers. date more flexible versions of side constraints. The first generalization is to replace the boolean AON constraint with divisibility down to q, the minimal quantity. In our network flow instance we simply need to turn the node of the constrained trader i (e.g., the node B3 in Figure 2) to a sink that can drain up to ¯qi − qi units of flow. The integer program (4) can be also easily changed to accommodate this extension.
Using gains, we can also apply batch size constraints. If a trader specifies a batch size β, we change the gain on the r arcs to β, and set the available flow of its origin to the maximal number of batches ¯qi/β.
A key assumption in handling aggregation up to this point is linear pricing, which enables us to limit attention to a single unit price. Divisibility without linear pricing allows expression of concave willingness-to-pay functions, corresponding to convex preference relations. Bidders may often wish to express non-convex offer sets, for example, due to fixed costs or switching costs in production settings [21].
We consider nonlinear pricing in the form of enumerated payment schedules-that is, defining values ˆu(x, q) for a select set of quantities q. For the indivisible case, these points are distinguished in the offer set by satisfying the following: ∃π. (x, q, i, ∗, π) ∈ OT i ∧ ¬∃q < q. (x, q , i, ∗, π) ∈ OT i . (cf. Definition 8, which defines the maximum quantity, ¯q, as the largest of these.) For the divisible case, the distinguished quantities are those where the unit price changes, which can be formalized similarly.
To handle nonlinear pricing, we augment the network to include flow possibilities corresponding to each of the enumerated quantities, plus additional structure to enforce exclusivity among them.
In other words, the network treats the offer for a given quantity as in Section 5.2, and embeds this in an XOR relation to ensure that each trader picks only one of these quantities. Since for each such quantity choice we can apply Theorem 3 or 4, the solution we get is in fact the solution to GMAP.
The network representation of the XOR relation (which can be embedded into the network of Figure 2) is depicted in Figure 3. For a trader i with K XOR quantity points, we define dummy variables, zk i , k = 1, . . . , K. Since we consider trades between every pair of quantity points we also have qk ij , k = 1, . . . , K. For buyer i ∈ AI with XOR points at quantities ¯qk i , we replace (4b) with the following constraints: X j∈T (i) qk ij = ¯qk i zk i k = 1, . . . , K KX k=1 zk i = ri zk i ∈ {0, 1} k = 1, . . . , K (5)
The model (4) handles constraints over the aggregation of quantities from different trading partners. When aggregation is allowed, the formulation permits trades involving arbitrary combinations of configurations. A homogeneity constraint [4] restricts such combinations, by requiring that configurations aggregated in an overall deal must agree on some or all attributes. 116 Figure 3: Extending the network flow model to express an XOR over quantities. B2 has 3 XOR points for 6, 3, or 5 units.
In the presence of homogeneity constraints, we can no longer apply the convenient separation of GMAP into MMP plus global bipartite optimization, as the solution to GMAP may include trades not part of any MMP solution. For example, let buyer b specify an offer for maximum quantity 10 of various acceptable configurations, with a homogeneity constraint over the attribute color.
This means b is willing to aggregate deals over different trading partners and configurations, as long as all are the same color. If seller s can provide 5 blue units or 5 green units, and seller s can provide only 5 green units, we may prefer that b and s trade on green units, even if the local surplus of a blue trade is greater.
Let {x1, . . . , xH} be attributes that some trader constrains to be homogeneous. To preserve the network flow framework, we need to consider, for each trader, every point in the product domain of these homogeneous attributes. Thus, for every assignment ˆx to the homogeneous attributes, we compute MMP(b, s) under the constraint that configurations are consistent with ˆx. We apply the same approach as in Section 5.3: solve the global optimization, such that the alternative ˆx assignments for each trader are combined under XOR semantics, thus enforcing homogeneity constraints.
The size of this network is exponential in the number of homogeneous attributes, since we need a node for each point in the product domain of all the homogeneous attributes of each trader.6 Hence this solution method will only be tractable in applications were the traders can be limited to a small number of homogeneous attributes.
It is important to note that the graph needs to include a node only for each point that potentially matches a point of the other side. It is therefore possible to make the problem tractable by limiting one of the sides to a less expressive bidding language, and by that limit the set of potential matches. For example, if sellers submit bounded sets of XOR points, we only need to consider the points in the combined set offered by the sellers, and the reduction to network flow is polynomial regardless of the number of homogeneous attributes.
If such simplifications do not apply, it may be preferable to solve the global problem directly as a single optimization problem. We provide the formulation for the special case of divisibility (with respect to integers) and configuration parity. Let i index buyers, j sellers, and H homogeneous attributes. Variable xh ij ∈ Xh represents the value of attribute Xh in the trade between buyer i and seller j. Integer variable qij represents the quantity of the trade (zero for no trade) between i and j. 6 If traders differ on which attributes they express such constraints, we can limit consideration to the relevant alternatives. The complexity will still be exponential, but in the maximum number of homogeneous attributes for any pair of traders. max X i∈B,j∈S [ˆuB i (xij , qij ) − ˆuS j (xij , qij )] X j∈S qij ≤ ¯qi i ∈ B X i∈B qij ≤ ¯qj j ∈ S xh 1j = xh 2j = · · · = x|B|j j ∈ S, h ∈ {1, . . . , H} xh i1 = xh i2 = · · · = xi|S| i ∈ B, h ∈ {1, . . . , H} (6) Table 1 summarizes the mapping we presented from allocation constraints to the complexity of solving GMAP. Configuration parity is assumed for all cases but the first.
We approach the experimental aspect of this work with two objectives. First, we seek a general idea of the sizes and types of clearing problems that can be solved under given time constraints. We also look to compare the performance of a straightforward integer program as in (4) with an integer program that is based on the network formulations developed here. Since we used CPLEX, a commercial optimization tool, the second objective could be achieved to the extent that CPLEX can take advantage of network structure present in a model.
We found that in addition to the problem size (in terms of number of traders), the number of aggregating traders plays a crucial role in determining complexity. When most of the traders are aggregating, problems of larger sizes can be solved quickly. For example, our IP model solved instances with 600 buyers and 500 sellers, where 90% of them are aggregating, in less than two minutes. When the aggregating ratio was reduced to 80% for the same data, solution time was just under five minutes.
These results motivated us to develop a new network model.
Rather than treat non-aggregating traders as a special case, the new model takes advantage of the single-unit nature of non-aggregating trades (treating the aggregating traders as a special case). This new model outperformed our other models on most problem instances, exceptions being those where aggregating traders constitute a vast majority (at least 80%).
This new model (Figure 4) has a single node for each non aggregating trader, with a single-unit arc designating a match to another non-aggregating trader. An aggregating trader has a node for each potential match, connected (via y arcs) to a mutual source node.
Unlike the previous model we allow fractional flow for this case, representing the traded fraction of the buyer"s total quantity.7 We tested all three models on random data in the form of bipartite graphs encoding MMP solutions. In our experiments, each trader has a maximum quantity uniformly distributed over [30, 70], and minimum quantity uniformly distributed from zero to maximal quantity. Each buyer/seller pair is selected as matching with probability 0.75, with matches assigned a surplus uniformly distributed over [10, 70]. Whereas the size of the problem is defined by the number of traders on each side, the problem complexity depends on the product |B| × |S|. The tests depicted in Figures 5-7 are for the worst case |B| = |S|, with each data point averaged over six samples. In the figures, the direct IP (4) is designated SW, our first network model (Figure 2) NW, and our revised network model (Figure 4) NW 2. 7 Traded quantity remains integer. 117 Aggregation Hom. attr. Divisibility linear pricing Technique Complexity No aggregation N/A Any Not required Assignment problem Polynomial All aggregate None Down to 0 Required Transpor. problem Polynomial One side None Aggr side div. Aggr. side GAP NP-hard Optional None Down to q, batch Required Generalized ntwrk flow NP-hard Optional Bounded Down to q, batch Bounded size schdl. Generalized ntwrk flow NP-hard Optional Not bounded Down to q, batch Not required Nonlinear opt Depends on ˆu(x, q) Table 1: Mapping from combinations of allocation constraints to the solution methods of GMAP. One Side means that one side aggregates and divisible, and the other side is restrictive. Batch means that traders may submit batch sizes.
Figure 4: Generalized network flow model. B1 is a buyer in AD, B2 ∈ AI, B3 ∈ NI, B4 ∈ ND. V 1 is a seller in AD,
V 2 ∈ AI, V 4 ∈ ND. The g values represent arc gains, and W values represent weights.
Figure 5: Average performance of models when 30% of traders aggregate.
Figure 6: Average performance of models when 50% of traders aggregate.
Figure 7: Average performance of models when 70% of traders aggregate. 118 Figure 8: Performance of models when varying percentage of aggregating traders Figure 8 shows how the various models are affected by a change in the percentage of aggregating traders, holding problem size fixed.8 Due to the integrality constraints, we could not test available algorithms specialized for network-flow problems on our test problems. Thus, we cannot fully evaluate the potential gain attributable to network structure. However, the model we built based on the insight from the network structure clearly provided a significant speedup, even without using a special-purpose algorithm. Model NW 2 provided speedups of a factor of 4-10 over the model SW.
This was consistent throughout the problem sizes, including the smaller sizes for which the speedup is not visually apparent on the chart.
The implementation and deployment of market exchanges requires the development of bidding languages, information feedback policies, and clearing algorithms that are suitable for the target domain, while paying heed to the incentive properties of the resulting mechanisms. For multiattribute exchanges, the space of feasible such mechanisms is constrained by computational limitations imposed by the clearing process. The extent to which the space of feasible mechanisms may be quantified a priori will facilitate the search for such exchanges in the full mechanism design problem.
In this work, we investigate the space of two-sided multiattribute auctions, focusing on the relationship between constraints on the offers traders can express through bids, and the resulting computational problem of determining an optimal set of trades. We developed a formal semantic framework for characterizing expressible offers, and introduced some basic classes of restrictions. Our key technical results identify sets of conditions under which the overall matching problem can be separated into first identifying optimal pairwise trades and subsequently optimizing combinations of those trades. Based on these results, we developed network flow models for the overall clearing problem, which facilitate classification of problem versions by computational complexity, and provide guidance for developing solution algorithms and relaxing bidding constraints.
This work was supported in part by NSF grant IIS-0205435, and the STIET program under NSF IGERT grant 0114368. We are 8 All tests were performed on Intel 3.4 GHz processors with 2048 KB cache. Test that did not complete by the one-hour time limit were recorded as 4000 seconds. grateful to comments from an anonymous reviewer. Some of the underlying ideas were developed while the first two authors worked at TradingDynamics Inc. and Ariba Inc. in 1999-2001 (cf. US Patent 6,952,682). We thank Yoav Shoham, Kumar Ramaiyer, and Gopal Sundaram for fruitful discussions about multiattribute auctions in that time frame.
[1] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin. Network Flows.
Prentice-Hall, 1993. [2] F. Bacchus and A. Grove. Graphical models for preference and utility. In Eleventh Conference on Uncertainty in Artificial Intelligence, pages 3-10, Montreal, 1995. [3] M. Bichler. The Future of e-Markets: Multi-Dimensional Market Mechanisms. Cambridge U. Press, New York, NY, USA, 2001. [4] M. Bichler and J. Kalagnanam. Configurable offers and winner determination in multi-attribute auctions. European Journal of Operational Research, 160:380-394, 2005. [5] M. Bichler, M. Kaukal, and A. Segev. Multi-attribute auctions for electronic procurement. In Proceedings of the 1st IBM IAC Workshop on Internet Based Negotiation Technologies, 1999. [6] C. Boutilier, T. Sandholm, and R. Shields. Eliciting bid taker non-price preferences in (combinatorial) auctions. In Nineteenth Natl. Conf. on Artificial Intelligence, pages 204-211, San Jose, 2004. [7] F. Branco. The design of multidimensional auctions. RAND Journal of Economics, 28(1):63-81, 1997. [8] Y.-K. Che. Design competition through multidimensional auctions.
RAND Journal of Economics, 24(4):668-680, 1993. [9] G. Debreu. Topological methods in cardinal utility theory. In K. Arrow, S. Karlin, and P. Suppes, editors, Mathematical Methods in the Social Sciences. Stanford University Press, 1959. [10] N. Economides and R. A. Schwartz. Electronic call market trading.
Journal of Portfolio Management, 21(3), 1995. [11] Y. Engel and M. P. Wellman. Multiattribute utility representation for willingness-to-pay functions. Tech. report, Univ. of Michigan, 2006. [12] E. Fink, J. Johnson, and J. Hu. Exchange market for complex goods: Theory and experiments. Netnomics, 6(1):21-42, 2004. [13] M. L. Fisher, R. Jaikumar, and L. N. Van Wassenhove. A multiplier adjustment method for the generalized assignment problem.
Management Science, 32(9):1095-1103, 1986. [14] J. Gong. Exchanges for complex commodities: Search for optimal matches. Master"s thesis, University of South Florida, 2002. [15] C. Gonzales and P. Perny. GAI networks for decision making under certainty. In IJCAI-05 workshop on preferences, Edinburgh, 2005. [16] J. R. Kalagnanam, A. J. Davenport, and H. S. Lee. Computational aspects of clearing continuous call double auctions with assignment constraints and indivisible demand. Electronic Commerce Research, 1(3):221-238, 2001. [17] R. L. Keeney and H. Raiffa. Decisions with Multiple Objectives: Preferences and Value Tradeoffs. Wiley, 1976. [18] N. Nisan. Bidding and allocation in combinatorial auctions. In Second ACM Conference on Electronic Commerce, pages 1-12,

In recent years there has been an explosive increase in the automation of modern equity markets. This increase has taken place both in the exchanges, which are increasingly computerized and offer sophisticated interfaces for order placement and management, and in the trading activity itself, which is ever more frequently undertaken by software. The so-called Electronic Communication Networks (or ECNs) that dominate trading in NASDAQ stocks are a common example of the automation of the exchanges. On the trading side, computer programs now are entrusted not only with the careful execution of large block trades for clients (sometimes referred to on Wall Street as program trading), but with the autonomous selection of stocks, direction (long or short) and volumes to trade for profit (commonly referred to as statistical arbitrage).
The vast majority of equity trading is done via the standard limit order market mechanism. In this mechanism, continuous trading takes place via the arrival of limit orders specifying whether the party wishes to buy or sell, the volume desired, and the price offered. Arriving limit orders that are entirely or partially executable with the best offers on the other side are executed immediately, with any volume not immediately executable being placed in an queue (or book) ordered by price on the appropriate side (buy or sell). (A detailed description of the limit order mechanism is given in Section 3.) While traders have always been able to view the prices at the top of the buy and sell books (known as the bid and ask), a relatively recent development in certain exchanges is the real-time revelation of the entire order book - the complete distribution of orders, prices and volumes on both sides of the exchange. With this revelation has come the opportunity - and increasingly, the needfor modeling and exploiting limit order data and dynamics. It is fair to say that market microstructure, as this area is generally known, is a topic commanding great interest both in the real markets and in the academic finance literature. The opportunities and needs span the range from the optimized execution of large trades to the creation of stand-alone proprietary strategies that attempt to profit from high-frequency microstructure signals.
In this paper we investigate a previously unexplored but fundamental aspect of limit order microstructure: the stability properties of the dynamics. Specifically, we are interested in the following natural question: To what extent are simple models of limit order markets either susceptible or immune to butterfly effects - that is, the infliction of large changes in important activity statistics (such as the 120 number of shares traded or the average price per share) by only minor perturbations of the order sequence?
To examine this question, we consider two stylized but natural models of the limit order arrival process. In the absolute price model, buyers and sellers arrive with limit order prices that are determined independently of the current state of the market (as represented by the order books), though they may depend on all manner of exogenous information or shocks, such as time, news events, announcements from the company whose shares are being traded, private signals or state of the individual traders, etc. This process models traditional fundamentals-based trading, in which market participants each have some inherent but possibly varying valuation for the good that in turn determines their limit price.
In contrast, in the relative price model, traders express their limit order prices relative to the best price offered in their respective book (buy or sell). Thus, a buyer would encode their limit order price as an offset ∆ (which may be positive, negative, or zero) from the current bid pb, which is then translated to the limit price pb +∆. Again, in addition to now depending on the state of the order books, prices may also depend on all manner of exogenous information.
The relative price model can be viewed as modeling traders who, in addition to perhaps incorporating fundamental external information on the stock, may also position their orders strategically relative to the other orders on their side of the book. A common example of such strategic behavior is known as penny-jumping on Wall Street, in which a trader who has in interest in buying shares quickly, but still at a discount to placing a market order, will deliberately position their order just above the current bid. More generally, the entire area of modern execution optimization [9, 10, 8] has come to rely heavily on the careful positioning of limit orders relative to the current order book state. Note that such positioning may depend on more complex features of the order books than just the current bid and ask, but the relative model is a natural and simplified starting point.
We remark that an alternate view of the two models is that all traders behave in a relative manner, but with absolute traders able to act only on a considerably slower time scale than the faster relative traders.
How do these two models differ? Clearly, given any fixed sequence of arriving limit order prices, we can choose to express these prices either as their original (absolute) values, or we can run the order book dynamical process and transform each order into a relative difference with the top of its book, and obtain identical results. The differences arise when we consider the stability question introduced above.
Intuitively, in the absolute model a small perturbation in the arriving limit price sequence should have limited (but still some) effects on the subsequent evolution of the order books, since prices are determined independently. For the relative model this intuition is less clear. It seems possible that a small perturbation could (for example) slightly modify the current bid, which in turn could slightly modify the price of the next arriving order, which could then slightly modify the price of the subsequent order, and so on, leading to an amplifying sequence of events.
Our main results demonstrate that these two models do indeed have dramatically different stability properties. We first show that for any fixed sequence of prices in the absolute model, the modification of a single order has a bounded and extremely limited impact on the subsequent evolution of the books. In particular, we define a natural notion of distance between order books and show that small modifications can result in only constant distance to the original books for all subsequent time steps. We then show that this implies that for almost any standard statistic of market activity - the executed volume, the average price execution price, and many others - the statistic can be influenced only infinitesimally by small perturbations.
In contrast, we show that the relative model enjoys no such stability properties. After giving specific (worst-case) relative price sequences in which small perturbations generate large changes in basic statistics (for example, altering the number of shares traded by a factor of two), we proceed to demonstrate that the difference in stability properties of the two models is more than merely theoretical. Using extensive INET (a major ECN for NASDAQ stocks) limit order data and order book reconstruction code, we investigate the empirical stability properties when the data is interpreted as containing either absolute prices, relative prices, or mixtures of the two. The theoretical predictions of stability and instability are strongly borne out by the subsequent experiments.
In addition to stability being of fundamental interest in any important dynamical system, we believe that the results described here provide food for thought on the topics of market impact and the backtesting of quantitative trading strategies (the attempt to determine hypothetical past performance using historical data). They suggest that one"s confidence that trading quietly and in small volumes will have minimal market impact is linked to an implicit belief in an absolute price model. Our results and the fact that in the real markets there is a large and increasing amount of relative behavior such as penny-jumping would seem to cast doubts on such beliefs. Similarly, in a purely or largely relative-price world, backtesting even low-frequency, low-volume strategies could result in historical estimates of performance that are not only unrelated to future performance (the usual concern), but are not even accurate measures of a hypothetical past.
The outline of the paper follows. In Section 2 we briefly review the large literature on market microstructure. In Section 3 we describe the limit order mechanism and our formal models. Section 4 presents our most important theoretical results, the 1-Modification Theorem for the absolute price model. This theorem is applied in Section 5 to derive a number of strong stability properties in the absolute model.
Section 6 presents specific examples establishing the worstcase instability of the relative model. Section 7 contains the simulation studies that largely confirm our theoretical findings on INET market data.
As was mentioned in the Introduction, market microstructure is an important and timely topic both in academic finance and on Wall Street, and consequently has a large and varied recent literature. Here we have space only to summarize the main themes of this literature and to provide pointers to further readings. To our knowledge the stability properties of detailed limit order microstructure dynamics have not been previously considered. (However, see Farmer and Joshi [6] for an example and survey of other price dynamic stability studies.) 121 On the more theoretical side, there is a rich line of work examining what might be considered the game-theoretic properties of limit order markets. These works model traders and market-makers (who provide liquidity by offering both buy and sell quotes, and profit on the difference) by utility functions incorporating tolerance for risks of price movement, large positions and other factors, and examine the resulting equilibrium prices and behaviors. Common findings predict negative price impacts for large trades, and price effects for large inventory holdings by market-makers. An excellent and comprehensive survey of results in this area can be found in [2].
There is a similarly large body of empirical work on microstructure. Major themes include the measurement of price impacts, statistical properties of limit order books, and attempts to establish the informational value of order books [4]. A good overview of the empirical work can be found in [7]. Of particular note for our interests is [3], which empirically studies the distribution of arriving limit order prices in several prominent markets. This work takes a view of arriving prices analogous to our relative model, and establishes a power-law form for the resulting distributions.
There is also a small but growing number of works examining market microstructure topics from a computer science perspective, including some focused on the use of microstructure in algorithms for optimized trade execution.
Kakade et al. [9] introduced limit order dynamics in competitive analysis for one-way and volume-weighted average price (VWAP) trading. Some recent papers have applied reinforcement learning methods to trade execution using order book properties as state variables [1, 5, 10].
The following expository background material is adapted from [9]. The market mechanism we examine in this paper is driven by the simple and standard concept of a limit order. Suppose we wish to purchase 1000 shares of Microsoft (MSFT) stock. In a limit order, we specify not only the desired volume (1000 shares), but also the desired price.
Suppose that MSFT is currently trading at roughly $24.07 a share (see Figure 1, which shows an actual snapshot of an MSFT order book on INET), but we are only willing to buy the 1000 shares at $24.04 a share or lower. We can choose to submit a limit order with this specification, and our order will be placed in a queue called the buy order book, which is ordered by price, with the highest offered unexecuted buy price at the top (often referred to as the bid). If there are multiple limit orders at the same price, they are ordered by time of arrival (with older orders higher in the book).
In the example provided by Figure 1, our order would be placed immediately after the extant order for 5,503 shares at $24.04; though we offer the same price, this order has arrived before ours. Similarly, a sell order book for sell limit orders is maintained, this time with the lowest sell price offered (often referred to as the ask) at its top.
Thus, the order books are sorted from the most competitive limit orders at the top (high buy prices and low sell prices) down to less competitive limit orders. The bid and ask prices together are sometimes referred to as the inside market, and the difference between them as the spread. By definition, the order books always consist exclusively of unexecuted orders - they are queues of orders hopefully waiting for the price to move in their direction.
Figure 1: Sample INET order books for MSFT.
How then do orders get (partially) executed? If a buy (sell, respectively) limit order comes in above the ask (below the bid, respectively) price, then the order is matched with orders on the opposing books until either the incoming order"s volume is filled, or no further matching is possible, in which case the remaining incoming volume is placed in the books.
For instance, suppose in the example of Figure 1 a buy order for 2000 shares arrived with a limit price of $24.08.
This order would be partially filled by the two 500-share sell orders at $24.069 in the sell books, the 500-share sell order at $24.07, and the 200-share sell order at $24.08, for a total of 1700 shares executed. The remaining 300 shares of the incoming buy order would become the new bid of the buy book at $24.08. It is important to note that the prices of executions are the prices specified in the limit orders already in the books, not the prices of the incoming order that is immediately executed. Thus in this example, the
this also means that in a pure limit order exchange such as INET, market orders can be simulated by limit orders with extreme price values. In exchanges such as INET, any order can be withdrawn or canceled by the party that placed it any time prior to execution.
Every limit order arrives atomically and instantaneously - there is a strict temporal sequence in which orders arrive, and two orders can never arrive simultaneously. This gives rise to the definition of the last price of the exchange, which is simply the last price at which the exchange executed an order. It is this quantity that is usually meant when people casually refer to the (ticker) price of a stock.
We now provide a formal model for the limit order pro122 cess described above. In this model, limit orders arrive in a temporal sequence, with each order specifying its limit price and an indication of its type (buy or sell). Like the actual exchanges, we also allow cancellation of a standing (unexecuted) order in the books any time prior to its execution.
Without loss of generality we limit attention to a model in which every order is for a single share; large order volumes can be represented by 1-share sequences.
Definition 3.1. Let Σ = σ1, ...σn be a sequence of limit orders, where each σi has the form ni, ti, vi . Here ni is an order identifier, ti is the order type (buy, sell, or cancel), and vi is the limit order value. In the case that ti is a cancel, ni matches a previously placed order and vi is ignored.
We have deliberately called vi in the definition above the limit order value rather than price, since our two models will differ in their interpretation of vi (as being absolute or relative). In the absolute model, we do indeed interpret vi as simply being the price of the limit order. In the relative model, if the current order book configuration is (A, B) (where A is the sell and B the buy book), the price of the order is ask(A) + vi if ti is sell, and bid(B) + vi if ti is buy, where by ask(X) and bid(X) we denote the price of the order at the top of the book X. (Note vi can be negative.) Our main interest in this paper is the effects that the modification of a small number of limit orders can have on the resulting dynamics. For simplicity we consider only modifications to the limit order values, but our results generalize to any modification.
Definition 3.2. A k-modification of Σ is a sequence Σ such that for exactly k indices i1, ..., ik vij = vij , tij = tij , and nij = nij . For every = ij , j ∈ {1, . . . , k} σ = σ .
We now define the various quantities whose stability properties we examine in the absolute and relative models. All of these are standard quantities of common interest in financial markets. • volume(Σ): Number of shares executed (traded) in the sequence Σ. • average(Σ): Average execution price. • close(Σ): Price of the last (closing) execution. • lastbid(Σ): Bid at the end of the sequence. • lastask(Σ): Ask at end of the sequence.
In this section we provide our most important technical result. It shows that in the absolute model, the effects that the modification of a single order has on the resulting evolution of the order books is extremely limited. We then apply this result to derive strong stability results for all of the aforementioned quantities in the absolute model.
Throughout this section, we consider an arbitrary order sequence Σ in the absolute model, and any 1-modification Σ of Σ. At any point (index) i in the two sequences we shall use (A1, B1) to denote the sell and buy books (respectively) in Σ, and (A2, B2) to denote the sell and buy books in Σ ; for notational convenience we omit explicitly superscripting by the current index i. We will shortly establish that at all times i, (A1, B1) and (A2, B2) are very close.
Although the order books are sorted by price, we will use (for example) A1 ∪ {a2} = A2 to indicate that A2 contains an order at some price a2 that is not present in A1, but that otherwise A1 and A2 are identical; thus deleting the order at a2 in A2 would render the books the same. Similarly,
B1 ∪ {b2} = B2 ∪ {b1} means B1 contains an order at price b1 not present in B2, B2 contains an order at price b2 not present in B1, and that otherwise B1 and B2 are identical.
Using this notation, we now define a set of stable system states, where each state is composed from the order books of the original and the modified sequences. Shortly we show that if we change only one order"s value (price), we remain in this set for any sequence of limit orders.
Definition 4.1. Let ab be the set of all states (A1, B1) and (A2, B2) such that A1 = A2 and B1 = B2. Let ¯ab be the set of states such that A1 ∪ {a2} = A2 ∪ {a1}, where a1 = a2, and B1 = B2. Let a¯b be the set of states such that B1∪{b2} = B2∪{b1}, where b1 = b2, and A1 = A2. Let ¯a¯b be the set of states in which A1 = A2∪{a1} and B1 = B2∪{b1}, or in which A2 = A1 ∪ {a2} and B2 = B1 ∪ {b2}. Finally we define S = ab ∪ ¯ab ∪ ¯ba ∪ ¯a¯b as the set of stable states.
Theorem 4.1. (1-Modification Theorem) Consider any sequence of orders Σ and any 1-modification Σ of Σ. Then the order books (A1, B1) and (A2, B2) determined by Σ and Σ lie in the set S of stable states at all times. ab ¯a¯b a¯b¯ab Figure 2: Diagram representing the set S of stable states and the possible movements transitions in it after the change.
The idea of the proof of this theorem is contained in Figure 2, which shows a state transition diagram labeled by the categories of stable states. This diagram describes all transitions that can take place after the arrival of the order on which Σ and Σ differ. The following establishes that immediately after the arrival of this differing order, the state lies in S.
Lemma 4.2. If at any time the current books (A1, B1) and (A2, B2) are in the set ab (and thus identical), then modifying the price of the next order keeps the state in S.
Proof. Suppose the arriving order is a sell order and we change it from a1 to a2; assume without loss of generality that a1 > a2. If neither order is executed immediately, then we move to state ¯ab; if both of them are executed then we stay in state ab; and if only a2 is executed then we move to state ¯a¯b. The analysis of an arriving buy order is similar.
Following the arrival of their only differing order, Σ and Σ are identical. We now give a sequence of lemmas showing 123 Executed with two orders Not executed in both Arrivng buy order Arriving buy order Arriving buy order Arriving sell order ¯ab ab ¯a¯b Executed only with a1 (not a1 and a2) Executed with a1 and a2 Figure 3: The state diagram when starting at state ¯ab. This diagram provides the intuition of Lemma
that following the initial difference covered by Lemma 4.2, the state remains in S forever on the remaining (identical) sequence. We first show that from state ¯ab we remain in S regardless the next order. The intuition of this lemma is demonstrated in Figure 3.
Lemma 4.3. If the current state is in the set ¯ab, then for any order the state will remain in S.
Proof. We first provide the analysis for the case of an arriving sell order. Note that in ¯ab the buy books are identical (B1 = B2). Thus either the arriving sell order is executed with the same buy order in both buy books, or it is not executed in both buy books. For the first case, the buy books remain identical (the bid is executed in both) and the sell books remain unchanged. For the second case, the buy books remain unchanged and identical, and the sell books have the new sell order added to both of them (and thus still differ by one order).
Next we provide an analysis of the more subtle case where the arriving item is a buy order. For this case we need to take care of several different scenarios. The first is when the top of both sell books (the ask) is identical. Then regardless of whether the new buy order is executed or not, the state remains in ¯ab (the analysis is similar to an arriving sell order).
We are left to deal with case where ask(A1) and ask(A2) are different. Here we discuss two subcases: (a) ask(A1) = a1 and ask(A2) = a2, and (b) ask(A1) = a1 and ask(A2) = a . Here a1 and a2 are as in the definition of ¯ab in Definition 4.1, and a is some other price. For subcase (a), by our assumption a1 < a2, then either (1) both asks get executed, the sell books become identical, and we move to state ab; (2) neither ask is executed and we remain in state ¯ab; or (3) only ask(A1) = a1 is executed, in which case we move to state ¯a¯b with A2 = A1 ∪ {a2} and B2 = B1 ∪ {b2}, where b2 is the arriving buy order price. For subcase (b), either (1) buy order is executed in neither sell book we remain in state ¯ab; or (2) the buy order is executed in both sell books and stay in state ¯ab with A1 ∪ {a } = A2 ∪ {a2}; or (3) only ask(A1) = a1 is executed and we move to state ¯a¯b.
Lemma 4.4. If the current state is in the set a¯b, then for any order the state will remain in S.
Lemma 4.5. If the current configuration is in the set ¯a¯b, then for any order the state will remain in S The proofs of these two lemmas are omitted, but are similar in spirit to that of Lemma 4.3. The next and final lemma deals with cancellations.
Lemma 4.6. If the current order book state lies in S, then following the arrival of a cancellation it remains in S.
Proof. When a cancellation order arrives, one of the following possibilities holds: (1) the order is still in both sets of books, (2) it is not in either of them and (3) it is only in one of them. For the first two cases it is easy to see that the cancellation effect is identical on both sets of books, and thus the state remains unchanged. For the case when the order appears only in one set of books, without loss of generality we assume that the cancellation cancels a buy order at b1.
Rather than removing b1 from the book we can change it to have price 0, meaning this buy order will never be executed and is effectively canceled. Now regardless the state that we were in, b1 is still only in one buy book (but with a different price), and thus we remain in the same state in S.
The proof of Theorem 4.1 follows from the above lemmas.
In this section we apply the 1-Modification Theorem to show strong stability properties for the absolute model. We begin with an examination of the executed volume.
Lemma 5.1. Let Σ be any sequence and Σ be any 1modification of Σ. Then the set of the executed orders (ID numbers) generated by the two sequences differs by at most
Proof. By Theorem 4.1 we know that at each stage the books differ by at most two orders. Now since the union of the IDs of the executed orders and the order books is always identical for both sequences, this implies that the executed orders can differ by at most two.
Corollary 5.2. Let Σ be any sequence and Σ be any kmodification of Σ. Then the set of the executed orders (ID numbers) generated by the two sequences differs by at most 2k.
An order sequence Σ is a k-extension of Σ if Σ can be obtained by deleting any k orders in Σ .
Lemma 5.3. Let Σ be any sequence and let Σ be any kextension of Σ. Then the set of the executed orders generated by Σ and Σ differ by at most 2k.
This lemma is the key to obtain our main absolute model volume result below. We use edit(Σ, Σ ) to denote the standard edit distance between the sequences Σ and Σ - the minimal number of substitutions, insertions and deletions or orders needed to change Σ to Σ .
Theorem 5.4. Let Σ and Σ be any absolute model order sequences. Then if edit(Σ, Σ ) ≤ k, the set of the executed orders generated by Σ and Σ differ by at most 4k. In particular, |volume(Σ) − volume(Σ )| ≤ 4k.
Proof. We first define the sequence ˜Σ which is the intersection of Σ and Σ . Since Σ and Σ are at most k apart,we have that by k insertions we change ˜Σ to either Σ or Σ , and by Lemma 5.3 its set of executed orders is at most 2k from each. Thus the set of executed orders in Σ and Σ is at most 4k apart. 124
Theorem 5.4 establishes strong stability for executed volume in the absolute model. We now turn to the quantities that involve execution prices as opposed to volume alone - namely, average(Σ), close(Σ), lastbid(Σ) and lastask(Σ).
For these results, unlike executed volume, a condition must hold on Σ in order for stability to occur. This condition is expressed in terms of a natural measure of the spread of the market, or the gap between the buyers and sellers. We motivate this condition by first showing that without it, by changing one order, we can change average(Σ) by any positive value x.
Lemma 5.5. There exists Σ such that for any x ≥ 0, there is a 1-modification Σ of Σ such that average(Σ ) = average(Σ) + x.
Proof. Let Σ be a sequence of alternating sell and buy orders in which each seller offers p and each buyer p + x, and the first order is a sell. Then all executions take place at the ask, which is always p, and thus average(Σ) = p.
Now suppose we modify only the first sell order to be at price p+1+x. This initial sell order will never be executed, and now all executions take place at the bid, which is always p + x.
Similar instability results can be shown to hold for the other price-based quantities. This motivates the introduction of a quantity we call the second spread of the order books, which is defined as the difference between the prices of the second order in the sell book and the second order in the buy book (as opposed to the bid-ask difference, which is commonly called the spread). We note that in a liquid stock, such as those we examine experimentally in Section 7, the second spread will typically be quite small and in fact almost always equal to the spread.
In this subsection we consider changes in the sequence only after an initialization period, and sequences such that the second spread is always defined after the time we make a change. We define s2(Σ) to be the maximum second spread in the sequence Σ following the change.
Theorem 5.6. Let Σ be a sequence and let Σ be any 1modification of Σ. Then
where s2(Σ) is the maximum over the second spread in Σ following the 1-modification.
Proof. We provide the proof for the last bid; the proof for the last ask is similar. The proof relies on Theorem 4.1 and considers states in the stable set S. For states ab and ¯ab, we have that the bid is identical. Let bid(X), sb(X), ask(X), be the bid, the second highest buy order, and the ask of a sequence X. Now recall that in state a¯b we have that the sell books are identical, and that the two buy books are identical except one different order. Thus bid(Σ)+s2(Σ) ≥ sb(Σ)+s2(Σ) ≥ ask(Σ) = ask(Σ ) ≥ bid(Σ ).
Now it remains to bound bid(Σ). Here we use the fact that the bid of the modified sequence is at least the second highest buy order in the original sequence, due to the fact that the books are different only in one order. Since bid(Σ ) ≥ sb(Σ) ≥ ask(Σ) − s2(Σ) ≥ bid(Σ) − s2(Σ) we have that |bid(Σ) − bid(Σ )| ≤ s2(Σ) as desired.
In state ¯a¯b we have that for one sequence the books contain an additional buy order and an additional sell order.
First suppose that the books containing the additional orders are the original sequence Σ. Now if the bid is not the additional order we are done, otherwise we have the following: bid(Σ) ≤ ask(Σ) ≤ sb(Σ) + s2(Σ) = bid(Σ ) + s2(Σ), where sb(Σ) ≤ bid(Σ ) since the original buy book has only one additional order.
Now assume that the books with the additional orders are for the modified sequence Σ . We have bid(Σ) + s2(Σ) ≥ ask(Σ) ≥ ask(Σ ) ≥ bid(Σ ), where we used the fact that ask(Σ) ≥ ask(Σ ) since the modified sequence has an additional order. Similarly we have that bid(Σ) ≤ bid(Σ ) since the modified buy book contains an additional order.
We note that the proof of Theorem 5.6 actually establishes that the bid and ask of the original and modified sequences are within s2(Σ) at all times.
Next we provide a technical lemma which relates the (first) spread of the modified sequence to the second spread of the original sequence.
Lemma 5.7. Let Σ be a sequence and let Σ be any 1modification of Σ. Then the spread of Σ is bounded by s2(Σ).
Proof. By the 1-Modification Theorem, we know that the books of the modified sequence and the original sequence can differ by at most one order in each book (buy and sell).
Therefore, the second-highest buy order in the original sequence is always at most the bid in the modified sequence, and the second-lowest sell order in the original sequence is always at least the ask of the modified sequence.
We are now ready to state a stability result for the average execution price in the absolute model. It establishes that in highly liquid markets, where the executed volume is large and the spread small, the average price is highly stable.
Theorem 5.8. Let Σ be a sequence and let Σ be any 1modification of Σ. Then |average(Σ) − average(Σ )| ≤ 2(pmax + s2(Σ)) volume(Σ) + s2(Σ) where pmax is the highest execution price in Σ.
Proof. The proof will show that every execution in Σ besides the execution of the modified order and the last execution has a matching execution in Σ with a price different by at most s2(Σ), and will use the fact that pmax + s2(Σ) is a bound on the price in Σ .
Referring to the proof of the 1-Modification Theorem, suppose we are in state ¯a¯b, where we have in one sequence (which can be either Σ or Σ ) an additional buy order b and an additional sell order a. Without loss of generality we assume that the sequence with the additional orders is Σ. If the next execution does not involve a or b then clearly we have the same execution in both Σ and Σ . Suppose that it involves a; there are two possibilities. Either a is the modified order, in which case we change the average price 125 difference by (pmax +s2(Σ))/volume(Σ), and this can happen only once; or a was executed before in Σ and the executions both involve an order whose limit price is a. By Lemma 5.7 the spread of both sequences is bounded by s2(Σ), which implies that the price of the execution in Σ was at most a + s2(Σ), while execution is in Σ is at price a, and thus the prices are different by at most s2(Σ).
In states ¯ab, a¯b as long as we have concurrent executions in the two sequences, we know that the prices can differ by at most s2(Σ). If we have an execution only in one sequence, we either match it in state ¯a¯b, or charge it by (pmax + s2(Σ))/volume(Σ) if we end at state ¯a¯b.
If we end in state ab, ¯ab or a¯b, then every execution in states ¯ab or a¯b were matched to an execution in state ¯a¯b. If we end up in state ¯a¯b, we have the one execution that is not matched and thus we charge it (pmax +s2(Σ))/volume(Σ).
We next give a stability result for the closing price. We first provide a technical lemma regarding the prices of consecutive executions.
Lemma 5.9. Let Σ be any sequence. Then the prices of two consecutive executions in Σ differ by at most s2(Σ).
Proof. Suppose the first execution is taken at time t; its price is bounded below by the current bid and above by the current ask. Now after this execution the bid is at least the second highest buy order at time t, if the former bid was executed and no higher buy orders arrived, and higher otherwise. Similarly, the ask is at most the second lowest sell order at time t. Therefore, the next execution price is at least the second bid at time t and at most the second ask at time t, which is at most s2(Σ) away from the bid/ask at time t.
Lemma 5.10. Let Σ be any sequence and let Σ be a 1modification of Σ. If the volume(Σ) ≥ 2, then |close(Σ) − close(Σ )| ≤ s2(Σ) Proof. We first deal with case where the last execution occurs in both sequences simultaneously. By Theorem 5.6, both the ask and the bid of Σ and Σ are at most s2(Σ) apart at every time t. Since the price of the last execution is their asks (bids) at time t we are done.
Next we deal with the case where the last execution among the two sequences occurs only in Σ. In this case we know that either the previous execution happened simultaneously in both sequences at time t, and thus all three executions are within the second spread of Σ at time t (the first execution in Σ by definition, the execution at Σ from identical arguments as in the former case, and the third by Lemma 5.9).
Otherwise the previous execution happened only in Σ at time t, in which case the two executions are within the the spread of Σ at time t (the execution of Σ from the same arguments as before, and the execution in Σ must be inside its spread in time t).
If the last execution happens only in Σ we know that the next execution of Σ will be at most s2(Σ) away from its previous execution by Lemma 5.9. Together with the fact that if an execution happens only in one sequence it implies that the order is in the spread of the second sequence as long as the sequences are 1-modification, the proof is completed.
As in the case of executed volume, we would like to extend the absolute model stability results for price-based quantities to the case where multiple orders are modified. Here our results are weaker and depend on the k-spread, the distance between the kth highest buy order and the kth lowest sell order, instead of the second spread. (Looking ahead to Section 7, we note that in actual market data for liquid stocks, this quantity is often very small as well.) We use sk(Σ) to denote the k-spread. As before, we assume that the k-spread is always defined after an initialization period.
We first state the following generalization of Lemma 5.7.
Lemma 5.11. Let Σ be a sequence and let Σ be any 1modification of Σ. For ≥ 1, if s +1(Σ) is always defined after the change, then s (Σ ) ≤ s +1(Σ).
The proof is similar to the proof of Lemma 5.7 and omitted. A simple application of this lemma is the following: Let Σ be any sequence which is an -modification of Σ. Then we have s2(Σ ) ≤ s +2(Σ). Now using the above lemma and by simple induction we can obtain the following theorem.
Theorem 5.12. Let Σ be a sequence and let Σ be any k-modification of Σ. Then
Pk =1 s +1(Σ) ≤ ksk+1(Σ)
Pk =1 s +1(Σ) ≤ ksk+1(Σ)
Pk =1 s +1(Σ) ≤ ksk+1(Σ)
Pk =1  2(pmax +s +1(Σ)) volume(Σ) + s +1(Σ)  where s (Σ) is the maximum over the -spread in Σ following the first modification.
We note that while these bounds depend on deeper measures of spread for more modifications, we are working in a 1-share order model. Thus in an actual market, where single orders contain hundreds or thousands of shares, the k-spread even for large k might be quite small and close to the standard 1-spread in liquid stocks.
In the relative model the underlying assumption is that traders try to exploit their knowledge of the books to strategically place their orders. Thus if a trader wants her buy order to be executed quickly, she may position it above the current bid and be the first in the queue; if the trader is patient and believes that the price trend is going to be downward she will place orders deeper in the buy book, and so on.
While in the previous sections we showed stability results for the absolute model, here we provide simple examples which show instability in the relative model for the executed volume, last bid, last ask, average execution price and the last execution price. In Section 7 we provide many simulations on actual market data that demonstrate that this instability is inherent to the relative model, and not due to artificial constructions. In the relative model we assume that for every sequence the ask and bid are always defined, so the books have a non-empty initial configuration. 126 We begin by showing that in the relative model, even a single modification can double the number of shares executed.
Theorem 6.1. There is a sequence Σ and a 1-modification Σ of Σ such that volume(Σ ) ≥ 2volume(Σ).
Proof. For concreteness we assume that at the beginning the ask is 10 and the bid is 8. The sequence Σ is composed from n buy orders with ∆ = 0, followed by n sell orders with ∆ = 0, and finally an alternating sequence of buy orders with ∆ = +1 and sell orders with ∆ = −1 of length 2n. Since the books before the alternating sequence contain n + 1 sell orders at 10 and n + 1 buy orders at 8, we have that each pair of buy sell order in the alternating part is matched and executed, but none of the initial 2n orders is executed, and thus volume(Σ) = n. Now we change the first buy order to have ∆ = +1. After the first 2n orders there are still no executions; however, the books are different. Now there are n + 1 sell orders at 10, n buy orders at 9 and one buy order at 8. Now each order in the alternating sequence is executed with one of the former orders and we have volume(Σ ) = 2n.
The next theorem shows that the spread-based stability results of Section 5.1 do not also hold in the relative model.
Before providing the proof, we give its intuition. At the beginning the sell book contains only two prices which are far apart and both contain only two orders, now several buy orders arrive, at the original sequence they are not being executed, while in the modified sequence they will be executed and leave the sell book with only the orders at the high price. Now many sell orders followed by many buy orders will arrive, such that in the original sequence they will be executed only at the low price and in the modified sequence they will executed at the high price.
Theorem 6.2. For any positive numbers s and x, there is sequence Σ such that s2(Σ) = s and a 1-modification Σ of Σ such that • |close(Σ) − close(Σ )| ≥ x • |average(Σ) − average(Σ )| ≥ x • |lastbid(Σ) − lastbid(Σ )| ≥ x • |lastask(Σ) − lastask(Σ )| ≥ x Proof. Without loss of generality let us consider sequences in which all prices are integer-valued, in which case the smallest possible value for the second spread is 1; we provide the proof for the case s2(Σ) = 2, but the s2(Σ) = 1 case is similar.
We consider a sequence Σ such that after an initialization period there have been no executions, the buy book has
price 12 and 2 orders with value 12+y, where y is a positive integer that will be determined by the analysis. The original sequence Σ is a buy order with ∆ = 0, followed by two buy orders with ∆ = +1, then 2y sell orders with ∆ = 0, and then 2y buy orders with ∆ = +1. We first note that s2(Σ) = 2, there are 2y executions, all at price 12, the last bid is 11 and the last ask is 12. Next we analyze a modified sequence. We change the first buy order from ∆ = 0 to ∆ = +1. Therefore, the next two buy orders with ∆ = +1 are executed, and afterwards we have that the bid is 11 and the ask is 12 + y. Now the 2y sell orders are accumulated at 12+y, and after the next y buy orders the bid is at 12+y−1.
Therefore, at the end we have that lastbid(Σ ) = 12 + y − 1, lastask(Σ ) = 12 + y, close(Σ ) = 12 + y, and average(Σ ) = y y+2 (12 + y) + 2 y+2 (12). Setting y = x + 2, we obtain the lemma for every property.
We note that while this proof was based on the fact that there are two consecutive orders in the books which are far (y) apart, we can provide a slightly more complicated example in which all orders are close (at most 2 apart), yet still one change results in large differences.
The results presented so far paint a striking contrast between the absolute and relative price models: while the absolute model enjoys provably strong stability over any fixed event sequence, there exist at least specific sequences demonstrating great instability in the relative model. The worstcase nature of these results raises the question of the extent to which such differences could actually occur in real markets. In this section we provide indirect evidence on this question by presenting simulation results exploiting a rich source of real-market historical limit order sequence data.
By interpreting arriving limit order prices as either absolute values, or by transforming them into differences with the current bid and ask (relative model), we can perform small modifications on the sequences and examine how different various outcomes (volume traded, average price, etc.) would be from what actually occurred in the market. These simulations provide an empirical counterpart to the theory we have developed. We emphasize that all such simulations interpret the actual historical data as falling into either the absolute or relative model, and are meaningful only within the confines of such an interpretation. Nevertheless, we feel they provide valuable empirical insight into the potential (in)stability properties of modern equity limit order markets, and demonstrate that one"s belief or hope in stability largely relies on an absolute model interpretation. We also investigate the empirical behavior of mixtures of absolute and relative prices.
The historical data used in our simulations is commercially available limit order data from INET, the previously mentioned electronic exchange for NASDAQ stocks. Broadly speaking, this data consists of practically every single event on INET regarding the trading of an individual stockevery arriving limit order (price, volume, and sequence ID number), every execution, and every cancellation of a standing order - all timestamped in milliseconds. It is data sufficient to recreate the precise INET order book in a given stock on a given day and time.
We will report stability properties for three stocks: Amazon, Nvidia, and Qualcomm (identified in the sequel by their tickers, AMZN, NVDA and QCOM). These three provide some range of liquidities (with QCOM having the greatest and NVDA the least liquidity on INET) and other trading properties. We note that the qualitative results of our simulations were similar for several other stocks we examined. 127
For our simulations we employed order-book reconstruction code operating on the underlying raw data. The basic format of each experiment was the following:
original INET data and compute the quantity of interest (volume traded, average price, etc.)
recompute the resulting value of the quantity of interest.
In the absolute model case, Step 2 is as simple as modifying the order in the original data and re-running the order book reconstruction. For the relative model, we must first pre-process the raw data and convert its prices to relative values, then make the modification and re-run the order book reconstruction on the relative values.
The type of modification we examined was extremely small compared to the volume of orders placed in these stocks: namely, the deletion of a single randomly chosen order from the sequence. Although a deletion is not 1-modification, its edit distance is 1 and we can apply Theorem 5.4. For each trading day examined,this single deleted order was selected among those arriving between 10 AM and 3 PM, and the quantities of interest were measured and compared at 3 PM. These times were chosen to include the busiest part of the trading day but avoid the half hour around the opening and closing of the official NASDAQ market (9:30 AM and 3:30 PM respectively), which are known to have different dynamics than the central portion of the day.
We run the absolute and relative model simulations on both the raw INET data and on a cleaned version of this data. In the cleaned we remove all limit orders that were canceled in the actual market prior to their execution (along with the cancellations themselves). The reason is that such cancellations may often be the first step in the repositioning of orders - that is, cancellations of the order that are followed by the submission of a replacement order at a different price. Not removing canceled orders allows the possibility of modified simulations in which the same order 1 is executed twice, which may magnify instability effects. Again, it is clear that neither the raw nor the cleaned data can perfectly reflect what would have happened under the deleted orders in the actual market.
However, the results both from the raw data and the clean data are qualitatively similar. The results mainly differ, as expected, in the executed volume, where the instability results for the relative model are much more dramatic in the raw data.
We begin with summary statistics capturing our overall stability findings. Each row of the tables below contains a ticker (e.g. AMZN) followed by either -R (for the uncleaned or raw data) or -C (for the data with canceled orders removed). For each of the approximately 250 trading days in 2003, 1000 trials were run in which a randomly selected order was deleted from the INET event sequence. For each quantity of interest (volume executed, average price, closing price and last bid), we show for the both the absolute and 1 Here same is in quotes since the two orders will actually have different sequence ID numbers, which is what makes such repositioning activity impossible to reliably detect in the data. relative model the average percentage change in the quantity induced by the deletion.
The results confirm rather strikingly the qualitative conclusions of the theory we have developed. In virtually every case (stock, raw or cleaned data, and quantity) the percentage change induced by a single deletion in the relative model is many orders of magnitude greater than in the absolute model, and shows that indeed butterfly effects may occur in a relative model market. As just one specific representative example, notice that for QCOM on the cleaned data, the relative model effect of just a single deletion on the closing price is in excess of a full percentage point. This is a variety of market impact entirely separate from the more traditional and expected kind generated by trading a large volume of shares.
Stock Date volume average Rel Abs Rel Abs AMZN-R 2003 15.1% 0.04% 0.3% 0.0002% AMZN-C 2003 0.69% 0.087% 0.36% 0.0007% NVDA-R 2003 9.09% 0.05 % 0.17% 0.0003% NVDA-C 2003 0.73% 0.09 % 0.35% 0.001% QCOM-R 2003 16.94% 0.035% 0.21% 0.0002% QCOM-C 2003 0.58% 0.06% 0.35% 0.0005% Stock Date close lastbid Rel Abs Rel Abs AMZN-R 2003 0.78% 0.0001% 0.78% 0.0007% AMZN-C 2003 1.10% 0.077% 1.11% 0.001% NVDA-R 2003 1.17% 0.002 % 1.18 % 0.08% NVDA-C 2003 0.45% 0.0003% 0.45% 0.0006% QCOM-R 2003 0.58% 0.0001% 0.58% 0.0004% QCOM-C 2003 1.05% 0.0006% 1.05% 0.06% In Figure 4 we examine how the change to one the quantities, the average execution price, grows with the introduction of greater perturbations of the event sequence in the two models. Rather than deleting only a single order between
of randomly chosen deletions was performed, and the percentage change to the average price measured. As suggested by the theory we have developed, for the absolute model the change to the average price grows linearly with the number of deletions and remains very small (note the vastly different scales of the y-axis in the panels for the absolute and relative models in the figure). For the relative model, it is interesting to note that while small numbers of changes have large effects (often causing average execution price changes well in excess of 0.1 percent), the effects of large numbers of changes levels off quite rapidly and consistently.
We conclude with an examination of experiments with a mixture model. Even if one accepts a world in which traders behave in either an absolute or relative manner, one would be likely to claim that the market contains a mixture of both.
We thus ran simulations in which each arriving order in the INET event streams was treated as an absolute price with probability α, and as a relative price with probability 1−α.
Representative results for the average execution price in this mixture model are shown in Figure 5 for AMZN and NVDA.
Perhaps as expected, we see a monotonic decrease in the percentage change (instability) as the fraction of absolute traders increases, with most of the reduction already being realized by the introduction of just a small population of absolute traders. Thus even in a largely relative-price world, a 128
0
1
2
x 10 −3 QCOM−R June 2004: Absolute Number of changes Averageprice
0
QCOM−R June 2004: Relative Number of changes Averageprice Figure 4: Percentage change to the average execution price (y-axis) as a function of the number of deletions to the sequence (x-axis). The left panel is for the absolute model, the right panel for the relative model, and each curve corresponds to a single day of QCOM trading in June 2004. Curves represent averages over 1000 trials. small minority of absolute traders can have a greatly stabilizing effect. Similar behavior is found for closing price and last bid.
0
AMZN−R Feburary 2004 α Averageprice
0
NVDA−R June 2004 α Averageprice Figure 5: Percentage change to the average execution price (y-axis) vs. probability of treating arriving INET orders as absolute prices (x-axis). Each curve corresponds to a single day of trading during a month of 2004. Curves represent averages over
For the executed volume in the mixture model, however, the findings are more curious. In Figure 6, we show how the percentage change to the executed volume varies with the absolute trader fraction α, for NVDA data that is both raw and cleaned of cancellations. We first see that for this quantity, unlike the others, the difference induced by the cleaned and uncleaned data is indeed dramatic, as already suggested by the summary statistics table above. But most intriguing is the fact that the stability is not monotonically increasing with α for either the cleaned or uncleaned datathe market with maximum instability is not a pure relative price market, but occurs at some nonzero value for α. It was in fact not obvious to us that sequences with this property could even be artificially constructed, much less that they would occur as actual market data. We have yet to find a satisfying explanation for this phenomenon and leave it to future research.
We are grateful to Yuriy Nevmyvaka of Lehman Brothers in New York for the use of his INET order book reconstruction code, and for valuable comments on the work presented
0
1
2
3
4 NVDA−C June 2004 α Volume
0 2 4 6 8 10 12 14 16 18 NVDA−R June 2004 α Volume Figure 6: Percentage change to the executed volume (y-axis) vs. probability of treating arriving INET orders as absolute prices (x-axis). The left panel is for NVDA using the raw data that includes cancellations, while the right panel is on the cleaned data.
Each curve corresponds to a single day of trading during June 2004. Curves represent averages over
here. Yishay Mansour was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778, by a grant from the Israel Science Foundation and an IBM faculty award.
[1] D. Bertsimas and A. Lo. Optimal control of execution costs. Journal of Financial Markets, 1:1-50, 1998. [2] B. Biais, L. Glosten, and C. Spatt. Market microstructure: a survey of microfoundations, empirical results and policy implications. Journal of Financial Markets, 8:217-264, 2005. [3] J.-P. Bouchaud, M. Mezard, and M. Potters.
Statistical properties of stock order books: empirical results and models. Quantitative Finance, 2:251-256,
[4] C. Cao, O.Hansch, and X. Wang. The informational content of an open limit order book, 2004. AFA 2005 Philadelphia Meetings, EFA Maastricht Meetings Paper No. 4311. [5] R. Coggins, A. Blazejewski, and M. Aitken. Optimal trade execution of equities in a limit order market. In International Conference on Computational Intelligence for Financial Engineering, pages 371-378,
March 2003. [6] D. Farmer and S. Joshi. The price dynamics of common trading strategies. Journal of Economic Behavior and Organization, 29:149-171, 2002. [7] J. Hasbrouck. Empirical market microstructure: Economic and statistical perspectives on the dynamics of trade in securities markets, 2004. Course notes,
Stern School of Business, New York University. [8] R. Kissell and M. Glantz. Optimal Trading Strategies.
Amacom, 2003. [9] S.Kakade, M. Kearns, Y. Mansour, and L. Ortiz.

A common feature of many online distributed systems is that individuals provide services for each other.
Peer-topeer (P2P) networks (such as Kazaa [25] or BitTorrent [3]) have proved popular as mechanisms for file sharing, and applications such as distributed computation and file storage are on the horizon; systems such as Seti@home [24] provide computational assistance; systems such as Slashdot [21] provide content, evaluations, and advice forums in which people answer each other"s questions. Having individuals provide each other with service typically increases the social welfare: the individual utilizing the resources of the system derives a greater benefit from it than the cost to the individual providing it. However, the cost of providing service can still be nontrivial. For example, users of Kazaa and BitTorrent may be charged for bandwidth usage; in addition, in some filesharing systems, there is the possibility of being sued, which can be viewed as part of the cost. Thus, in many systems there is a strong incentive to become a free rider and benefit from the system without contributing to it. This is not merely a theoretical problem; studies of the Gnutella [22] network have shown that almost 70 percent of users share no files and nearly 50 percent of responses are from the top
Having relatively few users provide most of the service creates a point of centralization; the disappearance of a small percentage of users can greatly impair the functionality of the system. Moreover, current trends seem to be leading to the elimination of the altruistic users on which these systems rely. These heavy users are some of the most expensive customers ISPs have. Thus, as the amount of traffic has grown, ISPs have begun to seek ways to reduce this traffic.
Some universities have started charging students for excessive bandwidth usage; others revoke network access for it [5]. A number of companies have also formed whose service is to detect excessive bandwidth usage [19].
These trends make developing a system that encourages a more equal distribution of the work critical for the continued viability of P2P networks and other distributed online systems. A significant amount of research has gone into designing reputation systems to give preferential treatment to users who are sharing files. Some of the P2P networks currently in use have implemented versions of these techniques. However, these approaches tend to fall into one of two categories: either they are barter-like or reputational.
By barter-like, we mean that each agent bases its decisions only on information it has derived from its own interactions.
Perhaps the best-known example of a barter-like system is BitTorrent, where clients downloading a file try to find other clients with parts they are missing so that they can trade, thus creating a roughly equal amount of work. Since the barter is restricted to users currently interested in a single file, this works well for popular files, but tends to have problems maintaining availability of less popular ones. An example of a barter-like system built on top of a more traditional file-sharing system is the credit system used by eMule 140 [8]. Each user tracks his history of interactions with other users and gives priority to those he has downloaded from in the past. However, in a large system, the probability that a pair of randomly-chosen users will have interacted before is quite small, so this interaction history will not be terribly helpful. Anagnostakis and Greenwald [2] present a more sophisticated version of this approach, but it still seems to suffer from similar problems.
A number of attempts have been made at providing general reputation systems (e.g. [12, 13, 17, 27]). The basic idea is to aggregate each user"s experience into a global number for each individual that intuitively represents the system"s view of that individual"s reputation. However, these attempts tend to suffer from practical problems because they implicitly view users as either good or bad, assume that the good users will act according to the specified protocol, and that there are relatively few bad users. Unfortunately, if there are easy ways to game the system, once this information becomes widely available, rational users are likely to make use of it. We cannot count on only a few users being bad (in the sense of not following the prescribed protocol).
For example, Kazaa uses a measure of the ratio of the number of uploads to the number of downloads to identify good and bad users. However, to avoid penalizing new users, they gave new users an average rating. Users discovered that they could use this relatively good rating to free ride for a while and, once it started to get bad, they could delete their stored information and effectively come back as a new user, thus circumventing the system (see [2] for a discussion and [11] for a formal analysis of this whitewashing). Thus Kazaa"s reputation system is ineffective.
This is a simple case of a more general vulnerability of such systems to sybil attacks [6], where a single user maintains multiple identities and uses them in a coordinated fashion to get better service than he otherwise would. Recent work has shown that most common reputation systems are vulnerable (in the worst case)to such attacks [4]; however, the degree of this vulnerability is still unclear. The analyses of the practical vulnerabilities and the existence of such systems that are immune to such attacks remains an area of active research (e.g., [4, 28, 14]).
Simple economic systems based on a scrip or money seem to avoid many of these problems, are easy to implement and are quite popular (see, e.g., [13, 15, 26]). However, they have a different set of problems. Perhaps the most common involve determining the amount of money in the system.
Roughly speaking, if there is too little money in the system relative to the number of agents, then relatively few users can afford to make request. On the other hand, if there is too much money, then users will not feel the need to respond to a request; they have enough money already. A related problem involves handling newcomers. If newcomers are each given a positive amount of money, then the system is open to sybil attacks. Perhaps not surprisingly, scrip systems end up having to deal with standard economic woes such as inflation, bubbles, and crashes [26].
In this paper, we provide a formal model in which to analyze scrip systems. We describe a simple scrip system and show that, under reasonable assumptions, for each fixed amount of money there is a nontrivial Nash equilibrium involving threshold strategies, where an agent accepts a request if he has less than $k for some threshold k.1 An interesting aspect of our analysis is that, in equilibrium, the distribution of users with each amount of money is the distribution that maximizes entropy (subject to the money supply constraint). This allows us to compute the money supply that maximizes efficiency (social welfare), given the number of agents. It also leads to a solution for the problem of dealing with newcomers: we simply assume that new users come in with no money, and adjust the price of service (which is equivalent to adjusting the money supply) to maintain the ratio that maximizes efficiency. While assuming that new users come in with no money will not work in all settings, we believe the approach will be widely applicable. In systems where the goal is to do work, new users can acquire money by performing work. It should also work in Kazaalike system where a user can come in with some resources (e.g., a private collection of MP3s).
The rest of the paper is organized as follows. In Section 2, we present our formal model and observe that it can be used to understand the effect of altruists. In Section 3, we examine what happens in the game under nonstrategic play, if all agents use the same threshold strategy. We show that, in this case, the system quickly converges to a situation where the distribution of money is characterized by maximum entropy. Using this analysis, we show in Section 4 that, under minimal assumptions, there is a nontrivial Nash equilibrium in the game where all agents use some threshold strategy.
Moreover, we show in Section 5 that the analysis leads to an understanding of how to choose the amount of money in the system (or, equivalently, the cost to fulfill a request) so as to maximize efficiency, and also shows how to handle new users. In Section 6, we discuss the extent to which our approach can handle sybils and collusion. We conclude in Section 7.
To begin, we formalize providing service in a P2P network as a non-cooperative game. Unlike much of the modeling in this area, our model will model the asymmetric interactions in a file sharing system in which the matching of players (those requesting a file with those who have that particular file) is a key part of the system. This is in contrast with much previous work which uses random matching in a prisoner"s dilemma. Such models were studied in the economics literature [18, 7] and first applied to online reputations in [11]; an application to P2P is found in [9].
This random-matching model fails to capture some salient aspects of a number of important settings. When a request is made, there are typically many people in the network who can potentially satisfy it (especially in a large P2P network), but not all can. For example, some people may not have the time or resources to satisfy the request. The randommatching process ignores the fact that some people may not be able to satisfy the request. Presumably, if the person matched with the requester could not satisfy the match, he would have to defect. Moreover, it does not capture the fact that the decision as to whether to volunteer to satisfy the request should be made before the matching process, not after. That is, the matching process does not capture 1 Although we refer to our unit of scrip as the dollar, these are not real dollars nor do we view them as convertible to dollars. 141 the fact that if someone is unwilling to satisfy the request, there will doubtless be others who can satisfy it. Finally, the actions and payoffs in the prisoner"s dilemma game do not obviously correspond to actual choices that can be made.
For example, it is not clear what defection on the part of the requester means. In our model we try to deal with all these issues.
Suppose that there are n agents. At each round, an agent is picked uniformly at random to make a request. Each other agent is able to satisfy this request with probability β > 0 at all times, independent of previous behavior. The term β is intended to capture the probability that an agent is busy, or does not have the resources to fulfill the request. Assuming that β is time-independent does not capture the intution that being an unable to fulfill a request at time t may well be correlated with being unable to fulfill it at time t+1. We believe that, in large systems, we should be able to drop the independence assumption, but we leave this for future work.
In any case, those agents that are able to satisfy the request must choose whether or not to volunteer to satisfy it. If at least one agent volunteers, the requester gets a benefit of 1 util (the job is done) and one of volunteers is chosen at random to fulfill the request. The agent that fulfills the request pays a cost of α < 1. As is standard in the literature, we assume that agents discount future payoffs by a factor of δ per time unit. This captures the intuition that a util now is worth more than a util tomorrow, and allows us to compute the total utility derived by an agent in an infinite game.
Lastly, we assume that with more players requests come more often. Thus we assume that the time between rounds is 1/n. This captures the fact that the systems we want to model are really processing many requests in parallel, so we would expect the number of concurrent requests to be proportional to the number of users.2 Let G(n, δ, α, β) denote this game with n agents, a discount factor of δ, a cost to satisfy requests of α, and a probability of being able to satisfy requests of β. When the latter two parameters are not relevant, we sometimes write G(n, δ).
We use the following notation throughout the paper: • pt denotes the agent chosen in round t. • Bt i ∈ {0, 1} denotes whether agent i can satisfy the request in round t. Bt i = 1 with probability β > 0 and Bt i is independent of Bt i for all t = t. • V t i ∈ {0, 1} denotes agent i"s decision about whether to volunteer in round t; 1 indicates volunteering. V t i is determined by agent i"s strategy. • vt ∈ {j | V t j Bt j = 1} denotes the agent chosen to satisfy the request. This agent is chosen uniformly at random from those who are willing (V t j = 1) and able (Bt j = 1) to satisfy the request. • ut i denotes agent i"s utility in round t.
A standard agent is one whose utility is determined as discussed in the introduction; namely, the agent gets 2 For large n, our model converges to one in which players make requests in real time, and the time between a player"s requests are exponentially distributed with mean 1. In addition, the time between requests served by a single player is also exponentially distributed. a utility of 1 for a fulfilled request and utility −α for fulfilling a request. Thus, if i is a standard agent, then ut i = 8 < :
P j=i V t j Bt j > 0 −α if i = vt
• Ui = P∞ t=0 δt/n ut i denotes the total utility for agent i. It is the discounted total of agent i"s utility in each round. Note that the effective discount factor is δ1/n since an increase in n leads to a shortening of the time between rounds.
Now that we have a model of making and satisfying requests, we use it to analyze free riding. Take an altruist to be someone who always fulfills requests. Agent i might rationally behave altruistically if agent i"s utility function has the following form, for some α > 0: ut i = 8 < :
P j=i V t j Bt j > 0 α if i = vt
Thus, rather than suffering a loss of utility when satisfying a request, an agent derives positive utility from satisfying it. Such a utility function is a reasonable representation of the pleasure that some people get from the sense that they provide the music that everyone is playing. For such altruistic agents, playing the strategy that sets V t i = 1 for all t is dominant. While having a nonstandard utility function might be one reason that a rational agent might use this strategy, there are certainly others. For example a naive user of filesharing software with a good connection might well follow this strategy. All that matters for the following discussion is that there are some agents that use this strategy, for whatever reason.
As we have observed, such users seem to exist in some large systems. Suppose that our system has a altruists.
Intuitively, if a is moderately large, they will manage to satisfy most of the requests in the system even if other agents do no work. Thus, there is little incentive for any other agent to volunteer, because he is already getting full advantage of participating in the system. Based on this intuition, it is a relatively straightforward calculation to determine a value of a that depends only on α, β, and δ, but not the number n of players in the system, such that the dominant strategy for all standard agents i is to never volunteer to satisfy any requests (i.e., V t i = 0 for all t).
Proposition 2.1. There exists an a that depends only on α, β, and δ such that, in G(n, δ, α, β) with at least a altruists, not volunteering in every round is a dominant strategy for all standard agents.
Proof. Consider the strategy for a standard player j in the presence of a altruists. Even with no money, player j will get a request satisfied with probability 1 − (1 − β)a just through the actions of these altruists. Thus, even if j is chosen to make a request in every round, the most additional expected utility he can hope to gain by having money isP∞ k=1(1 − β)a δk = (1 − β)a /(1 − δ). If (1 − β)a /(1 − δ) > α or, equivalently, if a > log1−β(α(1 − δ)), never volunteering is a dominant strategy.
Consider the following reasonable values for our parameters: β = .01 (so that each player can satisfy 1% of the requests), α = .1 (a low but non-negligible cost), δ = .9999/day 142 (which corresponds to a yearly discount factor of approximately 0.95), and an average of 1 request per day per player.
Then we only need a > 1145. While this is a large number, it is small relative to the size of a large P2P network.
Current systems all have a pool of users behaving like our altruists. This means that attempts to add a reputation system on top of an existing P2P system to influence users to cooperate will have no effect on rational users. To have a fair distribution of work, these systems must be fundamentally redesigned to eliminate the pool of altruistic users.
In some sense, this is not a problem at all. In a system with altruists, the altruists are presumably happy, as are the standard agents, who get almost all their requests satisfied without having to do any work. Indeed, current P2P network work quite well in terms of distributing content to people. However, as we said in the introduction, there is some reason to believe these altruists may not be around forever. Thus, it is worth looking at what can be done to make these systems work in their absence. For the rest of this paper we assume that all agents are standard, and try to maximize expected utility.
We are interested in equilibria based on a scrip system.
Each time an agent has a request satisfied he must pay the person who satisfied it some amount. For now, we assume that the payment is fixed; for simplicity, we take the amount to be $1. We denote by M the total amount of money in the system. We assume that M > 0 (otherwise no one will ever be able to get paid).
In principle, agents are free to adopt a very wide variety of strategies. They can make decisions based on the names of other agents or use a strategy that is heavily history dependant, and mix these strategies freely. To aid our analysis, we would like to be able to restrict our attention to a simpler class of strategies. The class of strategies we are interested in is easy to motivate. The intuitive reason for wanting to earn money is to cater for the possibility that an agent will run out before he has a chance to earn more. On the other hand, a rational agent with plenty of mone would not want to work, because by the time he has managed to spend all his money, the util will have less value than the present cost of working. The natural balance between these two is a threshold strategy. Let Sk be the strategy where an agent volunteers whenever he has less than k dollars and not otherwise. Note that S0 is the strategy where the agent never volunteers. While everyone playing S0 is a Nash equilibrium (nobody can do better by volunteering if no one else is willing to), it is an uninteresting one. As we will show in Section 4, it is sufficient to restrict our attention to this class of strategies.
We use Kt i to denote the amount of money agent i has at time t. Clearly Kt+1 i = Kt i unless agent i has a request satisfied, in which case Kt+1 i = Kt+1 i − 1 or agent i fulfills a request, in which case Kt+1 i = Kt+1 i + 1. Formally,
Kt+1 i = 8 < : Kt i − 1 if i = pt ,
P j=i V t j Bt j > 0, and Kt i > 0 Kt i + 1 if i = vt and Kt pt > 0 Kt i otherwise.
The threshold strategy Sk is the strategy such that V t i = 
pt > 0 and Kt i < k
PLAY Before we consider strategic play, we examine what happens in the system if everyone just plays the same strategy Sk. Our overall goal is to show that there is some distribution over money (i.e., the fraction of people with each amount of money) such that the system converges to this distribution in a sense to be made precise shortly.
Suppose that everyone plays Sk. For simplicity, assume that everyone has at most k dollars. We can make this assumption with essentially no loss of generality, since if someone has more than k dollars, he will just spend money until he has at most k dollars. After this point he will never acquire more than k. Thus, eventually the system will be in such a state. If M ≥ kn, no agent will ever be willing to work. Thus, for the purposes of this section we assume that M < kn.
From the perspective of a single agent, in (stochastic) equilibrium, the agent is undergoing a random walk.
However, the parameters of this random walk depend on the random walks of the other agents and it is quite complicated to solve directly. Thus we consider an alternative analysis based on the evolution of the system as a whole.
If everyone has at most k dollars, then the amount of money that an agent has is an element of {0, . . . , k}. If there are n agents, then the state of the game can be described by identifying how much money each agent has, so we can represent it by an element of Sk,n = {0, . . . , k}{1,...,n} . Since the total amount of money is constant, not all of these states can arise in the game. For example the state where each player has $0 is impossible to reach in any game with money in the system. Let mS(s) = P i∈{1...n} s(i) denote the total mount of money in the game at state s, where s(i) is the number of dollars that agent i has in state s. We want to consider only those states where the total money in the system is M, namely Sk,n,M = {s ∈ Sk,n | mS(s) = M}.
Under the assumption that all agents use strategy Sk, the evolution of the system can be treated as a Markov chain Mk,n,M over the state space Sk,n,M . It is possible to move from one state to another in a single round if by choosing a particular agent to make a request and a particular agent to satisfy it, the amounts of money possesed by each agent become those in the second state. Therefore the probability of a transition from a state s to t is 0 unless there exist two agents i and j such that s(i ) = t(i ) for all i /∈ {i, j}, t(i) = s(i) + 1, and t(j) = s(j) − 1. In this case the probability of transitioning from s to t is the probability of j being chosen to spend a dollar and has someone willing and able to satisfy his request ((1/n)(1 − (1 − β)|{i |s(i )=k}|−Ij ) multiplied by the probability of i being chosen to satisfy his request (1/(|({i | s(i ) = k}| − Ij )). Ij is 0 if j has k dollars and 1 otherwise (it is just a correction for the fact that j cannot satisfy his own request.) Let ∆k denote the set of probability distributions on {0, . . . , k}.
We can think of an element of ∆k as describing the fraction of people with each amount of money. This is a useful way of looking at the system, since we typically don"t care who has each amount of money, but just the fraction of people that have each amount. As before, not all elements of ∆k are possible, given our constraint that the total amount of 143 money is M. Rather than thinking in terms of the total amount of money in the system, it will prove more useful to think in terms of the average amount of money each player has. Of course, the total amount of money in a system with n agents is M iff the average amount that each player has is m = M/n. Let ∆k m denote all distributions d ∈ ∆k such that E(d) = m (i.e.,
Pk j=0 d(j)j = m). Given a state s ∈ Sk,n,M , let ds ∈ ∆k m denote the distribution of money in s. Our goal is to show that, if n is large, then there is a distribution d∗ ∈ ∆k m such that, with high probability, the Markov chain Mk,n,M will almost always be in a state s such that ds is close to d∗ . Thus, agents can base their decisions about what strategy to use on the assumption that they will be in such a state.
We can in fact completely characterize the distribution d∗ . Given a distribution d ∈ ∆k , let H(d) = − X {j:d(j)=0} d(j) log(d(j)) denote the entropy of d. If ∆ is a closed convex set of distributions, then it is well known that there is a unique distribution in ∆ at which the entropy function takes its maximum value in ∆. Since ∆k m is easily seen to be a closed convex set of distributions, it follows that there is a unique distribution in ∆k m that we denote d∗ k,m whose entropy is greater than that of all other distributions in ∆k m. We now show that, for n sufficiently large, the Markov chain Mk,n,M is almost surely in a state s such that ds is close to d∗ k,M/n. The statement is correct under a number of senses of close.
For definiteness, we consider the Euclidean distance. Given > 0, let Sk,n,m, denote the set of states s in Sk,n,mn such that Pk j=0 |ds (j) − d∗ k,m|2 < .
Given a Markov chain M over a state space S and S ⊆ S, let Xt,s,S be the random variable that denotes that M is in a state of S at time t, when started in state s.
Theorem 3.1. For all > 0, all k, and all m, there exists n such that for all n > n and all states s ∈ Sk,n,mn, there exists a time t∗ (which may depend on k, n, m, and ) such that for t > t∗ , we have Pr(Xt,s,Sk,n,m, ) > 1 − .
Proof. (Sketch) Suppose that at some time t, Pr(Xt,s,s ) is uniform for all s . Then the probability of being in a set of states is just the size of the set divided by the total number of states. A standard technique from statistical mechanics is to show that there is a concentration phenomenon around the maximum entropy distribution [16]. More precisely, using a straightforward combinatorial argument, it can be shown that the fraction of states not in Sk,n,m, is bounded by p(n)/ecn , where p is a polynomial. This fraction clearly goes to 0 as n gets large. Thus, for sufficiently large n,
Pr(Xt,s,Sk,n,m, ) > 1 − if Pr(Xt,s,s ) is uniform.
It is relatively straightforward to show that our Markov Chain has a limit distribution π over Sk,n,mn, such that for all s, s ∈ Sk,n,mn, limt→∞ Pr(Xt,s,s ) = πs . Let Pij denote the probability of transitioning from state i to state j. It is easily verified by an explicit computation of the transition probabilities that Pij = Pji for all states i and j. It immediatly follows from this symmetry that πs = πs , so π is uniform. After a sufficient amount of time, the distribution will be close enough to π, that the probabilities are again bounded by constant, which is sufficient to complete the theorem.
Euclidean Distance 2000 2500 3000 3500 4000 NumberofSteps Figure 1: Distance from maximum-entropy distribution with 1000 agents.
Number of Agents
MaximumDistance Figure 2: Maximum distance from maximumentropy distribution over 106 timesteps.
Number of Agents 0 20000 40000 60000 TimetoDistance.001 Figure 3: Average time to get within .001 of the maximum-entropy distribution. 144 We performed a number of experiments that show that the maximum entropy behavior described in Theorem 3.1 arises quickly for quite practical values of n and t. The first experiment showed that, even if n = 1000, we reach the maximum-entropy distribution quickly. We averaged 10 runs of the Markov chain for k = 5 where there is enough money for each agent to have $2 starting from a very extreme distribution (every agent has either $0 or $5) and considered the average time needed to come within various distances of the maximum entropy distribution. As Figure 1 shows, after 2,000 steps, on average, the Euclidean distance from the average distribution of money to the maximum-entropy distribution is .008; after 3,000 steps, the distance is down to .001. Note that this is really only 3 real time units since with 1000 players we have 1000 transactions per time unit.
We then considered how close the distribution stays to the maximum entropy distribution once it has reached it.
To simplify things, we started the system in a state whose distribution was very close to the maximum-entropy distribution and ran it for 106 steps, for various values of n.
As Figure 2 shows, the system does not move far from the maximum-entropy distribution once it is there. For example, if n = 5000, the system is never more than distance .001 from the maximum-entropy distribution; if n = 25, 000, it is never more than .0002 from the maximum-entropy distribution.
Finally, we considered how more carefully how quickly the system converges to the maximum-entropy distribution for various values of n. There are approximately kn  possible states, so the convergence time could in principle be quite large. However, we suspect that the Markov chain that arises here is rapidly mixing, which means that it will converge significantly faster (see [20] for more details about rapid mixing). We believe that the actually time needed is O(n). This behavior is illustrated in Figure 3, which shows that for our example chain (again averaged over 10 runs), after 3n steps, the Euclidean distance between the actual distribution of money in the system and the maximum-entropy distribution is less than .001.
We have seen that the system is well behaved if the agents all follow a threshold strategy; we now want to show that there is a nontrivial Nash equilibrium where they do so (that is, a Nash equilibrium where all the agents use Sk for some k > 0.) This is not true in general. If δ is small, then agents have no incentive to work. Intuitively, if future utility is sufficiently discounted, then all that matters is the present, and there is no point in volunteering to work. With small δ, S0 is the only equilibrium. However, we show that for δ sufficiently large, there is another equilibrium in threshold strategies. We do this by first showing that, if every other agent is playing a threshold strategy, then there is a best response that is also a threshold strategy (although not necessarily the same one). We then show that there must be some (mixed) threshold strategy for which this best response is the same strategy. It follows that this tuple of threshold strategies is a Nash equilibrium.
As a first step, we show that, for all k, if everyone other than agent i is playing Sk, then there is a threshold strategy Sk that is a best response for agent i. To prove this, we need to assume that the system is close to the steadystate distribution (i.e., the maximum-entropy distribution).
However, as long as δ is sufficiently close to 1, we can ignore what happens during the period that the system is not in steady state.3 We have thus far considered threshold strategies of the form Sk, where k is a natural number; this is a discrete set of strategies. For a later proof, it will be helpful to have a continuous set of strategies. If γ = k + γ , where k is a natural number and 0 ≤ γ < 1, let Sγ be the strategy that performs Sk with probability 1 − γ and Sk+1 with probability γ. (Note that we are not considering arbitrary mixed threshold strategies here, but rather just mixing between adjacent strategies for the sole purpose of making out strategies continuous in a natural way.) Theorem 3.1 applies to strategies Sγ (the same proof goes through without change), where γ is an arbitrary nonnegative real number.
Theorem 4.1. Fix a strategy Sγ and an agent i. There exists δ∗ < 1 and n∗ such that if δ > δ∗ , n > n∗ , and every agent other than i is playing Sγ in game G(n, δ), then there is an integer k such that the best response for agent i is Sk .
Either k is unique (that is, there is a unique best response that is also a threshold strategy), or there exists an integer k such that Sγ is a best response for agent i for all γ in the interval [k , k +1] (and these are the only best responses among threshold strategies).
Proof. (Sketch:) If δ is sufficiently large, we can ignore what happens before the system converges to the maximumentropy distribution. If n is sufficiently large, then the strategy played by one agent will not affect the distribution of money significantly. Thus, the probability of i moving from one state (dollar amount) to another depends only on i"s strategy (since we can take the probability that i will be chosen to make a request and the probability that i will be chosen to satisfy a request to be constant). Thus, from i"s point of view, the system is a Markov decision process (MDP), and i needs to compute the optimal policy (strategy) for this MDP. It follows from standard results [23,
Theorem 6.11.6] that there is an optimal policy that is a threshold policy.
The argument that the best response is either unique or there is an interval of best responses follows from a more careful analysis of the value function for the MDP.
We remark that there may be best responses that are not threshold strategies. All that Theorem 4.1 shows is that, among best responses, there is at least one that is a threshold strategy. Since we know that there is a best response that is a threshold strategy, we can look for a Nash equilibrium in the space of threshold strategies.
Theorem 4.2. For all M, there exists δ∗ < 1 and n∗ such that if δ > δ∗ and n > n∗ , there exists a Nash equilibrium in the game G(n, δ) where all agents play Sγ for some integer γ > 0.
Proof. It follows easily from the proof Theorem 4.1 that if br(δ, γ) is the minimal best response threshold strategy if all the other agents are playing Sγ and the discount factor is δ then, for fixed δ, br(δ, ·) is a step function. It also follows 3 Formally, we need to define the strategies when the system is far from equilibrium. However, these far from (stochastic) equilibrium strategies will not affect the equilibrium behavior when n is large and deviations from stochastic equilibrium are extremely rare. 145 from the theorem that if there are two best responses, then a mixture of them is also a best response. Therefore, if we can join the steps by a vertical line, we get a best-response curve. It is easy to see that everywhere that this bestresponse curve crosses the diagonal y = x defines a Nash equilibrium where all agents are using the same threshold strategy. As we have already observed, one such equilibrium occurs at 0. If there are only $M in the system, we can restrict to threshold strategies Sk where k ≤ M + 1. Since no one can have more than $M, all strategies Sk for k > M are equivalent to SM ; these are just the strategies where the agent always volunteers in response to request made by someone who can pay. Clearly br(δ, SM ) ≤ M for all δ, so the best response function is at or below the equilibrium at M. If k ≤ M/n, every player will have at least k dollars and so will be unwilling to work and the best response is just 0. Consider k∗ , the smallest k such that k > M/n. It is not hard to show that for k∗ there exists a δ∗ such that for all δ ≥ δ∗ , br(δ, k∗ ) ≥ k∗ . It follows by continuity that if δ ≥ δ∗ , there must be some γ such that br(δ, γ) = γ. This is the desired Nash equilibrium.
This argument also shows us that we cannot in general expect fixed points to be unique. If br(δ, k∗ ) = k∗ and br(δ, k + 1) > k + 1 then our argument shows there must be a second fixed point. In general there may be multiple fixed points even when br(δ, k∗ ) > k∗ , as illustrated in the Figure
Strategy of Rest of Agents 0 5 10 15 20 25 BestResponse Figure 4: The best response function for n = 1000 and M = 3000.
Theorem 4.2 allows us to restrict our design to agents using threshold strategies with the confidence that there will be a nontrivial equilibrium. However, it does not rule out the possibility that there may be other equilibria that do not involve threshold stratgies. It is even possible (although it seems unlikely) that some of these equilibria might be better.
Our theorems show that for each value of M and n, for sufficiently large δ, there is a nontrivial Nash equilibrium where all the agents use some threshold strategy Sγ(M,n).
From the point of view of the system designer, not all equilibria are equally good; we want an equilibrium where as few as possible agents have $0 when they get a chance to make a request (so that they can pay for the request) and relatively few agents have more than the threshold amount of money (so that there are always plenty of agents to fulfill the request). There is a tension between these objectives. It is not hard to show that as the fraction of agents with $0 increases in the maximum entropy distribution, the fraction of agents with the maximum amount of money decreases. Thus, our goal is to understand what the optimal amount of money should be in the system, given the number of agents. That is, we want to know the amount of money M that maximizes efficiency, i.e., the total expected utility if all the agents use Sγ(M,n). 4 We first observe that the most efficient equilibrium depends only on the ratio of M to n, not on the actual values of M and n.
Theorem 5.1. There exists n∗ such that for all games G(n1, δ) and G(n2, δ) where n1, n2 > n∗ , if M1/n1 = M2/n2, then Sγ(M1,n1) = Sγ(M2,n2).
Proof. Fix M/n = r. Theorem 3.1 shows that the maximum-entropy distribution depends only on k and the ratio M/n, not on M and n separately. Thus, given r, for each choice of k, there is a unique maximum entropy distribution dk,r. The best response br(δ, k) depends only on the distribution dk,r, not M or n. Thus, the Nash equilibrium depends only on the ratio r. That is, for all choices of M and n such that n is sufficiently large (so that Theorem 3.1 applies) and M/n = r, the equilibrium strategies are the same.
In light of Theorem 5.1, the system designer should ensure that there is enough money M in the system so that the ratio between M/n is optimal. We are currently exploring exactly what the optimal ratio is. As our very preliminary results for β = 1 show in Figure 5, the ratio appears to be monotone increasing in δ, which matches the intuition that we should provide more patient agents with the opportunity to save more money. Additionally, it appears to be relatively smooth, which suggests that it may have a nice analytic solution.
Discount Rate ∆ 5
6
7 OptimalRatioofMn Figure 5: Optimal average amount of money to the nearest .25 for β = 1 We remark that, in practice, it may be easier for the designer to vary the price of fulfilling a request rather than 4 If there are multiple equilibria, we take Sγ(M,n) to be the Nash equilibrium that has highest efficiency for fixed M and n. 146 injecting money in the system. This produces the same effect. For example, changing the cost of fulfilling a request from $1 to $2 is equivalent to halving the amount of money that each agent has. Similarly, halving the the cost of fulfilling a request is equivalent to doubling the amount of money that everyone has. With a fixed amount of money M, there is an optimal product nc of the number of agents and the cost c of fulfilling a request.
Theorem 5.1 also tells us how to deal with a dynamic pool of agents. Our system can handle newcomers relatively easily: simply allow them to join with no money. This gives existing agents no incentive to leave and rejoin as newcomers. We then change the price of fulfilling a request so that the optimal ratio is maintained. This method has the nice feature that it can be implemented in a distributed fashion; if all nodes in the system have a good estimate of n then they can all adjust prices automatically. (Alternatively, the number of agents in the system can be posted in a public place.) Approaches that rely on adjusting the amount of money may require expensive system-wide computations (see [26] for an example), and must be carefully tuned to avoid creating incentives for agents to manipulate the system by which this is done.
Note that, in principle, the realization that the cost of fulfilling a request can change can affect an agent"s strategy. For example, if an agent expects the cost to increase, then he may want to defer volunteering to fulfill a request.
However, if the number of agents in the system is always increasing, then the cost always decreases, so there is never any advantage in waiting.
There may be an advantage in delaying a request, but it is far more costly, in terms of waiting costs than in providing service, since we assume the need for a service is often subject to real waiting costs, while the need to supply the service is merely to augment a money supply. (Related issues are discussed in [10].) We ultimately hope to modify the mechanism so that the price of a job can be set endogenously within the system (as in real-world economies), with agents bidding for jobs rather than there being a fixed cost set externally. However, we have not yet explored the changes required to implement this change. Thus, for now, we assume that the cost is set as a function of the number of agents in the system (and that there is no possibility for agents to satisfy a request for less than the official cost or for requesters to offer to pay more than it).
In a naive sense, our system is essentially sybil-proof. To get d dollars, his sybils together still have to perform d units of work. Moreover, since newcomers enter the system with $0, there is no benefit to creating new agents simply to take advantage of an initial endowment. Nevertheless, there are some less direct ways that an agent could take advantage of sybils. First, by having more identities he will have a greater probability of getting chosen to make a request. It is easy to see that this will lead to the agent having higher total utility. However, this is just an artifact of our model.
To make our system simple to analyze, we have assumed that request opportunities came uniformly at random. In practice, requests are made to satisfy a desire. Our model implicitly assumed that all agents are equally likely to have a desire at any particular time. Having sybils should not increase the need to have a request satisfied. Indeed, it would be reasonable to assume that sybils do not make requests at all.
Second, having sybils makes it more likely that one of the sybils will be chosen to fulfill a request. This can allow a user to increase his utility by setting a lower threshold; that is, to use a strategy Sk where k is smaller than the k used by the Nash equilibrium strategy. Intuitively, the need for money is not as critical if money is easier to obtain.
Unlike the first concern, this seems like a real issue. It seems reasonable to believe that when people make a decision between a number of nodes to satisfy a request they do so at random, at least to some extent. Even if they look for advertised node features to help make this decision, sybils would allow a user to advertise a wide range of features.
Third, an agent can drive down the cost of fulfilling a request by introducing many sybils. Similarly, he could increase the cost (and thus the value of his money) by making a number of sybils leave the system. Concievably he could alternate between these techniques to magnify the effects of work he does. We have not yet calculated the exact effect of this change (it interacts with the other two effects of having sybils that we have already noted). Given the number of sybils that would be needed to cause a real change in the perceived size of a large P2P network, the practicality of this attack depends heavily on how much sybils cost an attacker and what resources he has available.
The second point raised regarding sybils also applies to collusion if we allow money to be loaned. If k agents collude, they can agree that, if one runs out of money, another in the group will loan him money. By pooling their money in this way, the k agents can again do better by setting a higher threshold. Note that the loan mechanism doesn"t need to be built into the system; the agents can simply use a fake transaction to transfer the money. These appear to be the main avenues for collusive attacks, but we are still exploring this issue.
We have given a formal analysis of a scrip system and have shown that the existence of a Nash equilibrium where all agents use a threshold strategy. Moreover, we can compute efficiency of equilibrium strategy and optimize the price (or money supply) to maximize efficiency. Thus, our analysis provides a formal mechanisms for solving some important problems in implementing scrip systems. It tells us that with a fixed population of rational users, such systems are very unlikely to become unstable. Thus if this stability is common belief among the agents we would not expect inflation, bubbles, or crashes because of agent speculation. However, we cannot rule out the possibility that that agents may have other beliefs that will cause them to speculate. Our analysis also tells us how to scale the system to handle an influx of new users without introducing these problems: scale the money supply to keep the average amount of money constant (or equivalently adjust prices to achieve the same goal).
There are a number of theoretical issues that are still open, including a characterization of the multiplicity of equilibria - are there usually 2? In addition, we expect that one should be able to compute analytic estimates for the best response function and optimal pricing which would allow us to understand the relationship between pricing and various parameters in the model. 147 It would also be of great interest to extend our analysis to handle more realistic settings. We mention a few possible extensions here: • We have assumed that the world is homogeneous in a number of ways, including request frequency, utility, and ability to satisfy requests. It would be interesting to examine how relaxing any of these assumptions would alter our results. • We have assumed that there is no cost to an agent to be a member of the system. Suppose instead that we imposed a small cost simply for being present in the system to reflect the costs of routing messages and overlay maintainance. This modification could have a significant impact on sybil attacks. • We have described a scrip system that works when there are no altruists and have shown that no system can work once there there are sufficiently many altruists. What happens between these extremes? • One type of irrational behavior encountered with scrip systems is hoarding. There are some similarities between hoarding and altruistic behavior. While an altruist provide service for everyone, a hoarder will volunteer for all jobs (in order to get more money) and rarely request service (so as not to spend money). It would be interesting to investigate the extent to which our system is robust against hoarders. Clearly with too many hoarders, there may not be enough money remaining among the non-hoarders to guarantee that, typically, a non-hoarder would have enough money to satisfy a request. • Finally, in P2P filesharing systems, there are overlapping communities of various sizes that are significantly more likely to be able to satisfy each other"s requests.
It would be interesting to investigate the effect of such communities on the equilibrium of our system.
There are also a number of implementation issues that would have to be resolved in a real system. For example, we need to worry about the possibility of agents counterfeiting money or lying about whether service was actually provided.
Karma [26] provdes techniques for dealing with both of these issues and a number of others, but some of Karma"s implementation decisions point to problems for our model. For example, it is prohibitively expensive to ensure that bank account balances can never go negative, a fact that our model does not capture. Another example is that Karma has nodes serve as bookkeepers for other nodes account balances. Like maintaining a presence in the network, this imposes a cost on the node, but unlike that, responsibility it can be easily shirked. Karma suggests several ways to incentivize nodes to perform these duties. We have not investigated whether these mechanisms be incorporated without disturbing our equilibrium.
We would like to thank Emin Gun Sirer, Shane Henderson, Jon Kleinberg, and 3 anonymous referees for helpful suggestions. EF, IK and JH are supported in part by NSF under grant ITR-0325453. JH is also supported in part by NSF under grants CTC-0208535 and IIS-0534064, by ONR under grant N00014-01-10-511, by the DoD Multidisciplinary University Research Initiative (MURI) program administered by the ONR under grants N00014-01-1-0795 and N00014-04-1-0725, and by AFOSR under grant F49620-021-0101.
[1] E. Adar and B. A. Huberman. Free riding on Gnutella. First Monday, 5(10), 2000. [2] K. G. Anagnostakis and M. Greenwald.
Exchange-based incentive mechanisms for peer-to-peer file sharing. In International Conference on Distributed Computing Systems (ICDCS), pages 524-533, 2004. [3] BitTorrent Inc. BitTorrent web site. http://www.bittorent.com. [4] A. Cheng and E. Friedman. Sybilproof reputation mechanisms. In Workshop on Economics of Peer-to-Peer Systems (P2PECON), pages 128-132,
[5] Cornell Information Technologies. Cornell"s ccommodity internet usage statistics. http://www.cit.cornell.edu/computer/students/ bandwidth/charts.html. [6] J. R. Douceur. The sybil attack. In International Workshop on Peer-to-Peer Systems (IPTPS), pages 251-260, 2002. [7] G. Ellison. Cooperation in the prisoner"s dilemma with anonymous random matching. Review of Economic Studies, 61:567-588, 1994. [8] eMule Project. eMule web site. http://www.emule-project.net/. [9] M. Feldman, K. Lai, I. Stoica, and J. Chuang. Robust incentive techniques for peer-to-peer networks. In ACM Conference on Electronic Commerce (EC), pages 102-111, 2004. [10] E. J. Friedman and D. C. Parkes. Pricing wifi at starbucks: issues in online mechanism design. In EC "03: Proceedings of the 4th ACM Conference on Electronic Commerce, pages 240-241. ACM Press,
[11] E. J. Friedman and P. Resnick. The social cost of cheap pseudonyms. Journal of Economics and Management Strategy, 10(2):173-199, 2001. [12] R. Guha, R. Kumar, P. Raghavan, and A. Tomkins.
Propagation of trust and distrust. In Conference on the World Wide Web(WWW), pages 403-412, 2004. [13] M. Gupta, P. Judge, and M. H. Ammar. A reputation system for peer-to-peer networks. In Network and Operating System Support for Digital Audio and Video(NOSSDAV), pages 144-152, 2003. [14] Z. Gyongi, P. Berkhin, H. Garcia-Molina, and J. Pedersen. Link spam detection based on mass estimation. Technical report, Stanford University,
[15] J. Ioannidis, S. Ioannidis, A. D. Keromytis, and V. Prevelakis. Fileteller: Paying and getting paid for file storage. In Financial Cryptography, pages 282-299,
[16] E. T. Jaynes. Where do we stand on maximum entropy? In R. D. Levine and M. Tribus, editors, The Maximum Entropy Formalism, pages 15-118. MIT Press, Cambridge, Mass., 1978. 148 [17] S. D. Kamvar, M. T. Schlosser, and H. Garcia-Molina.
The Eigentrust algorithm for reputation management in P2P networks. In Conference on the World Wide Web (WWW), pages 640-651, 2003. [18] M. Kandori. Social norms and community enforcement. Review of Economic Studies, 59:63-80,
[19] LogiSense Corporation. LogiSense web site. http://www.logisense.com/tm p2p.html. [20] L. Lovasz and P. Winkler. Mixing of random walks and other diffusions on a graph. In Surveys in Combinatorics, 1993, Walker (Ed.), London Mathematical Society Lecture Note Series 187,
Cambridge University Press. 1995. [21] Open Source Technology Group. Slashdot FAQcomments and moderation. http://slashdot.org/faq/com-mod.shtml#cm700. [22] OSMB LLC. Gnutella web site. http://www.gnutella.com/. [23] M. L. Puterman. Markov Decision Processes. Wiley,
[24] SETI@home. SETI@home web page. http://setiathome.ssl.berkeley.edu/. [25] Sharman Networks Ltd. Kazaa web site. http://www.kazaa.com/. [26] V. Vishnumurthy, S. Chandrakumar, and E. Sirer.
Karma: A secure economic framework for peer-to-peer resource sharing. In Workshop on Economics of Peer-to-Peer Systems (P2PECON), 2003. [27] L. Xiong and L. Liu. Building trust in decentralized peer-to-peer electronic communities. In Internation Conference on Electronic Commerce Research (ICECR), 2002. [28] H. Zhang, A. Goel, R. Govindan, K. Mason, and B. V.
Roy. Making eigenvector-based reputation systems robust to collusion. In Workshop on Algorithms and Models for the Web-Graph(WAW), pages 92-104,

Late October 1960. A smoky room. Democratic Party strategists huddle around a map. How should the Kennedy campaign allocate its remaining advertising budget? Should it focus on, say, California or New York? The Nixon campaign faces the same dilemma. Of course, neither campaign knows the effectiveness of its advertising in each state.
Perhaps Californians are susceptible to Nixon"s advertising, but are unresponsive to Kennedy"s. In light of this uncertainty, the Kennedy campaign may conduct a survey, at some cost, to estimate the effectiveness of its advertising. Moreover, the larger-and more expensive-the survey, the more accurate it will be. Is the cost of a survey worth the information that it provides? How should one balance the cost of acquiring more information against the risk of playing a game with higher uncertainty?
In this paper, we model situations of this type as Socratic games. As in traditional game theory, the players in a Socratic game choose actions to maximize their payoffs, but we model players with incomplete information who can make costly queries to reduce their uncertainty about the state of the world before they choose their actions. This approach contrasts with traditional game theory, in which players are usually modeled as having fixed, exogenously given information about the structure of the game and its payoffs. (In traditional games of incomplete and imperfect information, there is information that the players do not have; in Socratic games, unlike in these games, the players have a chance to acquire the missing information, at some cost.) A number of related models have been explored by economists and computer scientists motivated by similar situations, often with a focus on mechanism design and auctions; a sampling of this research includes the work of Larson and Sandholm [41, 42, 43, 44], Parkes [59], Fong [22], Compte and Jehiel [12],
Rezende [63], Persico and Matthews [48, 60], Cr´emer and Khalil [15], Rasmusen [62], and Bergemann and V¨alim¨aki [4, 5]. The model of Bergemann and V¨alim¨aki is similar in many regards to the one that we explore here; see Section 7 for some discussion.
A Socratic game proceeds as follows. A real world is cho150 sen randomly from a set of possible worlds according to a common prior distribution. Each player then selects an arbitrary query from a set of available costly queries and receives a corresponding piece of information about the real world. Finally each player selects an action and receives a payoff-a function of the players" selected actions and the identity of the real world-less the cost of the query that he or she made. Compared to traditional game theory, the distinguishing feature of our model is the introduction of explicit costs to the players for learning arbitrary partial information about which of the many possible worlds is the real world.
Our research was initially inspired by recent results in psychology on decision making, but it soon became clear that Socratic game theory is also a general tool for understanding the exploitation versus exploration tradeoff, well studied in machine learning, in a strategic multiplayer environment.
This tension between the risk arising from uncertainty and the cost of acquiring information is ubiquitous in economics, political science, and beyond.
Our results. We consider Socratic games under two models: an unobservable-query model where players learn only the response to their own queries and an observable-query model where players also learn which queries their opponents made. We give efficient algorithms to find Nash equilibriai.e., tuples of strategies from which no player has unilateral incentive to deviate-in broad classes of two-player Socratic games in both models. Our first result is an efficient algorithm to find Nash equilibria in unobservable-query Socratic games with constant-sum worlds, in which the sum of the players" payoffs is independent of their actions. Our techniques also yield Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds.
Strategically zero-sum games generalize constant-sum games by allowing the sum of the players" payoffs to depend on individual players" choices of strategy, but not on any interaction of their choices. Our second result is an efficient algorithm to find Nash equilibria in observable-query Socratic games with constant-sum worlds. Finally, we give an efficient algorithm to find correlated equilibria-a weaker but increasingly well-studied solution concept for games [2, 3, 32, 56, 57]-in observable-query Socratic games with strategically zero-sum worlds.
Like all games, Socratic games can be viewed as a special case of extensive-form games, which represent games by trees in which internal nodes represent choices made by chance or by the players, and the leaves represent outcomes that correspond to a vector of payoffs to the players.
Algorithmically, the generality of extensive-form games makes them difficult to solve efficiently, and the special cases that are known to be efficiently solvable do not include even simple Socratic games. Every (complete-information) classical game is a trivial Socratic game (with a single possible world and a single trivial query), and efficiently finding Nash equilibria in classical games has been shown to be hard [10, 11, 13, 16, 17, 27, 54, 55]. Therefore we would not expect to find a straightforward polynomial-time algorithm to compute Nash equilibria in general Socratic games. However, it is well known that Nash equilibria can be found efficiently via an LP for two-player constant-sum games [49, 71] (and strategically zero-sum games [51]). A Socratic game is itself a classical game, so one might hope that these results can be applied to Socratic games with constant-sum (or strategically zero-sum) worlds.
We face two major obstacles in extending these classical results to Socratic games. First, a Socratic game with constant-sum worlds is not itself a constant-sum classical game-rather, the resulting classical game is only strategically zero sum. Worse yet, a Socratic game with strategically zero-sum worlds is not itself classically strategically zero sum-indeed, there are no known efficient algorithmic techniques to compute Nash equilibria in the resulting class of classical games. (Exponential-time algorithms like Lemke/Howson, of course, can be used [45].) Thus even when it is easy to find Nash equilibria in each of the worlds of a Socratic game, we require new techniques to solve the Socratic game itself. Second, even when the Socratic game itself is strategically zero sum, the number of possible strategies available to each player is exponential in the natural representation of the game. As a result, the standard linear programs for computing equilibria have an exponential number of variables and an exponential number of constraints.
For unobservable-query Socratic games with strategically zero-sum worlds, we address these obstacles by formulating a new LP that uses only polynomially many variables (though still an exponential number of constraints) and then use ellipsoid-based techniques to solve it. For observablequery Socratic games, we handle the exponentiality by decomposing the game into stages, solving the stages separately, and showing how to reassemble the solutions efficiently. To solve the stages, it is necessary to find Nash equilibria in Bayesian strategically zero-sum games, and we give an explicit polynomial-time algorithm to do so.
In this section, we review background on game theory and formally introduce Socratic games. We present these models in the context of two-player games, but the multiplayer case is a natural extension. Throughout the paper, boldface variables will be used to denote a pair of variables (e.g., a = ai, aii ). Let Pr[x ← π] denote the probability that a particular value x is drawn from the distribution π, and let Ex∼π[g(x)] denote the expectation of g(x) when x is drawn from π.
Consider two players, Player I and Player II, each of whom is attempting to maximize his or her utility (or payoff). A (two-player) game is a pair A, u , where, for i ∈ {i,ii}, • Ai is the set of pure strategies for Player i, and A = Ai, Aii ; and • ui : A → R is the utility function for Player i, and u = ui, uii .
We require that A and u be common knowledge. If each Player i chooses strategy ai ∈ Ai, then the payoffs to Players I and II are ui(a) and uii(a), respectively. A game is constant sum if, for all a ∈ A, we have that ui(a) + uii(a) = c for some fixed c independent of a.
Player i can also play a mixed strategy αi ∈ Ai, where Ai denotes the space of probability measures over the set Ai.
Payoff functions are generalized as ui (α) = ui (αi, αii) := Ea∼α[ui (a)] = P a∈A α(a)ui (a), where the quantity α(a) = 151 αi(ai) · αii(aii) denotes the joint probability of the independent events that each Player i chooses action ai from the distribution αi. This generalization to mixed strategies is known as von Neumann/Morgenstern utility [70], in which players are indifferent between a guaranteed payoff x and an expected payoff of x.
A Nash equilibrium is a pair α of mixed strategies so that neither player has an incentive to change his or her strategy unilaterally. Formally, the strategy pair α is a Nash equilibrium if and only if both ui(αi, αii) = maxαi∈Ai ui(αi, αii) and uii(αi, αii) = maxαii∈Aii uii(αi, αii); that is, the strategies αi and αii are mutual best responses.
A correlated equilibrium is a distribution ψ over A that obeys the following: if a ∈ A is drawn randomly according to ψ and Player i learns ai, then no Player i has incentive to deviate unilaterally from playing ai. (A Nash equilibrium is a correlated equilibrium in which ψ(a) = αi(ai) · αii(aii) is a product distribution.) Formally, in a correlated equilibrium, for every a ∈ A we must have that ai is a best response to a randomly chosen ˆaii ∈ Aii drawn according to ψ(ai, ˆaii), and the analogous condition must hold for Player II.
In this section, we formally define Socratic games. A Socratic game is a 7-tuple A, W, u, S, Q, p, δ , where, for i ∈ {i,ii}: • Ai is, as before, the set of pure strategies for Player i. • W is a set of possible worlds, one of which is the real world wreal. • ui = {uw i : A → R | w ∈ W} is a set of payoff functions for Player i, one for each possible world. • S is a set of signals. • Qi is a set of available queries for Player i. When Player i makes query qi : W → S, he or she receives the signal qi(wreal). When Player i receives signal qi(wreal) in response to query qi, he or she can infer that wreal ∈ {w : qi(w) = qi(wreal)}, i.e., the set of possible worlds from which query qi cannot distinguish wreal. • p : W → [0, 1] is a probability distribution over the possible worlds. • δi : Qi → R≥0 gives the query cost for each available query for Player i.
Initially, the world wreal is chosen according to the probability distribution p, but the identity of wreal remains unknown to the players. That is, it is as if the players are playing the game A, uwreal but do not know wreal. The players make queries q ∈ Q, and Player i receives the signal qi(wreal). We consider both observable queries and unobservable queries. When queries are observable, each player learns which query was made by the other player, and the results of his or her own query-that is, each Player i learns qi, qii, and qi(wreal). For unobservable queries, Player i learns only qi and qi(wreal). After learning the results of the queries, the players select strategies a ∈ A and receive as payoffs u wreal i (a) − δi(qi).
In the Socratic game, a pure strategy for Player i consists of a query qi ∈ Qi and a response function mapping any result of the query qi to a strategy ai ∈ Ai to play. A player"s state of knowledge after a query is a point in R := Q × S or Ri := Qi × S for observable or unobservable queries, respectively. Thus Player i"s response function maps R or Ri to Ai. Note that the number of pure strategies is exponential, as there are exponentially many response functions. A mixed strategy involves both randomly choosing a query qi ∈ Qi and randomly choosing an action ai ∈ Ai in response to the results of the query. Formally, we will consider a mixed-strategy-function profile f = fquery , fresp to have two parts: • a function fquery i : Qi → [0, 1], where fquery i (qi) is the probability that Player i makes query qi. • a function fresp i that maps R or Ri to a probability distribution over actions. Player i chooses an action ai ∈ Ai according to the probability distribution fresp i (q, qi(w)) for observable queries, and according to fresp i (qi, qi(w)) for unobservable queries. (With unobservable queries, for example, the probability that Player I plays action ai conditioned on making query qi in world w is given by Pr[ai ← fresp i (qi, qi(w))].) Mixed strategies are typically defined as probability distributions over the pure strategies, but here we represent a mixed strategy by a pair fquery , fresp , which is commonly referred to as a behavioral strategy in the game-theory literature. As in any game with perfect recall, one can easily map a mixture of pure strategies to a behavioral strategy f = fquery , fresp that induces the same probability of making a particular query qi or playing a particular action after making a query qi in a particular world. Thus it suffices to consider only this representation of mixed strategies.
For a strategy-function profile f for observable queries, the (expected) payoff to Player i is given by X q∈Q,w∈W,a∈A 2 6 6 4 fquery i (qi) · fquery ii (qii) · p(w) · Pr[ai ← fresp i (q, qi(w))] · Pr[aii ← fresp ii (q, qii(w))] · (uw i (a) − δi(qi)) 3 7 7
The payoffs for unobservable queries are analogous, with fresp j (qj, qj(w)) in place of fresp j (q, qj(w)).
We can view a Socratic game G with constant-sum worlds as an exponentially large classical game, with pure strategies make query qi and respond according to fi.
However, this classical game is not constant sum. The sum of the players" payoffs varies depending upon their strategies, because different queries incur different costs. However, this game still has significant structure: the sum of payoffs varies only because of varying query costs. Thus the sum of payoffs does depend on players" choice of strategies, but not on the interaction of their choices-i.e., for fixed functions gi and gii, we have ui(q, f) + uii(q, f) = gi(qi, fi) + gii(qii, fii) for all strategies q, f . Such games are called strategically zero sum and were introduced by Moulin and Vial [51], who describe a notion of strategic equivalence and define strategically zero-sum games as those strategically equivalent to zero-sum games. It is interesting to note that two Socratic games with the same queries and strategically equivalent worlds are not necessarily strategically equivalent.
A game A, u is strategically zero sum if there exist labels (i, ai) for every Player i and every pure strategy ai ∈ Ai 152 such that, for all mixed-strategy profiles α, we have that the sum of the utilities satisfies ui(α)+uii(α) = X ai∈Ai αi(ai)· (i, ai)+ X aii∈Aii αii(aii)· (ii, aii).
Note that any constant-sum game is strategically zero sum as well.
It is not immediately obvious that one can efficiently decide if a given game is strategically zero sum. For completeness, we give a characterization of classical strategically zero-sum games in terms of the rank of a simple matrix derived from the game"s payoffs, allowing us to efficiently decide if a given game is strategically zero sum and, if it is, to compute the labels (i, ai).
Theorem 3.1. Consider a game G = A, u with Ai = {a1 i , . . . , ani i }. Let MG be the ni-by-nii matrix whose i, j th entry MG (i,j) satisfies log2 MG (i,j) = ui(ai i , aj ii) + uii(ai i , aj ii).
Then the following are equivalent: (i) G is strategically zero sum; (ii) there exist labels (i, ai) for every player i ∈ {i,ii} and every pure strategy ai ∈ Ai such that, for all pure strategies a ∈ A, we have ui(a) + uii(a) = (i, ai) + (ii, aii); and (iii) rank(MG ) = 1.
Proof Sketch. (i ⇒ ii) is immediate; every pure strategy is a trivially mixed strategy. For (ii ⇒ iii), let ci be the n-element column vector with jth component 2 (i,a j i ) ; then ci · cii T = MG . For (iii ⇒ i), if rank(MG ) = 1, then MG = u · vT . We can prove that G is strategically zero sum by choosing labels (i, aj i ) := log2 uj and (ii, aj ii) := log2 vj.
UNOBSERVABLE QUERIES We begin with Socratic games with unobservable queries, where a player"s choice of query is not revealed to her opponent. We give an efficient algorithm to solve unobservablequery Socratic games with strategically zero-sum worlds.
Our algorithm is based upon the LP shown in Figure 1, whose feasible points are Nash equilibria for the game. The LP has polynomially many variables but exponentially many constraints. We give an efficient separation oracle for the LP, implying that the ellipsoid method [28, 38] yields an efficient algorithm. This approach extends the techniques of Koller and Megiddo [39] (see also [40]) to solve constant-sum games represented in extensive form. (Recall that their result does not directly apply in our case; even a Socratic game with constant-sum worlds is not a constant-sum classical game.) Lemma 4.1. Let G = A, W, u, S, Q, p, δ be an arbitrary unobservable-query Socratic game with strategically zero-sum worlds. Any feasible point for the LP in Figure 1 can be efficiently mapped to a Nash equilibrium for G, and any Nash equilibrium for G can be mapped to a feasible point for the program.
Proof Sketch. We begin with a description of the correspondence between feasible points for the LP and Nash equilibria for G. First, suppose that strategy profile f = fquery , fresp forms a Nash equilibrium for G. Then the following setting for the LP variables is feasible: yi qi = fquery i (qi) xi ai,qi,w = Pr[ai ← fresp i (qi, qi(w))] · yi qi ρi = P w,q∈Q,a∈A p(w) · xi ai,qi,w · xii aii,qii,w · [uw i (a) − δi(qi)]. (We omit the straightforward calculations that verify feasibility.) Next, suppose xi ai,qi,w, yi qi , ρi is feasible for the LP.
Let f be the strategy-function profile defined as fquery i : qi → yi qi fresp i (qi, qi(w)) : ai → xi ai,qi,w/yi qi .
Verifying that this strategy profile is a Nash equilibrium requires checking that fresp i (qi, qi(w)) is a well-defined function (from constraint VI), that fquery i and fresp i (qi, qi(w)) are probability distributions (from constraints III and IV), and that each player is playing a best response to his or her opponent"s strategy (from constraints I and II). Finally, from constraints I and II, the expected payoff to Player i is at most ρi. Because the right-hand side of constraint VII is equal to the expected sum of the payoffs from f and is at most ρi + ρii, the payoffs are correct and imply the lemma.
We now give an efficient separation oracle for the LP in Figure 1, thus allowing the ellipsoid method to solve the LP in polynomial time. Recall that a separation oracle is a function that, given a setting for the variables in the LP, either returns feasible or returns a particular constraint of the LP that is violated by that setting of the variables.
An efficient, correct separation oracle allows us to solve the LP efficiently via the ellipsoid method.
Lemma 4.2. There exists a separation oracle for the LP in Figure 1 that is correct and runs in polynomial time.
Proof. Here is a description of the separation oracle SP.
On input xi ai,qi,w, yi qi , ρi :
and (VII). If any one of these constraints is violated, then return it.
fquery i : qi → yi qi fresp i (qi, qi(w)) : ai → xi ai,qi,w/yi qi For each query qi, we will compute a pure best-response function ˆf qi i for Player I to strategy fii after making query qi.
More specifically, given fii and the result qi(wreal) of the query qi, it is straightforward to compute the probability that, conditioned on the fact that the result of query qi is qi(w), the world is w and Player II will play action aii ∈ Aii. Therefore, for each query qi and response qi(w), Player I can compute the expected utility of each pure response ai to the induced mixed strategy over Aii for Player II. Player I can then select the ai maximizing this expected payoff.
Let ˆfi be the response function such that ˆfi(qi, qi(w)) = ˆf qi i (qi(w)) for every qi ∈ Qi. Similarly, compute ˆfii. 153 Player i does not prefer ‘make query qi, then play according to the function fi" : ∀qi ∈ Qi, fi : Ri → Ai : ρi ≥ P w∈W,aii∈Aii,qii∈Qii,ai=fi(qi,qi(w)) ` p(w) · xii aii,qii,w · [uw i (a) − δi(qi)] ´ (I) ∀qii ∈ Qii, fii : Rii → Aii : ρii ≥ P w∈W,ai∈Ai,qi∈Qi,aii=fii(qii,qii(w)) ` p(w) · xi ai,qi,w · [uw ii (a) − δii(qii)] ´ (II) Every player"s choices form a probability distribution in every world: ∀i ∈ {i,ii}, w ∈ W : 1 = P ai∈Ai,qi∈Qi xi ai,qi,w (III) ∀i ∈ {i,ii}, w ∈ W : 0 ≤ xi ai,qi,w (IV) Queries are independent of the world, and actions depend only on query output: ∀i ∈ {i,ii}, qi ∈ Qi, w ∈ W, w ∈ W such that qi(w) = qi(w ) : yi qi = P ai∈Ai xi ai,qi,w (V) xi ai,qi,w = xi ai,qi,w (VI) The payoffs are consistent with the labels (i, ai, w): ρi + ρii = P i∈{i,ii} P w∈W,qi∈Qi,ai∈Ai ` p(w) · xi ai,qi,w · [ (i, ai, w) − δi(qi)] ´ (VII) Figure 1: An LP to find Nash equilibria in unobservable-query Socratic games with strategically zero-sum worlds. The input is a Socratic game A, W, u, S, Q, p, δ so that world w is strategically zero sum with labels (i, ai, w). Player i makes query qi ∈ Qi with probability yi qi and, when the actual world is w ∈ W, makes query qi and plays action ai with probability xi ai,qi,w. The expected payoff to Player i is given by ρi.
qi i be the expected payoff to Player I using the strategy make query qi and play response function ˆfi if Player II plays according to fii.
Let ˆρi = maxqi∈Qq ˆρ qi i and let ˆqi = arg maxqi∈Qq ˆρ qi i .
Similarly, define ˆρ qii ii , ˆρii, and ˆqii.
(I-ˆqi- ˆfi) or (II-ˆqii- ˆfii) if either is violated. If both are satisfied, then return feasible.
We first note that the separation oracle runs in polynomial time and then prove its correctness. Steps 1 and 4 are clearly polynomial. For Step 2, we have described how to compute the relevant response functions by examining every action of Player I, every world, every query, and every action of Player II. There are only polynomially many queries, worlds, query results, and pure actions, so the running time of Steps 2 and
We now sketch the proof that the separation oracle works correctly. The main challenge is to show that if any constraint (I-qi-fi ) is violated then (I-ˆqi- ˆfi) is violated in Step 4.
First, we observe that, by construction, the function ˆfi computed in Step 3 must be a best response to Player II playing fii, no matter what query Player I makes. Therefore the strategy make query ˆqi, then play response function ˆfi must be a best response to Player II playing fii, by definition of ˆqi. The right-hand side of each constraint (I-qi-fi ) is equal to the expected payoff that Player I receives when playing the pure strategy make query qi and then play response function fi  against Player II"s strategy of fii. Therefore, because the pure strategy make query ˆqi and then play response function ˆfi is a best response to Player II playing fii, the right-hand side of constraint (I-ˆqi- ˆfi) is at least as large as the right hand side of any constraint (I-ˆqi-fi ).
Therefore, if any constraint (I-qi-fi ) is violated, constraint (I-ˆqi- ˆfi) is also violated. An analogous argument holds for Player II.
These lemmas and the well-known fact that Nash equilibria always exist [52] imply the following theorem: Theorem 4.3. Nash equilibria can be found in polynomial time for any two-player unobservable-query Socratic game with strategically zero-sum worlds.
OBSERVABLE QUERIES In this section, we give efficient algorithms to find (1) a Nash equilibrium for observable-query Socratic games with constant-sum worlds and (2) a correlated equilibrium in the broader class of Socratic games with strategically zero-sum worlds. Recall that a Socratic game G = A, W, u, S, Q, p, δ with observable queries proceeds in two stages: Stage 1: The players simultaneously choose queries q ∈ Q.
Player i receives as output qi, qii, and qi(wreal).
Stage 2: The players simultaneously choose strategies a ∈ A. The payoff to Player i is u wreal i (a) − δi(qi).
Using backward induction, we first solve Stage 2 and then proceed to the Stage-1 game.
For a query q ∈ Q, we would like to analyze the Stage-2 game ˆGq resulting from the players making queries q in Stage 1. Technically, however, ˆGq is not actually a game, because at the beginning of Stage 2 the players have different information about the world: Player I knows qi(wreal), and 154 Player II knows qii(wreal). Fortunately, the situation in which players have asymmetric private knowledge has been well studied in the game-theory literature. A Bayesian game is a quadruple A, T, r, u , where: • Ai is the set of pure strategies for Player i. • Ti is the set of types for Player i. • r is a probability distribution over T; r(t) denotes the probability that Player i has type ti for all i. • ui : A × T → R is the payoff function for Player i.
If the players have types t and play pure strategies a, then ui(a, t) denotes the payoff for Player i.
Initially, a type t is drawn randomly from T according to the distribution r. Player i learns his type ti, but does not learn any other player"s type. Player i then plays a mixed strategy αi ∈ Ai-that is, a probability distribution over Ai-and receives payoff ui(α, t). A strategy function is a function hi : Ti → Ai; Player i plays the mixed strategy hi(ti) ∈ Ai when her type is ti. A strategy-function profile h is a Bayesian Nash equilibrium if and only if no Player i has unilateral incentive to deviate from hi if the other players play according to h. For a two-player Bayesian game, if α = h(t), then the profile h is a Bayesian Nash equilibrium exactly when the following condition and its analogue for Player II hold: Et∼r[ui(α, t)] = maxhi Et∼r[ui( hi(ti), αii , t)]. These conditions hold if and only if, for all ti ∈ Ti occurring with positive probability, Player i"s expected utility conditioned on his type being ti is maximized by hi(ti). A Bayesian game is constant sum if for all a ∈ A and all t ∈ T, we have ui(a, t) + uii(a, t) = ct, for some constant ct independent of a. A Bayesian game is strategically zero sum if the classical game A, u(·, t) is strategically zero sum for every t ∈ T. Whether a Bayesian game is strategically zero sum can be determined as in Theorem 3.1. (For further discussion of Bayesian games, see [25, 31].) We now formally define the Stage-2 game as a Bayesian game. Given a Socratic game G = A, W, u, S, Q, p, δ and a query profile q ∈ Q, we define the Stage-2 Bayesian game Gstage2(q) := A, Tq , pstage2(q) , ustage2(q) , where: • Ai, the set of pure strategies for Player i, is the same as in the original Socratic game; • Tq i = {qi(w) : w ∈ W}, the set of types for Player i, is the set of signals that can result from query qi; • pstage2(q) (t) = Pr[q(w) = t | w ← p]; and • u stage2(q) i (a, t) = P w∈W Pr[w ← p | q(w) = t] · uw i (a).
We now define the Stage-1 game in terms of the payoffs for the Stage-2 games. Fix any algorithm alg that finds a Bayesian Nash equilibrium hq,alg := alg(Gstage2(q)) for each Stage-2 game. Define valuealg i (Gstage2(q)) to be the expected payoff received by Player i in the Bayesian game Gstage2(q) if each player plays according to hq,alg , that is, valuealg i (Gstage2(q)) := P w∈W p(w) · u stage2(q) i (hq,alg (q(w)), q(w)).
Define the game Galg stage1 := Astage1 , ustage1(alg) , where: • Astage1 := Q, the set of available queries in the Socratic game; and • u stage1(alg) i (q) := valuealg i (Gstage2(q)) − δi(qi).
I.e., players choose queries q and receive payoffs corresponding to valuealg (Gstage2(q)), less query costs.
Lemma 5.1. Consider an observable-query Socratic game G = A, W, u, S, Q, p, δ . Let Gstage2(q) be the Stage-2 games for all q ∈ Q, let alg be an algorithm finding a Bayesian Nash equilibrium in each Gstage2(q), and let Galg stage1 be the Stage-1 game. Let α be a Nash equilibrium for Galg stage1, and let hq,alg := alg(Gstage2(q)) be a Bayesian Nash equilibrium for each Gstage2(q). Then the following strategy profile is a Nash equilibrium for G: • In Stage 1, Player i makes query qi with probability αi(qi). (That is, set fquery (q) := α(q).) • In Stage 2, if q is the query in Stage 1 and qi(wreal) denotes the response to Player i"s query, then Player i chooses action ai with probability hq,alg i (qi(wreal)). (In other words, set fresp i (q, qi(w)) := hq,alg i (qi(w)).) We now find equilibria in the stage games for Socratic games with constant- or strategically zero-sum worlds. We first show that the stage games are well structured in this setting: Lemma 5.2. Consider an observable-query Socratic game G = A, W, u, S, Q, p, δ with constant-sum worlds. Then the Stage-1 game Galg stage1 is strategically zero sum for every algorithm alg, and every Stage-2 game Gstage2(q) is Bayesian constant sum. If the worlds of G are strategically zero sum, then every Gstage2(q) is Bayesian strategically zero sum.
We now show that we can efficiently compute equilibria for these well-structured stage games.
Theorem 5.3. There exists a polynomial-time algorithm BNE finding Bayesian Nash equilibria in strategically zerosum Bayesian (and thus classical strategically zero-sum or Bayesian constant-sum) two-player games.
Proof Sketch. Let G = A, T, r, u be a strategically zero-sum Bayesian game. Define an unobservable-query Socratic game G∗ with one possible world for each t ∈ T, one available zero-cost query qi for each Player i so that qi reveals ti, and all else as in G. Bayesian Nash equilibria in G correspond directly to Nash equilibria in G∗ , and the worlds of G∗ are strategically zero sum. Thus by Theorem 4.3 we can compute Nash equilibria for G∗ , and thus we can compute Bayesian Nash equilibria for G. (LP"s for zero-sum two-player Bayesian games have been previously developed and studied [61].) Theorem 5.4. We can compute a Nash equilibrium for an arbitrary two-player observable-query Socratic game G = A, W, u, S, Q, p, δ with constant-sum worlds in polynomial time.
Proof. Because each world of G is constant sum, Lemma
all Bayesian constant sum. Thus we can use algorithm BNE to compute a Bayesian Nash equilibrium hq,BNE := BNE(Gstage2(q)) for each q ∈ Q, by Theorem 5.3.
Furthermore, again by Lemma 5.2, the induced Stage-1 game GBNE stage1 is classical strategically zero sum. Therefore we can again use algorithm BNE to compute a Nash equilibrium α := BNE(GBNE stage1), again by Theorem 5.3. Therefore, by Lemma 5.1, we can assemble α and the hq,BNE "s into a Nash equilibrium for the Socratic game G. 155 We would like to extend our results on observable-query Socratic games to Socratic games with strategically zerosum worlds. While we can still find Nash equilibria in the Stage-2 games, the resulting Stage-1 game is not in general strategically zero sum. Thus, finding Nash equilibria in observable-query Socratic games with strategically zerosum worlds seems to require substantially new techniques.
However, our techniques for decomposing observable-query Socratic games do allow us to find correlated equilibria in this case.
Lemma 5.5. Consider an observable-query Socratic game G = A, W, u, S, Q, p, δ . Let alg be an arbitrary algorithm that finds a Bayesian Nash equilibrium in each of the derived Stage-2 games Gstage2(q), and let Galg stage1 be the derived Stage1 game. Let φ be a correlated equilibrium for Galg stage1, and let hq,alg := alg(Gstage2(q)) be a Bayesian Nash equilibrium for each Gstage2(q). Then the following distribution over pure strategies is a correlated equilibrium for G: ψ(q, f) := φ(q) Y i∈{i,ii} Y s∈S Pr h fi(q, s) ← hq,alg i (s) i .
Thus to find a correlated equilibrium in an observable-query Socratic game with strategically zero-sum worlds, we need only algorithm BNE from Theorem 5.3 along with an efficient algorithm for finding a correlated equilibrium in a general game. Such an algorithm exists (the definition of correlated equilibria can be directly translated into an LP [3]), and therefore we have the following theorem: Theorem 5.6. We can provide both efficient oracle access and efficient sampling access to a correlated equilibrium for any observable-query two-player Socratic game with strategically zero-sum worlds.
Because the support of the correlated equilibrium may be exponentially large, providing oracle and sampling access is the natural way to represent the correlated equilibrium.
By Lemma 5.5, we can also compute correlated equilibria in any observable-query Socratic game for which Nash equilibria are computable in the induced Gstage2(q) games (e.g., when Gstage2(q) is of constant size).
Another potentially interesting model of queries in Socratic games is what one might call public queries, in which both the choice and outcome of a player"s query is observable by all players in the game. (This model might be most appropriate in the presence of corporate espionage or media leaks, or in a setting in which the queries-and thus their results-are done in plain view.) The techniques that we have developed in this section also yield exactly the same results as for observable queries. The proof is actually simpler: with public queries, the players" payoffs are common knowledge when Stage 2 begins, and thus Stage 2 really is a complete-information game. (There may still be uncertainty about the real world, but all players use the observed signals to infer exactly the same set of possible worlds in which wreal may lie; thus they are playing a complete-information game against each other.) Thus we have the same results as in Theorems 5.4 and 5.6 more simply, by solving Stage 2 using a (non-Bayesian) Nash-equilibrium finder and solving Stage 1 as before.
Our results for observable queries are weaker than for unobservable: in Socratic games with worlds that are strategically zero sum but not constant sum, we find only a correlated equilibrium in the observable case, whereas we find a Nash equilibrium in the unobservable case. We might hope to extend our unobservable-query techniques to observable queries, but there is no obvious way to do so. The fundamental obstacle is that the LP"s payoff constraint becomes nonlinear if there is any dependence on the probability that the other player made a particular query. This dependence arises with observable queries, suggesting that observable Socratic games with strategically zero-sum worlds may be harder to solve.
Our work was initially motivated by research in the social sciences indicating that real people seem (irrationally) paralyzed when they are presented with additional options. In this section, we briefly review some of these social-science experiments and then discuss technical approaches related to Socratic game theory.
Prima facie, a rational agent"s happiness given an added option can only increase. However, recent research has found that more choices tend to decrease happiness: for example, students choosing among extra-credit options are more likely to do extra credit if given a small subset of the choices and, moreover, produce higher-quality work [35]. (See also [19].) The psychology literature explores a number of explanations: people may miscalculate their opportunity cost by comparing their choice to a component-wise maximum of all other options instead of the single best alternative [65], a new option may draw undue attention to aspects of the other options [67], and so on. The present work explores an economic explanation of this phenomenon: information is not free. When there are more options, a decision-maker must spend more time to achieve a satisfactory outcome.
See, e.g., the work of Skyrms [68] for a philosophical perspective on the role of deliberation in strategic situations.
Finally, we note the connection between Socratic games and modal logic [34], a formalism for the logic of possibility and necessity.
The observation that human players typically do not play rational strategies has inspired some attempts to model partially rational players. The typical model of this socalled bounded rationality [36, 64, 66] is to postulate bounds on computational power in computing the consequences of a strategy. The work on bounded rationality [23, 24, 53, 58] differs from the models that we consider here in that instead of putting hard limitations on the computational power of the agents, we instead restrict their a priori knowledge of the state of the world, requiring them to spend time (and therefore money/utility) to learn about it.
Partially observable stochastic games (POSGs) are a general framework used in AI to model situations of multi-agent planning in an evolving, unknown environment, but the generality of POSGs seems to make them very difficult [6].
Recent work has been done in developing algorithms for restricted classes of POSGs, most notably classes of cooperative POSGs-e.g., [20, 30]-which are very different from the competitive strategically zero-sum games we address in this paper.
The fundamental question in Socratic games is deciding on the comparative value of making a more costly but more informative query, or concluding the data-gathering phase and picking the best option, given current information. This tradeoff has been explored in a variety of other contexts; a sampling of these contexts includes aggregating results 156 from delay-prone information sources [8], doing approximate reasoning in intelligent systems [72], deciding when to take the current best guess of disease diagnosis from a beliefpropagation network and when to let it continue inference [33], among many others.
This issue can also be viewed as another perspective on the general question of exploration versus exploitation that arises often in AI: when is it better to actively seek additional information instead of exploiting the knowledge one already has? (See, e.g., [69].) Most of this work differs significantly from our own in that it considers single-agent planning as opposed to the game-theoretic setting. A notable exception is the work of Larson and Sandholm [41, 42, 43, 44] on mechanism design for interacting agents whose computation is costly and limited. They present a model in which players must solve a computationally intractable valuation problem, using costly computation to learn some hidden parameters, and results for auctions and bargaining games in this model.
Efficiently finding Nash equilibria in Socratic games with non-strategically zero-sum worlds is probably difficult because the existence of such an algorithm for classical games has been shown to be unlikely [10, 11, 13, 16, 17, 27, 54, 55]. There has, however, been some algorithmic success in finding Nash equilibria in restricted classical settings (e.g., [21, 46, 47, 57]); we might hope to extend our results to analogous Socratic games.
An efficient algorithm to find correlated equilibria in general Socratic games seems more attainable. Suppose the players receive recommended queries and responses. The difficulty is that when a player considers a deviation from his recommended query, he already knows his recommended response in each of the Stage-2 games. In a correlated equilibrium, a player"s expected payoff generally depends on his recommended strategy, and thus a player may deviate in Stage 1 so as to land in a Stage-2 game where he has been given a better than average recommended response. (Socratic games are succinct games of superpolynomial type, so Papadimitriou"s results [56] do not imply correlated equilibria for them.) Socratic games can be extended to allow players to make adaptive queries, choosing subsequent queries based on previous results. Our techniques carry over to O(1) rounds of unobservable queries, but it would be interesting to compute equilibria in Socratic games with adaptive observable queries or with ω(1) rounds of unobservable queries.
Special cases of adaptive Socratic games are closely related to single-agent problems like minimum latency [1, 7, 26], determining strategies for using priced information [9, 29, 37], and an online version of minimum test cover [18, 50].
Although there are important technical distinctions between adaptive Socratic games and these problems, approximation techniques from this literature may apply to Socratic games.
The question of approximation raises interesting questions even in non-adaptive Socratic games. An -approximate Nash equilibrium is a strategy profile α so that no player can increase her payoff by an additive by deviating from α.
Finding approximate Nash equilibria in both adaptive and non-adaptive Socratic games is an interesting direction to pursue.
Another natural extension is the model where query results are stochastic. In this paper, we model a query as deterministically partitioning the possible worlds into subsets that the query cannot distinguish. However, one could instead model a query as probabilistically mapping the set of possible worlds into the set of signals. With this modification, our unobservable-query model becomes equivalent to the model of Bergemann and V¨alim¨aki [4, 5], in which the result of a query is a posterior distribution over the worlds. Our techniques allow us to compute equilibria in such a stochastic-query model provided that each query is represented as a table that, for each world/signal pair, lists the probability that the query outputs that signal in that world. It is also interesting to consider settings in which the game"s queries are specified by a compact representation of the relevant probability distributions. (For example, one might consider a setting in which the algorithm has only a sampling oracle for the posterior distributions envisioned by Bergemann and V¨alim¨aki.) Efficiently finding equilibria in such settings remains an open problem.
Another interesting setting for Socratic games is when the set Q of available queries is given by Q = P(Γ)-i.e., each player chooses to make a set q ∈ P(Γ) of queries from a specified groundset Γ of queries. Here we take the query cost to be a linear function, so that δ(q) = P γ∈q δ({γ}).
Natural groundsets include comparison queries (if my opponent is playing strategy aii, would I prefer to play ai or ˆai?), strategy queries (what is my vector of payoffs if I play strategy ai?), and world-identity queries (is the world w ∈ W the real world?). When one can infer a polynomial bound on the number of queries made by a rational player, then our results yield efficient solutions. (For example, we can efficiently solve games in which every groundset element γ ∈ Γ has δ({γ}) = Ω(M − M), where M and M denote the maximum and minimum payoffs to any player in any world.) Conversely, it is NP-hard to compute a Nash equilibrium for such a game when every δ({γ}) ≤ 1/|W|2 , even when the worlds are constant sum and Player II has only a single available strategy. Thus even computing a best response for Player I is hard. (This proof proceeds by reduction from set cover; intuitively, for sufficiently low query costs, Player I must fully identify the actual world through his queries. Selecting a minimum-sized set of these queries is hard.) Computing Player I"s best response can be viewed as maximizing a submodular function, and thus a best response can be (1 − 1/e) ≈ 0.63 approximated greedily [14].
An interesting open question is whether this approximate best-response calculation can be leveraged to find an approximate Nash equilibrium.
Part of this work was done while all authors were at MIT CSAIL. We thank Erik Demaine, Natalia Hernandez Gardiol, Claire Monteleoni, Jason Rennie, Madhu Sudan, and Katherine White for helpful comments and discussions.
[1] Aaron Archer and David P. Williamson. Faster approximation algorithms for the minimum latency problem. In Proceedings of the Symposium on Discrete Algorithms, pages 88-96, 2003. [2] R. J. Aumann. Subjectivity and correlation in randomized strategies. J. Mathematical Economics, 1:67-96, 1974. 157 [3] Robert J. Aumann. Correlated equilibrium as an expression of Bayesian rationality. Econometrica, 55(1):1-18, January 1987. [4] Dick Bergemann and Juuso V¨alim¨aki. Information acquisition and efficient mechanism design.
Econometrica, 70(3):1007-1033, May 2002. [5] Dick Bergemann and Juuso V¨alim¨aki. Information in mechanism design. Technical Report 1532, Cowles Foundation for Research in Economics, 2005. [6] Daniel S. Bernstein, Shlomo Zilberstein, and Neil Immerman. The complexity of decentralized control of Markov Decision Processes. Mathematics of Operations Research, pages 819-840, 2002. [7] Avrim Blum, Prasad Chalasani, Don Coppersmith,
Bill Pulleyblank, Prabhakar Raghavan, and Madhu Sudan. The minimum latency problem. In Proceedings of the Symposium on the Theory of Computing, pages 163-171, 1994. [8] Andrei Z. Broder and Michael Mitzenmacher. Optimal plans for aggregation. In Proceedings of the Principles of Distributed Computing, pages 144-152, 2002. [9] Moses Charikar, Ronald Fagin, Venkatesan Guruswami, Jon Kleinberg, Prabhakar Raghavan, and Amit Sahai. Query strategies for priced information.
J. Computer and System Sciences, 64(4):785-819,
June 2002. [10] Xi Chen and Xiaotie Deng. 3-NASH is PPAD-complete. In Electronic Colloquium on Computational Complexity, 2005. [11] Xi Chen and Xiaotie Deng. Settling the complexity of 2-player Nash-equilibrium. In Electronic Colloquium on Computational Complexity, 2005. [12] Olivier Compte and Philippe Jehiel. Auctions and information acquisition: Sealed-bid or dynamic formats? Technical report, Centre d"Enseignement et de Recherche en Analyse Socio-´economique, 2002. [13] Vincent Conitzer and Tuomas Sandholm. Complexity results about Nash equilibria. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 765-771, 2003. [14] Gerard Cornuejols, Marshall L. Fisher, and George L.
Nemhauser. Location of bank accounts to optimize float: An analytic study of exact and approximate algorithms. Management Science, 23(8), April 1977. [15] Jacques Cr´emer and Fahad Khalil. Gathering information before signing a contract. American Economic Review, 82:566-578, 1992. [16] Constantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou. The complexity of computing a Nash equilbrium. In Electronic Colloquium on Computational Complexity, 2005. [17] Konstantinos Daskalakis and Christos H.
Papadimitriou. Three-player games are hard. In Electronic Colloquium on Computational Complexity,
[18] K. M. J. De Bontridder, B. V. Halld´orsson, M. M.
Halld´orsson, C. A. J. Hurkens, J. K. Lenstra, R. Ravi, and L. Stougie. Approximation algorithms for the test cover problem. Mathematical Programming, 98(1-3):477-491, September 2003. [19] Ap Dijksterhuis, Maarten W. Bos, Loran F. Nordgren, and Rick B. van Baaren. On making the right choice: The deliberation-without-attention effect. Science, 311:1005-1007, 17 February 2006. [20] Rosemary Emery-Montemerlo, Geoff Gordon, Jeff Schneider, and Sebastian Thrun. Approximate solutions for partially observable stochastic games with common payoffs. In Autonomous Agents and Multi-Agent Systems, 2004. [21] Alex Fabrikant, Christos Papadimitriou, and Kunal Talwar. The complexity of pure Nash equilibria. In Proceedings of the Symposium on the Theory of Computing, 2004. [22] Kyna Fong. Multi-stage Information Acquisition in Auction Design. Senior thesis, Harvard College, 2003. [23] Lance Fortnow and Duke Whang. Optimality and domination in repeated games with bounded players.
In Proceedings of the Symposium on the Theory of Computing, pages 741-749, 1994. [24] Yoav Freund, Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, and Robert E. Schapire.
Efficient algorithms for learning to play repeated games against computationally bounded adversaries.
In Proceedings of the Foundations of Computer Science, pages 332-341, 1995. [25] Drew Fudenberg and Jean Tirole. Game Theory. MIT,
[26] Michel X. Goemans and Jon Kleinberg. An improved approximation ratio for the minimum latency problem.
Mathematical Programming, 82:111-124, 1998. [27] Paul W. Goldberg and Christos H. Papadimitriou.
Reducibility among equilibrium problems. In Electronic Colloquium on Computational Complexity,
[28] M. Grotschel, L. Lovasz, and A. Schrijver. The ellipsoid method and its consequences in combinatorial optimization. Combinatorica, 1:70-89, 1981. [29] Anupam Gupta and Amit Kumar. Sorting and selection with structured costs. In Proceedings of the Foundations of Computer Science, pages 416-425,
[30] Eric A. Hansen, Daniel S. Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In National Conference on Artificial Intelligence (AAAI), 2004. [31] John C. Harsanyi. Games with incomplete information played by Bayesian players. Management Science, 14(3,5,7), 1967-1968. [32] Sergiu Hart and David Schmeidler. Existence of correlated equilibria. Mathematics of Operations Research, 14(1):18-25, 1989. [33] Eric Horvitz and Geoffrey Rutledge. Time-dependent utility and action under uncertainty. In Uncertainty in Artificial Intelligence, pages 151-158, 1991. [34] G. E. Hughes and M. J. Cresswell. A New Introduction to Modal Logic. Routledge, 1996. [35] Sheena S. Iyengar and Mark R. Lepper. When choice is demotivating: Can one desire too much of a good thing? J. Personality and Social Psychology, 79(6):995-1006, 2000. [36] Ehud Kalai. Bounded rationality and strategic complexity in repeated games. Game Theory and Applications, pages 131-157, 1990. 158 [37] Sampath Kannan and Sanjeev Khanna. Selection with monotone comparison costs. In Proceedings of the Symposium on Discrete Algorithms, pages 10-17, 2003. [38] L.G. Khachiyan. A polynomial algorithm in linear programming. Dokklady Akademiia Nauk SSSR, 244,
[39] Daphne Koller and Nimrod Megiddo. The complexity of two-person zero-sum games in extensive form.
Games and Economic Behavior, 4:528-552, 1992. [40] Daphne Koller, Nimrod Megiddo, and Bernhard von Stengel. Efficient computation of equilibria for extensive two-person games. Games and Economic Behavior, 14:247-259, 1996. [41] Kate Larson. Mechanism Design for Computationally Limited Agents. PhD thesis, CMU, 2004. [42] Kate Larson and Tuomas Sandholm. Bargaining with limited computation: Deliberation equilibrium.
Artificial Intelligence, 132(2):183-217, 2001. [43] Kate Larson and Tuomas Sandholm. Costly valuation computation in auctions. In Proceedings of the Theoretical Aspects of Rationality and Knowledge,
July 2001. [44] Kate Larson and Tuomas Sandholm. Strategic deliberation and truthful revelation: An impossibility result. In Proceedings of the ACM Conference on Electronic Commerce, May 2004. [45] C. E. Lemke and J. T. Howson, Jr. Equilibrium points of bimatrix games. J. Society for Industrial and Applied Mathematics, 12, 1964. [46] Richard J. Lipton, Evangelos Markakis, and Aranyak Mehta. Playing large games using simple strategies. In Proceedings of the ACM Conference on Electronic Commerce, pages 36-41, 2003. [47] Michael L. Littman, Michael Kearns, and Satinder Singh. An efficient exact algorithm for singly connected graphical games. In Proceedings of Neural Information Processing Systems, 2001. [48] Steven A. Matthews and Nicola Persico. Information acquisition and the excess refund puzzle. Technical Report 05-015, Department of Economics, University of Pennsylvania, March 2005. [49] Richard D. McKelvey and Andrew McLennan.
Computation of equilibria in finite games. In H. Amman, D. A. Kendrick, and J. Rust, editors,
Handbook of Compututational Economics, volume 1, pages 87-142. Elsevier, 1996. [50] B.M.E. Moret and H. D. Shapiro. On minimizing a set of tests. SIAM J. Scientific Statistical Computing, 6:983-1003, 1985. [51] H. Moulin and J.-P. Vial. Strategically zero-sum games: The class of games whose completely mixed equilibria cannot be improved upon. International J.
Game Theory, 7(3/4), 1978. [52] John F. Nash, Jr. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36:48-49, 1950. [53] Abraham Neyman. Finitely repeated games with finite automata. Mathematics of Operations Research, 23(3):513-552, August 1998. [54] Christos Papadimitriou. On the complexity of the parity argument and other inefficient proofs of existence. J. Computer and System Sciences, 48:498-532, 1994. [55] Christos Papadimitriou. Algorithms, games, and the internet. In Proceedings of the Symposium on the Theory of Computing, pages 749-753, 2001. [56] Christos H. Papadimitriou. Computing correlated equilibria in multi-player games. In Proceedings of the Symposium on the Theory of Computing, 2005. [57] Christos H. Papadimitriou and Tim Roughgarden.
Computing equilibria in multiplayer games. In Proceedings of the Symposium on Discrete Algorithms,
[58] Christos H. Papadimitriou and Mihalis Yannakakis.
On bounded rationality and computational complexity. In Proceedings of the Symposium on the Theory of Computing, pages 726-733, 1994. [59] David C. Parkes. Auction design with costly preference elicitation. Annals of Mathematics and Artificial Intelligence, 44:269-302, 2005. [60] Nicola Persico. Information acquisition in auctions.
Econometrica, 68(1):135-148, 2000. [61] Jean-Pierre Ponssard and Sylvain Sorin. The LP formulation of finite zero-sum games with incomplete information. International J. Game Theory, 9(2):99-105, 1980. [62] Eric Rasmussen. Strategic implications of uncertainty over one"s own private value in auctions. Technical report, Indiana University, 2005. [63] Leonardo Rezende. Mid-auction information acquisition. Technical report, University of Illinois,
[64] Ariel Rubinstein. Modeling Bounded Rationality. MIT,
[65] Barry Schwartz. The Paradox of Choice: Why More is Less. Ecco, 2004. [66] Herbert Simon. Models of Bounded Rationality. MIT,
[67] I. Simonson and A. Tversky. Choice in context: Tradeoff contrast and extremeness aversion. J.
Marketing Research, 29:281-295, 1992. [68] Brian Skyrms. Dynamic models of deliberation and the theory of games. In Proceedings of the Theoretical Aspects of Rationality and Knowledge, pages 185-200,
[69] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT, 1998. [70] John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton, 1957. [71] Bernhard von Stengel. Computing equilibria for two-person games. In R. J. Aumann and S. Hart, editors, Handbook of Game Theory with Econonic Applications, volume 3, pages 1723-1759. Elsevier,
[72] S. Zilberstein and S. Russell. Approximate reasoning using anytime algorithms. In S. Natarajan, editor,
Imprecise and Approximate Computation. Kluwer,

In environments with more than one agent, an agent"s outcome is generally affected by the actions of the other agent(s). Consequently, the optimal action of one agent can depend on the others. Game theory provides a normative framework for analyzing such strategic situations. In particular, it provides solution concepts that define what rational behavior is in such settings. The most famous and important solution concept is that of Nash equilibrium [36]. It is a strategy profile (one strategy for each agent) in which no agent has incentive to deviate to a different strategy.
However, for the concept to be operational, we need algorithmic techniques for finding an equilibrium.
Games can be classified as either games of perfect information or imperfect information. Chess and Go are examples of the former, and, until recently, most game playing work has been on games of this type. To compute an optimal strategy in a perfect information game, an agent traverses the game tree and evaluates individual nodes. If the agent is able to traverse the entire game tree, she simply computes an optimal strategy from the bottom-up, using the principle of backward induction.1 In computer science terms, this is done using minimax search (often in conjunction with αβ-pruning to reduce the search tree size and thus enhance speed). Minimax search runs in linear time in the size of the game tree.2 The differentiating feature of games of imperfect information, such as poker, is that they are not fully observable: when it is an agent"s turn to move, she does not have access to all of the information about the world. In such games, the decision of what to do at a point in time cannot generally be optimally made without considering decisions at all other points in time (including ones on other paths of play) because those other decisions affect the probabilities of being at different states at the current point in time. Thus the algorithms for perfect information games do not solve games of imperfect information.
For sequential games with imperfect information, one could try to find an equilibrium using the normal (matrix) form, where every contingency plan of the agent is a pure strategy for the agent.3 Unfortunately (even if equivalent strategies 1 This actually yields a solution that satisfies not only the Nash equilibrium solution concept, but a stronger solution concept called subgame perfect Nash equilibrium [45]. 2 This type of algorithm still does not scale to huge trees (such as in chess or Go), but effective game-playing agents can be developed even then by evaluating intermediate nodes using a heuristic evaluation and then treating those nodes as leaves. 3 An -equilibrium in a normal form game with any 160 are replaced by a single strategy [27]) this representation is generally exponential in the size of the game tree [52].
By observing that one needs to consider only sequences of moves rather than pure strategies [41, 46, 22, 52], one arrives at a more compact representation, the sequence form, which is linear in the size of the game tree.4 For 2-player games, there is a polynomial-sized (in the size of the game tree) linear programming formulation (linear complementarity in the non-zero-sum case) based on the sequence form such that strategies for players 1 and 2 correspond to primal and dual variables. Thus, the equilibria of reasonable-sized 2-player games can be computed using this method [52, 24, 25].5 However, this approach still yields enormous (unsolvable) optimization problems for many real-world games, such as poker.
In this paper, we take a different approach to tackling the difficult problem of equilibrium computation. Instead of developing an equilibrium-finding method per se, we instead develop a methodology for automatically abstracting games in such a way that any equilibrium in the smaller (abstracted) game corresponds directly to an equilibrium in the original game. Thus, by computing an equilibrium in the smaller game (using any available equilibrium-finding algorithm), we are able to construct an equilibrium in the original game. The motivation is that an equilibrium for the smaller game can be computed drastically faster than for the original game.
To this end, we introduce games with ordered signals (Section 2), a broad class of games that has enough structure which we can exploit for abstraction purposes. Instead of operating directly on the game tree (something we found to be technically challenging), we instead introduce the use of information filters (Section 2.1), which coarsen the information each player receives. They are used in our analysis and abstraction algorithm. By operating only in the space of filters, we are able to keep the strategic structure of the game intact, while abstracting out details of the game in a way that is lossless from the perspective of equilibrium finding. We introduce the ordered game isomorphism to describe strategically symmetric situations and the ordered game isomorphic abstraction transformation to take advantange of such symmetries (Section 3). As our main equilibrium result we have the following: constant number of agents can be constructed in quasipolynomial time [31], but finding an exact equilibrium is PPAD-complete even in a 2-player game [8]. The most prevalent algorithm for finding an equilibrium in a 2-agent game is Lemke-Howson [30], but it takes exponentially many steps in the worst case [44]. For a survey of equilibrium computation in 2-player games, see [53]. Recently, equilibriumfinding algorithms that enumerate supports (i.e., sets of pure strategies that are played with positive probability) have been shown efficient on many games [40], and efficient mixed integer programming algorithms that search in the space of supports have been developed [43]. For more than two players, many algorithms have been proposed, but they currently only scale to very small games [19, 34, 40]. 4 There were also early techniques that capitalized in different ways on the fact that in many games the vast majority of pure strategies are not played in equilibrium [54, 23]. 5 Recently this approach was extended to handle computing sequential equilibria [26] as well [35].
Theorem 2 Let Γ be a game with ordered signals, and let F be an information filter for Γ. Let F be an information filter constructed from F by one application of the ordered game isomorphic abstraction transformation, and let σ be a Nash equilibrium strategy profile of the induced game ΓF (i.e., the game Γ using the filter F ). If σ is constructed by using the corresponding strategies of σ , then σ is a Nash equilibrium of ΓF .
The proof of the theorem uses an equivalent characterization of Nash equilibria: σ is a Nash equilibrium if and only if there exist beliefs μ (players" beliefs about unknown information) at all points of the game reachable by σ such that σ is sequentially rational (i.e., a best response) given μ, where μ is updated using Bayes" rule. We can then use the fact that σ is a Nash equilibrium to show that σ is a Nash equilibrium considering only local properties of the game.
We also give an algorithm, GameShrink, for abstracting the game using our isomorphism exhaustively (Section 4).
Its complexity is ˜O(n2 ), where n is the number of nodes in a structure we call the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in the size of the game tree. We present several algorithmic and data structure related speed improvements (Section 4.1), and we demonstrate how a simple modification to our algorithm yields an approximation algorithm (Section 5).
Sequential games of imperfect information are ubiquitous, for example in negotiation and in auctions. Often aspects of a player"s knowledge are not pertinent for deciding what action the player should take at a given point in the game.
On the trivial end, some aspects of a player"s knowledge are never pertinent (e.g., whether it is raining or not has no bearing on the bidding strategy in an art auction), and such aspects can be completely left out of the model specification.
However, some aspects can be pertinent in certain states of the game while they are not pertinent in other states, and thus cannot be left out of the model completely.
Furthermore, it may be highly non-obvious which aspects are pertinent in which states of the game. Our algorithm automatically discovers which aspects are irrelevant in different states, and eliminates those aspects of the game, resulting in a more compact, equivalent game representation.
One broad application area that has this property is sequential negotiation (potentially over multiple issues).
Another broad application area is sequential auctions (potentially over multiple goods). For example, in those states of a 1-object auction where bidder A can infer that his valuation is greater than that of bidder B, bidder A can ignore all his other information about B"s signals, although that information would be relevant for inferring B"s exact valuation.
Furthermore, in some states of the auction, a bidder might not care which exact other bidders have which valuations, but cares about which valuations are held by the other bidders in aggregate (ignoring their identities). Many open-cry sequential auction and negotiation mechanisms fall within the game model studied in this paper (specified in detail later), as do certain other games in electronic commerce, such as sequences of take-it-or-leave-it offers [42].
Our techniques are in no way specific to an application.
The main experiment that we present in this paper is on 161 a recreational game. We chose a particular poker game as the benchmark problem because it yields an extremely complicated and enormous game tree, it is a game of imperfect information, it is fully specified as a game (and the data is available), and it has been posted as a challenge problem by others [47] (to our knowledge no such challenge problem instances have been proposed for electronic commerce applications that require solving sequential games).
Poker is an enormously popular card game played around the world. The 2005 World Series of Poker had over $103 million dollars in total prize money, including $56 million for the main event. Increasingly, poker players compete in online casinos, and television stations regularly broadcast poker tournaments. Poker has been identified as an important research area in AI due to the uncertainty stemming from opponents" cards, opponents" future actions, and chance moves, among other reasons [5].
Almost since the field"s founding, game theory has been used to analyze different aspects of poker [28; 37; 3; 51, pp. 186-219]. However, this work was limited to tiny games that could be solved by hand. More recently, AI researchers have been applying the computational power of modern hardware to computing game theory-based strategies for larger games.
Koller and Pfeffer determined solutions to poker games with up to 140,000 nodes using the sequence form and linear programming [25]. Large-scale approximations have been developed [4], but those methods do not provide any guarantees about the performance of the computed strategies.
Furthermore, the approximations were designed manually by a human expert. Our approach yields an automated abstraction mechanism along with theoretical guarantees on the strategies" performance.
Rhode Island Hold"em was invented as a testbed for computational game playing [47]. It was designed so that it was similar in style to Texas Hold"em, yet not so large that devising reasonably intelligent strategies would be impossible. (The rules of Rhode Island Hold"em, as well as a discussion of how Rhode Island Hold"em can be modeled as a game with ordered signals, that is, it fits in our model, is available in an extended version of this paper [13].) We applied the techniques developed in this paper to find an exact (minimax) solution to Rhode Island Hold"em, which has a game tree exceeding 3.1 billion nodes.
Applying the sequence form to Rhode Island Hold"em directly without abstraction yields a linear program with 91,224,226 rows, and the same number of columns. This is much too large for (current) linear programming algorithms to handle. We used our GameShrink algorithm to reduce this with lossless abstraction, and it yielded a linear program with 1,237,238 rows and columns-with 50,428,638 non-zero coefficients. We then applied iterated elimination of dominated strategies, which further reduced this to 1,190,443 rows and 1,181,084 columns. (Applying iterated elimination of dominated strategies without GameShrink yielded 89,471,986 rows and 89,121,538 columns, which still would have been prohibitively large to solve.) GameShrink required less than one second to perform the shrinking (i.e., to compute all of the ordered game isomorphic abstraction transformations). Using a 1.65GHz IBM eServer p5 570 with
needed 25 gigabytes), we solved it in 7 days and 17 hours using the interior-point barrier method of CPLEX version
Hold"em poker player at the AAAI-05 conference [14], and it is available for play on-line at http://www.cs.cmu.edu/ ~gilpin/gsi.html.
While others have worked on computer programs for playing Rhode Island Hold"em [47], no optimal strategy has been found before. This is the largest poker game solved to date by over four orders of magnitude.
We work with a slightly restricted class of games, as compared to the full generality of the extensive form. This class, which we call games with ordered signals, is highly structured, but still general enough to capture a wide range of strategic situations. A game with ordered signals consists of a finite number of rounds. Within a round, the players play a game on a directed tree (the tree can be different in different rounds). The only uncertainty players face stems from private signals the other players have received and from the unknown future signals. In other words, players observe each others" actions, but potentially not nature"s actions.
In each round, there can be public signals (announced to all players) and private signals (confidentially communicated to individual players). For simplicity, we assume-as is the case in most recreational games-that within each round, the number of private signals received is the same across players (this could quite likely be relaxed). We also assume that the legal actions that a player has are independent of the signals received. For example, in poker, the legal betting actions are independent of the cards received. Finally, the strongest assumption is that there is a partial ordering over sets of signals, and the payoffs are increasing (not necessarily strictly) in these signals. For example, in poker, this partial ordering corresponds exactly to the ranking of card hands.
Definition 1. A game with ordered signals is a tuple Γ = I, G, L, Θ, κ, γ, p, , ω, u where:
, . . . , Gr , Gj = ` V j , Ej ´ , is a finite collection of finite directed trees with nodes V j and edges Ej . Let Zj denote the leaf nodes of Gj and let Nj (v) denote the outgoing neighbors of v ∈ V j . Gj is the stage game for round j.
, . . . , Lr , Lj : V j \ Zj → I indicates which player acts (chooses an outgoing edge) at each internal node in round j.
, . . . , κr and γ = γ1 , . . . , γr are vectors of nonnegative integers, where κj and γj denote the number of public and private signals (per player), respectively, revealed in round j. Each signal θ ∈ Θ may only be revealed once, and in each round every player receives the same number of private signals, so we require Pr j=1 κj + nγj ≤ |Θ|. The public information revealed in round j is αj ∈ Θκj and the public information revealed in all rounds up through round j is ˜αj = ` α1 , . . . , αj ´ . The private information revealed to player i ∈ I in round j is βj i ∈ Θγj and the private information revaled to player i ∈ I in all rounds up through round j is ˜βj i = ` β1 i , . . . , βj i ´ . We 162 also write ˜βj =  ˜βj 1, . . . , ˜βj n  to represent all private information up through round j, and  ˜β j i , ˜βj −i  =  ˜βj 1, . . . , ˜βj i−1, ˜β j i , ˜βj i+1, . . . , ˜βj n  is ˜βj with ˜βj i replaced with ˜β j i . The total information revealed up through round j,  ˜αj , ˜βj  , is said to be legal if no signals are repeated.
for all θ ∈ Θ. Signals are drawn from Θ according to p without replacement, so if X is the set of signals already revealed, then p(x | X) = ( p(x)P y /∈X p(y) if x /∈ X
for at least those pairs required by u.
rS j=1 Zj → {over, continue} is a mapping of terminal nodes within a stage game to one of two values: over, in which case the game ends, or continue, in which case the game continues to the next round.
Clearly, we require ω(z) = over for all z ∈ Zr . Note that ω is independent of the signals. Let ωj over = {z ∈ Zj | ω(z) = over} and ωj cont = {z ∈ Zj | ω(z) = continue}.
, . . . , ur ), uj : j−1 k=1 ωk cont × ωj over × j k=1 Θκk × n i=1 j k=1 Θγk → Rn is a utility function such that for every j, 1 ≤ j ≤ r, for every i ∈ I, and for every ˜z ∈ j−1 k=1 ωk cont × ωj over, at least one of the following two conditions holds: (a) Utility is signal independent: uj i (˜z, ϑ) = uj i (˜z, ϑ ) for all legal ϑ, ϑ ∈ j k=1 Θκk × n i=1 j k=1 Θγk . (b) is defined for all legal signals (˜αj , ˜βj i ), (˜αj , ˜β j i ) through round j and a player"s utility is increasing in her private signals, everything else equal:  ˜αj , ˜βj i  ˜αj , ˜β j i  =⇒ ui  ˜z, ˜αj ,  ˜βj i , ˜βj −i  ≥ ui  ˜z, ˜αj ,  ˜β j i , ˜βj −i  .
We will use the term game with ordered signals and the term ordered game interchangeably.
In this subsection, we define an information filter for ordered games. Instead of completely revealing a signal (either public or private) to a player, the signal first passes through this filter, which outputs a coarsened signal to the player. By varying the filter applied to a game, we are able to obtain a wide variety of games while keeping the underlying action space of the game intact. We will use this when designing our abstraction techniques. Formally, an information filter is as follows.
Definition 2. Let Γ = I, G, L, Θ, κ, γ, p, , ω, u be an ordered game. Let Sj ⊆ j k=1 Θκk × j k=1 Θγk be the set of legal signals (i.e., no repeated signals) for one player through round j. An information filter for Γ is a collection F = F1 , . . . , Fr where each Fj is a function Fj : Sj → 2Sj such that each of the following conditions hold:
, ˜βj i ) ∈ Fj (˜αj , ˜βj i ) for all legal (˜αj , ˜βj i ).
is a partition of Sj .
distinguishable in round k, then they are distinguishable fpr each round j > k. Let mj = Pj l=1 κl +γl . We require that for all legal (θ1, . . . , θmk , . . . , θmj ) ⊆ Θ and (θ1, . . . , θmk , . . . , θmj ) ⊆ Θ: (θ1, . . . , θmk ) /∈ Fk (θ1, . . . , θmk ) =⇒ (θ1, . . . , θmk , . . . , θmj ) /∈ Fj (θ1, . . . , θmk , . . . , θmj ).
A game with ordered signals Γ and an information filter F for Γ defines a new game ΓF . We refer to such games as filtered ordered games. We are left with the original game if we use the identity filter Fj  ˜αj , ˜βj i  = n ˜αj , ˜βj i o . We have the following simple (but important) result: Proposition 1. A filtered ordered game is an extensive form game satisfying perfect recall.
A simple proof proceeds by constructing an extensive form game directly from the ordered game, and showing that it satisfies perfect recall. In determining the payoffs in a game with filtered signals, we take the average over all real signals in the filtered class, weighted by the probability of each real signal occurring.
We are now ready to define behavior strategies in the context of filtered ordered games.
Definition 3. A behavior strategy for player i in round j of Γ = I, G, L, Θ, κ, γ, p, , ω, u with information filter F is a probability distribution over possible actions, and is defined for each player i, each round j, and each v ∈ V j \Zj for Lj (v) = i: σj i,v : j−1 k=1 ωk cont×Range  Fj  → Δ n w ∈ V j | (v, w) ∈ Ej o . (Δ(X) is the set of probability distributions over a finite set X.) A behavior strategy for player i in round j is σj i = (σj i,v1 , . . . , σj i,vm ) for each vk ∈ V j \ Zj where Lj (vk) = i.
A behavior strategy for player i in Γ is σi = ` σ1 i , . . . , σr i ´ .
A strategy profile is σ = (σ1, . . . , σn). A strategy profile with σi replaced by σi is (σi, σ−i) = (σ1, . . . , σi−1, σi, σi+1, . . . , σn).
By an abuse of notation, we will say player i receives an expected payoff of ui(σ) when all players are playing the strategy profile σ. Strategy σi is said to be player i"s best response to σ−i if for all other strategies σi for player i we have ui(σi, σ−i) ≥ ui(σi, σ−i). σ is a Nash equilibrium if, for every player i, σi is a best response for σ−i. A Nash equilibrium always exists in finite extensive form games [36], and one exists in behavior strategies for games with perfect recall [29]. Using these observations, we have the following corollary to Proposition 1: 163 Corollary 1. For any filtered ordered game, a Nash equilibrium exists in behavior strateges.
ABSTRACTIONS In this section, we present our main technique for reducing the size of games. We begin by defining a filtered signal tree which represents all of the chance moves in the game. The bold edges (i.e. the first two levels of the tree) in the game trees in Figure 1 correspond to the filtered signal trees in each game.
Definition 4. Associated with every ordered game Γ = I, G, L, Θ, κ, γ, p, , ω, u and information filter F is a filtered signal tree, a directed tree in which each node corresponds to some revealed (filtered) signals and edges correspond to revealing specific (filtered) signals. The nodes in the filtered signal tree represent the set of all possible revealed filtered signals (public and private) at some point in time. The filtered public signals revealed in round j correspond to the nodes in the κj levels beginning at level Pj−1 k=1 ` κk + nγk ´ and the private signals revealed in round j correspond to the nodes in the nγj levels beginning at level Pj k=1 κk + Pj−1 k=1 nγk . We denote children of a node x as N(x). In addition, we associate weights with the edges corresponding to the probability of the particular edge being chosen given that its parent was reached.
In many games, there are certain situations in the game that can be thought of as being strategically equivalent to other situations in the game. By melding these situations together, it is possible to arrive at a strategically equivalent smaller game. The next two definitions formalize this notion via the introduction of the ordered game isomorphic relation and the ordered game isomorphic abstraction transformation.
Definition 5. Two subtrees beginning at internal nodes x and y of a filtered signal tree are ordered game isomorphic if x and y have the same parent and there is a bijection f : N(x) → N(y), such that for w ∈ N(x) and v ∈ N(y), v = f(w) implies the weights on the edges (x, w) and (y, v) are the same and the subtrees beginning at w and v are ordered game isomorphic. Two leaves (corresponding to filtered signals ϑ and ϑ up through round r) are ordered game isomorphic if for all ˜z ∈ r−1 j=1 ωj cont × ωr over, ur (˜z, ϑ) = ur (˜z, ϑ ).
Definition 6. Let Γ = I, G, L, Θ, κ, γ, p, , ω, u be an ordered game and let F be an information filter for Γ. Let ϑ and ϑ be two nodes where the subtrees in the induced filtered signal tree corresponding to the nodes ϑ and ϑ are ordered game isomorphic, and ϑ and ϑ are at either levelPj−1 k=1 ` κk + nγk ´ or Pj k=1 κk + Pj−1 k=1 nγk for some round j. The ordered game isomorphic abstraction transformation is given by creating a new information filter F : F j  ˜αj , ˜βj i  = 8 < : Fj  ˜αj , ˜βj i  if  ˜αj , ˜βj i  /∈ ϑ ∪ ϑ ϑ ∪ ϑ if  ˜αj , ˜βj i  ∈ ϑ ∪ ϑ .
Figure 1 shows the ordered game isomorphic abstraction transformation applied twice to a tiny poker game.
Theorem 2, our main equilibrium result, shows how the ordered game isomorphic abstraction transformation can be used to compute equilibria faster.
Theorem 2. Let Γ = I, G, L, Θ, κ, γ, p, , ω, u be an ordered game and F be an information filter for Γ. Let F be an information filter constructed from F by one application of the ordered game isomorphic abstraction transformation.
Let σ be a Nash equilibrium of the induced game ΓF . If we take σj i,v  ˜z, Fj  ˜αj , ˜βj i  = σ j i,v  ˜z, F j  ˜αj , ˜βj i  , σ is a Nash equilibrium of ΓF .
Proof. For an extensive form game, a belief system μ assigns a probability to every decision node x such thatP x∈h μ(x) = 1 for every information set h. A strategy profile σ is sequentially rational at h given belief system μ if ui(σi, σ−i | h, μ) ≥ ui(τi, σ−i | h, μ) for all other strategies τi, where i is the player who controls h. A basic result [33, Proposition 9.C.1] characterizing Nash equilibria dictates that σ is a Nash equilibrium if and only if there is a belief system μ such that for every information set h with Pr(h | σ) > 0, the following two conditions hold: (C1) σ is sequentially rational at h given μ; and (C2) μ(x) = Pr(x | σ) Pr(h | σ) for all x ∈ h. Since σ is a Nash equilibrium of Γ , there exists such a belief system μ for ΓF . Using μ , we will construct a belief system μ for Γ and show that conditions C1 and C2 hold, thus supporting σ as a Nash equilibrium.
Fix some player i ∈ I. Each of i"s information sets in some round j corresponds to filtered signals Fj  ˜α∗j , ˜β∗j i  , history in the first j − 1 rounds (z1, . . . , zj−1) ∈ j−1 k=1 ωk cont, and history so far in round j, v ∈ V j \ Zj . Let ˜z = (z1, . . . , zj−1, v) represent all of the player actions leading to this information set. Thus, we can uniquely specify this information set using the information  Fj  ˜α∗j , ˜β∗j i  , ˜z  .
Each node in an information set corresponds to the possible private signals the other players have received. Denote by ˜β some legal (Fj (˜αj , ˜βj 1), . . . , Fj (˜αj , ˜βj i−1), Fj (˜αj , ˜βj i+1), . . . , Fj (˜αj , ˜βj n)).
In other words, there exists (˜αj , ˜βj 1, . . . , ˜βj n) such that (˜αj , ˜βj i ) ∈ Fj (˜α∗j , ˜β∗j i ), (˜αj , ˜βj k) ∈ Fj (˜αj , ˜βj k) for k = i, and no signals are repeated. Using such a set of signals (˜αj , ˜βj 1, . . . , ˜βj n), let ˆβ denote (F j (˜αj , ˜βj 1), . . . ,
F j (˜αj , ˜βj i−1), F j (˜αj , ˜βj i+1), . . . , F j (˜αj , ˜βj n). (We will abuse notation and write F j −i  ˆβ  = ˆβ .) We can now compute μ directly from μ : μ  ˆβ | Fj  ˜αj , ˜βj i  , ˜z  = 8 >>>>>>< >>>>>>: μ  ˆβ | F j  ˜αj , ˜βj i  , ˜z  if Fj  ˜αj , ˜βj i  = F j  ˜αj , ˜βj i  or ˆβ = ˆβ p∗ μ  ˆβ | F j  ˜αj , ˜βj i  , ˜z  if Fj  ˜αj , ˜βj i  = F j  ˜αj , ˜βj i  and ˆβ = ˆβ 164 J1 J2 J2 K1 K1 K2 K2 c b C B F B f b c b C B F B f b c b C f b B BF c b C f b B BF c b C B F B f b c b C BF B f b c b C f b B BF c b C f b B BF c b C f b B BF c b C f b B BF c b C B F B f b c b C B F B f b
0-1 -1 -1 -1 -1 -1 -1 -1-1 -1 -1 -1 -1 -1 -1 -1 -1 -1-1 -1 -1 -1 -1 -10 0 0
0
0 -1 -2 -2 -1 -2 -2 -1 -2 -2 -1 -2 -2 1 2
2
2
2 2 J1 K1 K2 J1 J2 K2 J1 J2 K1 1 1 1
1 2 2 2 2 2 2 2 2 {{J1}, {J2}, {K1}, {K2}} {{J1,J2}, {K1}, {K2}} c b C BF B f b c b C f b B BF c b C B F B f b J1,J2 K1 K2 1 1 c b C f b B BF c b C BF B f b c b C BF B f b c b C B F B f b J1,J2 K1 K2 1 1 1 1 J1,J2 K2 J1,J2 K1
0-1 -1 -1 -1 -1 -1 -1 -2 -2 -1 -2 -2 2 2 2 2 2 2 -1 -1-1 -1
0 1 2 2 -1 -1-1 -1
0 1 2 2 c b C B F B f b -1 -10 0 0 c b B F B f b -1 -1-1 -2 -2 c b C BF B f b
0-1 -1 c b C BF B f b J1,J2 J1,J2 J1,J2K1,K2 K1,K2 K1,K2 -1 -1 1 2 2 2 2 2 2 {{J1,J2}, {K1,K2}} 1
1 1/4 1/4 1/4 1/4 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/3 1/4 1/41/2 1/3 1/3 1/3 1/32/3 1/32/3 1/2 1/2 1/3 2/3 2/3 1/3 Figure 1: GameShrink applied to a tiny two-person four-card (two Jacks and two Kings) poker game. Next to each game tree is the range of the information filter F. Dotted lines denote information sets, which are labeled by the controlling player. Open circles are chance nodes with the indicated transition probabilities.
The root node is the chance node for player 1"s card, and the next level is for player 2"s card. The payment from player 2 to player 1 is given below each leaf. In this example, the algorithm reduces the game tree from
where p∗ = Pr(ˆβ | F j (˜αj , ˜β j i )) Pr(ˆβ | F j (˜αj , ˜β j i )) . The following three claims show that μ as calculated above supports σ as a Nash equilibrium.
Claim 1. μ is a valid belief system for ΓF .
Claim 2. For all information sets h with Pr(h | σ) > 0, μ(x) = Pr(x | σ) Pr(h | σ) for all x ∈ h.
Claim 3. For all information sets h with Pr(h | σ) > 0, σ is sequentially rational at h given μ.
The proofs of Claims 1-3 are in an extended version of this paper [13]. By Claims 1 and 2, we know that condition C2 holds. By Claim 3, we know that condition C1 holds. Thus, σ is a Nash equilibrium.
model Our model does not capture general sequential games of imperfect information because it is restricted in two ways (as discussed above): 1) there is a special structure connecting the player actions and the chance actions (for one, the players are assumed to observe each others" actions, but nature"s actions might not be publicly observable), and 2) there is a common ordering of signals. In this subsection we show that removing either of these conditions can make our technique invalid.
First, we demonstrate a failure when removing the first assumption. Consider the game in Figure 2.6 Nodes a and b are in the same information set, have the same parent (chance) node, have isomorphic subtrees with the same payoffs, and nodes c and d also have similar structural properties. By merging the subtrees beginning at a and b, we get the game on the right in Figure 2. In this game, player 1"s only Nash equilibrium strategy is to play left. But in the original game, player 1 knows that node c will never be reached, and so should play right in that information set. 1/4 1/4 1/4 1/4
1 1
-10 10 1/2 1/4 1/4
1
a b
c d Figure 2: Example illustrating difficulty in developing a theory of equilibrium-preserving abstractions for general extensive form games.
Removing the second assumption (that the utility functions are based on a common ordering of signals) can also cause failure. Consider a simple three-card game with a deck containing two Jacks (J1 and J2) and a King (K), where player 1"s utility function is based on the ordering 6 We thank Albert Xin Jiang for providing this example. 165 K J1 ∼ J2 but player 2"s utility function is based on the ordering J2 K J1. It is easy to check that in the abstracted game (where Player 1 treats J1 and J2 as being equivalent) the Nash equilibrium does not correspond to a Nash equilibrium in the original game.7
ALGORITHM FOR COMPUTING ORDERED GAME ISOMORPHIC ABSTRACTION TRANSFORMATIONS This section presents an algorithm, GameShrink, for conducting the abstractions. It only needs to analyze the signal tree discussed above, rather than the entire game tree.
We first present a subroutine that GameShrink uses. It is a dynamic program for computing the ordered game isomorphic relation. Again, it operates on the signal tree.
Algorithm 1. OrderedGameIsomorphic? (Γ, ϑ, ϑ )
(a) If ur (ϑ | ˜z) = ur (ϑ | ˜z) for all ˜z ∈ r−1 j=1 ωj cont × ωr over, then return true. (b) Otherwise, return false.
N(ϑ) and V2 = N(ϑ ).
If OrderedGameIsomorphic? (Γ, v1, v2) Create edge (v1, v2)
return false.
By evaluating this dynamic program from bottom to top,
Algorithm 1 determines, in time polynomial in the size of the signal tree, whether or not any pair of equal depth nodes x and y are ordered game isomorphic. We can further speed up this computation by only examining nodes with the same parent, since we know (from step 1) that no nodes with different parents are ordered game isomorphic. The test in step 2(a) can be computed in O(1) time by consulting the relation from the specification of the game. Each call to OrderedGameIsomorphic? performs at most one perfect matching computation on a bipartite graph with O(|Θ|) nodes and O(|Θ|2 ) edges (recall that Θ is the set of signals). Using the Ford-Fulkerson algorithm [12] for finding a maximal matching, this takes O(|Θ|3 ) time. Let S be the maximum number of signals possibly revealed in the game (e.g., in Rhode Island Hold"em, S = 4 because each of the two players has one card in the hand plus there are two cards on the table). The number of nodes, n, in the signal tree is O(|Θ|S ). The dynamic program visits each node in the signal tree, with each visit requiring O(|Θ|2 ) calls to the OrderedGameIsomorphic? routine. So, it takes O(|Θ|S |Θ|3 |Θ|2 ) = O(|Θ|S+5 ) time to compute the entire ordered game isomorphic relation.
While this is exponential in the number of revealed signals, we now show that it is polynomial in the size of the signal tree-and thus polynomial in the size of the game tree 7 We thank an anonymous person for this example. because the signal tree is smaller than the game tree. The number of nodes in the signal tree is n = 1 + SX i=1 iY j=1 (|Θ| − j + 1) (Each term in the summation corresponds to the number of nodes at a specific depth of the tree.) The number of leaves is SY j=1 (|Θ| − j + 1) = |Θ| S ! S! which is a lower bound on the number of nodes. For large |Θ| we can use the relation `n k ´ ∼ nk k! to get |Θ| S ! S! ∼ „ |Θ|S S! « S! = |Θ|S and thus the number of leaves in the signal tree is Ω(|Θ|S ).
Thus, O(|Θ|S+5 ) = O(n|Θ|5 ), which proves that we can indeed compute the ordered game isomorphic relation in time polynomial in the number of nodes, n, of the signal tree.
The algorithm often runs in sublinear time (and space) in the size of the game tree because the signal tree is significantly smaller than the game tree in most nontrivial games. (Note that the input to the algorithm is not an explicit game tree, but a specification of the rules, so the algorithm does not need to read in the game tree.) See Figure 1. In general, if an ordered game has r rounds, and each round"s stage game has at least b nonterminal leaves, then the size of the signal tree is at most 1 br of the size of the game tree. For example, in Rhode Island Hold"em, the game tree has 3.1 billion nodes while the signal tree only has 6,632,705.
Given the OrderedGameIsomorphic? routine for determining ordered game isomorphisms in an ordered game, we are ready to present the main algorithm, GameShrink.
Algorithm 2. GameShrink (Γ)
For each pair of sibling nodes ϑ, ϑ at either levelPj−1 k=1 ` κk + nγk ´ or Pj k=1 κk + Pj−1 k=1 nγk in the filtered (according to F) signal tree: If OrderedGameIsomorphic?(Γ, ϑ, ϑ ), then Fj (ϑ) ← Fj (ϑ ) ← Fj (ϑ) ∪ Fj (ϑ ).
Given as input an ordered game Γ, GameShrink applies the shrinking ideas presented above as aggressively as possible. Once it finishes, there are no contractible nodes (since it compares every pair of nodes at each level of the signal tree), and it outputs the corresponding information filter F. The correctness of GameShrink follows by a repeated application of Theorem 2. Thus, we have the following result: Theorem 3. GameShrink finds all ordered game isomorphisms and applies the associated ordered game isomorphic abstraction transformations. Furthermore, for any Nash equilibrium, σ , of the abstracted game, the strategy profile constructed for the original game from σ is a Nash equilibrium.
The dominating factor in the run time of GameShrink is in the rth iteration of the main for-loop. There are at most 166 `|Θ| S ´ S! nodes at this level, where we again take S to be the maximum number of signals possibly revealed in the game.
Thus, the inner for-loop executes O „`|Θ| S ´ S! 2 « times. As discussed in the next subsection, we use a union-find data structure to represent the information filter F. Each iteration of the inner for-loop possibly performs a union operation on the data structure; performing M operations on a union-find data structure containing N elements takes O(α(M, N)) amortized time per operation, where α(M, N) is the inverse Ackermann"s function [1, 49] (which grows extremely slowly). Thus, the total time for GameShrink is O „`|Θ| S ´ S! 2 α „`|Θ| S ´ S! 2 , |Θ|S «« . By the inequality `n k ´ ≤ nk k! , this is O ` (|Θ|S )2 α ` (|Θ|S )2 , |Θ|S ´´ . Again, although this is exponential in S, it is ˜O(n2 ), where n is the number of nodes in the signal tree. Furthermore,
GameShrink tends to actually run in sublinear time and space in the size of the game tree because the signal tree is significantly smaller than the game tree in most nontrivial games, as discussed above.
We designed several speed enhancement techniques for GameShrink, and all of them are incorporated into our implementation. One technique is the use of the union-find data structure for storing the information filter F. This data structure uses time almost linear in the number of operations [49]. Initially each node in the signalling tree is its own set (this corresponds to the identity information filter); when two nodes are contracted they are joined into a new set. Upon termination, the filtered signals for the abstracted game correspond exactly to the disjoint sets in the data structure. This is an efficient method of recording contractions within the game tree, and the memory requirements are only linear in the size of the signal tree.
Determining whether two nodes are ordered game isomorphic requires us to determine if a bipartite graph has a perfect matching. We can eliminate some of these computations by using easy-to-check necessary conditions for the ordered game isomorphic relation to hold. One such condition is to check that the nodes have the same number of chances as being ranked (according to ) higher than, lower than, and the same as the opponents. We can precompute these frequencies for every game tree node. This substantially speeds up GameShrink, and we can leverage this database across multiple runs of the algorithm (for example, when trying different abstraction levels; see next section). The indices for this database depend on the private and public signals, but not the order in which they were revealed, and thus two nodes may have the same corresponding database entry. This makes the database significantly more compact. (For example in Texas Hold"em, the database is reduced by a factor `50 3 ´`47 1 ´`46 1 ´ / `50 5 ´ = 20.) We store the histograms in a 2-dimensional database. The first dimension is indexed by the private signals, the second by the public signals. The problem of computing the index in (either) one of the dimensions is exactly the problem of computing a bijection between all subsets of size r from a set of size n and integers in ˆ 0, . . . , `n r ´ − 1 ˜ . We efficiently compute this using the subsets" colexicographical ordering [6]. Let {c1, . . . , cr}, ci ∈ {0, . . . , n − 1}, denote the r signals and assume that ci < ci+1. We compute a unique index for this set of signals as follows: index(c1, . . . , cr) = Pr i=1 `ci i ´ .
Some games are too large to compute an exact equilibrium, even after using the presented abstraction technique.
This section discusses general techniques for computing approximately optimal strategy profiles. For a two-player game, we can always evaluate the worst-case performance of a strategy, thus providing some objective evaluation of the strength of the strategy. To illustrate this, suppose we know player 2"s planned strategy for some game. We can then fix the probabilities of player 2"s actions in the game tree as if they were chance moves. Then player 1 is faced with a single-agent decision problem, which can be solved bottomup, maximizing expected payoff at every node. Thus, we can objectively determine the expected worst-case performance of player 2"s strategy. This will be most useful when we want to evaluate how well a given strategy performs when we know that it is not an equilibrium strategy. (A variation of this technique may also be applied in n-person games where only one player"s strategies are held fixed.) This technique provides ex post guarantees about the worst-case performance of a strategy, and can be used independently of the method that is used to compute the strategies.
By slightly modifying GameShrink, we can obtain an algorithm that yields even smaller game trees, at the expense of losing the equilibrium guarantees of Theorem 2. Instead of requiring the payoffs at terminal nodes to match exactly, we can instead compute a penalty that increases as the difference in utility between two nodes increases.
There are many ways in which the penalty function could be defined and implemented. One possibility is to create edge weights in the bipartite graphs used in Algorithm 1, and then instead of requiring perfect matchings in the unweighted graph we would instead require perfect matchings with low cost (i.e., only consider two nodes to be ordered game isomorphic if the corresponding bipartite graph has a perfect matching with cost below some threshold). Thus, with this threshold as a parameter, we have a knob to turn that in one extreme (threshold = 0) yields an optimal abstraction and in the other extreme (threshold = ∞) yields a highly abstracted game (this would in effect restrict players to ignoring all signals, but still observing actions). This knob also begets an anytime algorithm. One can solve increasingly less abstracted versions of the game, and evaluate the quality of the solution at every iteration using the ex post method discussed above.
In the case of two-player zero-sum games, the equilibrium computation can be modeled as a linear program (LP), which can in turn be solved using the simplex method. This approach has inherent features which we can leverage into desirable properties in the context of solving games.
In the LP, primal solutions correspond to strategies of player 2, and dual solutions correspond to strategies of player
simplex and the dual simplex. The primal simplex maintains primal feasibility and proceeds by finding better and better primal solutions until the dual solution vector is feasible, 167 at which point optimality has been reached. Analogously, the dual simplex maintains dual feasibility and proceeds by finding increasingly better dual solutions until the primal solution vector is feasible. (The dual simplex method can be thought of as running the primal simplex method on the dual problem.) Thus, the primal and dual simplex methods serve as anytime algorithms (for a given abstraction) for players 2 and 1, respectively. At any point in time, they can output the best strategies found so far.
Also, for any feasible solution to the LP, we can get bounds on the quality of the strategies by examining the primal and dual solutions. (When using the primal simplex method, dual solutions may be read off of the LP tableau.) Every feasible solution of the dual yields an upper bound on the optimal value of the primal, and vice versa [9, p. 57]. Thus, without requiring further computation, we get lower bounds on the expected utility of each agent"s strategy against that agent"s worst-case opponent.
One problem with the simplex method is that it is not a primal-dual algorithm, that is, it does not maintain both primal and dual feasibility throughout its execution. (In fact, it only obtains primal and dual feasibility at the very end of execution.) In contrast, there are interior-point methods for linear programming that maintain primal and dual feasibility throughout the execution. For example, many interiorpoint path-following algorithms have this property [55, Ch. 5]. We observe that running such a linear programming method yields a method for finding -equilibria (i.e., strategy profiles in which no agent can increase her expected utility by more than by deviating). A threshold on can also be used as a termination criterion for using the method as an anytime algorithm. Furthermore, interior-point methods in this class have polynomial-time worst-case run time, as opposed to the simplex algorithm, which takes exponentially many steps in the worst case.
Functions that transform extensive form games have been introduced [50, 11]. In contrast to our work, those approaches were not for making the game smaller and easier to solve. The main result is that a game can be derived from another by a sequence of those transformations if and only if the games have the same pure reduced normal form.
The pure reduced normal form is the extensive form game represented as a game in normal form where duplicates of pure strategies (i.e., ones with identical payoffs) are removed and players essentially select equivalence classes of strategies [27]. An extension to that work shows a similar result, but for slightly different transformations and mixed reduced normal form games [21]. Modern treatments of this prior work on game transformations exist [38, Ch. 6], [10].
The recent notion of weak isomorphism in extensive form games [7] is related to our notion of restricted game isomorphism. The motivation of that work was to justify solution concepts by arguing that they are invariant with respect to isomorphic transformations. Indeed, the author shows, among other things, that many solution concepts, including Nash, perfect, subgame perfect, and sequential equilibrium, are invariant with respect to weak isomorphisms. However, that definition requires that the games to be tested for weak isomorphism are of the same size. Our focus is totally different: we find strategically equivalent smaller games. Also, their paper does not provide algorithms.
Abstraction techniques have been used in artificial intelligence research before. In contrast to our work, most (but not all) research involving abstraction has been for singleagent problems (e.g. [20, 32]). Furthermore, the use of abstraction typically leads to sub-optimal solutions, unlike the techniques presented in this paper, which yield optimal solutions. A notable exception is the use of abstraction to compute optimal strategies for the game of Sprouts [2].
However, a significant difference to our work is that Sprouts is a game of perfect information.
One of the first pieces of research to use abstraction in multi-agent settings was the development of partition search, which is the algorithm behind GIB, the world"s first expertlevel computer bridge player [17, 18]. In contrast to other game tree search algorithms which store a particular game position at each node of the search tree, partition search stores groups of positions that are similar. (Typically, the similarity of two game positions is computed by ignoring the less important components of each game position and then checking whether the abstracted positions are similar-in some domain-specific expert-defined sense-to each other.) Partition search can lead to substantial speed improvements over α-β-search. However, it is not game theory-based (it does not consider information sets in the game tree), and thus does not solve for the equilibrium of a game of imperfect information, such as poker.8 Another difference is that the abstraction is defined by an expert human while our abstractions are determined automatically.
There has been some research on the use of abstraction for imperfect information games. Most notably, Billings et al [4] describe a manually constructed abstraction for Texas Hold"em poker, and include promising results against expert players. However, this approach has significant drawbacks.
First, it is highly specialized for Texas Hold"em. Second, a large amount of expert knowledge and effort was used in constructing the abstraction. Third, the abstraction does not preserve equilibrium: even if applied to a smaller game, it might not yield a game-theoretic equilibrium. Promising ideas for abstraction in the context of general extensive form games have been described in an extended abstract [39], but to our knowledge, have not been fully developed.
We introduced the ordered game isomorphic abstraction transformation and gave an algorithm, GameShrink, for abstracting the game using the isomorphism exhaustively. We proved that in games with ordered signals, any Nash equilibrium in the smaller abstracted game maps directly to a Nash equilibrium in the original game.
The complexity of GameShrink is ˜O(n2 ), where n is the number of nodes in the signal tree. It is no larger than the game tree, and on nontrivial games it is drastically smaller, so GameShrink has time and space complexity sublinear in 8 Bridge is also a game of imperfect information, and partition search does not find the equilibrium for that game either. Instead, partition search is used in conjunction with statistical sampling to simulate the uncertainty in bridge. There are also other bridge programs that use search techniques for perfect information games in conjunction with statistical sampling and expert-defined abstraction [48]. Such (non-game-theoretic) techniques are unlikely to be competitive in poker because of the greater importance of information hiding and bluffing. 168 the size of the game tree. Using GameShrink, we found a minimax equilibrium to Rhode Island Hold"em, a poker game with 3.1 billion nodes in the game tree-over four orders of magnitude more than in the largest poker game solved previously.
To further improve scalability, we introduced an approximation variant of GameShrink, which can be used as an anytime algorithm by varying a parameter that controls the coarseness of abstraction. We also discussed how (in a two-player zero-sum game), linear programming can be used in an anytime manner to generate approximately optimal strategies of increasing quality. The method also yields bounds on the suboptimality of the resulting strategies. We are currently working on using these techniques for full-scale 2-player limit Texas Hold"em poker, a highly popular card game whose game tree has about 1018 nodes. That game tree size has required us to use the approximation version of GameShrink (as well as round-based abstraction) [16, 15].
[1] W. Ackermann. Zum Hilbertschen Aufbau der reellen Zahlen.
Math. Annalen, 99:118-133, 1928. [2] D. Applegate, G. Jacobson, and D. Sleator. Computer analysis of sprouts. Technical Report CMU-CS-91-144, 1991. [3] R. Bellman and D. Blackwell. Some two-person games involving bluffing. PNAS, 35:600-605, 1949. [4] D. Billings, N. Burch, A. Davidson, R. Holte, J. Schaeffer,
T. Schauenberg, and D. Szafron. Approximating game-theoretic optimal strategies for full-scale poker. In IJCAI, 2003. [5] D. Billings, A. Davidson, J. Schaeffer, and D. Szafron. The challenge of poker. Artificial Intelligence, 134:201-240, 2002. [6] B. Bollob´as. Combinatorics. Cambridge University Press, 1986. [7] A. Casajus. Weak isomorphism of extensive games.
Mathematical Social Sciences, 46:267-290, 2003. [8] X. Chen and X. Deng. Settling the complexity of 2-player Nash equilibrium. ECCC, Report No. 150, 2005. [9] V. Chv´atal. Linear Programming. W. H. Freeman & Co., 1983. [10] B. P. de Bruin. Game transformations and game equivalence.
Technical note x-1999-01, University of Amsterdam, Institute for Logic, Language, and Computation, 1999. [11] S. Elmes and P. J. Reny. On the strategic equivalence of extensive form games. J. of Economic Theory, 62:1-23, 1994. [12] L. R. Ford, Jr. and D. R. Fulkerson. Flows in Networks.
Princeton University Press, 1962. [13] A. Gilpin and T. Sandholm. Finding equilibria in large sequential games of imperfect information. Technical Report CMU-CS-05-158, Carnegie Mellon University, 2005. [14] A. Gilpin and T. Sandholm. Optimal Rhode Island Hold"em poker. In AAAI, pages 1684-1685, Pittsburgh, PA, USA, 2005. [15] A. Gilpin and T. Sandholm. A competitive Texas Hold"em poker player via automated abstraction and real-time equilibrium computation. Mimeo, 2006. [16] A. Gilpin and T. Sandholm. A Texas Hold"em poker player based on automated abstraction and real-time equilibrium computation. In AAMAS, Hakodate, Japan, 2006. [17] M. L. Ginsberg. Partition search. In AAAI, pages 228-233,
Portland, OR, 1996. [18] M. L. Ginsberg. GIB: Steps toward an expert-level bridge-playing program. In IJCAI, Stockholm, Sweden, 1999. [19] S. Govindan and R. Wilson. A global Newton method to compute Nash equilibria. J. of Econ. Theory, 110:65-86, 2003. [20] C. A. Knoblock. Automatically generating abstractions for planning. Artificial Intelligence, 68(2):243-302, 1994. [21] E. Kohlberg and J.-F. Mertens. On the strategic stability of equilibria. Econometrica, 54:1003-1037, 1986. [22] D. Koller and N. Megiddo. The complexity of two-person zero-sum games in extensive form. Games and Economic Behavior, 4(4):528-552, Oct. 1992. [23] D. Koller and N. Megiddo. Finding mixed strategies with small supports in extensive form games. International Journal of Game Theory, 25:73-92, 1996. [24] D. Koller, N. Megiddo, and B. von Stengel. Efficient computation of equilibria for extensive two-person games.
Games and Economic Behavior, 14(2):247-259, 1996. [25] D. Koller and A. Pfeffer. Representations and solutions for game-theoretic problems. Artificial Intelligence, 94(1):167-215,
July 1997. [26] D. M. Kreps and R. Wilson. Sequential equilibria.
Econometrica, 50(4):863-894, 1982. [27] H. W. Kuhn. Extensive games. PNAS, 36:570-576, 1950. [28] H. W. Kuhn. A simplified two-person poker. In Contributions to the Theory of Games, volume 1 of Annals of Mathematics Studies, 24, pages 97-103. Princeton University Press, 1950. [29] H. W. Kuhn. Extensive games and the problem of information.
In Contributions to the Theory of Games, volume 2 of Annals of Mathematics Studies, 28, pages 193-216. Princeton University Press, 1953. [30] C. Lemke and J. Howson. Equilibrium points of bimatrix games. Journal of the Society for Industrial and Applied Mathematics, 12:413-423, 1964. [31] R. Lipton, E. Markakis, and A. Mehta. Playing large games using simple strategies. In ACM-EC, pages 36-41, 2003. [32] C.-L. Liu and M. Wellman. On state-space abstraction for anytime evaluation of Bayesian networks. SIGART Bulletin, 7(2):50-57, 1996. [33] A. Mas-Colell, M. Whinston, and J. R. Green. Microeconomic Theory. Oxford University Press, 1995. [34] R. D. McKelvey and A. McLennan. Computation of equilibria in finite games. In Handbook of Computational Economics, volume 1, pages 87-142. Elsevier, 1996. [35] P. B. Miltersen and T. B. Sørensen. Computing sequential equilibria for two-player games. In SODA, pages 107-116, 2006. [36] J. Nash. Equilibrium points in n-person games. Proc. of the National Academy of Sciences, 36:48-49, 1950. [37] J. F. Nash and L. S. Shapley. A simple three-person poker game. In Contributions to the Theory of Games, volume 1, pages 105-116. Princeton University Press, 1950. [38] A. Perea. Rationality in extensive form games. Kluwer Academic Publishers, 2001. [39] A. Pfeffer, D. Koller, and K. Takusagawa. State-space approximations for extensive form games, July 2000. Talk given at the First International Congress of the Game Theory Society, Bilbao, Spain. [40] R. Porter, E. Nudelman, and Y. Shoham. Simple search methods for finding a Nash equilibrium. In AAAI, pages 664-669, San Jose, CA, USA, 2004. [41] I. Romanovskii. Reduction of a game with complete memory to a matrix game. Soviet Mathematics, 3:678-681, 1962. [42] T. Sandholm and A. Gilpin. Sequences of take-it-or-leave-it offers: Near-optimal auctions without full valuation revelation.
In AAMAS, Hakodate, Japan, 2006. [43] T. Sandholm, A. Gilpin, and V. Conitzer. Mixed-integer programming methods for finding Nash equilibria. In AAAI, pages 495-501, Pittsburgh, PA, USA, 2005. [44] R. Savani and B. von Stengel. Exponentially many steps for finding a Nash equilibrium in a bimatrix game. In FOCS, pages 258-267, 2004. [45] R. Selten. Spieltheoretische behandlung eines oligopolmodells mit nachfragetr¨agheit. Zeitschrift f¨ur die gesamte Staatswissenschaft, 12:301-324, 1965. [46] R. Selten. Evolutionary stability in extensive two-person games - correction and further development. Mathematical Social Sciences, 16:223-266, 1988. [47] J. Shi and M. Littman. Abstraction methods for game theoretic poker. In Computers and Games, pages 333-345.
Springer-Verlag, 2001. [48] S. J. J. Smith, D. S. Nau, and T. Throop. Computer bridge: A big win for AI planning. AI Magazine, 19(2):93-105, 1998. [49] R. E. Tarjan. Efficiency of a good but not linear set union algorithm. Journal of the ACM, 22(2):215-225, 1975. [50] F. Thompson. Equivalence of games in extensive form. RAND Memo RM-759, The RAND Corporation, Jan. 1952. [51] J. von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University Press, 1947. [52] B. von Stengel. Efficient computation of behavior strategies.
Games and Economic Behavior, 14(2):220-246, 1996. [53] B. von Stengel. Computing equilibria for two-person games. In Handbook of Game Theory, volume 3. North Holland,
Amsterdam, 2002. [54] R. Wilson. Computing equilibria of two-person games from the extensive form. Management Science, 18(7):448-460, 1972. [55] S. J. Wright. Primal-Dual Interior-Point Methods. SIAM,

When agents interact with one another, the value of their contribution is determined by what they can do with their skills and resources, rather than simply their identities.
Consider the problem of forming a soccer team. For a team to be successful, a team needs some forwards, midfielders, defenders, and a goalkeeper. The relevant attributes of the players are their skills at playing each of the four positions.
The value of a team depends on how well its players can play these positions. At a finer level, we can extend the model to consider a wider range of skills, such as passing, shooting, and tackling, but the value of a team remains solely a function of the attributes of its players.
Consider an example from the business world.
Companies in the metals industry are usually vertically-integrated and diversified. They have mines for various types of ores, and also mills capable of processing and producing different kinds of metal. They optimize their production profile according to the market prices for their products. For example, when the price of aluminum goes up, they will allocate more resources to producing aluminum. However, each company is limited by the amount of ores it has, and its capacities in processing given kinds of ores. Two or more companies may benefit from trading ores and processing capacities with one another. To model the metal industry, the relevant attributes are the amount of ores and the processing capacities of the companies. Given the exogenous input of market prices, the value of a group of companies will be determined by these attributes.
Many real-world problems can be likewise modeled by picking the right attributes. As attributes apply to both individual agents and groups of agents, we propose the use of coalitional game theory to understand what groups may form and what payoffs the agents may expect in such models.
Coalitional game theory focuses on what groups of agents can achieve, and thus connects strongly with e-commerce, as the Internet economies have significantly enhanced the abilities of business to identify and capitalize on profitable opportunities of cooperation. Our goal is to understand the computational aspects of computing the solution concepts (stable and/or fair distribution of payoffs, formally defined in Section 3) for coalitional games described using attributes. Our contributions can be summarized as follows: • We define a formal representation for coalitional games based on attributes, and relate this representation to others proposed in the literature. We show that when compared to other representations, there exists games for which a multi-attribute description can be exponentially more succinct, and for no game it is worse. • Given the generality of the model, positive results carry over to other representations. We discuss two positive results in the paper, one for the Shapley value and one for the core, and show how these imply related results in the literature. 170 • We study an approximation heuristic for the Shapley value when its exact values cannot be found efficiently.
We provide an explicit bound on the maximum error of the estimate, and show that the bound is asymptotically tight. We also carry out experiments to evaluate how the heuristic performs on random instances.1
Coalitional game theory has been well studied in economics [9, 10, 14]. A vast amount of literature have focused on defining and comparing solution concepts, and determining their existence and properties. The first algorithmic study of coalitional games, as far as we know, is performed by Deng and Papadimitriou in [5]. They consider coalitional games defined on graphs, where the players are the vertices and the value of coalition is determined by the sum of the weights of the edges spanned by these players. This can be efficiently modeled and generalized using attributes.
As a formal representation, multi-attribute coalitional games is closely related to the multi-issue representation of Conitzer and Sandholm [3] and our work on marginal contribution networks [7]. Both of these representations are based on dividing a coalitional game into subgames (termed issues in [3] and rules in [7]), and aggregating the subgames via linear combination. The key difference in our work is the unrestricted aggregation of subgames: the aggregation could be via a polynomial function of the attributes, or even by treating the subgames as input to another computational problem such as a min-cost flow problem. The relationship of these models will be made clear after we define the multiattribute representation in Section 4.
Another representation proposed in the literature is one specialized for superadditive games by Conitzer and Sandholm [2]. This representation is succinct, but to find the values of some coalitions may require solving an NP-hard problem. While it is possible for multi-attribute coalitional games to efficiently represent these games, it necessarily requires the solution to an NP-hard problem in order to find out the values of some coalitions. In this paper, we stay within the boundary of games that admits efficient algorithm for determining the value of coalitions. We will therefore not make further comparisons with [2].
The model of coalitional games with attributes has been considered in the works of Shehory and Kraus. They model the agents as possessing capabilities that indicates their proficiencies in different areas, and consider how to efficiently allocate tasks [12] and the dynamics of coalition formation [13]. Our work differs significantly as our focus is on reasoning about solution concepts. Our model also covers a wider scope as attributes generalize the notion of capabilities.
Yokoo et al. have also considered a model of coalitional games where agents are modeled by sets of skills, and these skills in turn determine the value of coalitions [15]. There are two major differences between their work and ours. Firstly,
Yokoo et al. assume that each skill is fundamentally different from another, hence no two agents may possess the same skill. Also, they focus on developing new solution concepts that are robust with respect to manipulation by agents. Our focus is on reasoning about traditional solution concepts. 1 We acknowledge that random instances may not be typical of what happens in practice, but given the generality of our model, it provides the most unbiased view.
Our work is also related to the study of cooperative games with committee control [4]. In these games, there is usually an underlying set of resources each controlled by a (possibly overlapping) set of players known as the committee, engaged in a simple game (defined in Section 3). multiattribute coalitional games generalize these by considering relationship between the committee and the resources beyond simple games. We note that when restricted to simple games, we derive similar results to that in [4].
In this section, we will review the relevant concepts of coalitional game theory and its two most important solution concepts - the Shapley value and the core. We will then define the computational questions that will be studied in the second half of the paper.
Throughout this paper, we assume that payoffs to groups of agents can be freely distributed among its members. This transferable utility assumption is commonly made in coalitional game theory. The canonical representation of a coalitional game with transferable utility is its characteristic form.
Definition 1. A coalition game with transferable utility in characteristic form is denoted by the pair N, v , where • N is the set of agents; and • v : 2N → R is a function that maps each group of agents S ⊆ N to a real-valued payoff.
A group of agents in a game is known as a coalition, and the entire set of agents is known as the grand coalition.
An important class of coalitional games is the class of monotonic games.
Definition 2. A coalitional game is monotonic if for all S ⊂ T ⊆ N, v(S) ≤ v(T).
Another important class of coalitional games is the class of simple games. In a simple game, a coalition either wins, in which case it has a value of 1, or loses, in which case it has a value of 0. It is often used to model voting situations.
Simple games are often assumed to be monotonic, i.e., if S wins, then for all T ⊇ S, T also wins. This coincides with the notion of using simple games as a model for voting. If a simple game is monotonic, then it is fully described by the set of minimal winning coalitions, i.e., coalitions S for which v(S) = 1 but for all coalitions T ⊂ S, v(T) = 0.
An outcome in a coalitional game specifies the utilities the agents receive. A solution concept assigns to each coalitional game a set of reasonable outcomes. Different solution concepts attempt to capture in some way outcomes that are stable and/or fair. Two of the best known solution concepts are the Shapley value and the core.
The Shapley value is a normative solution concept that prescribes a fair way to divide the gains from cooperation when the grand coalition is formed. The division of payoff to agent i is the average marginal contribution of agent i over all possible permutations of the agents. Formally,
Definition 3. The Shapley value of agent i, φi(v), in game N, v is given by the following formula φi(v) = S⊆N\{i} |S|!(|N| − |S| − 1)! |N|! (v(S ∪ {i}) − v(S)) 171 The core is a descriptive solution concept that focuses on outcomes that are stable. Stability under core means that no set of players can jointly deviate to improve their payoffs.
Definition 4. An outcome x ∈ R|N| is in the core of the game N, v if for all S ⊆ N, i∈S xi ≥ v(S) Note that the core of a game may be empty, i.e., there may not exist any payoff vector that satisfies the stability requirement for the given game.
We will study the following three problems related to solution concepts in coalitional games.
Problem 1. (Shapley Value) Given a description of the coalitional game and an agent i, compute the Shapley value of agent i.
Problem 2. (Core Membership) Given a description of the coalitional game and a payoff vector x such that È i∈N xi = v(N), determine if È i∈S xi ≥ v(S) for all S ⊆ N.
Problem 3. (Core Non-emptiness) Given a description of the coalitional game, determine if there exists any payoff vector x such that È i∈S xi ≥ V (S) for all S ⊆ N, andÈ i∈N xi = v(N).
Note that the complexity of the above problems depends on the how the game is described. All these problems will be easy if the game is described by its characteristic form, but only so because the description takes space exponential in the number of agents, and hence simple brute-force approach takes time polynomial to the input description.
To properly understand the computational complexity questions, we have to look at compact representation.
In this section, we will give a formal definition of multiattribute coalitional games, and show how it is related to some of the representations discussed in the literature. We will also discuss some limitations to our proposed approach.
A multi-attribute coalitional game (MACG) consists of two parts: a description of the attributes of the agents, which we termed an attribute model, and a function that assigns values to combination of attributes. Together, they induce a coalitional game over the agents. We first define the attribute model.
Definition 5. An attribute model is a tuple N, M, A , where • N denotes the set of agents, of size n; • M denotes the set of attributes, of size m; • A ∈ Rm×n , the attribute matrix, describes the values of the attributes of the agents, with Aij denoting the value of attribute i for agent j.
We can directly define a function that maps combinations of attributes to real values. However, for many problems, we can describe the function more compactly by computing it in two steps: we first compute an aggregate value for each attribute, then compute the values of combination of attributes using only the aggregated information. Formally,
Definition 6. An aggregating function (or aggregator) takes as input a row of the attribute matrix and a coalition S, and summarizes the attributes of the agents in S with a single number. We can treat it as a mapping from Rn × 2N → R.
Aggregators often perform basic arithmetic or logical operations. For example, it may compute the sum of the attributes, or evaluate a Boolean expression by treating the agents i ∈ S as true and j /∈ S as false. Analogous to the notion of simple games, we call an aggregator simple if its range is {0, 1}. For any aggregator, there is a set of relevant agents, and a set of irrelevant agents. An agent i is irrelevant to aggregator aj if aj (S ∪ {i}) = aj (S) for all S ⊆ N.
A relevant agent is one not irrelevant.
Given the attribute matrix, an aggregator assigns a value to each coalition S ⊆ N. Thus, each aggregator defines a game over N. For aggregator aj , we refer to this induced game as the game of attribute j, and denote it with aj (A).
When the attribute matrix is clear from the context, we may drop A and simply denote the game as aj . We may refer to the game as the aggregator when no ambiguities arise.
We now define the second step of the computation with the help of aggregators.
Definition 7. An aggregate value function takes as input the values of the aggregators and maps these to a real value.
In this paper, we will focus on having one aggregator per attribute. Therefore, in what follows, we will refer to the aggregate value function as a function over the attributes.
Note that when all aggregators are simple, the aggregate value function implicitly defines a game over the attributes, as it assigns a value to each set of attributes T ⊆ M. We refer to this as the game among attributes.
We now define multi-attribute coalitional game.
Definition 8. A multi-attribute coalitional game is defined by the tuple N, M, A, a, w , where • N, M, A is an attribute model; • a is a set of aggregators, one for each attribute; we can treat the set together as a vector function, mapping Rm×n × 2N → Rm • w : Rm → R is an aggregate value function.
This induces a coalitional game with transferable payoffs N, v with players N and the value function defined by v(S) = w(a(A, S)) Note that MACG as defined is fully capable of representing any coalitional game N, v . We can simply take the set of attributes as equal to the set of agents, i.e., M = N, an identity matrix for A, aggregators of sums, and the aggregate value function w to be v. 172
Let us illustrate how MACG can be used to represent a game with a simple example. Suppose there are four types of resources in the world: gold, silver, copper, and iron, that each agent is endowed with some amount of these resources, and there is a fixed price for each of the resources in the market. This game can be described using MACG with an attribute matrix A, where Aij denote the amount of resource i that agent j is endowed. For each resource, the aggregator sums together the amount of resources the agents have.
Finally, the aggregate value function takes the dot product between the market price vector and the aggregate vector.
Note the inherent flexibility in the model: only limited work would be required to update the game as the market price changes, or when a new agent arrives.
As briefly discussed in Section 2, MACG is closely related to two other representations in the literature, the multiissue representation of Conitzer and Sandholm [3], and our work on marginal contribution nets [7]. To make their relationships clear, we first review these two representations.
We have changed the notations from the original papers to highlight their similarities.
Definition 9. A multi-issue representation is given as a vector of coalitional games, (v1, v2, . . . vm), each possibly with a varying set of agents, say N1, . . . , Nm. The coalitional game N, v induced by multi-issue representation has player set N = Ëm i=1 Ni, and for each coalition S ⊆ N, v(S) = Èm i=1 v(S ∩ Ni). The games vi are assumed to be represented in characteristic form.
Definition 10. A marginal contribution net is given as a set of rules (r1, r2, . . . , rm), where rule ri has a weight wi, and a pattern pi that is a conjunction over literals (positive or negative). The agents are represented as literals. A coalition S is said to satisfy the pattern pi, if we treat the agents i ∈ S as true, an agent j /∈ S as false, pi(S) evaluates to true. Denote the set of literals involved in rule i by Ni. The coalitional game N, v induced by a marginal contribution net has player set N = Ëm i=1 Ni, and for each coalition S ⊆ N, v(S) = È i:pi(S)=true wi.
From these definitions, we can see the relationships among these three representations clearly. An issue of a multi-issue representation corresponds to an attribute in MACG.
Similarly, a rule of a marginal contribution net corresponds to an attribute in MACG. The aggregate value functions are simple sums and weighted sums for the respective representations. Therefore, it is clear that MACG will be no less succinct than either representation.
However, MACG differs in two important way. Firstly, there is no restriction on the operations performed by the aggregate value function over the attributes. This is an important generalization over the linear combination of issues or rules in the other two approaches. In particular, there are games for which MACG can be exponentially more compact.
The proof of the following proposition can be found in the Appendix.
Proposition 1. Consider the parity game N, v where coalition S ⊆ N has value v(S) = 1 if |S| is odd, and v(S) =
Both multi-issue representation and marginal contribution nets requires O(2n ) space.
A second important difference of MACG is that the attribute model and the value function is cleanly separated.
As suggested in the example in Section 4.2, this often allows us more efficient update of the values of the game as it changes. Also, the same attribute model can be evaluated using different value functions, and the same value function can be used to evaluate different attribute model. Therefore,
MACG is very suitable for representing multiple games. We believe the problems of updating games and representing multiple games are interesting future directions to explore.
Before focusing on one aggregator per attribute for the rest of the paper, it is natural to wonder if any is lost per such restriction. The unfortunate answer is yes, best illustrated by the following. Consider again the problem of forming a soccer team discussed in the introduction, where we model the attributes of the agents as their ability to take the four positions of the field, and the value of a team depends on the positions covered. If we first aggregate each of the attribute individually, we will lose the distributional information of the attributes. In other words, we will not be able to distinguish between two teams, one of which has a player for each position, the other has one player who can play all positions, but the rest can only play the same one position.
This loss of distributional information can be recovered by using aggregators that take as input multiple rows of the attribute matrix rather than just a single row.
Alternatively, if we leave such attributes untouched, we can leave the burden of correctly evaluating these attributes to the aggregate value function. However, for many problems that we found in the literature, such as the transportation domain of [12] and the flow game setting of [4], the distribution of attributes does not affect the value of the coalitions. In addition, the problem may become unmanageably complex as we introduce more complicated aggregators. Therefore, we will focus on the representation as defined in Definition 8.
In this section, we focus on computational issues of finding the Shapley value of a player in MACG. We first set up the problem with the use of oracles to avoid complexities arising from the aggregators. We then show that when attributes are linearly separable, the Shapley value can be efficiently computed. This generalizes the proofs of related results in the literature. For the non-linearly separable case, we consider a natural heuristic for estimating the Shapley value, and study the heuristic theoretically and empirically.
We start by noting that computing the Shapley value for simple aggregators can be hard in general. In particular, we can define aggregators to compute weighted majority over its input set of agents. As noted in [6], finding the Shapley value of a weighted majority game is #P-hard. Therefore, discussion of complexity of Shapley value for MACG with unrestricted aggregators is moot.
Instead of placing explicit restriction on the aggregator, we assume that the Shapley value of the aggregator can be 173 answered by an oracle. For notation, let φi(u) denote the Shapley value for some game u. We make the following assumption: Assumption 1. For each aggregator aj in a MACG, there is an associated oracle that answers the Shapley value of the game of attribute j. In other words, φi(aj ) is known.
For many aggregators that perform basic operations over its input, polynomial time oracle for Shapley value exists.
This include operations such as sum, and symmetric functions when the attributes are restricted to {0, 1}. Also, when only few agents have an effect on the aggregator, brute-force computation for Shapley value is feasible. Therefore, the above assumption is reasonable for many settings. In any case, such abstraction allows us to focus on the aggregate value function.
When the aggregate value function can be written as a linear function of the attributes, the Shapley value of the game can be efficiently computed.
Theorem 1. Given a game N, v represented as a MACG N, M, A, a, w , if the aggregate value function can be written as a linear function of its attributes, i.e., w(a(A, S)) = m j=1 cj aj (A, S) The Shapley value of agent i in N, v is given by φi(v) = m j=1 cj φi(aj ) (1) Proof. First, we note that Shapley value satisfies an additivity axiom [11].
The Shapley value satisfies additivity, namely, φi(a + b) = φi(a) + φi(b), where N, a + b is a game defined to be (a + b)(S) = a(S) + b(S) for all S ⊆ N.
It is also clear that Shapley value satisfies scaling, namely φi(αv) = αφi(v) where (αv)(S) = αv(S) for all S ⊆ N.
Since the aggregate value function can be expressed as a weighted sum of games of attributes, φi(v) = φi(w(a)) = φi( m j=1 cjaj ) = m j=1 cjφi(aj ) Many positive results regarding efficient computation of Shapley value in the literature depends on some form of linearity. Examples include the edge-spanning game on graphs by Deng and Papadimitriou [5], the multi-issue representation of [3], and the marginal contribution nets of [7]. The key to determine if the Shapley value can be efficiently computed depends on the linear separability of attributes. Once this is satisfied, as long as the Shapley value of the game of attributes can be efficiently determined, the Shapley value of the entire game can be efficiently computed.
Corollary 1. The Shapley value for the edge-spanning game of [5], games in multi-issue representation [3], and games in marginal contribution nets [7], can be computed in polynomial time.
When the aggregate value function cannot be expressed as a linear function of its attributes, computing the Shapley value exactly is difficult. Here, we will focus on aggregate value function that can be expressed as some polynomial of its attributes. If we do not place a limit on the degree of the polynomial, and the game N, v is not necessarily monotonic, the problem is #P-hard.
Theorem 2. Computing the Shapley value of a MACG N, M, A, a, w , when w can be an arbitrary polynomial of the aggregates a, is #P-hard, even when the Shapley value of each aggregator can be efficiently computed.
The proof is via reduction from three-dimensional matching, and details can be found in the Appendix.
Even if we restrict ourselves to monotonic games, and non-negative coefficients for the polynomial aggregate value function, computing the exact Shapley value can still be hard. For example, suppose there are two attributes. All agents in some set B ⊆ N possess the first attribute, and all agents in some set C ⊆ N possess the second, and B and C are disjoint. For a coalition S ⊆ N, the aggregator for the first evaluates to 1 if and only if |S ∩ B| ≥ b , and similarly, the aggregator for the second evaluates to 1 if and only if |S ∩ C| ≥ c . Let the cardinality of the sets B and C be b and c. We can verify that the Shapley value of an agent i in B equals φi = 1 b b −1 i=0  b i ¡  c c −1 ¡   b+c c +i−1 ¡ c − c + 1 b + c − c − i + 1 The equation corresponds to a weighted sum of probability values of hypergeometric random variables. The correspondence with hypergeometric distribution is due to sampling without replacement nature of Shapley value. As far as we know, there is no close-form formula to evaluate the sum above. In addition, as the number of attributes involved increases, we move to multi-variate hypergeometric random variables, and the number of summands grow exponentially in the number of attributes. Therefore, it is highly unlikely that the exact Shapley value can be determined efficiently.
Therefore, we look for approximation.
First, we need a criteria for evaluating how well an estimate, ˆφ, approximates the true Shapley value, φ. We consider the following three natural criteria: • Maximum underestimate: maxi φi/ˆφi • Maximum overestimate: maxi ˆφi/φi • Total variation: 1 2 È i |φi − ˆφi|, or alternatively maxS | È i∈S φi − È i∈S ˆφi| The total variation criterion is more meaningful when we normalize the game to having a value of 1 for the grand coalition, i.e., v(N) = 1. We can also define additive analogues of the under- and overestimates, especially when the games are normalized. 174 We will assume for now that the aggregate value function is a polynomial over the attributes with non-negative coefficients. We will also assume that the aggregators are simple. We will evaluate a specific heuristic that is analogous to Equation (1). Suppose the aggregate function can be written as a polynomial with p terms w(a(A, S)) = p j=1 cj aj(1) (A, S)aj(2) (A, S) · · · aj(kj ) (A, S) (2) For term j, the coefficient of the term is cj , its degree kj , and the attributes involved in the term are j(1), . . . , j(kj ).
We compute an estimate ˆφ to the Shapley value as ˆφi = p j=1 kj l=1 cj kj φi(aj(l) ) (3) The idea behind the estimate is that for each term, we divide the value of the term equally among all its attributes. This is represented by the factor cj kj . Then for for each attribute of an agent, we assign the player a share of value from the attribute. This share is determined by the Shapley value of the simple game of that attribute. Without considering the details of the simple games, this constitutes a fair (but blind) rule of sharing.
We can derive a simple and tight bound for the maximum (multiplicative) underestimate of the heuristic estimate.
Theorem 3. Given a game N, v represented as a MACG N, M, A, a, w , suppose w can be expressed as a polynomial function of its attributes (cf Equation (2)). Let K = maxjkj, i.e., the maximum degree of the polynomial. Let ˆφ denote the estimated Shapley value using Equation (3), and φ denote the true Shapley value. For all i ∈ N, φi ≥ K ˆφi.
Proof. We bound the maximum underestimate term-byterm. Let tj be the j-th term of the polynomial. We note that the term can be treated as a game among attributes, as it assigns a value to each coalition S ⊆ N. Without loss of generality, renumber attributes j(1) through j(kj ) as 1 through kj. tj (S) = cj kj l=1 al (A, S) To make the equations less cluttered, let B(N, S) = |S|!(|N| − |S| − 1)! |N|! and for a game a, contribution of agent i to group S : i /∈ S, ∆i(a, S) = a(S ∪ {i}) − a(S) The true Shapley value of the game tj is φi(tj) = cj S⊆N\{i} B(N, S)∆i(tj, S) For each coalition S, i /∈ S, ∆i(tj , S) = 1 if and only if for at least one attribute, say l∗ , ∆i(al∗ , S) = 1. Therefore, if we sum over all the attributes, we would have included l∗ for sure. φi(tj) ≤ cj kj j=1 S⊆N\{i} B(N, S)∆i(aj , S) = kj kj j=1 cj kj φi(aj ) = kj ˆφi(T) Summing over the terms, we see that the worst case underestimate is by the maximum degree.
Without loss of generality, since the bound is multiplicative, we can normalize the game to having v(N) = 1. As a corollary, because we cannot overestimate any set by more than K times, we obtain a bound on the total variation: Corollary 2. The total variation between the estimated Shapley value and the true Shapley value, for K-degree bounded polynomial aggregate value function, is K−1 K .
We can show that this bound is tight.
Example 1. Consider a game with n players and K attributes. Let the first (n−1) agents be a member of the first (K − 1) attributes, and that the corresponding aggregator returns 1 if any one of the first (K − 1) agents is present.
Let the n-th agent be the sole member of the K-th attribute.
The estimated Shapley will assign a value of K−1 K 1 n−1 to the first (n − 1) agents and 1 K to the n-th agent. However, the true Shapley value of the n-th agent tends to 1 as n → ∞, and the total variation approaches K−1 K .
In general, we cannot bound how much ˆφ may overestimate the true Shapley value. The problem is that ˆφi may be non-zero for agent i even though may have no influence over the outcome of a game when attributes are multiplied together, as illustrated by the following example.
Example 2. Consider a game with 2 players and 2 attributes, and let the first agent be a member of both attributes, and the other agent a member of the second attribute. For a coalition S, the first aggregator evaluates to
both agents are in S. While agent 2 is not a dummy with respect to the second attribute, it is a dummy with respect to the product of the attributes. Agent 2 will be assigned a value of 1 4 by the estimate.
As mentioned, for simple monotonic games, a game is fully described by its set of minimal winning coalitions. When the simple aggregators are represented as such, it is possible to check, in polynomial time, for agents turning dummies after attributes are multiplied together. Therefore, we can improve the heuristic estimate in this special case.
Due to a lack of benchmark problems for coalitional games, we have tested the heuristic on random instances. We believe more meaningful results can be obtained when we have real instances to test this heuristic on.
Our experiment is set up as follows. We control three parameters of the experiment: the number of players (6 − 10), 175 0
No. of Players TotalVariationDistance 2 3 4 5 (a) Effect of Max Degree 0
No. of Players TotalVariationDistance 4 5 6 (b) Effect of Number of Attributes Figure 1: Experimental results the number of attributes (3 − 8), and the maximum degree of the polynomial (2 − 5). For each attribute, we randomly sample one to three minimal winning coalitions. We then randomly generate a polynomial of the desired maximum degree with a random number (3 − 12) of terms, each with a random positive weight. We normalize each game to have v(N) = 1. The results of the experiments are shown in Figure 1. The y-axis of the graphs shows the total variation, and the x-axis the number of players. Each datapoint is an average of approximately 700 random samples.
Figure 1(a) explores the effect of the maximum degree and the number of players when the number of attributes is fixed (at six). As expected, the total variation increases as the maximum degree increases. On the other hand, there is only a very small increase in error as the number of players increases. The error is nowhere near the theoretical worstcase bound of 1 2 to 4 5 for polynomials of degrees 2 to 5.
Figure 1(b) explores the effect of the number of attributes and the number of players when the maximum degree of the polynomial is fixed (at three). We first note that these three lines are quite tightly clustered together, suggesting that the number of attributes has relatively little effect on the error of the estimate. As the number of attributes increases, the total variation decreases. We think this is an interesting phenomenon. This is probably due to the precise construct required for the worst-case bound, and so as more attributes are available, we have more diverse terms in the polynomial, and the diversity pushes away from the worst-case bound.
In this section, we look at the complexity of the two computational problems related to the core: Core Nonemptiness and Core Membership. We show that the nonemptiness of core of the game among attributes and the cores of the aggregators imply non-emptiness of the core of the game induced by the MACG. We also show that there appears to be no such general relationship that relates the core memberships of the game among attributes, games of attributes, and game induced by MACG.
There are many problems in the literature for which the questions of Core Non-emptiness and Core Membership are known to be hard [1]. For example, for the edgespanning game that Deng and Papadimitriou studied [5], both of these questions are coNP-complete. As MACG can model the edge-spanning game in the same amount of space, these hardness results hold for MACG as well.
As in the case for computing Shapley value, we attempt to find a way around the hardness barrier by assuming the existence of oracles, and try to build algorithms with these oracles. First, we consider the aggregate value function.
Assumption 2. For a MACG N, M, A, a, w , we assume there are oracles that answers the questions of Core Nonemptiness, and Core Membership for the aggregate value function w.
When the aggregate value function is a non-negative linear function of its attributes, the core is always non-empty, and core membership can be determined efficiently.
The concept of core for the game among attributes makes the most sense when the aggregators are simple games. We will further assume that these simple games are monotonic.
Assumption 3. For a MACG N, M, A, a, w , we assume all aggregators are monotonic and simple. We also assume there are oracles that answers the questions of Core Nonemptiness, and Core Membership for the aggregators.
We consider this a mild assumption. Recall that monotonic simple games are fully described by their set of minimal winning coalitions (cf Section 3). If the aggregators are represented as such, Core Non-emptiness and Core Membership can be checked in polynomial time. This is due to the following well-known result regarding simple games: Lemma 1. A simple game N, v has a non-empty core if and only if it has a set of veto players, say V , such that v(S) = 0 for all S ⊇ V . Further, A payoff vector x is in the core if and only if xi = 0 for all i /∈ V .
There is a strong connection between the non-emptiness of the cores of the games among attributes, games of the attributes, and the game induced by a MACG.
Theorem 4. Given a game N, v represented as a MACG N, M, A, a, w , if the core of the game among attributes, 176 M, w , is non-empty, and the cores of the games of attributes are non-empty, then the core of N, v is non-empty.
Proof. Let u be an arbitrary payoff vector in the core of the game among attributes, M, w . For each attribute j, let θj be an arbitrary payoff vector in the core of the game of attribute j. By Lemma 1, each attribute j must have a set of veto players; let this set be denoted by Pj . For each agent i ∈ N, let yi = È j ujθj i . We claim that this vector y is in the core of N, v . Consider any coalition S ⊆ N, v(S) = w(a(A, S)) ≤ j:S⊇P j uj (4) This is true because an aggregator cannot evaluate to 1 without all members of the veto set. For any attribute j, by Lemma 1,
È i∈P j θj i = 1. Therefore, j:S⊇P j uj = j:S⊇P j uj i∈P j θj i = i∈S j:S⊇P j ujθj i ≤ i∈S yi Note that the proof is constructive, and hence if we are given an element in the core of the game among attributes, we can construct an element of the core of the coalitional game. From Theorem 4, we can obtain the following corollaries that have been previously shown in the literature.
Corollary 3. The core of the edge-spanning game of [5] is non-empty when the edge weights are non-negative.
Proof. Let the players be the vertices, and their attributes the edges incident on them. For each attribute, there is a veto set - namely, both endpoints of the edges.
As previously observed, an aggregate value function that is a non-negative linear function of its aggregates has non-empty core. Therefore, the precondition of Theorem 4 is satisfied, and the edge-spanning game with non-negative edge weights has a non-empty core.
Corollary 4 (Theorem 1 of [4]). The core of a flow game with committee control, where each edge is controlled by a simple game with a veto set of players, is non-empty.
Proof. We treat each edge of the flow game as an attribute, and so each attribute has a veto set of players. The core of a flow game (without committee) has been shown to be non-empty in [8]. We can again invoke Theorem 4 to show the non-emptiness of core for flow games with committee control.
However, the core of the game induced by a MACG may be non-empty even when the core of the game among attributes is empty, as illustrated by the following example.
Example 3. Suppose the minimal winning coalition of all aggregators in a MACG N, M, A, a, w is N, then v(S) = 0 for all coalitions S ⊂ N. As long as v(N) ≥ 0, any nonnegative vector x that satisfies È i∈N xi = v(N) is in the core of N, v .
Complementary to the example above, when all the aggregators have empty cores, the core of N, v is also empty.
Theorem 5. Given a game N, v represented as a MACG N, M, A, a, w , if the cores of all aggregators are empty, v(N) > 0, and for each i ∈ N, v({i}) ≥ 0, then the core of N, v is empty.
Proof. Suppose the core of N, v is non-empty. Let x be a member of the core, and pick an agent i such that xi >
Lemma 1, there are at least two disjoint winning coalitions.
Pick the winning coalition Sj that does not include i for each attribute j. Let S∗ = Ë j Sj . Because S∗ is winning for all coalitions, v(S∗ ) = v(N). However, v(N) = j∈N xj = xi + j /∈N xj ≥ xi + j∈S∗ xj > j∈S∗ xj Therefore, v(S∗ ) > È j∈S∗ xj, contradicting the fact that x is in the core of N, v .
We do not have general results regarding the problem of Core Non-emptiness when some of the aggregators have non-empty cores while others have empty cores. We suspect knowledge about the status of the cores of the aggregators alone is insufficient to decide this problem.
Since it is possible for the game induced by the MACG to have a non-empty core when the core of the aggregate value function is empty (Example 3), we try to explore the problem of Core Membership assuming that the core of both the game among attributes, M, w , and the underlying game, N, v , is known to be non-empty, and see if there is any relationship between their members. One reasonable requirement is whether a payoff vector x in the core of N, v can be decomposed and re-aggregated to a payoff vector y in the core of M, w . Formally,
Definition 11. We say that a vector x ∈ Rn ≥0 can be decomposed and re-aggregated into a vector y ∈ Rm ≥0 if there exists Z ∈ Rm×n ≥0 , such that yi = n j=1 Zij for all i xj = m i=1 Zij for all j We may refer Z as shares.
When there is no restriction on the entries of Z, it is always possible to decompose a payoff vector x in the core of N, v to a payoff vector y in the core of M, w . However, it seems reasonable to restrict that if an agent j is irrelevant to the aggregator i, i.e., i never changes the outcome of aggregator j, then Zij should be restricted to be 0. Unfortunately, this restriction is already too strong.
Example 4. Consider a MACG N, M, A, a, w with two players and three attributes. Suppose agent 1 is irrelevant to attribute 1, and agent 2 is irrelevant to attributes 2 and
w(T) =
177 Since the core of a game with a finite number of players forms a polytope, we can verify that the set of vectors (4, 4, 2), (4, 2, 4), and (2, 4, 4), fully characterize the core C of M, w .
On the other hand, the vector (10, 0) is in the core of N, v .
This vector cannot be decomposed and re-aggregated to a vector in C under the stated restriction.
Because of the apparent lack of relationship among members of the core of N, v and that of M, w , we believe an algorithm for testing Core Membership will require more input than just the veto sets of the aggregators and the oracle of Core Membership for the aggregate value function.
Multi-attribute coalitional games constitute a very natural way of modeling problems of interest. Its space requirement compares favorably with other representations discussed in the literature, and hence it serves well as a prototype to study computational complexity of coalitional game theory for a variety of problems. Positive results obtained under this representation can easily be translated to results about other representations. Some of these corollary results have been discussed in Sections 5 and 6.
An important direction to explore in the future is the question of efficiency in updating a game, and how to evaluate the solution concepts without starting from scratch. As pointed out at the end of Section 4.3, MACG is very naturally suited for updates. Representation results regarding efficiency of updates, and algorithmic results regarding how to compute the different solution concepts from updates, will both be very interesting.
Our work on approximating the Shapley value when the aggregate value function is a non-linear function of the attributes suggests more work to be done there as well. Given the natural probabilistic interpretation of the Shapley value, we believe that a random sampling approach may have significantly better theoretical guarantees.
[1] J. M. Bilbao, J. R. Fern´andez, and J. J. L´opez.
Complexity in cooperative game theory. http://www.esi.us.es/~mbilbao. [2] V. Conitzer and T. Sandholm. Complexity of determining nonemptiness of the core. In Proc. 18th Int. Joint Conf. on Artificial Intelligence, pages 613-618, 2003. [3] V. Conitzer and T. Sandholm. Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains. In Proc. 19th Nat. Conf. on Artificial Intelligence, pages 219-225, 2004. [4] I. J. Curiel, J. J. Derks, and S. H. Tijs. On balanced games and games with committee control. OR Spectrum, 11:83-88, 1989. [5] X. Deng and C. H. Papadimitriou. On the complexity of cooperative solution concepts. Math. Oper. Res., 19:257-266, May 1994. [6] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, New York, 1979. [7] S. Ieong and Y. Shoham. Marginal contribution nets: A compact representation scheme for coalitional games. In Proc. 6th ACM Conf. on Electronic Commerce, pages 193-202, 2005. [8] E. Kalai and E. Zemel. Totally balanced games and games of flow. Math. Oper. Res., 7:476-478, 1982. [9] A. Mas-Colell, M. D. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, New York, 1995. [10] M. J. Osborne and A. Rubinstein. A Course in Game Theory. The MIT Press, Cambridge, Massachusetts,
[11] L. S. Shapley. A value for n-person games. In H. W.
Kuhn and A. W. Tucker, editors, Contributions to the Theory of Games II, number 28 in Annals of Mathematical Studies, pages 307-317. Princeton University Press, 1953. [12] O. Shehory and S. Kraus. Task allocation via coalition formation among autonomous agents. In Proc. 14th Int. Joint Conf. on Artificial Intelligence, pages 31-45,
[13] O. Shehory and S. Kraus. A kernel-oriented model for autonomous-agent coalition-formation in general environments: Implentation and results. In Proc. 13th Nat. Conf. on Artificial Intelligence, pages 134-140,
[14] J. von Neumann and O. Morgenstern. Theory of Games and Economic Behvaior. Princeton University Press, 1953. [15] M. Yokoo, V. Conitzer, T. Sandholm, N. Ohta, and A. Iwasaki. Coalitional games in open anonymous environments. In Proc. 20th Nat. Conf. on Artificial Intelligence, pages 509-515, 2005.
Appendix We complete the missing proofs from the main text here.
To prove Proposition 1, we need the following lemma.
Lemma 2. Marginal contribution nets when all coalitions are restricted to have values 0 or 1 have the same representation power as an AND/OR circuit with negation at the literal level (i.e., AC0 circuit) of depth two.
Proof. If a rule assigns a negative value in the marginal contribution nets, we can write the rule by a corresponding set of at most n rules, where n is the number of agents, such that each of which has positive values through application of De Morgan"s Law. With all values of the rules non-negative, we can treat the weighted summation step of marginal contribution nets can be viewed as an OR, and each rule as a conjunction over literals, possibly negated. This exactly match up with an AND/OR circuit of depth two.
Proof (Proposition 1). The parity game can be represented with a MACG using a single attribute, aggregator of sum, and an aggregate value function that evaluates that sum modulus two.
As a Boolean function, parity is known to require an exponential number of prime implicants. By Lemma 2, a prime implicant is the exact analogue of a pattern in a rule of marginal contribution nets. Therefore, to represent the parity function, a marginal contribution nets must be an exponential number of rules.
Finally, as shown in [7], a marginal contribution net is at worst a factor of O(n) less compact than multi-issue representation. Therefore, multi-issue representation will also 178 take exponential space to represent the parity game. This is assuming that each issue in the game is represented in characteristic form.
Proof (Theorem 2). An instance of three-dimensional matching is as follows [6]: Given set P ⊆ W × X × Y , where W , X, Y are disjoint sets having the same number q of elements, does there exist a matching P ⊆ P such that |P | = q and no two elements of P agree in any coordinate. For notation, let P = {p1, p2, . . . , pK}. We construct a MACG N, M, A, a, w as follows: • M: Let attributes 1 to q correspond to elements in W , (q+1) to 2q correspond to elements in X, (2q+1) to 3q corresponds to element in Y , and let there be a special attribute (3q + 1). • N: Let player i corresponds to pi, and let there be a special player . • A: Let Aji = 1 if the element corresponding to attribute j is in pi. Thus, for the first K columns, there are exactly three non-zero entries. We also set A(3q+1) = 1. • a: for each aggregator j, aj (A(S)) = 1 if and only if sum of row j of A(S) equals 1. • w: product over all aj .

Electronic markets represent an application of information systems that has generated significant new trading opportunities while allowing for the dynamic pricing of goods. In addition to marketplaces such as eBay, electronic marketplaces are increasingly used for business-to-consumer auctions (e.g. to sell surplus inventory [19]).
Many authors have written about a future in which commerce is mediated by online, automated trading agents [10, 25, 1]. There is still little evidence of automated trading in e-markets, though. We believe that one leading place of resistance is in the lack of provably optimal bidding strategies for any but the simplest of market designs. Without this, we do not expect individual consumers, or firms, to be confident in placing their business in the hands of an automated agent.
One of the most common examples today of an electronic marketplace is eBay, where the gross merchandise volume (i.e., the sum of all successfully closed listings) during 2005 was $44B. Among items listed on eBay, many are essentially identical. This is especially true in the Consumer Electronics category [9], which accounted for roughly $3.5B of eBay"s gross merchandise volume in 2005. This presence of essentially identical items can expose bidders, and sellers, to risks because of the sequential auction problem.
For example, Alice may want an LCD monitor, and could potentially bid in either a 1 o"clock or 3 o"clock eBay auction. While Alice would prefer to participate in whichever auction will have the lower winning price, she cannot determine beforehand which auction that may be, and could end up winning the wrong auction. This is a problem of multiple copies.
Another problem bidders may face is the exposure problem. As investigated by Bykowsky et al. [6], exposure problems exist when buyers desire a bundle of goods but may only participate in single-item auctions.1 For example, if Alice values a video game console by itself for $200, a video game by itself for $30, and both a console and game for $250,
Alice must determine how much of the $20 of synergy value she might include in her bid for the console alone. Both problems arise in eBay as a result of sequential auctions of single items coupled with patient bidders with substitutes or complementary valuations.
Why might the sequential auction problem be bad?
Complex games may lead to bidders employing costly strategies and making mistakes. Potential bidders who do not wish to bear such costs may choose not to participate in the 1 The exposure problem has been primarily investigated by Bykowsky et al. in the context of simultaneous single-item auctions. The problem is also a familiar one of online decision making. 180 market, inhibiting seller revenue opportunities.
Additionally, among those bidders who do choose to participate, the mistakes made may lead to inefficient allocations, further limiting revenue opportunities.
We are interested in creating modifications to eBay-style markets that simplify the bidder problem, leading to simple equilibrium strategies, and preferably better efficiency and revenue properties.
Retail stores have developed policies to assist their customers in addressing sequential purchasing problems.
Return policies alleviate the exposure problem by allowing customers to return goods at the purchase price. Price matching alleviates the multiple copies problem by allowing buyers to receive from sellers after purchase the difference between the price paid for a good and a lower price found elsewhere for the same good [7, 15, 18]. Furthermore, price matching can reduce the impact of exactly when a seller brings an item to market, as the price will in part be set by others selling the same item. These two retail policies provide the basis for the scheme proposed in this paper.2 We extend the proxy bidding technology currently employed by eBay. Our super-proxy extension will take advantage of a new, real options-based, market infrastructure that enables simple, yet optimal, bidding strategies. The extensions are computationally simple, handle temporal issues, and retain seller autonomy in deciding when to enter the market and conduct individual auctions.
A seller sells an option for a good, which will ultimately lead to either a sale of the good or the return of the option.
Buyers interact through a proxy agent, defining a value on all possible bundles of goods in which they have interest together with the latest time period in which they are willing to wait to receive the good(s). The proxy agents use this information to determine how much to bid for options, and follow a dominant bidding strategy across all relevant auctions. A proxy agent exercises options held when the buyer"s patience has expired, choosing options that maximize a buyer"s payoff given the reported valuation. All other options are returned to the market and not exercised. The options-based protocol makes truthful and immediate revelation to a proxy a dominant strategy for buyers, whatever the future auction dynamics.
We conduct an empirical analysis of eBay, collecting data on over four months of bids for Dell LCD screens (model E193FP) starting in the Summer of 2005. LCD screens are a high-ticket item, for which we demonstrate evidence of the sequential bidding problem. We first infer a conservative model for the arrival time, departure time and value of bidders on eBay for LCD screens during this period. This model is used to simulate the performance of the optionsbased infrastructure, in order to make direct comparisons to the actual performance of eBay in this market.
We also extend the work of Haile and Tamer [11] to estimate an upper bound on the distribution of value of eBay bidders, taking into account the sequential auction problem when making the adjustments. Using this estimate, one can approximate how much greater a bidder"s true value is 2 Prior work has shown price matching as a potential mechanism for colluding firms to set monopoly prices. However, in our context, auction prices will be matched, which are not explicitly set by sellers but rather by buyers" bids. from the maximum bid they were observed to have placed on eBay. Based on this approximation, revenue generated in a simulation of the options-based scheme exceeds revenue on eBay for the comparable population and sequence of auctions by 14.8%, while the options-based scheme demonstrates itself as being 7.5% more efficient.
A number of authors [27, 13, 28, 29] have analyzed the multiple copies problem, often times in the context of categorizing or modeling sniping behavior for reasons other than those first brought forward by Ockenfels and Roth [20]. These papers perform equilibrium analysis in simpler settings, assuming bidders can participate in at most two auctions. Peters & Severinov [21] extend these models to allow buyers to consider an arbitrary number of auctions, and characterize a perfect Bayesian equilibrium. However, their model does not allow auctions to close at distinct times and does not consider the arrival and departure of bidders.
Previous work have developed a data-driven approach toward developing a taxonomy of strategies employed by bidders in practice when facing multi-unit auctions, but have not considered the sequential bidding problem [26, 2].
Previous work has also sought to provide agents with smarter bidding strategies [4, 3, 5, 1]. Unfortunately, it seems hard to design artificial agents with equilibrium bidding strategies, even for a simple simultaneous ascending price auction.
Iwasaki et al. [14] have considered the role of options in the context of a single, monolithic, auction design to help bidders with marginal-increasing values avoid exposure in a multi-unit, homogeneous item auction problem. In other contexts, options have been discussed for selling coal mine leases [23], or as leveled commitment contracts for use in a decentralized market place [24]. Most similar to our work,
Gopal et al. [9] use options for reducing the risks of buyers and sellers in the sequential auction problem. However, their work uses costly options and does not remove the sequential bidding problem completely.
Work on online mechanisms and online auctions [17, 12, 22] considers agents that can dynamically arrive and depart across time. We leverage a recent price-based characterization by Hajiaghayi et al. [12] to provide a dominant strategy equilibrium for buyers within our options-based protocol.
The special case for single-unit buyers is equivalent to the protocol of Hajiaghayi et al., albeit with an options-based interpretation.
Jiang and Leyton-Brown [16] use machine learning techniques for bid identification in online auctions.
The most common type of auction held on eBay is a singleitem proxy auction. Auctions open at a given time and remain open for a set period of time (usually one week).
Bidders bid for the item by giving a proxy a value ceiling. The proxy will bid on behalf of the bidder only as much as is necessary to maintain a winning position in the auction, up to the ceiling received from the bidder. Bidders may communicate with the proxy multiple times before an auction closes.
In the event that a bidder"s proxy has been outbid, a bidder may give the proxy a higher ceiling to use in the auction. eBay"s proxy auction implements an incremental version of a Vickrey auction, with the item sold to the highest bidder for the second-highest bid plus a small increment. 181 10 0 10 1 10 2 10 3 10 4 10 0 10 1 10 2 10 3 10 4 Number of Auctions NumberofBidders Auctions Available Auctions in Which Bid Figure 1: Histogram of number of LCD auctions available to each bidder and number of LCD auctions in which a bidder participates.
The market analyzed in this paper is that of a specific model of an LCD monitor, a 19 Dell LCD model E193FP.
This market was selected for a variety of reasons including: • The mean price of the monitor was $240 (with standard deviation $32), so we believe it reasonable to assume that bidders on the whole are only interested in acquiring one copy of the item on eBay.3 • The volume transacted is fairly high, at approximately
• The item is not usually bundled with other items. • The item is typically sold as new, and so suitable for the price-matching of the options-based scheme.
Raw auction information was acquired via a PERL script.
The script accesses the eBay search engine,4 and returns all auctions containing the terms ‘Dell" and ‘LCD" that have closed within the past month.5 Data was stored in a text file for post-processing. To isolate the auctions in the domain of interest, queries were made against the titles of eBay auctions that closed between 27 May, 2005 through 1 October, 2005.6 Figure 1 provides a general sense of how many LCD auctions occur while a bidder is interested in pursuing a monitor.7 8,746 bidders (86%) had more than one auction available between when they first placed a bid on eBay and the 3 For reference, Dell"s October 2005 mail order catalogue quotes the price of the monitor as being $379 without a desktop purchase, and $240 as part of a desktop purchase upgrade. 4 http://search.ebay.com 5 The search is not case-sensitive. 6 Specifically, the query found all auctions where the title contained all of the following strings: ‘Dell," ‘LCD" and ‘E193FP," while excluding all auctions that contained any of the following strings: ‘Dimension," ‘GHZ," ‘desktop," ‘p4" and ‘GB." The exclusion terms were incorporated so that the only auctions analyzed would be those selling exclusively the LCD of interest. For example, the few bundled auctions selling both a Dell Dimension desktop and the E193FP LCD are excluded. 7 As a reference, most auctions close on eBay between noon and midnight EDT, with almost two auctions for the Dell LCD monitor closing each hour on average during peak time periods. Bidders have an average observed patience of 3.9 days (with a standard deviation of 11.4 days). latest closing time of an auction in which they bid (with an average of 78 auctions available). Figure 1 also illustrates the number of auctions in which each bidder participates.
Only 32.3% of bidders who had more than one auction available are observed to bid in more than one auction (bidding in 3.6 auctions on average). A simple regression analysis shows that bidders tend to submit maximal bids to an auction that are $1.22 higher after spending twice as much time in the system, as well as bids that are $0.27 higher in each subsequent auction.
Among the 508 bidders that won exactly one monitor and participated in multiple auctions, 201 (40%) paid more than $10 more than the closing price of another auction in which they bid, paying on average $35 more (standard deviation $21) than the closing price of the cheapest auction in which they bid but did not win. Furthermore, among the 2,216 bidders that never won an item despite participating in multiple auctions, 421 (19%) placed a losing bid in one auction that was more than $10 higher than the closing price of another auction in which they bid, submitting a losing bid on average $34 more (standard deviation $23) than the closing price of the cheapest auction in which they bid but did not win. Although these measures do not say a bidder that lost could have definitively won (because we only consider the final winning price and not the bid of the winner to her proxy), or a bidder that won could have secured a better price, this is at least indicative of some bidder mistakes.
AUCTION PROBLEM While the eBay analysis was for simple bidders who desire only a single item, let us now consider a more general scenario where people may desire multiple goods of different types, possessing general valuations over those goods.
Consider a world with buyers (sometimes called bidders) B and K different types of goods G1...GK . Let T = {0, 1, ...} denote time periods. Let L denote a bundle of goods, represented as a vector of size K, where Lk ∈ {0, 1} denotes the quantity of good type Gk in the bundle.8 The type of a buyer i ∈ B is (ai, di, vi), with arrival time ai ∈ T, departure time di ∈ T, and private valuation vi(L) ≥ 0 for each bundle of goods L received between ai and di, and zero value otherwise. The arrival time models the period in which a buyer first realizes her demand and enters the market, while the departure time models the period in which a buyer loses interest in acquiring the good(s). In settings with general valuations, we need an additional assumption: an upper bound on the difference between a buyer"s arrival and departure, denoted ΔMax. Buyers have quasi-linear utilities, so that the utility of buyer i receiving bundle L and paying p, in some period no later than di, is ui(L, p) = vi(L) − p. Each seller j ∈ S brings a single item kj to the market, has no intrinsic value and wants to maximize revenue. Seller j has an arrival time, aj, which models the period in which she is first interested in listing the item, while the departure time, dj, models the latest period in which she is willing to consider having an auction for the item close. A seller will receive payment by the end of the reported departure of the winning buyer. 8 We extend notation whereby a single item k of type Gk refers to a vector L : Lk = 1. 182 We say an individual auction in a sequence is locally strategyproof (LSP) if truthful bidding is a dominant strategy for a buyer that can only bid in that auction. Consider the following example to see that LSP is insufficient for the existence of a dominant bidding strategy for buyers facing a sequence of auctions.
Example 1. Alice values one ton of Sand with one ton of Stone at $2, 000. Bob holds a Vickrey auction for one ton of Sand on Monday and a Vickrey auction for one ton of Stone on Tuesday. Alice has no dominant bidding strategy because she needs to know the price for Stone on Tuesday to know her maximum willingness to pay for Sand on Monday.
Definition 1. The sequential auction problem. Given a sequence of auctions, despite each auction being locally strategyproof, a bidder has no dominant bidding strategy.
Consider a sequence of auctions. Generally, auctions selling the same item will be uncertainly-ordered, because a buyer will not know the ordering of closing prices among the auctions. Define the interesting bundles for a buyer as all bundles that could maximize the buyer"s profit for some combination of auctions and bids of other buyers.9 Within the interesting bundles, say that an item has uncertain marginal value if the marginal value of an item depends on the other goods held by the buyer.10 Say that an item is oversupplied if there is more than one auction offering an item of that type. Say two bundles are substitutes if one of those bundles has the same value as the union of both bundles.11 Proposition 1. Given locally strategyproof single-item auctions, the sequential auction problem exists for a bidder if and only if either of the following two conditions is true: (1) within the set of interesting bundles (a) there are two bundles that are substitutes, (b) there is an item with uncertain marginal value, or (c) there is an item that is over-supplied; (2) a bidder faces competitors" bids that are conditioned on the bidder"s past bids.
Proof. (Sketch.)(⇐) A bidder does not have a dominant strategy when (a) she does not know which bundle among substitutes to pursue, (b) she faces the exposure problem, or (c) she faces the multiple copies problem. Additionally, a bidder does not have a dominant strategy when she does not how to optimally influence the bids of competitors.(⇒) By contradiction. A bidder has a dominant strategy to bid its constant marginal value for a given item in each auction available when conditions (1) and (2) are both false.
For example, the following buyers all face the sequential auction problem as a result of condition (a), (b) and (c) respectively: a buyer who values one ton of Sand for $1,000, or one ton of Stone for $2,000, but not both Sand and Stone; a buyer who values one ton of Sand for $1,000, one ton of Stone for $300, and one ton of Sand and one ton of Stone for $1,500, and can participate in an auction for Sand before an auction for Stone; a buyer who values one ton of Sand for $1,000 and can participate in many auctions selling Sand. 9 Assume that the empty set is an interesting bundle. 10 Formally, an item k has uncertain marginal value if |{m : m = vi(Q) − vi(Q − k), ∀Q ⊆ L ∈ InterestingBundle, Q ⊇ k}| > 1. 11 Formally, two bundles A and B are substitutes if vi(A ∪ B) = max(vi(A), vi(B)), where A ∪ B = L where Lk = max(Ak, Bk).
The novel solution proposed in this work to resolve the sequential auction problem consists of two primary components: richer proxy agents, and options with price matching.
In finance, a real option is a right to acquire a real good at a certain price, called the exercise price. For instance, Alice may obtain from Bob the right to buy Sand from him at an exercise price of $1, 000. An option provides the right to purchase a good at an exercise price but not the obligation.
This flexibility allows buyers to put together a collection of options on goods and then decide which to exercise.
Options are typically sold at a price called the option price. However, options obtained at a non-zero option price cannot generally support a simple, dominant bidding strategy, as a buyer must compute the expected value of an option to justify the cost [8]. This computation requires a model of the future, which in our setting requires a model of the bidding strategies and the values of other bidders.
This is the very kind of game-theoretic reasoning that we want to avoid.
Instead, we consider costless options with an option price of zero. This will require some care as buyers are weakly better off with a costless option than without one, whatever its exercise price. However, multiple bidders pursuing options with no intention of exercising them would cause the efficiency of an auction for options to unravel. This is the role of the mandatory proxy agents, which intermediate between buyers and the market. A proxy agent forces a link between the valuation function used to acquire options and the valuation used to exercise options. If a buyer tells her proxy an inflated value for an item, she runs the risk of having the proxy exercise options at a price greater than her value.
After her arrival, a buyer submits her valuation ˆvi (perhaps untruthfully) to her proxy in some period ˆai ≥ ai, along with a claim about her departure time ˆdi ≥ ˆai. All transactions are intermediated via proxy agents. Each auction is modified to sell an option on that good to the highest bidding proxy, with an initial exercise price set to the second-highest bid received.12 When an option in which a buyer is interested becomes available for the first time, the proxy determines its bid by computing the buyer"s maximum marginal value for the item, and then submits a bid in this amount. A proxy does not bid for an item when it already holds an option. The bid price is: bidt i(k) = max L [ˆvi(L + k) − ˆvi(L)] (1) By having a proxy compute a buyer"s maximum marginal value for an item and then bidding only that amount, a buyer"s proxy will win any auction that could possibly be of benefit to the buyer and only lose those auctions that could never be of value to the buyer. 12 The system can set a reserve price for each good, provided that the reserve is universal for all auctions selling the same item. Without a universal reserve price, price matching is not possible because of the additional restrictions on prices that individual sellers will accept. 183 Buyer Type Monday Tuesday Molly (Mon, Tues, $8) 6Nancy 6Nancy → 4Polly Nancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Table 1: Three-buyer example with each wanting a single item and one auction occurring on Monday and Tuesday. XY  implies an option with exercise price X and bookkeeping that a proxy has prevented Y from currently possessing an option. → is the updating of exercise price and bookkeeping.
When a proxy wins an auction for an option, the proxy will store in its local memory the identity (which may be a pseudonym) of the proxy not holding an option because of the proxy"s win (i.e., the proxy that it ‘bumped" from winning, if any). This information will be used for price matching.
Sellers agree by joining the market to allow the proxy representing a buyer to adjust the exercise price of an option that it holds downwards if the proxy discovers that it could have achieved a better price by waiting to bid in a later auction for an option on the same good. To assist in the implementation of the price matching scheme each proxy tracks future auctions for an option that it has already won and will determine who would be bidding in that auction had the proxy delayed its entry into the market until this later auction. The proxy will request price matching from the seller that granted it an option if the proxy discovers that it could have secured a lower price by waiting. To reiterate, the proxy does not acquire more than one option for any good. Rather, it reduces the exercise price on its already issued option if a better deal is found.
The proxy is able to discover these deals by asking each future auction to report the identities of the bidders in that auction together with their bids. This needs to be enforced by eBay, as the central authority. The highest bidder in this later auction, across those whose identity is not stored in the proxy"s memory for the given item, is exactly the bidder against whom the proxy would be competing had it delayed its entry until this auction. If this high bid is lower than the current option price held, the proxy price matches down to this high bid price.
After price matching, one of two adjustments will be made by the proxy for bookkeeping purposes. If the winner of the auction is the bidder whose identity has been in the proxy"s local memory, the proxy will replace that local information with the identity of the bidder whose bid it just price matched, as that is now the bidder the proxy has prevented from obtaining an option. If the auction winner"s identity is not stored in the proxy"s local memory the memory may be cleared. In this case, the proxy will simply price match against the bids of future auction winners on this item until the proxy departs.
Example 2 (Table 1). Molly"s proxy wins the Monday auction, submitting a bid of $8 and receiving an option for $6. Molly"s proxy adds Nancy to its local memory as Nancy"s proxy would have won had Molly"s proxy not bid.
On Tuesday, only Nancy"s and Polly"s proxy bid (as Molly"s proxy holds an option), with Nancy"s proxy winning an opBuyer Type Monday Tuesday Truth: Molly (Mon, Mon, $8) 6NancyNancy (Mon, Tues, $6) - 4Polly Polly (Mon, Tues, $4) -Misreport: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8Molly 8Molly → 4φ Polly (Mon, Tues, $4) - 0φ Misreport & match low: Molly (Mon, Mon, $8) -Nancy (Mon, Tues, $10) 8 8 → 0 Polly (Mon, Tues, $4) - 0 Table 2: Examples demonstrating why bookkeeping will lead to a truthful system whereas simply matching to the lowest winning price will not. tion for $4 and noting that it bumped Polly"s proxy. At this time, Molly"s proxy will price match its option down to $4 and replace Nancy with Polly in its local memory as per the price match algorithm, as Polly would be holding an option had Molly never bid.
At the reported departure time the proxy chooses which options to exercise. Therefore, a seller of an option must wait until period ˆdw for the option to be exercised and receive payment, where w was the winner of the option.13 For bidder i, in period ˆdi, the proxy chooses the option(s) that maximize the (reported) utility of the buyer: θ∗ t = argmax θ⊆Θ (ˆvi(γ(θ)) − π(θ)) (2) where Θ is the set of all options held, γ(θ) are the goods corresponding to a set of options, and π(θ) is the sum of exercise prices for a set of options. All other options are returned.14 No options are exercised when no combination of options have positive utility.
One may believe that an alternative method for implementing a price matching scheme could be to simply have proxies match the lowest winning price they observe after winning an option. However, as demonstrated in Table 2, such a simple price matching scheme will not lead to a truthful system.
The first scenario in Table 2 demonstrates the outcome if all agents were to truthfully report their types. Molly 13 While this appears restrictive on the seller, we believe it not significantly different than what sellers on eBay currently endure in practice. An auction on eBay closes at a specific time, but a seller must wait until a buyer relinquishes payment before being able to realize the revenue, an amount of time that could easily be days (if payment is via a money order sent through courier) to much longer (if a buyer is slow but not overtly delinquent in remitting her payment). 14 Presumably, an option returned will result in the seller holding a new auction for an option on the item it still possesses. However, the system will not allow a seller to re-auction an option until ΔMax after the option had first been issued in order to maintain a truthful mechanism. 184 would win the Monday auction and receive an option with an exercise price of $6 (subsequently exercising that option at the end of Monday), and Nancy would win the Tuesday auction and receive an option with an exercise price of $4 (subsequently exercising that option at the end of Tuesday).
The second scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, using the proposed bookkeeping method. Nancy would win the Monday auction and receive an option with an exercise price of $8. On Tuesday, Polly would win the auction and receive an option with an exercise price of $0. Nancy"s proxy would observe that the highest bid submitted on Tuesday among those proxies not stored in local memory is Polly"s bid of $4, and so Nancy"s proxy would price match the exercise price of its option down to $4. Note that the exercise price Nancy"s proxy has obtained at the end of Tuesday is the same as when she truthfully revealed her type to her proxy.
The third scenario in Table 2 demonstrates the outcome if Nancy were to misreport her value for the good by reporting an inflated value of $10, if the price matching scheme were for proxies to simply match their option price to the lowest winning price at any time while they are in the system.
Nancy would win the Monday auction and receive an option with an exercise price of $8. On Tuesday, Polly would win the auction and receive an option with an exercise price of $0. Nancy"s proxy would observe that the lowest price on Tuesday was $0, and so Nancy"s proxy would price match the exercise price of its option down to $0. Note that the exercise price Nancy"s proxy has obtained at the end of Tuesday is lower than when she truthfully revealed her type to the proxy.
Therefore, a price matching policy of simply matching the lowest price paid may not elicit truthful information from buyers.
An XOR-valuation of size M for buyer i is a set of M terms, < L1 , v1 i > ...< LM , vM i >, that maps distinct bundles to values, where i is interested in acquiring at most one such bundle. For any bundle S, vi(S) = maxLm⊆S(vm i ).
Theorem 1. Given an XOR-valuation which possesses M terms, there is an O(KM2 ) algorithm for computing all maximum marginal values, where K is the number of different item types in which a buyer may be interested.
Proof. For each item type, recall Equation 1 which defines the maximum marginal value of an item. For each bundle L in the M-term valuation, vi(L + k) may be found by iterating over the M terms. Therefore, the number of terms explored to determine the maximum marginal value for any item is O(M2 ), and so the total number of bundle comparisons to be performed to calculate all maximum marginal values is O(KM2 ).
Theorem 2. The total memory required by a proxy for implementing price matching is O(K), where K is the number of distinct item types. The total work performed by a proxy to conduct price matching in each auction is O(1).
Proof. By construction of the algorithm, the proxy stores one maximum marginal value for each item for bidding, of which there are O(K); at most one buyer"s identity for each item, of which there are O(K); and one current option exercise price for each item, of which there are O(K). For each auction, the proxy either submits a precomputed bid or price matches, both of which take O(1) work.
Proxies transform the market into a direct revelation mechanism, where each buyer i interacts with the proxy only once,15 and does so by declaring a bid, bi, which is defined as an announcement of her type, (ˆai, ˆdi, ˆvi), where the announcement may or may not be truthful. We denote all received bids other than i"s as b−i. Given bids, b = (bi, b−i), the market determines allocations, xi(b), and payments, pi(b) ≥ 0, to each buyer (using an online algorithm).
A dominant strategy equilibrium for buyers requires that vi(xi(bi, b−i))−pi(bi, b−i) ≥ vi(xi(bi, b−i))−pi(bi, b−i), ∀bi = bi, ∀b−i.
We now establish that it is a dominant strategy for a buyer to reveal her true valuation and true departure time to her proxy agent immediately upon arrival to the system. The proof builds on the price-based characterization of strategyproof single-item online auctions in Hajiaghayi et al. [12].
Define a monotonic and value-independent price function psi(ai, di, L, v−i) which can depend on the values of other agents v−i. Price psi(ai, di, L, v−i) will represent the price available to agent i for bundle L in the mechanism if it announces arrival time ai and departure time di. The price is independent of the value vi of agent i, but can depend on ai, di and L as long as it satisfies a monotonicity condition.
Definition 2. Price function psi(ai, di, L, v−i) is monotonic if psi(ai, di, L , v−i) ≤ psi(ai, di, L, v−i) for all ai ≤ ai, all di ≥ di, all bundles L ⊆ L and all v−i.
Lemma 1. An online combinatorial auction will be strategyproof (with truthful reports of arrival, departure and value a dominant strategy) when there exists a monotonic and value-independent price function, psi(ai, di, L, v−i), such that for all i and all ai, di ∈ T and all vi, agent i is allocated bundle L∗ = argmaxL [vi(L) − psi(ai, di, L, v−i)] in period di and makes payment psi(ai, di, L∗ , v−i).
Proof. Agent i cannot benefit from reporting a later departure ˆdi because the allocation is made in period ˆdi and the agent would have no value for this allocation. Agent i cannot benefit from reporting a later arrival ˆai ≥ ai or earlier departure ˆdi ≤ di because of price monotonicity.
Finally, the agent cannot benefit from reporting some ˆvi = vi because its reported valuation does not change the prices it faces and the mechanism maximizes its utility given its reported valuation and given the prices.
Lemma 2. At any given time, there is at most one buyer in the system whose proxy does not hold an option for a given item type because of buyer i"s presence in the system, and the identity of that buyer will be stored in i"s proxy"s local memory at that time if such a buyer exists.
Proof. By induction. Consider the first proxy that a buyer prevents from winning an option. Either (a) the 15 For analysis purposes, we view the mechanism as an opaque market so that the buyer cannot condition her bid on bids placed by others. 185 bumped proxy will leave the system having never won an option, or (b) the bumped proxy will win an auction in the future. If (a), the buyer"s presence prevented exactly that one buyer from winning an option, but will have not prevented any other proxies from winning an option (as the buyer"s proxy will not bid on additional options upon securing one), and will have had that bumped proxy"s identity in its local memory by definition of the algorithm. If (b), the buyer has not prevented the bumped proxy from winning an option after all, but rather has prevented only the proxy that lost to the bumped proxy from winning (if any), whose identity will now be stored in the proxy"s local memory by definition of the algorithm. For this new identity in the buyer"s proxy"s local memory, either scenario (a) or (b) will be true, ad infinitum.
Given this, we show that the options-based infrastructure implements a price-based auction with a monotonic and value-independent price schedule to every agent.
Theorem 3. Truthful revelation of valuation, arrival and departure is a dominant strategy for a buyer in the optionsbased market.
Proof. First, define a simple agent-independent price function pk i (t, v−i) as the highest bid by the proxies not holding an option on an item of type Gk at time t, not including the proxy representing i herself and not including any proxies that would have already won an option had i never entered the system (i.e., whose identity is stored in i"s proxy"s local memory)(∞ if no supply at t). This set of proxies is independent of any declaration i makes to its proxy (as the set explicitly excludes the at most one proxy (see Lemma 2) that i has prevented from holding an option), and each bid submitted by a proxy within this set is only a function of their own buyer"s declared valuation (see Equation 1). Furthermore, i cannot influence the supply she faces as any options returned by bidders due to a price set by i"s proxy"s bid will be re-auctioned after i has departed the system. Therefore, pk i (t, v−i) is independent of i"s declaration to its proxy. Next, define psk i (ˆai, ˆdi, v−i) = minˆai≤τ≤ ˆdi [pk i (τ, v−i)] (possibly ∞) as the minimum price over pk i (t, v−i), which is clearly monotonic.
By construction of price matching, this is exactly the price obtained by a proxy on any option that it holds at departure. Define psi(ˆai, ˆdi, L, v−i) = Èk=K k=1 psk i (ˆai, ˆdi, v−i)Lk, which is monotonic in ˆai, ˆdi and L since psk i (ˆai, ˆdi, v−i) is monotonic in ˆai and ˆdi and (weakly) greater than zero for each k. Given the set of options held at ˆdi, which may be a subset of those items with non-infinite prices, the proxy exercises options to maximize the reported utility. Left to show is that all bundles that could not be obtained with options held are priced sufficiently high as to not be preferred. For each such bundle, either there is an item priced at ∞ (in which case the bundle would not be desired) or there must be an item in that bundle for which the proxy does not hold an option that was available. In all auctions for such an item there must have been a distinct bidder with a bid greater than bidt i(k), which subsequently results in psk i (ˆai, ˆdi, v−i) > bidt i(k), and so the bundle without k would be preferred to the bundle.
Theorem 4. The super proxy, options-based scheme is individually-rational for both buyers and sellers.
Price σ(Price) Value Surplus eBay $240.24 $32 $244 $4 Options $239.66 $12 $263 $23 Table 3: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as the simulated options-based market using worst-case estimates of bidders" true value.
Proof. By construction, the proxy exercises the profit maximizing set of options obtained, or no options if no set of options derives non-negative surplus. Therefore, buyers are guaranteed non-negative surplus by participating in the scheme. For sellers, the price of each option is based on a non-negative bid or zero.
INFRASTRUCTURE A goal of the empirical benchmarking and a reason to collect data from eBay is to try and build a realistic model of buyers from which to estimate seller revenue and other market effects under the options-based scheme.
We simulate a sequence of auctions that match the timing of the Dell LCD auctions on eBay.16 When an auction successfully closes on eBay, we simulate a Vickrey auction for an option on the item sold in that period. Auctions that do not successfully close on eBay are not simulated. We estimate the arrival, departure and value of each bidder on eBay from their observed behavior.17 Arrival is estimated as the first time that a bidder interacts with the eBay proxy, while departure is estimated as the latest closing time among eBay auctions in which a bidder participates.
We initially adopt a particularly conservative estimate for bidder value, estimating it as the highest bid a bidder was observed to make on eBay. Table 3 compares the distribution of closing prices on eBay and in the simulated options scheme. While the average revenue in both schemes is virtually the same ($239.66 in the options scheme vs. $240.24 on eBay), the winners in the options scheme tend to value the item won 7% more than the winners on eBay ($263 in the options scheme vs. $244 on eBay).
We extend the work of Haile and Tamer [11] to sequential auctions to get a better view of underlying bidder values.
Rather than assume for bidders an equilibrium behavior as in standard econometric techniques, Haile and Tamer do not attempt to model how bidders" true values get mapped into a bid in any given auction. Rather, in the context of repeated 16 When running the simulations, the results of the first and final ten days of auctions are not recorded to reduce edge effects that come from viewing a discrete time window of a continuous process. 17 For the 100 bidders that won multiple times on eBay, we have each one bid a constant marginal value for each additional item in each auction until the number of options held equals the total number of LCDs won on eBay, with each option available for price matching independently. This bidding strategy is not a dominant strategy (falling outside the type space possible for buyers on which the proof of truthfulness has been built), but is believed to be the most appropriate first order action for simulation. 186
0
1 Value ($) CDF Observed Max Bids Upper Bound of True Value Figure 2: CDF of maximum bids observed and upper bound estimate of the bidding population"s distribution for maximum willingness to pay. The true population distribution lies below the estimated upper bound. single-item auctions with distinct bidder populations, Haile and Tamer make only the following two assumptions when estimating the distribution of true bidder values:
they are willing to beat.
From the first of their two assumptions, given the bids placed by each bidder in each auction, Haile and Tamer derive a method for estimating an upper bound of the bidding population"s true value distribution (i.e., the bound that lies above the true value distribution). From the second of their two assumptions, given the winning price of each auction,
Haile and Tamer derive a method for estimating a lower bound of the bidding population"s true value distribution.
It is only the upper-bound of the distribution that we utilize in our work.
Haile and Tamer assume that bidders only participate in a single auction, and require independence of the bidding population from auction to auction. Neither assumption is valid here: the former because bidders are known to bid in more than one auction, and the latter because the set of bidders in an auction is in all likelihood not a true i.i.d. sampling of the overall bidding population. In particular, those who win auctions are less likely to bid in successive auctions, while those who lose auctions are more likely to remain bidders in future auctions.
In applying their methods we make the following adjustments: • Within a given auction, each individual bidder"s true willingness to pay is assumed weakly greater than the maximum bid that bidder submits across all auctions for that item (either past or future). • When estimating the upper bound of the value distribution, if a bidder bids in more than one auction, randomly select one of the auctions in which the bidder bid, and only utilize that one observation during the estimation.18 18 In current work, we assume that removing duplicate bidders is sufficient to make the buying populations independent i.i.d. draws from auction to auction. If one believes that certain portions of the population are drawn to cerPrice σ(Price) Value Surplus eBay $240.24 $32 $281 $40 Options $275.80 $14 $302 $26 Table 4: Average price paid, standard deviation of prices paid, average bidder value among winners, and average winning bidder surplus on eBay for Dell E193FP LCD screens as well as in the simulated options-based market using an adjusted Haile and Tamer estimate of bidders" true values being 15% higher than their maximum observed bid.
Figure 2 provides the distribution of maximum bids placed by bidders on eBay as well as the estimated upper bound of the true value distribution of bidders based on the extended Haile and Tamer method.19 As can be seen, the smallest relative gap between the two curves meaningfully occurs near the 80th percentile, where the upper bound is 1.17 times the maximum bid. Therefore, adopted as a less conservative model of bidder values is a uniform scaling factor of
We now present results from this less conservative analysis. Table 4 shows the distribution of closing prices in auctions on eBay and in the simulated options scheme. The mean price in the options scheme is now significantly higher, 15% greater, than the prices on eBay ($276 in the options scheme vs. $240 on eBay), while the standard deviation of closing prices is lower among the options scheme auctions ($14 in the options scheme vs. $32 on eBay). Therefore, not only is the expected revenue stream higher, but the lower variance provides sellers a greater likelihood of realizing that higher revenue.
The efficiency of the options scheme remains higher than on eBay. The winners in the options scheme now have an average estimated value 7.5% higher at $302.
In an effort to better understand this efficiency, we formulated a mixed integer program (MIP) to determine a simple estimate of the allocative efficiency of eBay. The MIP computes the efficient value of the oﬄine problem with full hindsight on all bids and all supply.20 Using a scaling of 1.15, the total value allocated to eBay winners is estimated at $551,242, while the optimal value (from the MIP) is $593,301. This suggests an allocative efficiency of 92.9%: while the typical value of a winner on eBay is $281, an average value of $303 was possible.21 Note the options-based tain auctions, then further adjustments would be required in order to utilize these techniques. 19 The estimation of the points in the curve is a minimization over many variables, many of which can have smallnumbers bias. Consequently, Haile and Tamer suggest using a weighted average over all terms yi of È i yi exp(yiρ)Èj exp(yj ρ) to approximate the minimum while reducing the small number effects. We used ρ = −1000 and removed observations of auctions with 17 bidders or more as they occurred very infrequently. However, some small numbers bias still demonstrated itself with the plateau in our upper bound estimate around a value of $300. 20 Buyers who won more than one item on eBay are cloned so that they appear to be multiple bidders of identical type. 21 As long as one believes that every bidder"s true value is a constant factor α away from their observed maximum bid, the 92.9% efficiency calculation holds for any value of α. In practice, this belief may not be reasonable. For example, if losing bidders tend to have true values close to their observed 187 scheme comes very close to achieving this level of efficiency [at 99.7% efficient in this estimate] even though it operates without the benefit of hindsight.
Finally, although the typical winning bidder surplus decreases between eBay and the options-based scheme, some surplus redistribution would be possible because the total market efficiency is improved.22
The biggest concern with our scheme is that proxy agents who may be interested in many different items may acquire many more options than they finally exercise. This can lead to efficiency loss. Notice that this is not an issue when bidders are only interested in a single item (as in our empirical study), or have linear-additive values on items.
To fix this, we would prefer to have proxy agents use more caution in acquiring options and use a more adaptive bidding strategy than that in Equation 1. For instance, if a proxy is already holding an option with an exercise price of $3 on some item for which it has value of $10, and it values some substitute item at $5, the proxy could reason that in no circumstance will it be useful to acquire an option on the second item.
We formulate a more sophisticated bidding strategy along these lines. Let Θt be the set of all options a proxy for bidder i already possesses at time t. Let θt ⊆ Θt, be a subset of those options, the sum of whose exercise prices are π(θt), and the goods corresponding to those options being γ(θt).
Let Π(θt) = ˆvi(γ(θt)) − π(θt) be the (reported) available surplus associated with a set of options. Let θ∗ t be the set of options currently held that would maximize the buyer"s surplus; i.e., θ∗ t = argmaxθt⊆Θt Π(θt).
Let the maximal willingness to pay for an item k represent a price above which the agent knows it would never exercise an option on the item given the current options held. This can be computed as follows: bidt i(k) = max L [0, min[ˆvi(L + k) − Π(θ∗ t ), ˆvi(L + k) − ˆvi(L)]] (3) where ˆvi(L+k)−Π(θ∗ t ) considers surplus already held, ˆvi(L+ k)−ˆvi(L) considers the marginal value of a good, and taking the max[0, .] considers the overall use of pursuing the good.
However, and somewhat counter intuitively, we are not able to implement this bidding scheme without forfeiting truthfulness. The Π(θ∗ t ) term in Equation 3 (i.e., the amount of guaranteed surplus bidder i has already obtained) can be influenced by proxy j"s bid. Therefore, bidder j may have the incentive to misrepresent her valuation to her proxy if she believes doing so will cause i to bid differently in the future in a manner beneficial to j. Consider the following example where the proxy scheme is refined to bid the maximum willingness to pay.
Example 3. Alice values either one ton of Sand or one ton of Stone for $2,000. Bob values either one ton of Sand or one ton of Stone for $1,500. All bidders have a patience maximum bids while eBay winners have true values much greater than their observed maximum bids then downward bias is introduced in the efficiency calculation at present. 22 The increase in eBay winner surplus between Tables 3 and
estimated value of the eBay winners while holding the prices at which they won constant. of 2 days. On day one, a Sand auction is held, where Alice"s proxy bids $2,000 and Bob"s bids $1,500. Alice"s proxy wins an option to purchase Sand for $1,500. On day two, a Stone auction is held, where Alice"s proxy bids $1,500 [as she has already obtained a guaranteed $500 of surplus from winning a Sand option, and so reduces her Stone bid by this amount], and Bob"s bids $1,500. Either Alice"s proxy or Bob"s proxy will win the Stone option. At the end of the second day,
Alice"s proxy holds an option with an exercise price of $1,500 to obtain a good valued for $2,000, and so obtains $500 in surplus.
Now, consider what would have happened had Alice declared that she valued only Stone.
Example 4. Alice declares valuing only Stone for $2,000.
Bob values either one ton of Sand or one ton of Stone for $1,500. All bidders have a patience of 2 days. On day one, a Sand auction is held, where Bob"s proxy bids $1,500. Bob"s proxy wins an option to purchase Sand for $0. On day two, a Stone auction is held, where Alice"s proxy bids $2,000, and Bob"s bids $0 [as he has already obtained a guaranteed $1,500 of surplus from winning a Sand option, and so reduces his Stone bid by this amount]. Alice"s proxy wins the Stone option for $0. At the end of the second day, Alice"s proxy holds an option with an exercise price of $0 to obtain a good valued for $2,000, and so obtains $2,000 in surplus.
By misrepresenting her valuation (i.e., excluding her value of Sand), Alice was able to secure higher surplus by guiding Bob"s bid for Stone to $0. An area of immediate further work by the authors is to develop a more sophisticated proxy agent that can allow for bidding of maximum willingness to pay (Equation 3) while maintaining truthfulness.
An additional, practical, concern with our proxy scheme is that we assume an available, trusted, and well understood method to characterize goods (and presumably the quality of goods). We envision this happening in practice by sellers defining a classification for their item upon entering the market, for instance via a UPC code. Just as in eBay, this would allow an opportunity for sellers to improve revenue by overstating the quality of their item (new vs. like new), and raises the issue of how well a reputation scheme could address this.
We introduced a new sales channel, consisting of an optionsbased and proxied auction protocol, to address the sequential auction problem that exists when bidders face multiple auctions for substitutes and complements goods. Our scheme provides bidders with a simple, dominant and truthful bidding strategy even though the market remains open and dynamic.
In addition to exploring more sophisticated proxies that bid in terms of maximum willingness to pay, future work should aim to better model seller incentives and resolve the strategic problems facing sellers. For instance, does the options scheme change seller incentives from what they currently are on eBay?
Acknowledgments We would like to thank Pai-Ling Yin. Helpful comments have been received from William Simpson, attendees at Har188 vard University"s EconCS and ITM seminars, and anonymous reviewers. Thank you to Aaron L. Roth and KangXing Jin for technical support. All errors and omissions remain our own.
[1] P. Anthony and N. R. Jennings. Developing a bidding agent for multiple heterogeneous auctions. ACM Trans. On Internet Technology, 2003. [2] R. Bapna, P. Goes, A. Gupta, and Y. Jin. User heterogeneity and its impact on electronic auction market design: An empirical exploration. MIS Quarterly, 28(1):21-43, 2004. [3] D. Bertsimas, J. Hawkins, and G. Perakis. Optimal bidding in on-line auctions. Working Paper, 2002. [4] C. Boutilier, M. Goldszmidt, and B. Sabata.
Sequential auctions for the allocation of resources with complementarities. In Proc. 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pages 527-534, 1999. [5] A. Byde, C. Preist, and N. R. Jennings. Decision procedures for multiple auctions. In Proc. 1st Int.
Joint Conf. on Autonomous Agents and Multiagent Systems (AAMAS-02), 2002. [6] M. M. Bykowsky, R. J. Cull, and J. O. Ledyard.
Mutually destructive bidding: The FCC auction design problem. Journal of Regulatory Economics, 17(3):205-228, 2000. [7] Y. Chen, C. Narasimhan, and Z. J. Zhang. Consumer heterogeneity and competitive price-matching guarantees. Marketing Science, 20(3):300-314, 2001. [8] A. K. Dixit and R. S. Pindyck. Investment under Uncertainty. Princeton University Press, 1994. [9] R. Gopal, S. Thompson, Y. A. Tung, and A. B.
Whinston. Managing risks in multiple online auctions: An options approach. Decision Sciences, 36(3):397-425, 2005. [10] A. Greenwald and J. O. Kephart. Shopbots and pricebots. In Proc. 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pages 506-511, 1999. [11] P. A. Haile and E. Tamer. Inference with an incomplete model of English auctions. Journal of Political Economy, 11(1), 2003. [12] M. T. Hajiaghayi, R. Kleinberg, M. Mahdian, and D. C. Parkes. Online auctions with re-usable goods. In Proc. ACM Conf. on Electronic Commerce, 2005. [13] K. Hendricks, I. Onur, and T. Wiseman. Preemption and delay in eBay auctions. University of Texas at Austin Working Paper, 2005. [14] A. Iwasaki, M. Yokoo, and K. Terada. A robust open ascending-price multi-unit auction protocol against false-name bids. Decision Support Systems, 39:23-40,
[15] E. G. James D. Hess. Price-matching policies: An empirical case. Managerial and Decision Economics, 12(4):305-315, 1991. [16] A. X. Jiang and K. Leyton-Brown. Estimating bidders" valuation distributions in online auctions. In Workshop on Game Theory and Decision Theory (GTDT) at IJCAI, 2005. [17] R. Lavi and N. Nisan. Competitive analysis of incentive compatible on-line auctions. In Proc. 2nd ACM Conf. on Electronic Commerce (EC-00), 2000. [18] Y. J. Lin. Price matching in a model of equilibrium price dispersion. Southern Economic Journal, 55(1):57-69, 1988. [19] D. Lucking-Reiley and D. F. Spulber.
Business-to-business electronic commerce. Journal of Economic Perspectives, 15(1):55-68, 2001. [20] A. Ockenfels and A. Roth. Last-minute bidding and the rules for ending second-price auctions: Evidence from eBay and Amazon auctions on the Internet.
American Economic Review, 92(4):1093-1103, 2002. [21] M. Peters and S. Severinov. Internet auctions with many traders. Journal of Economic Theory (Forthcoming), 2005. [22] R. Porter. Mechanism design for online real-time scheduling. In Proceedings of the 5th ACM conference on Electronic commerce, pages 61-70. ACM Press,
[23] M. H. Rothkopf and R. Engelbrecht-Wiggans.
Innovative approaches to competitive mineral leasing.
Resources and Energy, 14:233-248, 1992. [24] T. Sandholm and V. Lesser. Leveled commitment contracts and strategic breach. Games and Economic Behavior, 35:212-270, 2001. [25] T. W. Sandholm and V. R. Lesser. Issues in automated negotiation and electronic commerce: Extending the Contract Net framework. In Proc. 1st International Conference on Multi-Agent Systems (ICMAS-95), pages 328-335, 1995. [26] H. S. Shah, N. R. Joshi, A. Sureka, and P. R.
Wurman. Mining for bidding strategies on eBay.
Lecture Notes on Artificial Intelligence, 2003. [27] M. Stryszowska. Late and multiple bidding in competing second price Internet auctions.

Major search engines like Google, Yahoo!, and MSN sell advertisements by auctioning off space on keyword search results pages. For example, when a user searches the web for iPod, the highest paying advertisers (for example, Apple or Best Buy) for that keyword may appear in a separate sponsored section of the page above or to the right of the algorithmic results. The sponsored results are displayed in a format similar to algorithmic results: as a list of items each containing a title, a text description, and a hyperlink to a web page. Generally, advertisements that appear in a higher position on the page garner more attention and more clicks from users. Thus, all else being equal, advertisers prefer higher positions to lower positions.
Advertisers bid for placement on the page in an auctionstyle format where the larger their bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally bid and pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked.
Overture Services, formerly GoTo.com and now owned by Yahoo! Inc., is credited with pioneering sponsored search advertising. Overture"s success prompted a number of companies to adopt similar business models, most prominently Google, the leading web search engine today. Microsoft"s MSN, previously an affiliate of Overture, now operates its own keyword auction marketplace. Sponsored search is one of the fastest growing, most effective, and most profitable forms of advertising, generating roughly $7 billion in revenue in 2005 after nearly doubling every year for the previous five years.
The search engine evaluates the advertisers" bids and allocates the positions on the page accordingly. Notice that, although bids are expressed as payments per click, the search engine cannot directly allocate clicks, but rather allocates impressions, or placements on the screen. Clicks relate only stochastically to impressions. Until recently, Yahoo! ranked bidders in decreasing order of advertisers" stated values per click, while Google ranks in decreasing order of advertisers" stated values per impression. In Google"s case, value per impression is computed by multiplying the advertiser"s (perclick) bid by the advertisement"s expected click-through rate, where this expectation may consider a number of unspecified factors including historical click-through rate, position on the page, advertiser identity, user identity, and the context of other items on the page. We refer to these rules as rank-by-bid and rank-by-revenue, respectively.1 We analyze a family of ranking rules that contains the Yahoo! and Google models as special cases. We consider rank1 These are industry terms. We will see, however, that rankby-revenue is not necessarily revenue-optimal. 50 ing rules where bidders are ranked in decreasing order of score eq b, where e denotes an advertiser"s click-through rate (normalized for position) and b his bid. Notice that q = 0 corresponds to Yahoo!"s rank-by-bid rule and q = 1 corresponds to Google"s rank-by-revenue rule. Our premise is that bidders are playing a symmetric equilibrium, as defined by Edelman, Ostrovsky, and Schwarz [3] and Varian [11].
We show through simulation that although q = 1 yields the efficient allocation, settings of q considerably less than 1 can yield superior revenue in equilibrium under certain conditions. The key parameter is the correlation between advertiser value and click-through rate. If this correlation is strongly positive, then smaller q are revenue-optimal. Our simulations are based on distributions fitted to data from Yahoo! keyword auctions. We propose that search engines set thresholds of acceptable loss in advertiser satisfaction and user experience, then choose the revenue-optimal q consistent with these constraints. We also compare the potential gains from tuning q with the gains from setting reserve prices, and find that the former may be much more significant.
In Section 2 we give a formal model of keyword auctions, and establish its equilibrium properties in Section 3. In Section 4 we note that giving agents bidding credits can have the same effect as tuning the ranking rule explicitly. In Section 5 we give a general formulation of the optimal keyword auction design problem as an optimization problem, in a manner analogous to the single-item auction setting. We then provide some theoretical insight into how tuning q can improve revenue, and why the correlation between bidders" values and click-through rates is relevant. In Section 6 we consider the effect of q on advertiser satisfaction and user experience. In Section 7 we describe our simulations and interpret their results.
Related work. As mentioned the papers of Edelman et al. [3] and Varian [11] lay the groundwork for our study.
Both papers independently define an appealing refinement of Nash equilibrium for keyword auctions and analyze its equilibrium properties. They called this refinement locally envy-free equilibrium and symmetric equilibrium, respectively. Varian also provides some empirical analysis.
The general model of keyword auctions used here, where bidders are ranked according to a weight times their bid, was introduced by Aggarwal, Goel, and Motwani [1]. That paper also makes a connection between the revenue of keyword auctions in incomplete information settings with the revenue in symmetric equilibrium. Iyengar and Kumar [5] study the optimal keyword auction design problem in a setting of incomplete information, and also make the connection to symmetric equilibrium. We make use of this connection when formulating the optimal auction design problem in our setting.
The work most closely related to ours is that of Feng,
Bhargava, and Pennock [4]. They were the first to realize that the correlation between bidder values and click-through rates should be a key parameter affecting the revenue performance of various ranking mechanisms. For simplicity, they assume bidders bid their true values, so their model is very different from ours and consequently so are their findings. According to their simulations, rank-by-revenue always (weakly) dominates rank-by-bid in terms of revenue, whereas our results suggest that rank-by-bid may do much better for negative correlations.
Lahaie [8] gives an example that suggests rank-by-bid should yield more revenue when values and click-through rates are positively correlated, whereas rank-by-revenue should do better when the correlation is negative. In this work we make a deeper study of this conjecture.
There are K positions to be allocated among N bidders, where N > K. We assume that the (expected) click-through rate of bidder s in position t is of the form esxt, i.e. separable into an advertiser effect es ∈ [0, 1] and position effect xt ∈ [0, 1]. We assume that x1 > x2 > . . . > xK > 0 and let xt = 0 for t > K. We also refer to es as the relevance of bidder s. It is useful to interpret xt as the probability that an ad in position t will be noticed, and es as the probability that it will be clicked on if noticed.
Bidder s has value vs for each click. Bidders have quasilinear utility, so that the utility to bidder s of obtaining position t at a price of p per click is esxt(vs − p).
A weight ws is associated with agent s, and agents bid for position. If agent s bids bs, his corresponding score is wsbs.
Agents are ranked by score, so that the agent with highest score is ranked first, and so on. We assume throughout that agents are numbered such that agent s obtains position s.
An agent pays per click the lowest bid necessary to retain his position, so that the agent in slot s pays ws+1 ws bs+1. The auctioneer may introduce a reserve score of r, so that an agent"s ad appears only if his score is at least r. For agent s, this translates into a reserve price (minimum bid) of r/ws.
We consider the pure-strategy Nash equilibria of the auction game. This is a full-information concept. The motivation for this choice is that in a keyword auction, bidders are allowed to continuously adjust their bids over time, and hence obtain estimates of their profits in various positions.
As a result it is reasonable to assume that if bids stabilize, bidders should be playing best-responses to each other"s bids [2, 3, 11]. Formally, in a Nash equilibrium of this game the following inequalities hold. esxs „ vs − ws+1 ws bs+1 « ≥ esxt „ vs − wt+1 ws bt+1 « ∀t > s (1) esxs „ vs − ws+1 ws bs+1 « ≥ esxt „ vs − wt ws bt « ∀t < s (2) Inequalities (1) and (2) state that bidder s does not prefer a lower or higher position to his own, respectively. It can be hard to derive any theoretical insight into the properties of these Nash equilibria-multiple allocations of positions to bidders can potentially arise in equilibrium [2].
Edelman, Ostrovsky, and Schwarz [3] introduced a refinement of Nash equilibrium called locally envy-free equilibrium that is more tractable to analyze; Varian [11] independently proposed this solution concept and called it symmetric equilibrium. In a symmetric equilibrium, inequality (1) holds for all s, t rather than just for t > s. So for all s and all t = s, we have esxs „ vs − ws+1 ws bs+1 « ≥ esxt „ vs − wt+1 ws bt+1 « , 51 or equivalently xs(wsvs − ws+1bs+1) ≥ xt(wsvs − wt+1bt+1). (3) Edelman et al. [3] note that this equilibrium arises if agents are raising their bids to increase the payments of those above them, a practice which is believed to be common in actual keyword auctions. Varian [11] provides some empirical evidence that Google bid data agrees well with the hypothesis that bidders are playing a symmetric equilibrium.
Varian does a thorough analysis of the properties of symmetric equilibrium, assuming ws = es = 1 for all bidders.
It is straightforward to adapt his analysis to the case where bidders are assigned arbitrary weights and have separable click-through rates.2 As a result we find that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs.
To be clear, although the auctioneer only has access to the bids bs and not the values vs, in symmetric equilibrium the bids are such that ranking according to wsbs is equivalent to ranking according to wsvs.
The smallest possible bid profile that can arise in symmetric equilibrium is given by the recursion xsws+1bs+1 = (xs − xs+1)ws+1vs+1 + xs+1ws+2bs+2.
In this work we assume that bidders are playing the smallest symmetric equilibrium. This is an appropriate selection for our purposes: by optimizing revenue in this equilibrium, we are optimizing a lower bound on the revenue in any symmetric equilibrium. Unraveling the recursion yields xsws+1bs+1 = KX t=s (xt − xt+1)wt+1vt+1. (4) Agent s"s total expected payment is es/ws times the quantity on the left-hand side of (4). The base case of the recursion occurs for s = K, where we find that the first excluded bidder bids his true value, as in the original analysis.
Multiplying each of the inequalities (4) by the corresponding es/ws to obtain total payments, and summing over all positions, we obtain a total equilibrium revenue of KX s=1 KX t=s wt+1 ws es(xt − xt+1)vt+1. (5) To summarize, the minimum possible revenue in symmetric equilibrium can be computed as follows, given the agents" relevance-value pairs (es, vs): first rank the agents in decreasing order of wsvs, and then evaluate (5).
With a reserve score of r, it follows from inequality (3) that no bidder with wsvs < r would want to participate in the auction. Let K(r) be the number of bidders with wsvs ≥ r, and assume it is at most K. We can impose a reserve score of r by introducing a bidder with value r and weight 1, and making him the first excluded bidder (who in symmetric equilibrium bids truthfully). In this case the recursion yields xsws+1bs+1 = K(r)−1 X t=s (xt − xt+1)wt+1vt+1 + xK(r)r and the revenue formula is adapted similarly. 2 If we redefine wsvs to be vs and wsbs to be bs, we recover Varian"s setup and his original analysis goes through unchanged.
An indirect way to influence the allocation is to introduce bidding credits.3 Suppose bidder s is only required to pay a fraction cs ∈ [0, 1] of the price he faces, or equivalently a (1 − cs) fraction of his clicks are received for free. Then in a symmetric equilibrium, we have esxs „ vs − ws+1 ws csbs+1 « ≥ esxt „ vs − wt+1 ws csbt+1 « or equivalently xs „ ws cs vs − ws+1bs+1 « ≥ xt „ ws cs vs − wt+1bt+1 « .
If we define ws = ws cs and bs = csbs, we recover inequality (3). Hence the equilibrium revenue will be as if we had used weights w rather than w. The bids will be scaled versions of the bids that arise with weights w (and no credits), where each bid is scaled by the corresponding factor 1/cs.
This technique allows one to use credits instead of explicit changes in the weights to affect revenue. For instance, rankby-revenue will yield the same revenue as rank-by-bid if we set credits to cs = es.
We are interested in setting the weights w to achieve optimal expected revenue. The setup is as follows. The auctioneer chooses a function g so that the weighting scheme is ws ≡ g(es). We do not consider weights that also depend on the agents" bids because this would invalidate the equilibrium analysis of the previous section.4 A pool of N bidders is then obtained by i.i.d. draws of value-relevance pairs from a common probability density f(es, vs). We assume the density is continuous and has full support on [0, 1]×[0, ∞). The revenue to the auctioneer is then the revenue generated in symmetric equilibrium under weighting scheme w. This assumes the auctioneer is patient enough not to care about revenue until bids have stabilized.
The problem of finding an optimal weighting scheme can be formulated as an optimization problem very similar to the one derived by Myerson [9] for the single-item auction case (with incomplete information). Let Qsk(e, v; w) = 1 if agent s obtains slot k in equilibrium under weighting scheme w, where e = (e1, . . . , eN ) and v = (v1, . . . , vN ), and let it be 0 otherwise.
Note that the total payment of agent s in equilibrium is esxs ws+1 ws bs+1 = KX t=s es(xt − xt+1) wt+1 ws vt+1 = esxsvs − Z vs 0 KX k=1 esxkQsk(es, e−s, y, v−s; w) dy.
The derivation then continues just as in the case of a singleitem auction [7, 9]. We take the expectation of this payment, 3 Hal Varian suggested to us that bidding credits could be used to affect revenue in keyword auctions, which prompted us to look into this connection. 4 The analysis does not generalize to weights that depend on bids. It is unclear whether an equilibrium would exist at all with such weights. 52 and sum over all agents to obtain the objective Z ∞ 0 Z ∞ 0 " NX s=1 KX k=1 esxkψ(es, vs)Qsk(e, v; w) # f(e, v) dv de, where ψ is the virtual valuation ψ(es, vs) = vs − 1 − F(vs|es) f(vs|es) .
According to this analysis, we should rank bidders by virtual score esψ(es, vs) to optimize revenue (and exclude any bidders with negative virtual score). However, unlike in the incomplete information setting, here we are constrained to ranking rules that correspond to a certain weighting scheme ws ≡ g(es). We remark that the virtual score cannot be reproduced exactly via a weighting scheme.
Lemma 1. There is no weighting scheme g such that the virtual score equals the score, for any density f.
Proof. Assume there is a g such that eψ(e, v) = g(e)v. (The subscript s is suppressed for clarity.) This is equivalent to d dv log(1 − F(v|e)) = h(e)/v, (6) where h(e) = (g(e)/e−1)−1 . Let ¯v be such that F(¯v|e) < 1; under the assumption of full support, there is always such a ¯v. Integrating (6) with respect to v from 0 to ¯v, we find that the left-hand side converges whereas the right-hand side diverges, a contradiction.
Of course, to rank bidders by virtual score, we only need g(es)vs = h(esψ(es, vs)) for some monotonically increasing transformation h. (A necessary condition for this is that ψ(es, vs) be increasing in vs for all es.) Absent this regularity condition, the optimization problem seems quite difficult because it is so general: we need to maximize expected revenue over the space of all functions g.
To simplify matters, we now restrict our attention to the family of weights ws = eq s for q ∈ (−∞, +∞). It should be much simpler to find the optimum within this family, since it is just one-dimensional. Note that it covers rank-by-bid (q = 0) and rank-by-revenue (q = 1) as special cases.
To see how tuning q can improve matters, consider again the equilibrium revenue: R(q) = KX s=1 KX t=s „ et+1 es «q es(xt − xt+1)vt+1. (7) If the bidders are ranked in decreasing order of relevance, then et es ≤ 1 for t > s and decreasing q slightly without affecting the allocation will increase revenue. Similarly, if bidders are ranked in increasing order of relevance, increasing q slightly will yield an improvement. Now suppose there is perfect positive correlation between value and relevance.
In this case, rank-by-bid will always lead to the same allocation as rank-by-revenue, and bidders will always be ranked in decreasing order of relevance. It then follows from (7) that q = 0 will yield more revenue in equilibrium than q = 1.5 5 It may appear that this contradicts the revenue-equivalence theorem [7, 9], because mechanisms that always lead to the same allocation in equilibrium should yield the same revenue. Note though that with perfect correlation, there are If a good estimate of f is available, Monte-Carlo simulations can be used to estimate the revenue curve as a function of q, and the optimum can be located. Simulations can also be used to quantify the effect of correlation on the location of the optimum. We do this in Section 7.
In principle the revenue-optimal parameter q may lie anywhere in (−∞, ∞). However, tuning the ranking rule also has consequences for advertiser satisfaction and user experience, and taking these into account reduces the range of allowable q.
The total relevance of the equilibrium allocation is L(q) = KX s=1 esxs, i.e. the aggregate click-through rate. Presumably users find the ad display more interesting and less of a nuisance if they are more inclined to click on the ads, so we adopt total relevance as a measure of user experience.
Let ps = ws+1 ws bs+1 be the price per click faced by bidder s. The total value (efficiency) generated by the auction in equilibrium is V (q) = KX s=1 esxsvs = KX s=1 esxs(vs − ps) + KX s=1 esxsps.
As we see total value can be reinterpreted as total profits to the bidders and auctioneer combined. Since we only consider deviations from maximum efficiency that increase the auctioneer"s profits, any decrease in efficiency in our setting corresponds to a decrease in bidder profits. We therefore adopt efficiency as a measure of advertiser satisfaction.
We would expect total relevance to increase with q, since more weight is placed on each bidder"s individual relevance.
We would expect efficiency to be maximized at q = 1, since in this case a bidder"s weight is exactly his relevance.
Proposition 1. Total relevance is non-decreasing in q.
Proof. Recall that in symmetric equilibrium, bidders are ranked in order of decreasing wsvs. Let > 0. Perform an exchange sort to obtain the ranking that arises with q + starting from the ranking that arises with q (for a description of exchange sort and its properties, see Knuth [6] pp. 106110). Assume that is large enough to make the rankings distinct. Agents s and t, where s is initially ranked lower than t, are swapped in the process if and only if the following conditions hold: eq svs ≤ eq t vt eq+ s vs > eq+ t vt which together imply that es > et and hence es > et as > 0. At some point in the sort, agent s occupies some slot α, β such that vs = αes + β. So the assumption of full support is violated, which is necessary for revenue equivalence.
Recall that a density has full support over a given domain if every point in the domain has positive density. 53 k while agent t occupies slot k − 1. After the swap, total relevance will have changed by the amount esxk−1 + etxk − etxk−1 − esxk = (es − et)(xk−1 − xk) > 0 As relevance strictly increases with each swap in the sort, total relevance is strictly greater when using q + rather than q.
Proposition 2. Total value is non-decreasing in q for q ≤ 1 and non-increasing in q for q ≥ 1.
Proof. Let q ≥ 1 and let > 0. Perform an exchange sort to obtain the second ranking from the first as in the previous proof. If agents s and t are swapped, where s was initially ranked lower than t, then es > et. This follows by the same reasoning as in the previous proof. Now e1−q s ≤ e1−q t as 1 − q ≤ 0. This together with eq svs ≤ eq t vt implies that esvs ≤ etvt. Hence after swapping agents s and t, total value has not increased. The case for q ≤ 1 is similar.
Since the trends described in Propositions 1 and 2 hold pointwise (i.e. for any set of bidders), they also hold in expectation. Proposition 2 confirms that efficiency is indeed maximized at q = 1.
These results motivate the following approach. Although tuning q can optimize current revenue, this may come at the price of future revenue because advertisers and users may be lost, seeing as their satisfaction decreases. To guarantee future revenue will not be hurt too much, the auctioneer can impose bounds on the percent efficiency and relevance loss he is willing to tolerate, with q = 1 being a natural baseline. By Proposition 2, a lower bound on efficiency will yield upper and lower bounds on the search space for q. By Proposition 1, a lower bound on relevance will yield another lower bound on q. The revenue curve can then be plotted within the allowable range of q to find the revenue-optimal setting.
To add a measure of reality to our simulations, we fit distributions for value and relevance to Yahoo! bid and clickthrough rate data for a certain keyword that draws over a million searches per month. (We do not reveal the identity of the keyword to respect the privacy of the advertisers.) We obtained click and impression data for the advertisers bidding on the keyword. From this we estimated advertiser and position effects using a maximum-likelihood criterion.
We found that, indeed, position effects are monotonically decreasing with lower rank. We then fit a beta distribution to the advertiser effects resulting in parameters a = 2.71 and b = 25.43.
We obtained bids of advertisers for the keyword. Using Varian"s [11] technique, we derived bounds on the bidders" actual values given these bids. By this technique, upper and lower bounds are obtained on bidder values given the bids according to inequality (3). If the interval for a given value is empty, i.e. its upper bound lies below its lower bound, then we compute the smallest perturbation to the bids necessary to make the interval non-empty, which involves solving a quadratic program. We found that the mean absolute deviation required to fit bids to symmetric equilibrium was 0 1 2 3 4 5 6 7 0
Value Density 0 0.05 0.1 0.15 0.2 0.25 0 2 4 6 8 10 Relevance Density Figure 1: Empirical marginal distributions of value and relevance. always at most 0.08, and usually significantly less, over different days in a period of two weeks.6 We fit a lognormal distribution to the lower bounds on the bidders" values, resulting in parameters μ = 0.35 and σ = 0.71.
The empirical distributions of value and relevance together with the fitted lognormal and beta curves are given in Figure 1. It appears that mixtures of beta and lognormal distributions might be better fits, but since these distributions are used mainly for illustration purposes, we err on the side of simplicity.
We used a Gaussian copula to create dependence between value and relevance.7 Given the marginal distributions for value and relevance together with this copula, we simulated the revenue effect of varying q for different levels of Spearman correlation, with 12 slots and 13 bidders. The results are shown in Figure 2.8 It is apparent from the figure that the optimal choice of q moves to the right as correlation decreases; this agrees with our intuition from Section 5. The choice is very sensitive to the level of correlation. If choosing only between rankby-bid and rank-by-revenue, rank-by-bid is best for positive correlation whereas rank-by-revenue is best for negative correlation. At zero correlation, they give about the same expected revenue in this instance. Figure 2 also shows that in principle, the optimal q may be negative. It may also occur beyond 1 for different distributions, but we do not know if these would be realistic. The trends in efficiency and relevance are as described in the results from Section 6. (Any small deviation from these trends is due to the randomness inherent in the simulations.) The curves level off as q → +∞ because eventually agents are ranked purely according to relevance, and similarly as q → −∞.
A typical Spearman correlation between value and relevance for the keyword was about 0.4-for different days in a week the correlation lay within [0.36, 0.55]. Simulation results with this correlation are in Figure 3. In this instance rank-by-bid is in fact optimal, yielding 25% more revenue than rank-by-revenue. However, at q = 0 efficiency and relevance are 9% and 17% lower than at q = 1, respectively.
Imposing a bound of, say, 5% on efficiency and relevance loss from the baseline at q = 1, the optimal setting is q = 0.6, yielding 11% more revenue than the baseline. 6 See Varian [11] for a definition of mean absolute deviation. 7 A copula is a function that takes marginal distributions and gives a joint distribution with these marginals. It can be designed so that the variables are correlated. See for example Nelsen [10]. 8 The y-axes in Figures 2-4 have been normalized because the simulations are based on proprietary data. Only relative values are meaningful. 54 0 1 2 3 4 5 6 7 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) q Revenue 0 1 2 3 4 5 6 7 8 9 10 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) q Efficiency
1
2
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) q Relevance Figure 2: Revenue, efficiency, and relevance for different parameters q under varying Spearman correlation (key at right). Estimated standard errors are less than 1% of the values shown. -1 -0.5 0
1 1
2
3
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 R(q) q Revenue 2
3
4
5
6
7
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 V(q) q Efficiency
1
2
-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 L(q) q Relevance Figure 3: Revenue, efficiency, and relevance for different parameters q with Spearman correlation of 0.4. Estimated standard errors are less than 1% of the values shown.
We also looked into the effect of introducing a reserve score. Results are shown in Figure 4. Naturally, both efficiency and relevance suffer with an increasing reserve score.
The optimal setting is r = 0.2, which gives only an 8% increase in revenue from r = 0. However, it results in a 13% efficiency loss and a 26% relevance loss. Tuning weights seems to be a much more desirable approach than introducing a reserve score in this instance.
The reason why efficiency and relevance suffer more with a reserve score is that this approach will often exclude bidders entirely, whereas this never occurs when tuning weights.
The two approaches are not mutually exclusive, however, and some combination of the two might prove better than either alone, although we did not investigate this possibility.
In this work we looked into the revenue properties of a family of ranking rules that contains the Yahoo! and Google models as special cases. In practice, it should be very simple to move between rules within the family: this simply involves changing the exponent q applied to advertiser effects.
We also showed that, in principle, the same effect could be obtained by using bidding credits. Despite the simplicity of the rule change, simulations revealed that properly tuning q can significantly improve revenue. In the simulations, the revenue improvements were greater than what could be obtained using reserve prices.
On the other hand, we showed that advertiser satisfaction and user experience could suffer if q is made too small.
We proposed that the auctioneer set bounds on the decrease in advertiser and user satisfaction he is willing to tolerate, which would imply bounds on the range of allowable q. With appropriate estimates for the distributions of value and relevance, and knowledge of their correlation, the revenue curve can then be plotted within this range to locate the optimum.
There are several ways to push this research further. It would be interesting to do this analysis for a variety of keywords, to see if the optimal setting of q is always so sensitive to the level of correlation. If it is, then simply using rank-bybid where there is positive correlation, and rank-by-revenue where there is negative correlation, could be fine to a first approximation and already improve revenue. It would also be interesting to compare the effects of tuning q versus reserve pricing for keywords that have few bidders. In this instance reserve pricing should be more competitive, but this is still an open question.
In principle the minimum revenue in Nash equilibrium can be found by linear programming. However, many allocations can arise in Nash equilibrium, and a linear program needs to be solved for each of these. There is as yet no efficient way to enumerate all possible Nash allocations, so finding the minimum revenue is currently infeasible. If this problem could be solved, we could run simulations for Nash equilibrium instead of symmetric equilibrium, to see if our insights are robust to the choice of solution concept.
Larger classes of ranking rules could be relevant. For instance, it is possible to introduce discounts ds and rank according to wsbs − ds; the equilibrium analysis generalizes to this case as well. With this larger class the virtual score can equal the score, e.g. in the case of a uniform marginal distribution over values. It is unclear, though, whether such extensions help with more realistic distributions. 55 0
1
2
3 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 R(r) r Revenue 0 1 2 3 4 5 6 7 8 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 V(r) r Efficiency 0
1
2
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 L(r) r Relevance Figure 4: Revenue, efficiency, and relevance for different reserve scores r, with Spearman correlation of 0.4 and q = 1. Estimates are averaged over 1000 samples.
Acknowledgements We thank Pavel Berkhin, Chad Carson, Yiling Chen, Ashvin Kannan, Darshan Kantak, Chris LuVogt, Jan Pedersen, Michael Schwarz, Tong Zhang, and other members of Yahoo! Research and Yahoo! Search Marketing.
[1] G. Aggarwal, A. Goel, and R. Motwani. Truthful auctions for pricing search keywords. In Proceedings of the 7th ACM Conference on Electronic Commerce,
Ann Arbor, MI, 2006. [2] T. B¨orgers, I. Cox, M. Pesendorfer, and V. Petricek.
Equilibrium bids in auctions of sponsored links: Theory and evidence. Working paper, November 2006. [3] B. Edelman, M. Ostrovsky, and M. Schwarz. Internet advertising and the Generalized Second Price auction: Selling billions of dollars worth of keywords. American Economic Review, forthcoming. [4] J. Feng, H. K. Bhargava, and D. M. Pennock.
Implementing sponsored search in Web search engines: Computational evaluation of alternative mechanisms.
INFORMS Journal on Computing, forthcoming. [5] G. Iyengar and A. Kumar. Characterizing optimal keyword auctions. In Proceedings of the 2nd Workshop on Sponsored Search Auctions, Ann Arbor, MI, 2006. [6] D. Knuth. The Art of Computer Programming, volume 3. Addison-Wesley, 1997. [7] V. Krishna. Auction Theory. Academic Press, 2002. [8] S. Lahaie. An analysis of alternative slot auction designs for sponsored search. In Proceedings of the 7th ACM Conference on Electronic Commerce, Ann Arbor, MI, 2006. [9] R. B. Myerson. Optimal auction design. Mathematics of Operations Research, 6(1), February 1981. [10] R. B. Nelsen. An Introduction to Copulas. Springer,

In this paper, we introduce and examine a natural extension of classical evolutionary game theory (EGT) to a setting in which pairwise interactions are restricted to the edges of an undirected graph or network. This extension generalizes the classical setting, in which all pairs of organisms in an infinite population are equally likely to interact. The classical setting can be viewed as the special case in which the underlying network is a clique.
There are many obvious reasons why one would like to examine more general graphs, the primary one being in that many scenarios considered in evolutionary game theory, all interactions are in fact not possible. For example, geographical restrictions may limit interactions to physically proximate pairs of organisms. More generally, as evolutionary game theory has become a plausible model not only for biological interaction, but also economic and other kinds of interaction in which certain dynamics are more imitative than optimizing (see [2, 16] and chapter 4 of [19]), the network constraints may come from similarly more general sources.
Evolutionary game theory on networks has been considered before, but not in the generality we will do so here (see Section 4).
We generalize the definition of an evolutionary stable strategy (ESS) to networks, and show a pair of complementary results that exhibit the power of randomization in our setting: subject to degree or edge density conditions, the classical ESS of any game are preserved when the graph is chosen randomly and the mutation set is chosen adversarially, or when the graph is chosen adversarially and the mutation set is chosen randomly. We examine natural strengthenings of our generalized ESS definition, and show that similarly strong results are not possible for them.
The work described here is part of recent efforts examining the relationship between graph topology or structure and properties of equilibrium outcomes. Previous works in this line include studies of the relationship of topology to properties of correlated equilibria in graphical games [11], and studies of price variation in graph-theoretic market exchange models [12]. More generally, this work contributes to the line of graph-theoretic models for game theory investigated in both computer science [13] and economics [10].
The fundamental concept of evolutionary game theory is the evolutionarily stable strategy (ESS). Intuitively, an ESS is a strategy such that if all the members of a population adopt it, then no mutant strategy could invade the population [17]. To make this more precise, we describe the basic model of evolutionary game theory, in which the notion of an ESS resides.
The standard model of evolutionary game theory considers an infinite population of organisms, each of which plays a strategy in a fixed, 2-player, symmetric game. The game is defined by a fitness function F. All pairs of members of the infinite population are equally likely to interact with one another. If two organisms interact, one playing strategy s 200 and the other playing strategy t, the s-player earns a fitness of F(s|t) while the t-player earns a fitness of F(t|s).
In this infinite population of organisms, suppose there is a
incumbents; and suppose there is an fraction who play t, and call these organisms mutants. Assume two organisms are chosen uniformly at random to play each other. The strategy s is an ESS if the expected fitness of an organism playing s is higher than that of an organism playing t, for all t = s and all sufficiently small . Since an incumbent will meet another incumbent with probability 1 − and it will meet a mutant with probability , we can calculate the expected fitness of an incumbent, which is simply (1 − )F(s|s) + F(s|t). Similarly, the expected fitness of a mutant is (1 − )F(t|s) + F(t|t). Thus we come to the formal definition of an ESS [19].
Definition 2.1. A strategy s is an evolutionarily stable strategy (ESS) for the 2-player, symmetric game given by fitness function F, if for every strategy t = s, there exists an t such that for all 0 < < t, (1 − )F(s|s) + F(s|t) > (1 − )F(t|s) + F(t|t).
A consequence of this definition is that for s to be an ESS, it must be the case that F(s|s) ≥ F(t|s), for all strategies t. This inequality means that s must be a best response to itself, and thus any ESS strategy s must also be a Nash equilibrium. In general the notion of ESS is more restrictive than Nash equilibrium, and not all 2-player, symmetric games have an ESS.
In this paper our interest is to examine what kinds of network structure preserve the ESS strategies for those games that do have a standard ESS. First we must of course generalize the definition of ESS to a network setting.
In our setting, we will no longer assume that two organisms are chosen uniformly at random to interact. Instead, we assume that organisms interact only with those in their local neighborhood, as defined by an undirected graph or network. As in the classical setting (which can be viewed as the special case of the complete network or clique), we shall assume an infinite population, by which we mean we examine limiting behavior in a family of graphs of increasing size.
Before giving formal definitions, some comments are in order on what to expect in moving from the classical to the graph-theoretic setting. In the classical (complete graph) setting, there exist many symmetries that may be broken in moving to the the network setting, at both the group and individual level. Indeed, such asymmetries are the primary interest in examining a graph-theoretic generalization.
For example, at the group level, in the standard ESS definition, one need not discuss any particular set of mutants of population fraction . Since all organisms are equally likely to interact, the survival or fate of any specific mutant set is identical to that of any other. In the network setting, this may not be true: some mutant sets may be better able to survive than others due to the specific topologies of their interactions in the network. For instance, foreshadowing some of our analysis, if s is an ESS but F(t|t) is much larger than F(s|s) and F(s|t), a mutant set with a great deal of internal interaction (that is, edges between mutants) may be able to survive, whereas one without this may suffer. At the level of individuals, in the classical setting, the assertion that one mutant dies implies that all mutants die, again by symmetry. In the network setting, individual fates may differ within a group all playing a common strategy. These observations imply that in examining ESS on networks we face definitional choices that were obscured in the classical model.
If G is a graph representing the allowed pairwise interactions between organisms (vertices), and u is a vertex of G playing strategy su, then the fitness of u is given by F(u) = P v∈Γ(u) F(su|sv) |Γ(u)| .
Here sv is the strategy being played by the neighbor v, and Γ(u) = {v ∈ V : (u, v) ∈ E}. One can view the fitness of u as the average fitness u would obtain if it played each if its neighbors, or the expected fitness u would obtain if it were assigned to play one of its neighbors chosen uniformly at random.
Classical evolutionary game theory examines an infinite, symmetric population. Graphs or networks are inherently finite objects, and we are specifically interested in their asymmetries, as discussed above. Thus all of our definitions shall revolve around an infinite family G = {Gn}∞ n=0 of finite graphs Gn over n vertices, but we shall examine asymptotic (large n) properties of such families.
We first give a definition for a family of mutant vertex sets in such an infinite graph family to contract.
Definition 3.1. Let G = {Gn}∞ n=0 be an infinite family of graphs, where Gn has n vertices. Let M = {Mn}∞ n=0 be any family of subsets of vertices of the Gn such that |Mn| ≥ n for some constant > 0. Suppose all the vertices of Mn play a common (mutant) strategy t, and suppose the remaining vertices in Gn play a common (incumbent) strategy s. We say that Mn contracts if for sufficiently large n, for all but o(n) of the j ∈ Mn, j has an incumbent neighbor i such that F(j) < F(i).
A reasonable alternative would be to ask that the condition above hold for all mutants rather than all but o(n).
Note also that we only require that a mutant have one incumbent neighbor of higher fitness in order to die; one might considering requiring more. In Sections 6.1 and 6.2 we consider these stronger conditions and demonstrate that our results can no longer hold.
In order to properly define an ESS for an infinite family of finite graphs in a way that recovers the classical definition asymptotically in the case of the family of complete graphs, we first must give a definition that restricts attention to families of mutant vertices that are smaller than some invasion threshold n, yet remain some constant fraction of the population. This prevents invasions that survive merely by constituting a vanishing fraction of the population.
Definition 3.2. Let > 0, and let G = {Gn}∞ n=0 be an infinite family of graphs, where Gn has n vertices. Let M = {Mn}∞ n=0 be any family of (mutant) vertices in Gn.
We say that M is -linear if there exists an , > > 0, such that for all sufficiently large n, n > |Mn| > n.
We can now give our definition for a strategy to be evolutionarily stable when employed by organisms interacting with their neighborhood in a graph. 201 Definition 3.3. Let G = {Gn}∞ n=0 be an infinite family of graphs, where Gn has n vertices. Let F be any 2-player, symmetric game for which s is a strategy. We say that s is an ESS with respect to F and G if for all mutant strategies t = s, there exists an t > 0 such that for any t-linear family of mutant vertices M = {Mn}∞ n=0 all playing t, for n sufficiently large, Mn contracts.
Thus, to violate the ESS property for G, one must witness a family of mutations M in which each Mn is an arbitrarily small but nonzero constant fraction of the population of Gn, but does not contract (i.e. every mutant set has a subset of linear size that survives all of its incumbent interactions).
In Section A.1 we show that the definition given coincides with the classical one in the case where G is the family of complete graphs, in the limit of large n. We note that even in the classical model, small sets of mutants were allowed to have greater fitness than the incumbents, as long as the size of the set was o(n) [18].
In the definition above there are three parameters: the game F, the graph family G and the mutation family M.
Our main results will hold for any 2-player, symmetric game F. We will also study two rather general settings for G and M: that in which G is a family of random graphs and M is arbitrary, and that in which G is nearly arbitrary and M is randomly chosen. In both cases, we will see that, subject to conditions on degree or edge density (essentially forcing connectivity of G but not much more), for any 2-player, symmetric game, the ESS of the classical settings, and only those strategies, are always preserved. Thus a common theme of these results is the power of randomization: as long as either the network itself is chosen randomly, or the mutation set is chosen randomly, classical ESS are preserved.
There has been previous work that analyzes which strategies are resilient to mutant invasions with respect to various types of graphs. What sets our work apart is that the model we consider encompasses a significantly more general class of games and graph topologies. We will briefly survey this literature and point out the differences in the previous models and ours.
In [8], [3], and [4], the authors consider specific families of graphs, such as cycles and lattices, where players play specific games, such as 2 × 2-games or k × k-coordination games. In these papers the authors specify a simple, local dynamic for players to improve their payoffs by changing strategies, and analyze what type of strategies will grow to dominate the population. The model we propose is more general than both of these, as it encompasses a larger class of graphs as well as a richer set of games.
Also related to our work is that of [14], where the authors propose two models. The first assumes organisms interact according to a weighted, undirected graph. However, the fitness of each organism is simply assigned and does not depend on the actions of each organism"s neighborhood. The second model has organisms arranged around a directed cycle, where neighbors play a 2 × 2-game. With probability proportional to its fitness, an organism is chosen to reproduce by placing a replica of itself in its neighbors position, thereby killing the neighbor. We consider more general games than the first model and more general graphs than the second.
Finally, the works most closely related to ours are [7], [15], and [6]. The authors consider 2-action, coordination games played by players in a general undirected graph. In these three works, the authors specify a dynamic for a strategy to reproduce, and analyze properties of the graph that allow a strategy to overrun the population. Here again, one can see that our model is more general than these, as it allows for organisms to play any 2-player, symmetric game.
We now proceed to state and prove two complementary results in the network ESS model defined in Section 3. First, we consider a setting where the graphs are generated via the Gn,p model of Erd˝os and R´enyi [5]. In this model, every pair of vertices are joined by an edge independently and with probability p (where p may depend on n). The mutant set, however, will be constructed adversarially (subject to the linear size constraint given by Definition 3.3). For these settings, we show that for any 2-player, symmetric game, s is a classical ESS of that game, if and only if s is an ESS for {Gn,p}∞ n=0, where p = Ω(1/nc ) and 0 ≤ c < 1, and any mutant family {Mn}∞ n=0, where each Mn has linear size. We note that under these settings, if we let c = 1 − γ for small γ > 0, the expected number of edges in Gn is n1+γ or larger - that is, just superlinear in the number of vertices and potentially far smaller than O(n2 ). It is easy to convince oneself that once the graphs have only a linear number of edges, we are flirting with disconnectedness, and there may simply be large mutant sets that can survive in isolation due to the lack of any incumbent interactions in certain games.
Thus in some sense we examine the minimum plausible edge density.
The second result is a kind of dual to the first, considering a setting where the graphs are chosen arbitrarily (subject to conditions) but the mutant sets are chosen randomly. It states that for any 2-player, symmetric game, s is a classical ESS for that game, if and only if s is an ESS for any {Gn = (Vn, En)}∞ n=0 in which for all v ∈ Vn, deg(v) = Ω(nγ ) (for any constant γ > 0), and a family of mutant sets {Mn}∞ n=0, that is chosen randomly (that is, in which each organism is labeled a mutant with constant probability > 0).
Thus, in this setting we again find that classical ESS are preserved subject to edge density restrictions. Since the degree assumption is somewhat strong, we also prove another result which only assumes that |En| ≥ n1+γ , and shows that there must exist at least 1 mutant with an incumbent neighbor of higher fitness (as opposed to showing that all but o(n) mutants have an incumbent neighbor of higher fitness). As will be discussed, this rules out stationary mutant invasions.
Now we state and prove a theorem which shows that if s is a classical ESS, then s will be an ESS for random graphs, where a linear sized set of mutants is chosen by an adversary.
Theorem 5.1. Let F be any 2-player, symmetric game, and suppose s is a classical ESS of F. Let the infinite graph family {Gn}∞ n=0 be drawn according to Gn,p, where p = Ω(1/nc ) and 0 ≤ c < 1. Then with probability 1, s is an ESS.
The main idea of the proof is to divide mutants into 2 categories, those with normal fitness and those with ab202 normal fitness. First, we show all but o(n) of the population (incumbent or mutant) have an incumbent neighbor of normal fitness. This will imply that all but o(n) of the mutants of normal fitness have an incumbent neighbor of higher fitness. The vehicle for proving this is Theorem 2.15 of [5], which gives an upper bound on the number of vertices not connected to a sufficiently large set. This theorem assumes that the size of this large set is known with equality, which necessitates the union bound argument below. Secondly, we show that there can be at most o(n) mutants with abnormal fitness. Since there are so few of them, even if none of them have an incumbent neighbor of higher fitness, s will still be an ESS with respect to F and G.
Proof. (Sketch) Let t = s be the mutant strategy. Since s is a classical ESS, there exists an t such that (1− )F(s|s)+ F(s|t) > (1 − )F(t|s) + F(t|t), for all 0 < < t. Let M be any mutant family that is t-linear. Thus for any fixed value of n that is sufficiently large, there exists an such that |Mn| = n and t > > 0. Also, let In = Vn \ Mn and let I ⊆ In be the set of incumbents that have fitness in the range (1 ± τ)[(1 − )F(s|s) + F(s|t)] for some constant τ,
(1 − )n − 24 log n τ2p . Finally, let TI = {x ∈ V \ I : Γ(x) ∩ I = ∅}. (For the sake of clarity we suppress the subscript n on the sets I and T.) The union bound gives us Pr(|TI | ≥ δn) ≤ (1− )n X i=(1− )n− 24 log n τ2p Pr(|TI | ≥ δn and |I | = i) (1) Letting δ = n−γ for some γ > 0 gives δn = o(n). We will apply Theorem 2.15 of [5] to the summand on the right hand side of Equation 1. If we let γ = (1−c)/2, and combine this with the fact that 0 ≤ c < 1, all of the requirements of this theorem will be satisfied (details omitted). Now when we apply this theorem to Equation 1, we get Pr(|TI | ≥ δn) ≤ (1− )n X i=(1− )n− 24 log n τ2p exp „ − 1 6 Cδn « (2) = o(1) This is because equation 2 has only 24 log n τ2p terms, and Theorem 2.15 of [5] gives us that C ≥ (1 − )n1−c − 24 log n τ2 .
Thus we have shown, with probability tending to 1 as n → ∞, at most o(n) individuals are not attached to an incumbent which has fitness in the range (1 ± τ)[(1 − )F(s|s) + F(s|t)]. This implies that the number of mutants of approximately normal fitness, not attached to an incumbent of approximately normal fitness, is also o(n).
Now those mutants of approximately normal fitness that are attached to an incumbent of approximately normal fitness have fitness in the range (1±τ)[(1− )F(t|s)+ F(t|t)].
The incumbents that they are attached to have fitness in the range (1±τ)[(1− )F(s|s)+ F(s|t)]. Since s is an ESS of F, we know (1− )F(s|s)+ F(s|t) > (1− )F(t|s)+ F(t|t), thus if we choose τ small enough, we can ensure that all but o(n) mutants of normal fitness have a neighboring incumbent of higher fitness.
Finally by Lemma 5.1, we know there are at most o(n) mutants of abnormal fitness. So even if all of them are more fit than their respective incumbent neighbors, we have shown all but o(n) of the mutants have an incumbent neighbor of higher fitness.
We now state and prove the lemma used in the proof above.
Lemma 5.1. For almost every graph Gn,p with (1 − )n incumbents, all but 24 log n δ2p incumbents have fitness in the range (1±δ)[(1− )F(s|s)+ F(s|t)], where p = Ω(1/nc ) and , δ and c are constants satisfying 0 < < 1, 0 < δ < 1/6,
but 24 log n δ2p mutants have fitness in the range (1 ± δ)[(1 − )F(t|s) + F(t|t)].
Proof. We define the mutant degree of a vertex to be the number of mutant neighbors of that vertex, and incumbent degree analogously. Observe that the only way for an incumbent to have fitness far from its expected value of (1− )F(s|s)+ F(s|t) is if it has a fraction of mutant neighbors either much higher or much lower than . Theorem
incumbents. It states that the number of incumbents with mutant degree outside the range (1 ± δ)p|M| is at most 12 log n δ2p . By the same theorem, the number of incumbents with incumbent degree outside the range (1 ± δ)p|I| is at most 12 log n δ2p .
From the linearity of fitness as a function of the fraction of mutant or incumbent neighbors, one can show that for those incumbents with mutant and incumbent degree in the expected range, their fitness is within a constant factor of (1 − )F(s|s) + F(s|t), where that constant goes to 1 as n tends to infinity and δ tends to 0. The proof for the mutant case is analogous.
We note that if in the statement of Theorem 5.1 we let c = 0, then p = 1. This, in turn, makes G = {Kn}∞ n=0, where Kn is a clique of n vertices. Then for any Kn all of the incumbents will have identical fitness and all of the mutants will have identical fitness. Furthermore, since s was an ESS for G, the incumbent fitness will be higher than the mutant fitness. Finally, one can show that as n → ∞, the incumbent fitness converges to (1 − )F(s|s) + F(s|t), and the mutant fitness converges to (1 − )F(t|s) + F(t|t). In other words, s must be a classical ESS, providing a converse to Theorem 5.1. We rigorously present this argument in Section A.1.
We now move on to our second main result. Here we show that if the graph family, rather than being chosen randomly, is arbitrary subject to a minimum degree requirement, and the mutation sets are randomly chosen, classical ESS are again preserved. A modified notion of ESS allows us to considerably weaken the degree requirement to a minimum edge density requirement.
Theorem 5.2. Let G = {Gn = (Vn, En)}∞ n=0 be an infinite family of graphs in which for all v ∈ Vn, deg(v) = Ω(nγ ) (for any constant γ > 0). Let F be any 2-player, symmetric game, and suppose s is a classical ESS of F. Let t be any mutant strategy, and let the mutant family M = {Mn}∞ n=0 be chosen randomly by labeling each vertex a mutant with constant probability , where t > > 0. Then with probability 1, s is an ESS with respect to F, G and M. 203 Proof. Let t = s be the mutant strategy and let X be the event that every incumbent has fitness within the range (1 ± τ)[(1 − )F(s|s) + F(s|t)], for some constant τ > 0 to be specified later. Similarly, let Y be the event that every mutant has fitness within the range (1 ± τ)[(1 − )F(t|s) + F(t|t)]. Since Pr(X ∩ Y ) = 1 − Pr(¬X ∪ ¬Y ), we proceed by showing Pr(¬X ∪ ¬Y ) = o(1). ¬X is the event that there exists an incumbent with fitness outside the range (1±τ)[(1− )F(s|s)+ F(s|t)]. If degM (v) denotes the number of mutant neighbors of v, similarly, degI (v) denotes the number of incumbent neighbors of v, then an incumbent i has fitness degI (i) deg(i) F(s|s)+ degM (i) deg(i) F(s|t).
Since F(s|s) and F(s|t) are fixed quantities, the only variation in an incumbents fitness can come from variation in the terms degI (i) deg(i) and degM (i) deg(i) . One can use the Chernoff bound followed by the union bound to show that for any incumbent i,
Pr(F(i) /∈ (1 ± τ)[(1 − )F(s|s) + F(s|t)]) < 4 exp „ − deg(i)τ2 3 « .
Next one can use the union bound again to bound the probability of the event ¬X,
Pr(¬X) ≤ 4n exp „ − diτ2 3 « where di = mini∈V \M deg(i), 0 < ≤ 1/2. An analogous argument can be made to show Pr(¬Y ) < 4n exp(− dj τ2 3 ), where dj = minj∈M deg(j) and 0 < ≤ 1/2. Thus, by the union bound,
Pr(¬X ∪ ¬Y ) < 8n exp „ − dτ2 3 « where d = minv∈V deg(v), 0 < ≤ 1/2. Since deg(v) = Ω(nγ ), for all v ∈ V , and , τ and γ are all constants greater than 0, lim n→∞ 8n exp ( dτ2/3) = 0, so Pr(¬X∪¬Y ) = o(1). Thus, we can choose τ small enough such that (1 + τ)[(1 − )F(t|s) + F(t|t)] < (1 − τ)[(1 − )F(s|s)+ F(s|t)], and then choose n large enough such that with probability 1 − o(1), every incumbent will have fitness in the range (1±τ)[(1− )F(s|s)+F(s|t)], and every mutant will have fitness in the range (1 ± τ)[(1 − )F(t|s) + F(t|t)].
So with high probability, every incumbent will have a higher fitness than every mutant.
By arguments similar to those following the proof of Theorem 5.1, if we let G = {Kn}∞ n=0, each incumbent will have the same fitness and each mutant will have the same fitness.
Furthermore, since s is an ESS for G, the incumbent fitness must be higher than the mutant fitness. Here again, one has to show show that as n → ∞, the incumbent fitness converges to (1 − )F(s|s) + F(s|t), and the mutant fitness converges to (1 − )F(t|s) + F(t|t). Observe that the exact fraction mutants of Vn is now a random variable. So to prove this convergence we use an argument similar to one that is used to prove that sequence of random variables that converges in probability also converges in distribution (details omitted). This in turn establishes that s must be a classical ESS, and we thus obtain a converse to Theorem 5.2. This argument is made rigorous in Section A.2.
The assumption on the degree of each vertex of Theorem 5.2 is rather strong. The following theorem relaxes this requirement and only necessitates that every graph have n1+γ edges, for some constant γ > 0, in which case it shows there will alway be at least 1 mutant with an incumbent neighbor of higher fitness. A strategy that is an ESS in this weakened sense will essentially rule out stable, static sets of mutant invasions, but not more complex invasions.
An example of more complex invasions are mutant sets that survive, but only by perpetually migrating through the graph under some natural evolutionary dynamics, akin to gliders in the well-known Game of Life [1].
Theorem 5.3. Let F be any game, and let s be a classical ESS of F, and let t = s be a mutant strategy. For any graph family G = {Gn = (Vn, En)}∞ n=0 in which |En| ≥ n1+γ (for any constant γ > 0), and any mutant family M = {Mn}∞ n=0 which is determined by labeling each vertex a mutant with probability , where t > > 0, the probability that there exists a mutant with an incumbent neighbor of higher fitness approaches 1 as n → ∞.
Proof. (Sketch) The main idea behind the proof is to show that with high probability, over only the choice of mutants, there will be an incumbent-mutant edge in which both vertices have high degree. If their degree is high enough, we can show that close to an fraction of their neighbors are mutants, and thus their fitnesses are very close to what we expect them to be in the classical case. Since s is an ESS, the fitness of the incumbent will be higher than the mutant.
We call an edge (i, j) ∈ En a g(n)-barbell if deg(i) ≥ g(n) and deg(j) ≥ g(n). Suppose Gn has at most h(n) edges that are g(n)-barbells. This means there are at least |En| − h(n) edges in which at least one vertex has degree at most g(n).
We call these vertices light vertices. Let (n) be the number of light vertices in Gn. Observe that |En|−h(n) ≤ (n)g(n).
This is because each light vertex is incident on at most g(n) edges. This gives us that |En| ≤ h(n) + (n)g(n) ≤ h(n) + ng(n).
So if we choose h(n) and g(n) such that h(n) + ng(n) = o(n1+γ ), then |En| = o(n1+γ ). This contradicts the assumption that |En| = Ω(n1+γ ). Thus, subject to the above constraint on h(n) and g(n), Gn must contain at least h(n) edges that are g(n)-barbells.
Now let Hn denote the subgraph induced by the barbell edges of Gn. Note that regardless of the structure of Gn, there is no reason that Hn should be connected. Thus, let m be the number of connected components of Hn, and let c1, c2, . . . , cm be the number of vertices in each of these connected components. Note that since Hn is an edge-induced subgraph we have ck ≥ 2 for all components k. Let us choose the mutant set by first flipping the vertices in Hn only. We now show that the probability, with respect to the random mutant set, that none of the components of Hn have an incumbent-mutant edge is exponentially small in n. Let An be the event that every component of Hn contains only mutants or only incumbents. Then algebraic manipulations can establish that Pr[An] = Πm k=1( ck + (1 − )ck ) ≤ (1 − )(1− β2 2 ) Pm k=1 ck 204 where β is a constant. Thus for sufficiently small the bound decreases exponentially with Pm k=1 ck. Furthermore, sincePm k=1 `ck 2 ´ ≥ h(n) (with equality achieved by making each component a clique), one can show that Pm k=1 ck ≥ p h(n).
Thus, as long as h(n) → ∞ with n, the probability that all components are uniformly labeled will go to 0.
Now assuming that there exists a non-uniformly labeled component, by construction that component contains an edge (i, j) where i is an incumbent and j is a mutant, that is a g(n)-barbell. We also assume that the h(n) vertices already labeled have been done so arbitrarily, but that the remaining g(n) − h(n) vertices neighboring i and j are labeled mutants independently with probability . Then via a standard Chernoff bound argument, one can show that with high probability, the fraction of mutants neighboring i and the fraction of mutants neighboring j is in the range (1 ± τ)(g(n)−h(n)) g(n) . Similarly, one can show that the fraction of incumbents neighboring i and the fraction of mutants neighboring j is in the range 1 − (1 ± τ)(g(n)−h(n)) g(n) .
Since s is an ESS, there exists a ζ > 0 such that (1 − )F(s|s) + F(s|t) = (1 − )F(t|s) + F(t|t) + ζ. If we choose g(n) = nγ , and h(n) = o(g(n)), we can choose n large enough and τ small enough to force F(i) > F(j), as desired.
In this section we show that if one tried to strengthen the model described in Section 3 in two natural ways, one would not be able to prove results as strong as Theorems 5.1 and 5.2, which hold for every 2-player, symmetric game.
In Section 3 we alluded to the fact that we made certain design decisions in arriving at Definitions 3.1, 3.2 and 3.3.
One such decision was to require that all but o(n) mutants have incumbent neighbors of higher fitness. Instead, we could have required that all mutants have an incumbent neighbor of higher fitness. The two theorems in this subsection show that if one were to strengthen our notion of contraction for the mutant set, given by Definition 3.1, in this way, it would be impossible to prove theorems analogous to Theorems 5.1 and 5.3.
Recall that Definition 3.1 gave the notion of contraction for a linear sized subset of mutants. In what follows, we will say an edge (i, j) contracts if i is an incumbent, j is a mutant, and F(i) > F(j). Also, recall that Theorem 5.1 stated that if s is a classical ESS, then it is an ESS for random graphs with adversarial mutations. Next, we prove that if we instead required every incumbent-mutant edge to contract, this need not be the case.
Theorem 6.1. Let F be a 2-player, symmetric game that has a classical ESS s for which there exists a mutant strategy t = s with F(t|t) > F(s|s) and F(t|t) > F(s|t). Let G = {Gn}∞ n=0 be an infinite family of random graphs drawn according to Gn,p, where p = Ω(1/nc ) for any constant
there exists a mutant family M = {Mn}∞ n=0, where tn > |Mn| > n and t, > 0, in which there is an edge that does not contract.
Proof. (Sketch) With probability approaching 1 as n → ∞, there exists a vertex j where deg(j) is arbitrarily close to n. So label j mutant, label one of its neighbors incumbent, denoted i, and label the rest of j"s neighborhood mutant. Also, label all of i"s neighbors incumbent, with the exception of j and j"s neighbors (which were already labeled mutant). In this setting, one can show that F(j) will be arbitrarily close to F(t|t) and F(i) will be a convex combination of F(s|s) and F(s|t), which are both strictly less than F(t|t).
Theorem 5.3 stated that if s is a classical ESS, then for graphs where |En| ≥ n1+γ , for some γ > 0, and where each organism is labeled a mutant with probability , one edge must contract. Below we show that, for certain graphs and certain games, there will always exist one edge that will not contract.
Theorem 6.2. Let F be a 2-player, symmetric game that has a classical ESS s, such that there exists a mutant strategy t = s where F(t|s) > F(s|t). There exists an infinite family of graphs {Gn = (Vn, En)}∞ n=0, where |En| = Θ(n2 ), such that for a mutant family M = {Mn}∞ n=0, which is determined by labeling each vertex a mutant with probability > 0, the probability there exists an edge in En that does not contract approaches 1 as n → ∞.
Proof. (Sketch) Construct Gn as follows. Pick n/4 vertices u1, u2, . . . , un/4 and add edges such that they from a clique. Then, for each ui, i ∈ [n/4] add edges (ui, vi), (vi, wi) and (wi, xi). With probability 1 as n → ∞, there exists an i such that ui and wi are mutants and vi and xi are incumbents. Observe that F(vi) = F(xi) = F(s|t) and F(wi) = F(t|s).
The model of Section 3 requires that for an edge (i, j) to contract, the fitness of i must be greater than the fitness of j.
One way to strengthen this notion of contraction would be to require that the maximum fitness incumbent in the neighborhood of j be more fit than the maximum fitness mutant in the neighborhood of j. This models the idea that each organism is trying to take over each place in its neighborhood, but only the most fit organism in the neighborhood of a vertex gets the privilege of taking it. If we assume that we adopt this notion of contraction for individual mutants, and require that all incumbent-mutant edges contract, we will next show that Theorems 6.1 and 6.2 still hold, and thus it is still impossible to get results such as Theorems 5.1 and 5.3 which hold for every 2-player, symmetric game.
In the proof of Theorem 6.1 we proved that F(i) is strictly less than F(j). Observe that maximum fitness mutant in the neighborhood of j must have fitness at least F(j). Also observe that there is only 1 incumbent in the neighborhood of j, namely i. So under this stronger notion of contraction, the edge (i, j) will not contract.
Similarly, in the proof of Theorem 6.2, observe that the only mutant in the neighborhood of wi is wi itself, which has fitness F(t|s). Furthermore, the only incumbents in the neighborhood of wi are vi and xi, both of which have fitness F(s|t). By assumption, F(t|s) > F(s|t), thus, under this stronger notion of contraction, neither of the incumbentmutant edges, (vi, wi) and (xi, wi), will contract.

Today, Internet giants Google and Yahoo! boast a combined market capitalization of over $150 billion, largely on the strength of sponsored search, the fastest growing component of a resurgent online advertising industry.
PricewaterhouseCoopers estimates that 2004 industry-wide sponsored search revenues were $3.9 billion, or 40% of total Internet advertising revenues.1 Industry watchers expect 2005 revenues to reach or exceed $7 billion.2 Roughly 80% of Google"s estimated $4 billion in 2005 revenue and roughly 45% of Yahoo!"s estimated $3.7 billion in 2005 revenue will likely be attributable to sponsored search.3 A number of other companies-including LookSmart, FindWhat,
InterActiveCorp (Ask Jeeves), and eBay (Shopping.com)-earn hundreds of millions of dollars of sponsored search revenue annually.
Sponsored search is a form of advertising where merchants pay to appear alongside web search results. For example, when a user searches for used honda accord san diego in a web search engine, a variety of commercial entities (San Diego car dealers, Honda Corp, automobile information portals, classified ad aggregators, eBay, etc...) may bid to to have their listings featured alongside the standard algorithmic search listings. Advertisers bid for placement on the page in an auction-style format where the higher they bid the more likely their listing will appear above other ads on the page. By convention, sponsored search advertisers generally pay per click, meaning that they pay only when a user clicks on their ad, and do not pay if their ad is displayed but not clicked. Though many people claim to systematically ignore sponsored search ads, Majestic Research reports that 1 www.iab.net/resources/adrevenue/pdf/IAB PwC 2004full.pdf 2 battellemedia.com/archives/002032.php 3 These are rough back of the envelope estimates. Google and Yahoo! 2005 revenue estimates were obtained from Yahoo! Finance. We assumed $7 billion in 2005 industry-wide sponsored search revenues. We used Nielsen/NetRatings estimates of search engine market share in the US, the most monetized market: wired-vig.wired.com/news/technology/0,1282,69291,00.html Using comScore"s international search engine market share estimates would yield different estimates: www.comscore.com/press/release.asp?press=622 218 as many as 17% of Google searches result in a paid click, and that Google earns roughly nine cents on average for every search query they process.4 Usually, sponsored search results appear in a separate section of the page designated as sponsored above or to the right of the algorithmic results. Sponsored search results are displayed in a format similar to algorithmic results: as a list of items each containing a title, a text description, and a hyperlink to a corresponding web page. We call each position in the list a slot. Generally, advertisements that appear in a higher ranked slot (higher on the page) garner more attention and more clicks from users. Thus, all else being equal, merchants generally prefer higher ranked slots to lower ranked slots.
Merchants bid for placement next to particular search queries; for example, Orbitz and Travelocity may bid for las vegas hotel while Dell and HP bid for laptop computer. As mentioned, bids are expressed as a maximum willingness to pay per click. For example, a forty-cent bid by HostRocket for web hosting means HostRocket is willing to pay up to forty cents every time a user clicks on their ad.5 The auctioneer (the search engine6 ) evaluates the bids and allocates slots to advertisers. In principle, the allocation decision can be altered with each new incoming search query, so in effect new auctions clear continuously over time as search queries arrive.
Many allocation rules are plausible. In this paper, we investigate two allocation rules, roughly corresponding to the two allocation rules used by Yahoo! and Google. The rank by bid (RBB) allocation assigns slots in order of bids, with higher ranked slots going to higher bidders. The rank by revenue (RBR) allocation assigns slots in order of the product of bid times expected relevance, where relevance is the proportion of users that click on the merchant"s ad after viewing it. In our model, we assume that an ad"s expected relevance is known to the auctioneer and the advertiser (but not necessarily to other advertisers), and that clickthrough rate decays monotonically with lower ranked slots. In practice, the expected clickthrough rate depends on a number of factors, including the position on the page, the ad text (which in turn depends on the identity of the bidder), the nature and intent of the user, and the context of other ads and algorithmic results on the page, and must be learned over time by both the auctioneer and the bidder [13]. As of this writing, to a rough first-order approximation, Yahoo! employs a RBB allocation and Google employs a RBR allocation, though numerous caveats apply in both cases when it comes to the vagaries of real-world implementations.7 Even when examining a one-shot version of a slot auction, the mechanism differs from a standard multi-item auc4 battellemedia.com/archives/001102.php 5 Usually advertisers also set daily or monthly budget caps; in this paper we do not model budget constraints. 6 In the sponsored search industry, the auctioneer and search engine are not always the same entity. For example Google runs the sponsored search ads for AOL web search, with revenue being shared. Similarly, Yahoo! currently runs the sponsored search ads for MSN web search, though Microsoft will begin independent operations soon. 7 Here are two among many exceptions to the Yahoo! = RBB and Google = RBR assertion: (1) Yahoo! excludes ads deemed insufficiently relevant either by a human editor or due to poor historical click rate; (2) Google sets differing reserve prices depending on Google"s estimate of ad quality. tion in subtle ways. First, a single bid per merchant is used to allocate multiple non-identical slots. Second, the bid is communicated not as a direct preference over slots, but as a preference for clicks that depend stochastically on slot allocation.
We investigate a number of economic properties of RBB and RBR slot auctions. We consider the short-run incomplete information case in Section 3, adapting and extending standard analyses of single-item auctions. In Section 4 we turn to the long-run complete information case; our characterization results here draw on techniques from linear programming. Throughout, important observations are highlighted as claims supported by examples. Our contributions are as follows: • We show that with multiple slots, bidders do not reveal their true values with either RBB or RBR, and with either first- or second-pricing. • With incomplete information, we find that the informational requirements of playing the equilibrium bid are much weaker for RBB than for RBR, because bidders need not know any information about each others" relevance (or even their own) with RBB. • With incomplete information, we prove that RBR is efficient but that RBB is not. • We show via a simple example that no general revenue ranking of RBB and RBR is possible. • We prove that in a complete-information setting, firstprice slot auctions have no pure strategy Nash equilibrium, but that there always exists a pure-strategy equilibrium with second pricing. • We provide a constant-factor bound on the deviation from efficiency that can occur in the equilibrium of a second-price slot auction.
In Section 2 we specify our model of bidders and the various slot auction formats.
In Section 3.1 we study the incentive properties of each format, asking in which cases agents would bid truthfully.
There is possible confusion here because the second-price design for slot auctions is reminiscent of the Vickrey auction for a single item; we note that for slot auctions the Vickrey mechanism is in fact very different from the second-price mechanism, and so they have different incentive properties.8 In Section 3.2 we derive the Bayes-Nash equilibrium bids for the various auction formats. This is useful for the efficiency and revenue results in later sections. It should become clear in this section that slot auctions in our model are a straightforward generalization of single-item auctions.
Sections 3.3 and 3.4 address questions of efficiency and revenue under incomplete information, respectively.
In Section 4.1 we determine whether pure-strategy equilibria exist for the various auction formats, under complete information. In Section 4.2 we derive bounds on the deviation from efficiency in the pure-strategy equilibria of secondprice slot auctions.
Our approach is positive rather than normative. We aim to clarify the incentive, efficiency, and revenue properties of two slot auction designs currently in use, under settings of 8 Other authors have also made this observation [5, 6]. 219 incomplete and complete information. We do not attempt to derive the optimal mechanism for a slot auction.
Related work. Feng et al. [7] compare the revenue performance of various ranking mechanisms for slot auctions in a model with incomplete information, much as we do in Section 3.4, but they obtain their results via simulations whereas we perform an equilibrium analysis.
Liu and Chen [12] study properties of slot auctions under incomplete information. Their setting is essentially the same as ours, except they restrict their attention to a model with a single slot and a binary type for bidder relevance (high or low). They find that RBR is efficient, but that no general revenue ranking of RBB and RBR is possible, which agrees with our results. They also take a design approach and show how the auctioneer should assign relevance scores to optimize its revenue.
Edelman et al. [6] model the slot auction problem both as a static game of complete information and a dynamic game of incomplete information. They study the locally envyfree equilibria of the static game of complete information; this is a solution concept motivated by certain bidding behaviors that arise due to the presence of budget constraints.
They do not view slot auctions as static games of incomplete information as we do, but do study them as dynamic games of incomplete information and derive results on the uniqueness and revenue properties of the resulting equilibria. They also provide a nice description of the evolution of the market for sponsored search.
Varian [18] also studies slot auctions under a setting of complete information. He focuses on symmetric equilibria, which are a refinement of Nash equilibria appropriate for slot auctions. He provides bounds on the revenue obtained in equilibrium. He also gives bounds that can be used to infer bidder values given their bids, and performs some empirical analysis using these results. In contrast, we focus instead on efficiency and provide bounds on the deviation from efficiency in complete-information equilibria.
We focus on a slot auction for a single keyword. In a setting of incomplete information, a bidder knows only distributions over others" private information (value per click and relevance). With complete information, a bidder knows others" private information, and so does not need to rely on distributions to strategize. We first describe the model for the case with incomplete information, and drop the distributional information from the model when we come to the complete-information case in Section 4.
There is a fixed number K of slots to be allocated among N bidders. We assume without loss of generality that K ≤ N, since superfluous slots can remain blank. Bidder i assigns a value of Xi to each click received on its advertisement, regardless of this advertisement"s rank.9 The probability that i"s advertisement will be clicked if viewed is Ai ∈ [0, 1].
We refer to Ai as bidder i"s relevance. We refer to Ri = AiXi as bidder i"s revenue. The Xi, Ai, and Ri are random 9 Indeed Kitts et al. [10] find that in their sample of actual click data, the correlation between rank and conversion rate is not statistically significant. However, for the purposes of our model it is also important that bidders believe that conversion rate does not vary with rank. variables and we denote their realizations by xi, αi, and ri respectively. The probability that an advertisement will be viewed if placed in slot j is γj ∈ [0, 1]. We assume γ1 > γ2 > . . . > γK. Hence bidder i"s advertisement will have a clickthrough rate of γjαi if placed in slot j. Of course, an advertisement does not receive any clicks if it is not allocated a slot.
Each bidder"s value and relevance pair (Xi, Ai) is independently and identically distributed on [0, ¯x] × [0, 1] according to a continuous density function f that has full support on its domain. The density f and slot probabilities γ1, . . . , γK are common knowledge. Only bidder i knows the realization xi of its value per click Xi. Both bidder i and the seller know the realization αi of Ai, but this realization remains unobservable to the other bidders.
We assume that bidders have quasi-linear utility functions. That is, the expected utility to bidder i of obtaining the slot of rank j at a price of b per click is ui(j, b) = γjαi(xi − b) If the advertising firms bidding in the slot auction are riskneutral and have ample liquidity, quasi-linearity is a reasonable assumption.
The assumptions of independence, symmetry, and riskneutrality made above are all quite standard in single-item auction theory [11, 19]. The assumption that clickthrough rate decays monotonically with lower slots-by the same factors for each agent-is unique to the slot auction problem. We view it as a main contribution of our work to show that this assumption allows for tractable analysis of the slot auction problem using standard tools from singleitem auction theory. It also allows for interesting results in the complete information case. A common model of decaying clickthrough rate is the exponential decay model, where γk = 1 δk−1 with decay δ > 1. Feng et al. [7] state that their actual clickthrough data is fitted extremely well by an exponential decay model with δ = 1.428.
Our model lacks budget constraints, which are an important feature of real slot auctions. With budget constraints keyword auctions cannot be considered independently of one another, because the budget must be allocated across multiple keywords-a single advertiser typically bids on multiple keywords relevant to his business. Introducing this element into the model is an important next step for future work.10
In a slot auction a bidder provides to the seller a declared value per click ˜xi(xi, αi) which depends on his true value and relevance. We often denote this declared value (bid) by ˜xi for short. Since a bidder"s relevance αi is observable to the seller, the bidder cannot misrepresent it. We denote the kth highest of the N declared values by ˜x(k) , and the kth highest of the N declared revenues by ˜r(k) , where the declared revenue of bidder i is ˜ri = αi ˜xi. We consider two types of allocation rules, rank by bid (RBB) and rank by revenue (RBR): 10 Models with budget constraints have begun to appear in this research area. Abrams [1] and Borgs et al. [3] design multi-unit auctions for budget-constrained bidders, which can be interpreted as slot auctions, with a focus on revenue optimization and truthfulness. Mehta et al. [14] address the problem of matching user queries to budget-constrained advertisers so as to maximize revenue. 220 RBB. Slot k goes to bidder i if and only if ˜xi = ˜x(k) .
RBR. Slot k goes to bidder i if and only if ˜ri = ˜r(k) .
We will commonly represent an allocation by a one-to-one function σ : [K] → [N], where [n] is the set of integers {1, 2, . . . , n}. Hence slot k goes to bidder σ(k).
We also consider two different types of payment rules.
Note that no matter what the payment rule, a bidder that is not allocated a slot will pay 0 since his listing cannot receive any clicks.
First-price. The bidder allocated slot k, namely σ(k), pays ˜xσ(k) per click under both the RBB and RBR allocation rules.
Second-price. If k < N, bidder σ(k) pays ˜xσ(k+1) per click under the RBB rule, and pays ˜rσ(k+1)/ασ(k) per click under the RBR rule. If k = N, bidder σ(k) pays 0 per click.11 Intuitively, a second-price payment rule sets a bidder"s payment to the lowest bid it could have declared while maintaining the same ranking, given the allocation rule used.
Overture introduced the first slot auction design in 1997, using a first-price RBB scheme. Google then followed in
(at this point acquired by Yahoo!) then switched to second pricing but still allocates using RBB. One possible reason for the switch is given in Section 4.
We assume that ties are broken as follows in the event that two agents make the exact same bid or declare the same revenue. There is a permutation of the agents κ : [N] → [N] that is fixed beforehand. If the bids of agents i and j are tied, then agent i obtains a higher slot if and only if κ(i) < κ(j). This is consistent with the practice in real slot auctions where ties are broken by the bidders" order of arrival.
It should be clear that with a first-price payment rule, truthful bidding is neither a dominant strategy nor an ex post Nash equilibrium using either RBB or RBR, because this guarantees a payoff of 0. There is always an incentive to shade true values with first pricing.
The second-price payment rule is reminiscent of the secondprice (Vickrey) auction used for selling a single item, and in a Vickrey auction it is a dominant strategy for a bidder to reveal his true value for the item [19]. However, using a second-price rule in a slot auction together with either allocation rule above does not yield an incentive-compatible mechanism, either in dominant strategies or ex post Nash equilibrium.12 With a second-price rule there is no incentive for a bidder to bid higher than his true value per click using either RBB or RBR: this either leads to no change 11 We are effectively assuming a reserve price of zero, but in practice search engines charge a non-zero reserve price per click. 12 Unless of course there is only a single slot available, since this is the single-item case. With a single slot both RBB and RBR with a second-price payment rule are dominantstrategy incentive-compatible. in the outcome, or a situation in which he will have to pay more than his value per click for each click received, resulting in a negative payoff.13 However, with either allocation rule there may be an incentive to shade true values with second pricing.
Claim 1. With second pricing and K ≥ 2, truthful bidding is not a dominant strategy nor an ex post Nash equilibrium for either RBB or RBR.
Example. There are two agents and two slots. The agents have relevance α1 = α2 = 1, whereas γ1 = 1 and γ2 = 1/2. Agent 1 has a value of x1 = 6 per click, and agent
RBB rule. Suppose agent 2 bids truthfully. If agent 1 also bids truthfully, he wins the first slot and obtains a payoff of
the second slot at a cost of 0 per click yielding a payoff of
situation holds with the RBR rule. Hence truthful bidding is not a dominant strategy in either format, and neither is it an ex post Nash equilibrium.
To find payments that make RBB and RBR dominantstrategy incentive-compatible, we can apply Holmstrom"s lemma [9] (see also chapter 3 in Milgrom [15]). Under the restriction that a bidder with value 0 per click does not pay anything (even if he obtains a slot, which can occur if there are as many slots as bidders), this lemma implies that there is a unique payment rule that achieves dominant-strategy incentive compatibility for either allocation rule. For RBB, the bidder allocated slot k is charged per click KX i=k+1 (γi−1 − γi)˜x(i) + γK ˜x(K+1) (1) Note that if K = N, ˜x(K+1) = 0 since there is no K + 1th bidder. For RBR, the bidder allocated slot k is charged per click 1 ασ(k) KX i=k+1 (γi−1 − γi)˜r(i) + γK ˜r(K+1) ! (2) Using payment rule (2) and RBR, the auctioneer is aware of the true revenues of the bidders (since they reveal their values truthfully), and hence ranks them according to their true revenues. We show in Section 3.3 that this allocation is in fact efficient. Since the VCG mechanism is the unique mechanism that is efficient, truthful, and ensures bidders with value 0 pay nothing (by the Green-Laffont theorem [8]), the RBR rule and payment scheme (2) constitute exactly the VCG mechanism.
In the VCG mechanism an agent pays the externality he imposes on others. To understand payment (2) in this sense, note that the first term is the added utility (due to an increased clickthrough rate) agents in slots k + 1 to K would receive if they were all to move up a slot; the last term is the utility that the agent with the K +1st revenue would receive by obtaining the last slot as opposed to nothing. The leading coefficient simply reduces the agent"s expected payment to a payment per click. 13 In a dynamic setting with second pricing, there may be an incentive to bid higher than one"s true value in order to exhaust competitors" budgets. This phenomenon is commonly called bid jamming or antisocial bidding [4]. 221
To understand the efficiency and revenue properties of the various auction formats, we must first understand which rankings of the bidders occur in equilibrium with different allocation and payment rule combinations. The following lemma essentially follows from the Monotonic Selection Theorem by Milgrom and Shannon [16].
Lemma 1. In a RBB (RBR) auction with either a firstor second-price payment rule, the symmetric Bayes-Nash equilibrium bid is strictly increasing with value ( revenue).
As a consequence of this lemma, we find that RBB and RBR auctions allocate the slots greedily by the true values and revenues of the agents, respectively (whether using firstor second-price payment rules). This will be relevant in Section 3.3 below. For a first-price payment rule, we can explicitly derive the symmetric Bayes-Nash equilibrium bid functions for RBB and RBR auctions. The purpose of this exercise is to lend qualitative insights into the parameters that influence an agent"s bidding, and to derive formulae for the expected revenue in RBB and RBR auctions in order to make a revenue ranking of these two allocation rules (in Section 3.4).
Let G(y) be the expected resulting clickthrough rate, in a symmetric equilibrium of the RBB auction (with either payment rule), to a bidder with value y and relevance α =
revenue y and relevance 1 in a RBR auction. By Lemma 1, a bidder with value y will obtain slot k in a RBB auction if y is the kth highest of the true realized values. The same applies in a RBR auction when y is the kth highest of the true realized revenues. Let FX (y) be the distribution function for value, and let FR(y) be the distribution function for revenue.
The probability that y is the kth highest out of N values is N − 1 k − 1 ! (1 − FX (y))k−1 FX (y)N−k whereas the probability that y is the kth highest out of N revenues is the same formula with FR replacing FX . Hence we have G(y) = KX k=1 γk N − 1 k − 1 ! (1 − FX (y))k−1 FX (y)N−k The H function is analogous to G with FR replacing FX .
In the two propositions that follow, g and h are the derivatives of G and H respectively. We omit the proof of the next proposition, because it is almost identical to the derivation of the equilibrium bid in the single-item case (see Krishna [11], Proposition 2.2).
Proposition 1. The symmetric Bayes-Nash equilibrium strategies in a first-price RBB auction are given by ˜xB (x, α) = 1 G(x) Z x 0 y g(y)dy The first-price equilibrium above closely parallels the firstprice equilibrium in the single-item model. With a single item g is the density of the second highest value among all N agent values, whereas in a slot auction it is a weighted combination of the densities for the second, third, etc. highest values.
Note that the symmetric Bayes-Nash equilibrium bid in a first-price RBB auction does not depend on a bidder"s relevance α. To see clearly why, note that a bidder chooses a bid b so as to maximize the objective αG(˜x−1 (b))(x − b) and here α is just a leading constant factor. So dropping it does not change the set of optimal solutions. Hence the equilibrium bid depends only on the value x and function G, and G in turn depends only on the marginal cumulative distribution of value FX . So really only the latter needs to be common knowledge to the bidders. On the other hand, we will now see that information about relevance is needed for bidders to play the equilibrium in the first-price RBR auction. So the informational requirements for a first-price RBB auction are much weaker than for a first-price RBR auction: in the RBB auction a bidder need not know his own relevance, and need not know any distributional information over others" relevance in order to play the equilibrium.
Again we omit the next proposition"s proof since it is so similar to the one above.
Proposition 2. The symmetric Bayes-Nash equilibrium strategies in a first-price RBR auction are given by ˜xR (x, α) = 1 αH(αx) Z αx 0 y h(y) dy Here it can be seen that the equilibrium bid is increasing with x, but not necessarily with α. This should not be much of a concern to the auctioneer, however, because in any case the declared revenue in equilibrium is always increasing in the true revenue.
It would be interesting to obtain the equilibrium bids when using a second-price payment rule, but it appears that the resulting differential equations for this case do not have a neat analytical solution. Nonetheless, the same conclusions about the informational requirements of the RBB and RBR rules still hold, as can be seen simply by inspecting the objective function associated with an agent"s bidding problem for the second-price case.
A slot auction is efficient if in equilibrium the sum of the bidders" revenues from their allocated slots is maximized.
Using symmetry as our equilibrium selection criterion, we find that the RBB auction is not efficient with either payment rule.
Claim 2. The RBB auction is not efficient with either first or second pricing.
Example. There are two agents and one slot, with γ1 =
α1 = 1/2. Agent 2 has a value of x2 = 4 per click and relevance α2 = 1. By Lemma 1, agents are ranked greedily by value. Hence agent 1 obtains the lone slot, for a total revenue of 3 to the agents. However, it is most efficient to allocate the slot to agent 2, for a total revenue of 4.
Examples with more agents or more slots are simple to construct along the same lines. On the other hand, under our assumptions on how clickthrough rate decreases with lower rank, the RBR auction is efficient with either payment rule. 222 Theorem 1. The RBR auction is efficient with either first- or second-price payments rules.
Proof. Since by Lemma 1 the agents" equilibrium bids are increasing functions of their revenues in the RBR auction, slots are allocated greedily according to true revenues.
Let σ be a non-greedy allocation. Then there are slots s, t with s < t and rσ(s) < rσ(t). We can switch the agents in slots s and t to obtain a new allocation, and the difference between the total revenue in this new allocation and the original allocation"s total revenue is ` γtrσ(s) + γsrσ(t) ´ − ` γsrσ(s) + γtrσ(t) ´ = (γs − γt) ` rσ(t) − rσ(s) ´ Both parenthesized terms above are positive. Hence the switch has increased the total revenue to the bidders. If we continue to perform such switches, we will eventually reach a greedy allocation of greater revenue than the initial allocation. Since the initial allocation was arbitrary, it follows that a greedy allocation is always efficient, and hence the RBR auction"s allocation is efficient.
Note that the assumption that clickthrough rate decays montonically by the same factors γ1, . . . , γK for all agents is crucial to this result. A greedy allocation scheme does not necessarily find an efficient solution if the clickthrough rates are monotonically decreasing in an independent fashion for each agent.
To obtain possible revenue rankings for the different auction formats, we first note that when the allocation rule is fixed to RBB, then using either a first-price, second-price, or truthful payment rule leads to the same expected revenue in a symmetric, increasing Bayes-Nash equilibrium. Because a RBB auction ranks agents by their true values in equilibrium for any of these payment rules (by Lemma 1), it follows that expected revenue is the same for all these payment rules, following arguments that are virtually identical to those used to establish revenue equivalence in the singleitem case (see e.g. Proposition 3.1 in Krishna [11]). The same holds for RBR auctions; however, the revenue ranking of the RBB and RBR allocation rules is still unclear.
Because of this revenue equivalence principle, we can choose whichever payment rule is most convenient for the purpose of making revenue comparisons.
Using Propositions 1 and 2, it is a simple matter to derive formulae for the expected revenue under both allocation rules. The payment of an agent in a RBB auction is mB (x, α) = αG(x)˜xV (x, α) The expected revenue is then N · E ˆ mV (X, A) ˜ , where the expectation is taken with respect to the joint density of value and relevance. The expected revenue formula for RBR auctions is entirely analogous using ˜xR (x, α) and the H function. With these in hand we can obtain revenue rankings for specific numbers of bidders and slots, and specific distributions over values and relevance.
Claim 3. For fixed K, N, and fixed γ1, . . . , γK, no revenue ranking of RBB and RBR is possible for an arbitrary density f.
Example. Assume there are 2 bidders, 2 slots, and that γ1 = 1, γ2 = 1/2. Assume that value-relevance pairs are uniformly distributed over [0, 1]× [0, 1]. For such a distribution with a closed-form formula, it is most convenient to use the revenue formulae just derived. RBB dominates RBR in terms of revenue for these parameters. The formula for the expected revenue in a RBB auction yields 1/12, whereas for RBR auctions we have 7/108.
Assume instead that with probability 1/2 an agent"s valuerelevance pair is (1, 1/2), and that with probability 1/2 it is (1/2, 1). In this scenario it is more convenient to appeal to formulae (1) and (2). In a truthful auction the second agent will always pay 0. According to (1), in a truthful RBB auction the first agent makes an expected payment of E ˆ (γ1 − γ2)Aσ(1)Xσ(2) ˜ = 1 2 E ˆ Aσ(1) ˜ E ˆ Xσ(2) ˜ where we have used the fact that value and relevance are independently distributed for different agents. The expected relevance of the agent with the highest value is E ˆ Aσ(1) ˜ = 5/8. The expected second highest value is also E ˆ Xσ(2) ˜ = 5/8. The expected revenue for a RBB auction here is then 25/128. According to (2), in a truthful RBR auction the first agent makes an expected payment of E ˆ (γ1 − γ2)Rσ(2) ˜ = 1 2 E ˆ Rσ(2) ˜ In expectation the second highest revenue is E ˆ Rσ(2) ˜ = 1/2, so the expected revenue for a RBR auction is 1/4.
Hence in this case the RBR auction yields higher expected revenue.1415 This example suggests the following conjecture: when value and relevance are either uncorrelated or positively correlated, RBB dominates RBR in terms of revenue. When value and relevance are negatively correlated, RBR dominates.
In typical slot auctions such as those run by Yahoo! and Google, bidders can adjust their bids up or down at any time. As B¨orgers et al. [2] and Edelman et al. [6] have noted, this can be viewed as a continuous-time process in which bidders learn each other"s bids. If the process stabilizes the result can then be modeled as a Nash equilibrium in pure strategies of the static one-shot game of complete information, since each bidder will be playing a best-response to the others" bids.16 This argument seems especially appropriate for Yahoo!"s slot auction design where all bids are 14 To be entirely rigorous and consistent with our initial assumptions, we should have constructed a continuous probability density with full support over an appropriate domain.
Taking the domain to be e.g. [0, 1] × [0, 1] and a continuous density with full support that is sufficiently concentrated around (1, 1/2) and (1/2, 1), with roughly equal mass around both, would yield the same conclusion. 15 Claim 3 should serve as a word of caution, because Feng et al. [7] find through their simulations that with a bivariate normal distribution over value-relevance pairs, and with
terms of revenue for any level of correlation between value and relevance. However, they assume that bidding behavior in a second-price slot auction can be well approximated by truthful bidding. 16 We do not claim that bidders will actually learn each others" private information (value and relevance), just that for a stable set of bids there is a corresponding equilibrium of the complete information game. 223 made public. Google keeps bids private, but experimentation can allow one to discover other bids, especially since second pricing automatically reveals to an agent the bid of the agent ranked directly below him.
In this section we ask whether a pure-strategy Nash equilibrium exists in a RBB or RBR slot auction, with either first or second pricing.
Before dealing with the first-price case there is a technical issue involving ties. In our model we allow bids to be nonnegative real numbers for mathematical convenience, but this can become problematic because there is then no bid that is just higher than another. We brush over such issues by assuming that an agent can bid infinitesimally higher than another. This is imprecise but allows us to focus on the intuition behind the result that follows. See Reny [17] for a full treatment of such issues.
For the remainder of the paper, we assume that there are as many slots as bidders. The following result shows that there can be no pure-strategy Nash equilibrium with first pricing.17 Note that the argument holds for both RBB and RBR allocation rules. For RBB, bids should be interpreted as declared values, and for RBR as declared revenues.
Theorem 2. There exists no complete information Nash equilibrium in pure strategies in the first-price slot auction, for any possible values of the agents, whether using a RBB or RBR allocation rule.
Proof. Let σ : [K] → [N] be the allocation of slots to the agents resulting from their bids. Let ri and bi be the revenue and bid of the agent ranked ith , respectively. Note that we cannot have bi > bi+1, or else the agent in slot i can make a profitable deviation by instead bidding bi − > bi+1 for small enough > 0. This does not change its allocation, but increases its profit. Hence we must have bi = bi+1 (i.e. with one bidder bidding infinitesimally higher than the other). Since this holds for any two consecutive bidders, it follows that in a Nash equilibrium all bidders must be bidding 0 (since the bidder ranked last matches the bid directly below him, which is 0 by default because there is no such bid). But this is impossible: consider the bidder ranked last. The identity of this bidder is always clear given the deterministic tie-breaking rule. This bidder can obtain the top spot and increase his revenue by (γ1 −γK)rK > 0 by bidding some > 0, and for small enough this is necessarily a profitable deviation. Hence there is no Nash equilibrium in pure strategies.
On the other hand, we find that in a second-price slot auction there can be a multitude of pure strategy Nash equilibria. The next two lemmas give conditions that characterize the allocations that can occur as a result of an equilibrium profile of bids, given fixed agent values and revenues. Then if we can exhibit an allocation that satisfies these conditions, there must exist at least one equilibrium. We first consider the RBR case. 17 B¨orgers et al. [2] have proven this result in a model with three bidders and three slots, and we generalize their argument. Edelman et al. [6] also point out this non-existence phenomenon. They only illustrate the fact with an example because the result is quite immediate.
Lemma 2. Given an allocation σ, there exists a Nash equilibrium profile of bids b leading to σ in a second-price RBR slot auction if and only if „
γi γj+1 « rσ(i) ≤ rσ(j) for 1 ≤ j ≤ N − 2 and i ≥ j + 2.
Proof. There exists a desired vector b which constitutes a Nash equilibrium if and only if the following set of inequalities can be satisfied (the variables are the πi and bj): πi ≥ γj(rσ(i) − bj) ∀i, ∀j < i (3) πi ≥ γj(rσ(i) − bj+1) ∀i, ∀j > i (4) πi = γi(rσ(i) − bi+1) ∀i (5) bi ≥ bi+1 1 ≤ i ≤ N − 1 (6) πi ≥ 0, bi ≥ 0 ∀i Here rσ(i) is the revenue of the agent allocated slot i, and πi and bi may be interpreted as this agent"s surplus and declared revenue, respectively. We first argue that constraints (6) can be removed, because the inequalities above can be satisfied if and only if the inequalities without (6) can be satisfied. The necessary direction is immediate. Assume we have a vector (π, b) which satisfies all inequalities above except (6). Then there is some i for which bi < bi+1.
Construct a new vector (π, b ) identical to the original except with bi+1 = bi. We now have bi = bi+1. An agent in slot k < i sees the price of slot i decrease from bi+1 to bi+1 = bi, but this does not make i more preferred than k to this agent because we have πk ≥ γi−1(rσ(k) − bi) ≥ γi(rσ(k) − bi) = γi(rσ(k) −bi+1) (i.e. because the agent in slot k did not originally prefer slot i − 1 at price bi, he will not prefer slot i at price bi). A similar argument applies for agents in slots k > i + 1. The agent in slot i sees the price of this slot go down, which only makes it more preferred. Finally, the agent in slot i + 1 sees no change in the price of any slot, so his slot remains most preferred. Hence inequalities (3)-(5) remain valid at (π, b ). We first make this change to the bi+1 where bi < bi+1 and index i is smallest. We then recursively apply the change until we eventually obtain a vector that satisfies all inequalities.
We safely ignore inequalities (6) from now on. By the Farkas lemma, the remaining inequalities can be satisfied if and only if there is no vector z such that X i,j (γj rσ(i)) zσ(i)j > 0 X i>j γjzσ(i)j + X i<j γj−1zσ(i)j−1 ≤ 0 ∀j (7) X j zσ(i)j ≤ 0 ∀i (8) zσ(i)j ≥ 0 ∀i, ∀j = i zσ(i)i free ∀i Note that a variable of the form zσ(i)i appears at most once in a constraint of type (8), so such a variable can never be positive. Also, zσ(i)1 = 0 for all i = 1 by constraint (7), since such variables never appear with another of the form zσ(i)i.
Now if we wish to raise zσ(i)j above 0 by one unit for j = i, we must lower zσ(i)i by one unit because of the constraint of type (8). Because γjrσ(i) ≤ γirσ(i) for i < j, raising 224 zσ(i)j with i < j while adjusting other variables to maintain feasibility cannot make the objective P i,j(γjrσ(i))zσ(i)j positive. If this objective is positive, then this is due to some component zσ(i)j with i > j being positive.
Now for the constraints of type (7), if i > j then zσ(i)j appears with zσ(j−1)j−1 (for 1 < j < N). So to raise the former variable γ−1 j units and maintain feasibility, we must (I) lower zσ(i)i by γ−1 j units, and (II) lower zσ(j−1)j−1 by γ−1 j−1 units. Hence if the following inequalities hold: rσ(i) ≤ „ γi γj « rσ(i) + rσ(j−1) (9) for 2 ≤ j ≤ N − 1 and i > j, raising some zσ(i)j with i > j cannot make the objective positive, and there is no z that satisfies all inequalities above. Conversely, if some inequality (9) does not hold, the objective can be made positive by raising the corresponding zσ(i)j and adjusting other variables so that feasibility is just maintained. By a slight reindexing, inequalities (9) yield the statement of the theorem.
The RBB case is entirely analogous.
Lemma 3. Given an allocation σ, there exists a Nash equilibrium profile of bids b leading to σ in a second-price RBB slot auction if and only if „
γi γj+1 « xσ(i) ≤ xσ(j) for 1 ≤ j ≤ N − 2 and i ≥ j + 2.
Proof Sketch. The proof technique is the same as in the previous lemma. The desired Nash equilibrium exists if and only if a related set of inequalities can be satisfied; by the Farkas lemma, this occurs if and only if an alternate set of inequalities cannot be satisfied. The conditions that determine whether the latter holds are given in the statement of the lemma.
The two lemmas above immediately lead to the following result.
Theorem 3. There always exists a complete information Nash equilibrium in pure strategies in the second-price RBB slot auction. There always exists an efficient complete information Nash equilibrium in pure strategies in the secondprice RBR slot auction.
Proof. First consider RBB. Suppose agents are ranked according to their true values. Since xσ(i) ≤ xσ(j) for i > j, the system of inequalities in Lemma 3 is satisfied, and the allocation is the result of some Nash equilibrium bid profile.
By the same type of argument but appealing to Lemma 2 for RBR, there exists a Nash equilibrium bid profile such that bidders are ranked according to their true revenues.
By Theorem 1, this latter allocation is efficient.
This theorem establishes existence but not uniqueness.
Indeed we expect that in many cases there will be multiple allocations (and hence equilibria) which satisfy the conditions of Lemmas 2 and 3. In particular, not all equilibria of a second-price RBR auction will be efficient. For instance, according to Lemma 2, with two agents and two slots any allocation can arise in a RBR equilibrium because no constraints apply.
Theorems 2 and 3 taken together provide a possible explanation for Yahoo!"s switch from first to second pricing.
We saw in Section 3.1 that this does not induce truthfulness from bidders. With first pricing, there will always be some bidder that feels compelled to adjust his bid. Second pricing is more convenient because an equilibrium can be reached, and this reduces the cost of bid management.
For a given allocation rule, we call the allocation that would result if the bidders reported their values truthfully the standard allocation. Hence in the standard RBB allocation bidders are ranked by true values, and in the standard RBR allocation they are ranked by true revenues.
According to Lemmas 2 and 3, a ranking that results from a Nash equilibrium profile can only deviate from the standard allocation by having agents with relatively similar values or revenues switch places. That is, if ri > rj then with RBR agent j can be ranked higher than i only if the ratio rj/ri is sufficiently large; similarly for RBB. This suggests that the value of an equilibrium allocation cannot differ too much from the value obtained in the standard allocation, and the following theorems confirms this.
For an allocation σ of slots to agents, we denote its total value by f(σ) = PN i=1 γirσ(i). We denote by g(σ) = PN i=1 γixσ(i) allocation σ"s value when assuming all agents have identical relevance, normalized to 1. Let L = min i=1,...,N−1 min  γi+1 γi , 1 − γi+2 γi+1 ff (where by default γN+1 = 0). Let ηx and ηr be the standard allocations when using RBB and RBR, respectively.
Theorem 4. For an allocation σ that results from a purestrategy Nash equilibrium of a second-price RBR slot auction, we have f(σ) ≥ Lf(ηr).
Proof. We number the agents so that agent i has the ith highest revenue, so r1 ≥ r2 ≥ . . . ≥ rN . Hence the standard allocation has value f(ηr) = PN i=1 γiri. To prove the theorem, we will make repeated use of the fact thatP k akP k bk ≥ mink ak bk when the ak and bk are positive. Note that according to Lemma 2, if agent i lies at least two slots below slot j, then rσ(j) ≥ ri
γj+2 γj+1  .
It may be the case that for some slot i, we have σ(i) > i and for slots k > i + 1 we have σ(k) > i. We then say that slot i is inverted. Let S be the set of agents with indices at least i + 1; there are N − i of these. If slot i is inverted, it is occupied by some agent from S. Also all slots strictly lower than i + 1 must be occupied by the remaining agents from S, since σ(k) > i for k ≥ i + 2. The agent in slot i + 1 must then have an index σ(i + 1) ≤ i (note this means slot i + 1 cannot be inverted). Now there are two cases. In the first case we have σ(i) = i + 1. Then γirσ(i) + γi+1rσ(i+1) γiri + γi+1ri+1 ≥ γi+1ri + γiri+1 γiri + γi+1ri+1 ≥ min  γi+1 γi , γi γi+1 ff = γi+1 γi In the second case we have σ(i) > i+1. Then since all agents in S except the one in slot i lie strictly below slot i + 1, and 225 the agent in slot i is not agent i + 1, it must be that agent i+1 is in a slot strictly below slot i+1. This means that it is at least two slots below the agent that actually occupies slot i, and by Lemma 2 we then have rσ(i) ≥ ri+1
γi+2 γi+1  .
Thus, γirσ(i) + γi+1rσ(i+1) γiri + γi+1ri+1 ≥ γi+1ri + γirσ(i) γiri + γi+1ri+1 ≥ min  γi+1 γi , 1 − γi+2 γi+1 ff If slot i is not inverted, then on one hand we may have σ(i) ≤ i, in which case rσ(i)/ri ≥ 1. On the other hand we may have σ(i) > i but there is some agent with index j ≤ i that lies at least two slots below slot i. Then by Lemma 2, rσ(i) ≥ rj
γi+2 γi+1  ≥ ri
γi+2 γi+1  .
We write i ∈ I if slot i is inverted, and i ∈ I if neither i nor i − 1 are inverted. By our arguments above two consecutive slots cannot be inverted, so we can write f(σ) f(γr) = P i∈I ` γirσ(i) + γi+1rσ(i+1) ´ + P i∈I γirσ(i) P i∈I (γiri + γi+1ri+1) + P i∈I γiri ≥ min  min i∈I  γirσ(i) + γi+1rσ(i+1) γiri + γi+1ri+1 ff , min i∈I  γirσ(i) γiri ffff ≥ L and this completes the proof.
Note that for RBR, the standard value is also the efficient value by Theorem 1. Also note that for an exponential decay model, L = min ˘1 δ , 1 − 1 δ ¯ . With δ = 1.428 (see Section 2.1), the factor is L ≈ 1/3.34, so the total value in a pure-strategy Nash equilibrium of a second-price RBR slot auction is always within a factor of 3.34 of the efficient value with such a discount.
Again for RBB we have an analogous result.
Theorem 5. For an allocation σ that results from a purestrategy Nash equilibrium of a second-price RBB slot auction, we have g(σ) ≥ Lg(ηx).
Proof Sketch. Simply substitute bidder values for bidder revenues in the proof of Theorem 4, and appeal to Lemma 3.
This paper analyzed stylized versions of the slot auction designs currently used by Yahoo! and Google, namely rank by bid (RBB) and rank by revenue (RBR), respectively.
We also considered first and second pricing rules together with each of these allocation rules, since both have been used historically. We first studied the short-run setting with incomplete information, corresponding to the case where agents have just approached the mechanism. Our equilibrium analysis revealed that RBB has much weaker informational requirements than RBR, because bidders need not know any information about relevance (even their own) to play the Bayes-Nash equilibrium. However, RBR leads to an efficient allocation in equilibrium, whereas RBB does not.
We showed that for an arbitrary distribution over value and relevance, no revenue ranking of RBB and RBR is possible.
We hope that the tools we used to establish these results (revenue equivalence, the form of first-price equilibria, the truthful payments rules) will help others wanting to pursue further analyses of slot auctions.
We also studied the long-run case where agents have experimented with their bids and each settled on one they find optimal. We argued that a stable set of bids in this setting can be modeled as a pure-strategy Nash equilibrium of the static game of complete information. We showed that no pure-strategy equilibrium exists with either RBB or RBR using first pricing, but that with second pricing there always exists such an equilibrium (in the case of RBR, an efficient equilibrium). In general second pricing allows for multiple pure-strategy equilibria, but we showed that the value of such equilibria diverges by only a constant factor from the value obtained if all agents bid truthfully (which in the case of RBR is the efficient value).
Introducing budget constraints into the model is a natural next step for future work. The complication here lies in the fact that budgets are often set for entire campaigns rather than single keywords. Assuming that the optimal choice of budget can be made independent of the choice of bid for a specific keyword, it can be shown that it is a dominant-strategy to report this optimal budget with one"s bid. The problem is then to ascertain that bids and budgets can indeed be optimized separately, or to find a plausible model where deriving equilibrium bids and budgets together is tractable.
Identifying a condition on the distribution over value and relevance that actually does yield a revenue ranking of RBB and RBR (such as correlation between value and relevance, perhaps) would yield a more satisfactory characterization of their relative revenue properties. Placing bounds on the revenue obtained in a complete information equilibrium is also a relevant question.
Because the incomplete information case is such a close generalization of the most basic single-item auction model, it would be interesting to see which standard results from single-item auction theory (e.g. results with risk-averse bidders, an endogenous number of bidders, asymmetries, etc...) automatically generalize and which do not, to fully understand the structural differences between single-item and slot auctions.
Acknowledgements David Pennock provided valuable guidance throughout this project. I would also like to thank David Parkes for helpful comments.

With consumers showing increasing resistance to traditional forms of advertising such as TV or newspaper ads, marketers have turned to alternate strategies, including viral marketing. Viral marketing exploits existing social networks by encouraging customers to share product information with their friends. Previously, a few in depth studies have shown that social networks affect the adoption of individual innovations and products (for a review see [15] or [16]). But until recently it has been difficult to measure how influential person-to-person recommendations actually are over a wide range of products. We were able to directly measure and model the effectiveness of recommendations by studying one online retailer"s incentivised viral marketing program. The website gave discounts to customers recommending any of its products to others, and then tracked the resulting purchases and additional recommendations.
Although word of mouth can be a powerful factor influencing purchasing decisions, it can be tricky for advertisers to tap into. Some services used by individuals to communicate are natural candidates for viral marketing, because the product can be observed or advertised as part of the communication. Email services such as Hotmail and Yahoo had very fast adoption curves because every email sent through them contained an advertisement for the service and because they were free. Hotmail spent a mere $50,000 on traditional marketing and still grew from zero to 12 million users in 18 months [7]. Google"s Gmail captured a significant part of market share in spite of the fact that the only way to sign up for the service was through a referral.
Most products cannot be advertised in such a direct way.
At the same time the choice of products available to consumers has increased manyfold thanks to online retailers who can supply a much wider variety of products than traditional brick-and-mortar stores. Not only is the variety of products larger, but one observes a ‘fat tail" phenomenon, where a large fraction of purchases are of relatively obscure items. On Amazon.com, somewhere between 20 to
products [2]. Rhapsody, a streaming-music service, streams more tracks outside than inside its top 10,000 tunes [1].
Effectively advertising these niche products using traditional advertising approaches is impractical. Therefore using more targeted marketing approaches is advantageous both to the merchant and the consumer, who would benefit from learning about new products.
The problem is partly addressed by the advent of online product and merchant reviews, both at retail sites such as EBay and Amazon, and specialized product comparison sites such as Epinions and CNET. Quantitative marketing techniques have been proposed [12], and the rating of products and merchants has been shown to effect the likelihood of an item being bought [13, 4]. Of further help to the consumer are collaborative filtering recommendations of the form people who bought x also bought y feature [11].
These refinements help consumers discover new products 228 and receive more accurate evaluations, but they cannot completely substitute personalized recommendations that one receives from a friend or relative. It is human nature to be more interested in what a friend buys than what an anonymous person buys, to be more likely to trust their opinion, and to be more influenced by their actions. Our friends are also acquainted with our needs and tastes, and can make appropriate recommendations. A Lucid Marketing survey found that 68% of individuals consulted friends and relatives before purchasing home electronics - more than the half who used search engines to find product information [3].
Several studies have attempted to model just this kind of network influence. Richardson and Domingos [14] used Epinions" trusted reviewer network to construct an algorithm to maximize viral marketing efficiency assuming that individuals" probability of purchasing a product depends on the opinions on the trusted peers in their network. Kempe,
Kleinberg and Tardos [8] evaluate the efficiency of several algorithms for maximizing the size of influence set given various models of adoption. While these models address the question of maximizing the spread of influence in a network, they are based on assumed rather than measured influence effects.
In contrast, in our study we are able to directly observe the effectiveness of person to person word of mouth advertising for hundreds of thousands of products for the first time.
We find that most recommendation chains do not grow very large, often terminating with the initial purchase of a product. However, occasionally a product will propagate through a very active recommendation network. We propose a simple stochastic model that seems to explain the propagation of recommendations. Moreover, the characteristics of recommendation networks influence the purchase patterns of their members. For example, individuals" likelihood of purchasing a product initially increases as they receive additional recommendations for it, but a saturation point is quickly reached. Interestingly, as more recommendations are sent between the same two individuals, the likelihood that they will be heeded decreases. We also propose models to identify products for which viral marketing is effective: We find that the category and price of product plays a role, with recommendations of expensive products of interest to small, well connected communities resulting in a purchase more often.
We also observe patterns in the timing of recommendations and purchases corresponding to times of day when people are likely to be shopping online or reading email. We report on these and other findings in the following sections.
Our analysis focuses on the recommendation referral program run by a large retailer. The program rules were as follows. Each time a person purchases a book, music, or a movie he or she is given the option of sending emails recommending the item to friends. The first person to purchase the same item through a referral link in the email gets a 10% discount. When this happens the sender of the recommendation receives a 10% credit on their purchase.
The recommendation dataset consists of 15,646,121 recommendations made among 3,943,084 distinct users. The data was collected from June 5 2001 to May 16 2003. In total, 548,523 products were recommended, 99% of them belonging to 4 main product groups: Books, DVDs, Music and Videos. In addition to recommendation data, we also crawled the retailer"s website to obtain product categories, reviews and ratings for all products. Of the products in our data set, 5813 (1%) were discontinued (the retailer no longer provided any information about them).
Although the data gives us a detailed and accurate view of recommendation dynamics, it does have its limitations.
The only indication of the success of a recommendation is the observation of the recipient purchasing the product through the same vendor. We have no way of knowing if the person had decided instead to purchase elsewhere, borrow, or otherwise obtain the product. The delivery of the recommendation is also somewhat different from one person simply telling another about a product they enjoy, possibly in the context of a broader discussion of similar products.
The recommendation is received as a form email including information about the discount program. Someone reading the email might consider it spam, or at least deem it less important than a recommendation given in the context of a conversation. The recipient may also doubt whether the friend is recommending the product because they think the recipient might enjoy it, or are simply trying to get a discount for themselves. Finally, because the recommendation takes place before the recommender receives the product, it might not be based on a direct observation of the product. Nevertheless, we believe that these recommendation networks are reflective of the nature of word of mouth advertising, and give us key insights into the influence of social networks on purchasing decisions.
For each recommendation, the dataset included the product and product price, sender ID, receiver ID, the sent date, and a buy-bit, indicating whether the recommendation resulted in a purchase and discount. The sender and receiver ID"s were shadowed. We represent this data set as a directed multi graph. The nodes represent customers, and a directed edge contains all the information about the recommendation. The edge (i, j, p, t) indicates that i recommended product p to customer j at time t.
The typical process generating edges in the recommendation network is as follows: a node i first buys a product p at time t and then it recommends it to nodes j1, . . . , jn. The j nodes can they buy the product and further recommend it. The only way for a node to recommend a product is to first buy it. Note that even if all nodes j buy a product, only the edge to the node jk that first made the purchase (within a week after the recommendation) will be marked by a buy-bit. Because the buy-bit is set only for the first person who acts on a recommendation, we identify additional purchases by the presence of outgoing recommendations for a person, since all recommendations must be preceded by a purchase. We call this type of evidence of purchase a buyedge. Note that buy-edges provide only a lower bound on the total number of purchases without discounts. It is possible for a customer to not be the first to act on a recommendation and also to not recommend the product to others.
Unfortunately, this was not recorded in the data set. We consider, however, the buy-bits and buy-edges as proxies for the total number of purchases through recommendations.
For each product group we took recommendations on all products from the group and created a network. Table 1 229
x 10 6 0 2 4 6 8 10 12 x 10 4 number of nodes sizeofgiantcomponent by month quadratic fit
0 2 4 x 10 6 m (month) n # nodes
6 m 10 0 10 1 10 2 10 3 10 1 10 2 10 3 10 4 10 5 10 6 kp (recommendations by a person for a product) N(x>=k p ) level 0 γ = 2.6 level 1 γ = 2.0 level 2 γ = 1.5 level 3 γ = 1.2 level 4 γ = 1.2 (a) Network growth (b) Recommending by level Figure 1: (a) The size of the largest connected component of customers over time. The inset shows the linear growth in the number of customers n over time. (b) The number of recommendations sent by a user with each curve representing a different depth of the user in the recommendation chain. A power law exponent γ is fitted to all but the tail. (first 7 columns) shows the sizes of various product group recommendation networks with p being the total number of products in the product group, n the total number of nodes spanned by the group recommendation network and e the number of edges (recommendations). The column eu shows the number of unique edges - disregarding multiple recommendations between the same source and recipient.
In terms of the number of different items, there are by far the most music CDs, followed by books and videos. There is a surprisingly small number of DVD titles. On the other hand, DVDs account for more half of all recommendations in the dataset. The DVD network is also the most dense, having about 10 recommendations per node, while books and music have about 2 recommendations per node and videos have only a bit more than 1 recommendation per node.
Music recommendations reached about the same number of people as DVDs but used more than 5 times fewer recommendations to achieve the same coverage of the nodes. Book recommendations reached by far the most people - 2.8 million. Notice that all networks have a very small number of unique edges. For books, videos and music the number of unique edges is smaller than the number of nodes - this suggests that the networks are highly disconnected [5].
Figure 1(a) shows the fraction of nodes in largest weakly connected component over time. Notice the component is very small. Even if we compose a network using all the recommendations in the dataset, the largest connected component contains less than 2.5% (100,420) of the nodes, and the second largest component has only 600 nodes. Still, some smaller communities, numbering in the tens of thousands of purchasers of DVDs in categories such as westerns, classics and Japanese animated films (anime), had connected components spanning about 20% of their members.
The insert in figure 1(a) shows the growth of the customer base over time. Surprisingly it was linear, adding on average 165,000 new users each month, which is an indication that the service itself was not spreading epidemically.
Further evidence of non-viral spread is provided by the relatively high percentage (94%) of users who made their first recommendation without having previously received one.
Back to table 1: given the total number of recommendations e and purchases (bb + be) influenced by recommendations we can estimate how many recommendations need to be independently sent over the network to induce a new purchase. Using this metric books have the most influential recommendations followed by DVDs and music. For books one out of 69 recommendations resulted in a purchase. For DVDs it increases to 108 recommendations per purchase and further increases to 136 for music and 203 for video.
Even with these simple counts we can make the first few observations. It seems that some people got quite heavily involved in the recommendation program, and that they tended to recommend a large number of products to the same set of friends (since the number of unique edges is so small). This shows that people tend to buy more DVDs and also like to recommend them to their friends, while they seem to be more conservative with books. One possible reason is that a book is bigger time investment than a DVD: one usually needs several days to read a book, while a DVD can be viewed in a single evening.
One external factor which may be affecting the recommendation patterns for DVDs is the existence of referral websites (www.dvdtalk.com). On these websites people, who want to buy a DVD and get a discount, would ask for recommendations. This way there would be recommendations made between people who don"t really know each other but rather have an economic incentive to cooperate. We were not able to find similar referral sharing sites for books or CDs.
Not all people who make a purchase also decide to give recommendations. So we estimate what fraction of people that purchase also decide to recommend forward. To obtain this information we can only use the nodes with purchases that resulted in a discount.
The last 3 columns of table 1 show that only about a third of the people that purchase also recommend the product forward. The ratio of forward recommendations is much higher for DVDs than for other kinds of products. Videos also have a higher ratio of forward recommendations, while books have the lowest. This shows that people are most keen on recommending movies, while more conservative when recommending books and music.
Figure 1(b) shows the cumulative out-degree distribution, that is the number of people who sent out at least kp recommendations, for a product. It shows that the deeper an individual is in the cascade, if they choose to make recommendations, they tend to recommend to a greater number of people on average (the distribution has a higher variance). This effect is probably due to only very heavily recommended products producing large enough cascades to reach a certain depth. We also observe that the probability of an individual making a recommendation at all (which can only occur if they make a purchase), declines after an initial increase as one gets deeper into the cascade.
As customers continue forwarding recommendations, they contribute to the formation of cascades. In order to identify cascades, i.e. the causal propagation of recommendations, we track successful recommendations as they influence purchases and further recommendations. We define a recommendation to be successful if it reached a node before its first purchase. We consider only the first purchase of an item, because there are many cases when a person made multiple 230 Group p n e eu bb be Purchases Forward Percent Book 103,161 2,863,977 5,741,611 2,097,809 65,344 17,769 65,391 15,769 24.2 DVD 19,829 805,285 8,180,393 962,341 17,232 58,189 16,459 7,336 44.6 Music 393,598 794,148 1,443,847 585,738 7,837 2,739 7,843 1,824 23.3 Video 26,131 239,583 280,270 160,683 909 467 909 250 27.6 Total 542,719 3,943,084 15,646,121 3,153,676 91,322 79,164 90,602 25,179 27.8 Table 1: Product group recommendation statistics. p: number of products, n: number of nodes, e: number of edges (recommendations), eu: number of unique edges, bb: number of buy bits, be: number of buy edges. Last
of nodes that purchased. Forward: nodes that purchased and then also recommended the product. 973 938 (a) Medical book (b) Japanese graphic novel Figure 2: Examples of two product recommendation networks: (a) First aid study guide First Aid for the USMLE Step, (b) Japanese graphic novel (manga) Oh My Goddess!: Mara Strikes Back. 10 0 10 5 10 0 10 2 10 4 10 6 10 8 Number of recommendations Count = 3.4e6 x−2.30 R2 =0.96 10 0 10 1 10 2 10 3 10 4 10 0 10 2 10 4 10 6 10 8 Number of purchases Count = 4.1e6 x−2.49 R2 =0.99 (a) Recommendations (b) Purchases Figure 3: Distribution of the number of recommendations and number of purchases made by a node. purchases of the same product, and in between those purchases she may have received new recommendations. In this case one cannot conclude that recommendations following the first purchase influenced the later purchases.
Each cascade is a network consisting of customers (nodes) who purchased the same product as a result of each other"s recommendations (edges). We delete late recommendations - all incoming recommendations that happened after the first purchase of the product. This way we make the network time increasing or causal - for each node all incoming edges (recommendations) occurred before all outgoing edges.
Now each connected component represents a time obeying propagation of recommendations.
Figure 2 shows two typical product recommendation networks: (a) a medical study guide and (b) a Japanese graphic novel. Throughout the dataset we observe very similar patters. Most product recommendation networks consist of a large number of small disconnected components where we do not observe cascades. Then there is usually a small number of relatively small components with recommendations successfully propagating.
This observation is reflected in the heavy tailed distribution of cascade sizes (see figure 4), having a power-law exponent close to 1 for DVDs in particular.
We also notice bursts of recommendations (figure 2(b)).
Some nodes recommend to many friends, forming a star like pattern. Figure 3 shows the distribution of the recommendations and purchases made by a single node in the recommendation network. Notice the power-law distributions and long flat tails. The most active person made 83,729 recommendations and purchased 4,416 different items. Finally, we also sometimes observe ‘collisions", where nodes receive recommendations from two or more sources. A detailed enumeration and analysis of observed topological cascade patterns for this dataset is made in [10].
A simple model can help explain how the wide variance we observe in the number of recommendations made by individuals can lead to power-laws in cascade sizes (figure 4). The model assumes that each recipient of a recommendation will forward it to others if its value exceeds an arbitrary threshold that the individual sets for herself. Since exceeding this value is a probabilistic event, let"s call pt the probability that at time step t the recommendation exceeds the thresh231 10 0 10 1 10 2 10 0 10 2 10 4 10 6 = 1.8e6 x−4.98 R2 =0.99 10 0 10 1 10 2 10 3 10 0 10 2 10 4 = 3.4e3 x−1.56 R2 =0.83 10 0 10 1 10 2 10 0 10 2 10 4 = 4.9e5 x−6.27 R2 =0.97 10 0 10 1 10 2 10 0 10 2 10 4 = 7.8e4 x−5.87 R2 =0.97 (a) Book (b) DVD (c) Music (d) Video Figure 4: Size distribution of cascades (size of cascade vs. count). Bold line presents a power-fit. old. In that case the number of recommendations Nt+1 at time (t + 1) is given in terms of the number of recommendations at an earlier time by Nt+1 = ptNt (1) where the probability pt is defined over the unit interval.
Notice that, because of the probabilistic nature of the threshold being exceeded, one can only compute the final distribution of recommendation chain lengths, which we now proceed to do.
Subtracting from both sides of this equation the term Nt and diving by it we obtain N(t+1) − Nt Nt = pt − 1 (2) Summing both sides from the initial time to some very large time T and assuming that for long times the numerator is smaller than the denominator (a reasonable assumption) we get dN N = pt (3) The left hand integral is just ln(N), and the right hand side is a sum of random variables, which in the limit of a very large uncorrelated number of recommendations is normally distributed (central limit theorem).
This means that the logarithm of the number of messages is normally distributed, or equivalently, that the number of messages passed is log-normally distributed. In other words the probability density for N is given by P(N) = 1 N √ 2πσ2 exp −(ln(N) − μ)2 2σ2 (4) which, for large variances describes a behavior whereby the typical number of recommendations is small (the mode of the distribution) but there are unlikely events of large chains of recommendations which are also observable.
Furthermore, for large variances, the lognormal distribution can behave like a power law for a range of values. In order to see this, take the logarithms on both sides of the equation (equivalent to a log-log plot) and one obtains ln(P(N)) = − ln(N) − ln( √ 2πσ2) − (ln (N) − μ)2 2σ2 (5) So, for large σ, the last term of the right hand side goes to zero, and since the the second term is a constant one obtains a power law behavior with exponent value of minus one. There are other models which produce power-law distributions of cascade sizes, but we present ours for its simplicity, since it does not depend on network topology [6] or critical thresholds in the probability of a recommendation being accepted [18].
So far we only looked into the aggregate statistics of the recommendation network. Next, we ask questions about the effectiveness of recommendations in the recommendation network itself. First, we analyze the probability of purchasing as one gets more and more recommendations. Next, we measure recommendation effectiveness as two people exchange more and more recommendations. Lastly, we observe the recommendation network from the perspective of the sender of the recommendation. Does a node that makes more recommendations also influence more purchases?
incoming recommendations First, we examine how the probability of purchasing changes as one gets more and more recommendations. One would expect that a person is more likely to buy a product if she gets more recommendations. On the other had one would also think that there is a saturation point - if a person hasn"t bought a product after a number of recommendations, they are not likely to change their minds after receiving even more of them. So, how many recommendations are too many?
Figure 5 shows the probability of purchasing a product as a function of the number of incoming recommendations on the product. As we move to higher numbers of incoming recommendations, the number of observations drops rapidly.
For example, there were 5 million cases with 1 incoming recommendation on a book, and only 58 cases where a person got 20 incoming recommendations on a particular book.
The maximum was 30 incoming recommendations. For these reasons we cut-off the plot when the number of observations becomes too small and the error bars too large.
Figure 5(a) shows that, overall, book recommendations are rarely followed. Even more surprisingly, as more and more recommendations are received, their success decreases.
We observe a peak in probability of buying at 2 incoming recommendations and then a slow drop.
For DVDs (figure 5(b)) we observe a saturation around 10 incoming recommendations. This means that after a person gets 10 recommendations on a particular DVD, they become immune to them - their probability of buying does not increase anymore. The number of observations is 2.5 million at 1 incoming recommendation and 100 at 60 incoming recommendations. The maximal number of received recommendations is 172 (and that person did not buy) 232
0
Incoming Recommendations ProbabilityofBuying
0
Incoming Recommendations ProbabilityofBuying (a) Books (b) DVD Figure 5: Probability of buying a book (DVD) given a number of incoming recommendations.
4 6 8 10 12 x 10 −3 Exchanged recommendations Probabilityofbuying
Exchanged recommendations Probabilityofbuying (a) Books (b) DVD Figure 6: The effectiveness of recommendations with the total number of exchanged recommendations.
Next, we analyze how the effectiveness of recommendations changes as two persons exchange more and more recommendations. A large number of exchanged recommendations can be a sign of trust and influence, but a sender of too many recommendations can be perceived as a spammer. A person who recommends only a few products will have her friends" attention, but one who floods her friends with all sorts of recommendations will start to loose her influence.
We measure the effectiveness of recommendations as a function of the total number of previously exchanged recommendations between the two nodes. We construct the experiment in the following way. For every recommendation r on some product p between nodes u and v, we first determine how many recommendations were exchanged between u and v before recommendation r. Then we check whether v, the recipient of recommendation, purchased p after recommendation r arrived. For the experiment we consider only node pairs (u, v), where there were at least a total of 10 recommendations sent from u to v. We perform the experiment using only recommendations from the same product group.
Figure 6 shows the probability of buying as a function of the total number of exchanged recommendations between two persons up to that point. For books we observe that the effectiveness of recommendation remains about constant up to 3 exchanged recommendations. As the number of exchanged recommendations increases, the probability of buying starts to decrease to about half of the original value and then levels off. For DVDs we observe an immediate and consistent drop. This experiment shows that recommendations start to lose effect after more than two or three are passed between two people. We performed the experiment also for video and music, but the number of observations was too low and the measurements were noisy.
In previous sections we examined the data from the viewpoint of the receiver of the recommendation. Now we look from the viewpoint of the sender. The two interesting questions are: how does the probability of getting a 10% credit change with the number of outgoing recommendations; and given a number of outgoing recommendations, how many purchases will they influence?
One would expect that recommendations would be the most effective when recommended to the right subset of friends. If one is very selective and recommends to too few friends, then the chances of success are slim. One the other hand, recommending to everyone and spamming them with recommendations may have limited returns as well.
The top row of figure 7 shows how the average number of purchases changes with the number of outgoing recommendations. For books, music, and videos the number of purchases soon saturates: it grows fast up to around 10 outgoing recommendations and then the trend either slows or starts to drop. DVDs exhibit different behavior, with the expected number of purchases increasing throughout. But if we plot the probability of getting a 10% credit as a function of the number of outgoing recommendations, as in the bottom row of figure 7, we see that the success of DVD recommendations saturates as well, while books, videos and music have qualitatively similar trends. The difference in the curves for DVD recommendations points to the presence of collisions in the dense DVD network, which has 10 recommendations per node and around 400 per product - an order of magnitude more than other product groups. This means that many different individuals are recommending to the same person, and after that person makes a purchase, even though all of them made a ‘successful recommendation" 233
0
Outgoing Recommendations NumberofPurchases
0 1 2 3 4 5 6 7 Outgoing Recommendations NumberofPurchases
0
Outgoing Recommendations NumberofPurchases
0
Outgoing Recommendations NumberofPurchases
0
Outgoing Recommendations ProbabilityofCredit
0
Outgoing Recommendations ProbabilityofCredit
0
Outgoing Recommendations ProbabilityofCredit
0
Outgoing Recommendations ProbabilityofCredit (a) Books (b) DVD (c) Music (d) Video Figure 7: Top row: Number of resulting purchases given a number of outgoing recommendations. Bottom row: Probability of getting a credit given a number of outgoing recommendations.
0
Lag [day] ProportionofPurchases
0
Lag [day] ProportionofPurchases (a) Books (b) DVD Figure 8: The time between the recommendation and the actual purchase. We use all purchases. by our definition, only one of them receives a credit.
PURCHASES The recommendation referral program encourages people to purchase as soon as possible after they get a recommendation, since this maximizes the probability of getting a discount. We study the time lag between the recommendation and the purchase of different product groups, effectively how long it takes a person to both receive a recommendation, consider it, and act on it.
We present the histograms of the thinking time, i.e. the difference between the time of purchase and the time the last recommendation was received for the product prior to the purchase (figure 8). We use a bin size of 1 day. Around 35%40% of book and DVD purchases occurred within a day after the last recommendation was received. For DVDs 16% purchases occur more than a week after last recommendation, while this drops to 10% for books. In contrast, if we consider the lag between the purchase and the first recommendation, only 23% of DVD purchases are made within a day, while the proportion stays the same for books. This reflects a greater likelihood for a person to receive multiple recommendations for a DVD than for a book. At the same time, DVD recommenders tend to send out many more recommendations, only one of which can result in a discount. Individuals then often miss their chance of a discount, which is reflected in the high ratio (78%) of recommended DVD purchases that did not a get discount (see table 1, columns bb and be). In contrast, for books, only 21% of purchases through recommendations did not receive a discount.
We also measure the variation in intensity by time of day for three different activities in the recommendation system: recommendations (figure 9(a)), all purchases (figure 9(b)), and finally just the purchases which resulted in a discount (figure 9(c)). Each is given as a total count by hour of day.
The recommendations and purchases follow the same pattern. The only small difference is that purchases reach a sharper peak in the afternoon (after 3pm Pacific Time, 6pm Eastern time). The purchases that resulted in a discount look like a negative image of the first two figures. This means that most of discounted purchases happened in the morning when the traffic (number of purchases/recommendations) on the retailer"s website was low. This makes a lot of sense since most of the recommendations happened during the day, and if the person wanted to get the discount by being the first one to purchase, she had the highest chances when the traffic on the website was the lowest.
BY BOOK CATEGORY Social networks are a product of the contexts that bring people together. Some contexts result in social ties that are more effective at conducting an action. For example, in small world experiments, where participants attempt to reach a target individual through their chain of acquaintances, profession trumped geography, which in turn was more useful in locating a target than attributes such as religion or hobbies [9, 17]. In the context of product recommendations, we can ask whether a recommendation for a work of fiction, which may be made by any friend or neighbor, is 234
0 2 4 6 8 10 x 10 5 Hour of the Day Recommendtions
0
1
2 x 10 4 Hour of the Day AllPurchases
0 1000 2000 3000 4000 5000 6000 7000 Hour of the Day DiscountedPurchases (a) Recommendations (b) Purchases (c) Purchases with Discount Figure 9: Time of day for purchases and recommendations. (a) shows the distribution of recommendations over the day, (b) shows all purchases and (c) shows only purchases that resulted in getting discount. more or less influential than a recommendation for a technical book, which may be made by a colleague at work or school.
Table 2 shows recommendation trends for all top level book categories by subject. An analysis of other product types can be found in the extended version of the paper. For clarity, we group the results by 4 different category types: fiction, personal/leisure, professional/technical, and nonfiction/other. Fiction encompasses categories such as Sci-Fi and Romance, as well as children"s and young adult books.
Personal/Leisure encompasses everything from gardening, photography and cooking to health and religion.
First, we compare the relative number of recommendations to reviews posted on the site (column cav/rp1 of table 2). Surprisingly, we find that the number of people making personal recommendations was only a few times greater than the number of people posting a public review on the website. We observe that fiction books have relatively few recommendations compared to the number of reviews, while professional and technical books have more recommendations than reviews. This could reflect several factors. One is that people feel more confident reviewing fiction than technical books. Another is that they hesitate to recommend a work of fiction before reading it themselves, since the recommendation must be made at the point of purchase. Yet another explanation is that the median price of a work of fiction is lower than that of a technical book. This means that the discount received for successfully recommending a mystery novel or thriller is lower and hence people have less incentive to send recommendations.
Next, we measure the per category efficacy of recommendations by observing the ratio of the number of purchases occurring within a week following a recommendation to the number of recommenders for each book subject category (column b of table 2). On average, only 2% of the recommenders of a book received a discount because their recommendation was accepted, and another 1% made a recommendation that resulted in a purchase, but not a discount.
We observe marked differences in the response to recommendation for different categories of books. Fiction in general is not very effectively recommended, with only around 2% of recommenders succeeding. The efficacy was a bit higher (around 3%) for non-fiction books dealing with personal and leisure pursuits, but is significantly higher in the professional and technical category. Medical books have nearly double the average rate of recommendation acceptance. This could be in part attributed to the higher median price of medical books and technical books in general. As we will see in Section 6, a higher product price increases the chance that a recommendation will be accepted.
Recommendations are also more likely to be accepted for certain religious categories: 4.3% for Christian living and theology and 4.8% for Bibles. In contrast, books not tied to organized religions, such as ones on the subject of new age (2.5%) and occult (2.2%) spirituality, have lower recommendation effectiveness. These results raise the interesting possibility that individuals have greater influence over one another in an organized context, for example through a professional contact or a religious one. There are exceptions of course. For example, Japanese anime DVDs have a strong following in the US, and this is reflected in their frequency and success in recommendations. Another example is that of gardening. In general, recommendations for books relating to gardening have only a modest chance of being accepted, which agrees with the individual prerogative that accompanies this hobby. At the same time, orchid cultivation can be a highly organized and social activity, with frequent ‘shows" and online communities devoted entirely to orchids. Perhaps because of this, the rate of acceptance of orchid book recommendations is twice as high as those for books on vegetable or tomato growing.
SUCCESS We have examined the properties of recommendation network in relation to viral marketing, but one question still remains: what determines the product"s viral marketing success? We present a model which characterizes product categories for which recommendations are more likely to be accepted. We use a regression of the following product attributes to correlate them with recommendation success: • r: number of recommendations • ns: number of senders of recommendations • nr: number of recipients of recommendations • p: price of the product • v: number of reviews of the product • t: average product rating 235 category np n cc rp1 vav cav/ pm b ∗ 100 rp1 Books general 370230 2,860,714 1.87 5.28 4.32 1.41 14.95 3.12 Fiction Children"s Books 46,451 390,283 2.82 6.44 4.52 1.12 8.76 2.06** Literature & Fiction 41,682 502,179 3.06 13.09 4.30 0.57 11.87 2.82* Mystery and Thrillers 10,734 123,392 6.03 20.14 4.08 0.36 9.60 2.40** Science Fiction & Fantasy 10,008 175,168 6.17 19.90 4.15 0.64 10.39 2.34** Romance 6,317 60,902 5.65 12.81 4.17 0.52 6.99 1.78** Teens 5,857 81,260 5.72 20.52 4.36 0.41 9.56 1.94** Comics & Graphic Novels 3,565 46,564 11.70 4.76 4.36 2.03 10.47 2.30* Horror 2,773 48,321 9.35 21.26 4.16 0.44 9.60 1.81** Personal/Leisure Religion and Spirituality 43,423 441,263 1.89 3.87 4.45 1.73 9.99 3.13 Health Mind and Body 33,751 572,704 1.54 4.34 4.41 2.39 13.96 3.04 History 28,458 28,3406 2.74 4.34 4.30 1.27 18.00 2.84 Home and Garden 19,024 180,009 2.91 1.78 4.31 3.48 15.37 2.26** Entertainment 18,724 258,142 3.65 3.48 4.29 2.26 13.97 2.66* Arts and Photography 17,153 179,074 3.49 1.56 4.42 3.85 20.95 2.87 Travel 12,670 113,939 3.91 2.74 4.26 1.87 13.27 2.39** Sports 10,183 120,103 1.74 3.36 4.34 1.99 13.97 2.26** Parenting and Families 8,324 182,792 0.73 4.71 4.42 2.57 11.87 2.81 Cooking Food and Wine 7,655 146,522 3.02 3.14 4.45 3.49 13.97 2.38* Outdoors & Nature 6,413 59,764 2.23 1.93 4.42 2.50 15.00 3.05 Professional/Technical Professional & Technical 41,794 459,889 1.72 1.91 4.30 3.22 32.50 4.54** Business and Investing 29,002 476,542 1.55 3.61 4.22 2.94 20.99 3.62** Science 25,697 271,391 2.64 2.41 4.30 2.42 28.00 3.90** Computers and Internet 18,941 375,712 2.22 4.51 3.98 3.10 34.95 3.61** Medicine 16,047 175,520 1.08 1.41 4.40 4.19 39.95 5.68** Engineering 10,312 107,255 1.30 1.43 4.14 3.85 59.95 4.10** Law 5,176 53,182 2.64 1.89 4.25 2.67 24.95 3.66* Nonfiction-other Nonfiction 55,868 560,552 2.03 3.13 4.29 1.89 18.95 3.28** Reference 26,834 371,959 1.94 2.49 4.19 3.04 17.47 3.21 Biographies and Memoirs 18,233 277,356 2.80 7.65 4.34 0.90 14.00 2.96 Table 2: Statistics by book category: np:number of products in category, n number of customers, cc percentage of customers in the largest connected component, rp1 av. # reviews in 2001 - 2003, rp2 av. # reviews 1st 6 months 2005, vav average star rating, cav average number of people recommending product, cav/rp1 ratio of recommenders to reviewers, pm median price, b ratio of the number of purchases resulting from a recommendation to the number of recommenders. The symbol ** denotes statistical significance at the 0.01 level, * at the 0.05 level.
From the original set of half a million products, we compute a success rate s for the 48,218 products that had at least one purchase made through a recommendation and for which a price was given. In section 5 we defined recommendation success rate s as the ratio of the total number purchases made through recommendations and the number of senders of the recommendations. We decided to use this kind of normalization, rather than normalizing by the total number of recommendations sent, in order not to penalize communities where a few individuals send out many recommendations (figure 2(b)). Since the variables follow a heavy tailed distribution, we use the following model: s = exp( i βi log(xi) + i) where xi are the product attributes (as described on previous page), and i is random error.
We fit the model using least squares and obtain the coefficients βi shown on table 3. With the exception of the average rating, they are all significant. The only two attributes with a positive coefficient are the number of recommendations and price. This shows that more expensive and more recommended products have a higher success rate.
The number of senders and receivers have large negative coefficients, showing that successfully recommended products are more likely to be not so widely popular. They have relatively many recommendations with a small number of senders and receivers, which suggests a very dense recommendation network where lots of recommendations were exchanged between a small community of people.
These insights could be to marketers - personal recommendations are most effective in small, densely connected communities enjoying expensive products. 236 Variable Coefficient βi const -0.940 (0.025)** r 0.426 (0.013)** ns -0.782 (0.004)** nr -1.307 (0.015)** p 0.128 (0.004)** v -0.011 (0.002)** t -0.027 (0.014)* R2
Table 3: Regression using the log of the recommendation success rate, ln(s), as the dependent variable.
For each coefficient we provide the standard error and the statistical significance level (**:0.01, *:0.1).
Although the retailer may have hoped to boost its revenues through viral marketing, the additional purchases that resulted from recommendations are just a drop in the bucket of sales that occur through the website. Nevertheless, we were able to obtain a number of interesting insights into how viral marketing works that challenge common assumptions made in epidemic and rumor propagation modeling.
Firstly, it is frequently assumed in epidemic models that individuals have equal probability of being infected every time they interact. Contrary to this we observe that the probability of infection decreases with repeated interaction.
Marketers should take heed that providing excessive incentives for customers to recommend products could backfire by weakening the credibility of the very same links they are trying to take advantage of.
Traditional epidemic and innovation diffusion models also often assume that individuals either have a constant probability of ‘converting" every time they interact with an infected individual or that they convert once the fraction of their contacts who are infected exceeds a threshold. In both cases, an increasing number of infected contacts results in an increased likelihood of infection. Instead, we find that the probability of purchasing a product increases with the number of recommendations received, but quickly saturates to a constant and relatively low probability. This means individuals are often impervious to the recommendations of their friends, and resist buying items that they do not want.
In network-based epidemic models, extremely highly connected individuals play a very important role. For example, in needle sharing and sexual contact networks these nodes become the super-spreaders by infecting a large number of people. But these models assume that a high degree node has as much of a probability of infecting each of its neighbors as a low degree node does. In contrast, we find that there are limits to how influential high degree nodes are in the recommendation network. As a person sends out more and more recommendations past a certain number for a product, the success per recommendation declines. This would seem to indicate that individuals have influence over a few of their friends, but not everybody they know.
We also presented a simple stochastic model that allows for the presence of relatively large cascades for a few products, but reflects well the general tendency of recommendation chains to terminate after just a short number of steps.
We saw that the characteristics of product reviews and effectiveness of recommendations vary by category and price, with more successful recommendations being made on technical or religious books, which presumably are placed in the social context of a school, workplace or place of worship.
Finally, we presented a model which shows that smaller and more tightly knit groups tend to be more conducive to viral marketing. So despite the relative ineffectiveness of the viral marketing program in general, we found a number of new insights which we hope will have general applicability to marketing strategies and to future models of viral information spread.
[1] Anonymous. Profiting from obscurity: What the long tail means for the economics of e-commerce.
Economist, 2005. [2] E. Brynjolfsson, Y. Hu, and M. D. Smith. Consumer surplus in the digital economy: Estimating the value of increased product variety at online booksellers.
Management Science, 49(11), 2003. [3] K. Burke. As consumer attitudes shift, so must marketing strategies. 2003. [4] J. Chevalier and D. Mayzlin. The effect of word of mouth on sales: Online book reviews. 2004. [5] P. Erd¨os and A. R´enyi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci., 1960. [6] D. Gruhl, R. Guha, D. Liben-Nowell, and A. Tomkins.
Information diffusion through blogspace. In WWW "04, 2004. [7] S. Jurvetson. What exactly is viral marketing? Red Herring, 78:110-112, 2000. [8] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of infuence in a social network. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2003. [9] P. Killworth and H. Bernard. Reverse small world experiment. Social Networks, 1:159-192, 1978. [10] J. Leskovec, A. Singh, and J. Kleinberg. Patterns of influence in a recommendation network. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), 2006. [11] G. Linden, B. Smith, and J. York. Amazon.com recommendations: item-to-item collaborative filtering.
IEEE Internet Computing, 7(1):76-80, 2003. [12] A. L. Montgomery. Applying quantitative marketing techniques to the internet. Interfaces, 30:90-108, 2001. [13] P. Resnick and R. Zeckhauser. Trust among strangers in internet transactions: Empirical analysis of ebays reputation system. In The Economics of the Internet and E-Commerce. Elsevier Science, 2002. [14] M. Richardson and P. Domingos. Mining knowledge-sharing sites for viral marketing. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2002. [15] E. M. Rogers. Diffusion of Innovations. Free Press,

Recommender systems have become integral to e-commerce, providing technology that suggests products to a visitor based on previous purchases or rating history.
Collaborative filtering, a common form of recommendation, predicts a user"s rating for an item by combining (other) ratings of that user with other users" ratings. Significant research has been conducted in implementing fast and accurate collaborative filtering algorithms [2, 7], designing interfaces for presenting recommendations to users [1], and studying the robustness of these algorithms [8]. However, with the exception of a few studies on the influence of users [10], little attention has been paid to unraveling the inner workings of a recommender in terms of the individual ratings and the roles they play in making (good) recommendations. Such an understanding will give an important handle to monitoring and managing a recommender system, to engineer mechanisms to sustain the recommender, and thereby ensure its continued success.
Our motivation here is to disaggregate global recommender performance metrics into contributions made by each individual rating, allowing us to characterize the many roles played by ratings in nearest-neighbor collaborative filtering.
We identify three possible roles: (scouts) to connect the user into the system to receive recommendations, (promoters) to connect an item into the system to be recommended, and (connectors) to connect ratings of these two kinds. Viewing ratings in this way, we can define the contribution of a rating in each role, both in terms of allowing recommendations to occur, and in terms of influence on the quality of recommendations. In turn, this capability helps support scenarios such as:
ratings may inadvertently connect the user to a neighborhood for which the user"s tastes may not be a perfect match. Identifying ratings responsible for such bad recommendations and suggesting new items to rate can help situate the user in a better neighborhood.
lack of user participation, especially in cold-start scenarios [13] involving newly arrived items. Identifying users who can be encouraged to rate specific items helps ensure coverage of the recommender system.
and its stakeholders: A recommender system is constantly under change: growing with new users and 250 items, shrinking with users leaving the system, items becoming irrelevant, and parts of the system under attack. Tracking the roles of a rating and its evolution over time provides many insights into the health of the system, and how it could be managed and improved.
These include being able to identify rating subspaces that do not contribute (or contribute negatively) to system performance, and could be removed; to enumerate users who are in danger of leaving, or have left the system; and to assess the susceptibility of the system to attacks such as shilling [5].
As we show, the characterization of rating roles presented here provides broad primitives to manage a recommender system and its community. The rest of the paper is organized as follows. Background on nearest-neighbor collaborative filtering and algorithm evaluation is discussed in Section 2. Section 3 defines and discusses the roles of a rating, and Section 4 defines measures of the contribution of a rating in each of these roles. In Section 5, we illustrate the use of these roles to address the goals outlined above.
Nearest-neighbor collaborative filtering algorithms either use neighborhoods of users or neighborhoods of items to compute a prediction. An algorithm of the first kind is called user-based, and one of the second kind is called itembased [12]. In both families of algorithms, neighborhoods are formed by first computing the similarity between all pairs of users (for user-based) or items (for item-based).
Predictions are then computed by aggregating ratings, which in a user-based algorithm involves aggregating the ratings of the target item by the user"s neighbors and, in an item-based algorithm, involves aggregating the user"s ratings of items that are neighbors of the target item. Algorithms within these families differ in the definition of similarity, formation of neighborhoods, and the computation of predictions.
We consider a user-based algorithm based on that defined for GroupLens [11] with variations from Herlocker et al. [2], and an item-based algorithm similar to that of Sarwar et al. [12].
The algorithm used by Resnick et al. [11] defines the similarity of two users u and v as the Pearson correlation of their common ratings: sim(u, v) = P i∈Iu∩Iv (ru,i − ¯ru)(rv,i − ¯rv) qP i∈Iu (ru,i − ¯ru)2 qP i∈Iv (rv,i − ¯rv)2 , where Iu is the set of items rated by user u, ru,i is user u"s rating for item i, and ¯ru is the average rating of user u (similarly for v). Similarity computed in this manner is typically scaled by a factor proportional to the number of common ratings, to reduce the chance of making a recommendation made on weak connections: sim (u, v) = max(|Iu ∩ Iv|, γ) γ · sim(u, v), where γ ≈ 5 is a constant used as a lower limit in scaling [2].
These new similarities are then used to define a static neighborhood Nu for each user u consisting of the top K users most similar to user u. A prediction for user u and item i is computed by a weighted average of the ratings by the neighbors pu,i = ¯ru + P v∈V sim (u, v)(rv,i − ¯rv) P v∈V sim (u, v) (1) where V = Nu ∩ Ui is the set of users most similar to u who have rated i.
The item-based algorithm we use is the one defined by Sarwar et al. [12]. In this algorithm, similarity is defined as the adjusted cosine measure sim(i, j) = P u∈Ui∩Uj (ru,i − ¯ru)(ru,j − ¯ru) qP u∈Ui (ru,i − ¯ru)2 qP u∈Uj (ru,j − ¯ru)2 (2) where Ui is the set of users who have rated item i. As for the user-based algorithm, the similarity weights are adjusted proportionally to the number of users that have rated the items in common sim (i, j) = max(|Ui ∩ Uj|, γ) γ · sim(i, j). (3) Given the similarities, the neighborhood Ni of an item i is defined as the top K most similar items for that item. A prediction for user u and item i is computed as the weighted average pu,i = ¯ri + P j∈J sim (i, j)(ru,j − ¯rj) P j∈J sim (i, j) (4) where J = Ni ∩ Iu is the set of items rated by u that are most similar to i.
Recommender algorithms have typically been evaluated using measures of predictive accuracy and coverage [3].
Studies on recommender algorithms, notably Herlocker et al. [2] and Sarwar et al. [12], typically compute predictive accuracy by dividing a set of ratings into training and test sets, and compute the prediction for an item in the test set using the ratings in the training set. A standard measure of predictive accuracy is mean absolute error (MAE), which for a test set T = {(u, i)} is defined as,
MAE = P (u,i)∈T |pu,i − ru,i| |T | . (5) Coverage has a number of definitions, but generally refers to the proportion of items that can be predicted by the algorithm [3].
A practical issue with predictive accuracy is that users typically are presented with recommendation lists, and not individual numeric predictions. Recommendation lists are lists of items in decreasing order of prediction (sometimes stated in terms of star-ratings), and so predictive accuracy may not be reflective of the accuracy of the list. So, instead we can measure recommendation or rank accuracy, which indicates the extent to which the list is in the correct order. Herlocker et al. [3] discuss a number of rank accuracy measures, which range from Kendall"s Tau to measures that consider the fact that users tend to only look at a prefix of the list [5]. Kendall"s Tau measures the number of inversions when comparing ordered pairs in the true user ordering of 251 Jim Tom Jeff My Cousin Vinny The Matrix Star Wars The Mask Figure 1: Ratings in simple movie recommender. items and the recommended order, and is defined as τ = C − D p (C + D + TR)(C + D + TP) (6) where C is the number of pairs that the system predicts in the correct order, D the number of pairs the system predicts in the wrong order, TR the number of pairs in the true ordering that have the same ratings, and TP is the number of pairs in the predicted ordering that have the same ratings [3]. A shortcoming of the Tau metric is that it is oblivious to the position in the ordered list where the inversion occurs [3]. For instance, an inversion toward the end of the list is given the same weight as one in the beginning.
One solution is to consider inversions only in the top few items in the recommended list or to weight inversions based on their position in the list.
Our basic observation is that each rating plays a different role in each prediction in which it is used. Consider a simplified movie recommender system with three users Jim,
Jeff, and Tom and their ratings for a few movies, as shown in Fig. 1. (For this initial discussion we will not consider the rating values involved.) The recommender predicts whether Tom will like The Mask using the other already available ratings. How this is done depends on the algorithm:
constructs a neighborhood of movies around The Mask by using the ratings of users who rated The Mask and other movies similarly (e.g., Jim"s ratings of The Matrix and The Mask; and Jeff"s ratings of Star Wars and The Mask). Tom"s ratings of those movies are then used to make a prediction for The Mask.
construct a neighborhood around Tom by tracking other users whose rating behaviors are similar to Tom"s (e.g.,
Tom and Jeff have rated Star Wars; Tom and Jim have rated The Matrix). The prediction of Tom"s rating for The Mask is then based on the ratings of Jeff and Tim.
Although the nearest-neighbor algorithms aggregate the ratings to form neighborhoods used to compute predictions, we can disaggregate the similarities to view the computation of a prediction as simultaneously following parallel paths of ratings. So, irrespective of the collaborative filtering algorithm used, we can visualize the prediction of Tom"s rating of The Mask as walking through a sequence of ratings. In Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Figure 2: Ratings used to predict The Mask for Tom.
Jim Tom Jeff The Matrix Star Wars The Mask q1 q2 q3 p1 p2 p3 Jerry r2 r3 Figure 3: Prediction of The Mask for Tom in which a rating is used more than once. this example, two paths were used for this prediction as depicted in Fig. 2: (p1, p2, p3) and (q1, q2, q3). Note that these paths are undirected, and are all of length 3. Only the order in which the ratings are traversed is different between the item-based algorithm (e.g., (p3, p2, p1), (q3, q2, q1)) and the user-based algorithm (e.g., (p1, p2, p3), (q1, q2, q3).) A rating can be part of many paths for a single prediction as shown in Fig. 3, where three paths are used for a prediction, two of which follow p1: (p1, p2, p3) and (p1, r2, r3).
Predictions in a collaborative filtering algorithms may involve thousands of such walks in parallel, each playing a part in influencing the predicted value. Each prediction path consists of three ratings, playing roles that we call scouts, promoters, and connectors. To illustrate these roles, consider the path (p1, p2, p3) in Fig. 2 used to make a prediction of The Mask for Tom:
from Tom to other ratings that can be used to predict Tom"s rating for The Mask. This rating serves as a scout in the bipartite graph of ratings to find a path that leads to The Mask.
recommend The Mask to Tom by connecting the scout to the promoter.
The Mask, and, therefore, promotes this movie to Tom.
Formally, given a prediction pu,a of a target item a for user u, a scout for pu,a is a rating ru,i such that there exists a user v with ratings rv,a and rv,i for some item i; a promoter for pu,a is a rating rv,a for some user v, such that there exist ratings rv,i and ru,i for an item i, and; a connector for pu,a 252 Jim Tom Jeff Jerry My Cousin Vinny The Matrix Star Wars The Mask Jurasic Park Figure 4: Scouts, promoters, and connectors. is a rating rv,i by some user v and rating i, such that there exists ratings ru,i and rv,a. The scouts, connectors, and promoters for the prediction of Tom"s rating of The Mask are p1 and q1, p2 and q2, and p3 and q3 (respectively). Each of these roles has a value in the recommender to the user, the user"s neighborhood, and the system in terms of allowing recommendations to be made.
Ratings that act as scouts tend to help the recommender system suggest more movies to the user, though the extent to which this is true depends on the rating behavior of other users. For example, in Fig. 4 the rating Tom → Star Wars helps the system recommend only The Mask to him, while Tom → The Matrix helps recommend The Mask, Jurassic Park, and My Cousin Vinny. Tom makes a connection to Jim who is a prolific user of the system, by rating The Matrix. However, this does not make The Matrix the best movie to rate for everyone. For example, Jim is benefited equally by both The Mask and The Matrix, which allow the system to recommend Star Wars to him. His rating of The Mask is the best scout for Jeff, and Jerry"s only scout is his rating of Star Wars. This suggests that good scouts allow a user to build similarity with prolific users, and thereby ensure they get more from the system.
While scouts represent beneficial ratings from the perspective of a user, promoters are their duals, and are of benefit to items. In Fig. 4, My Cousin Vinny benefits from Jim"s rating, since it allows recommendations to Jeff and Tom.
The Mask is not so dependent on just one rating, since the ratings by Jim and Jeff help it. On the other hand, Jerry"s rating of Star Wars does not help promote it to any other user. We conclude that a good promoter connects an item to a broader neighborhood of other items, and thereby ensures that it is recommended to more users.
Connectors serve a crucial role in a recommender system that is not as obvious. The movies My Cousin Vinny and Jurassic Park have the highest recommendation potential since they can be recommended to Jeff, Jerry and Tom based on the linkage structure illustrated in Fig. 4. Beside the fact that Jim rated these movies, these recommendations are possible only because of the ratings Jim → The Matrix and Jim → The Mask, which are the best connectors. A connector improves the system"s ability to make recommendations with no explicit gain for the user.
Note that every rating can be of varied benefit in each of these roles. The rating Jim → My Cousin Vinny is a poor scout and connector, but is a very good promoter. The rating Jim → The Mask is a reasonably good scout, a very good connector, and a good promoter. Finally, the rating Jerry → Star Wars is a very good scout, but is of no value as a connector or promoter. As illustrated here, a rating can have different value in each of the three roles in terms of whether a recommendation can be made or not. We could measure this value by simply counting the number of times a rating is used in each role, which alone would be helpful in characterizing the behavior of a system. But we can also measure the contribution of each rating to the quality of recommendations or health of the system. Since every prediction is a combined effort of several recommendation paths, we are interested in discerning the influence of each rating (and, hence, each path) in the system towards the system"s overall error. We can understand the dynamics of the system at a finer granularity by tracking the influence of a rating according to the role played. The next section describes the approach to measuring the values of a rating in each role.
As we"ve seen, a rating may play different roles in different predictions and, in each prediction, contribute to the quality of a prediction in different ways. Our approach can use any numeric measure of a property of system health, and assigns credit (or blame) to each rating proportional to its influence in the prediction. By tracking the role of each rating in a prediction, we can accumulate the credit for a rating in each of the three roles, and also track the evolution of the roles of rating over time in the system.
This section defines the methodology for computing the contribution of ratings by first defining the influence of a rating, and then instantiating the approach for predictive accuracy, and then rank accuracy. We also demonstrate how these contributions can be aggregated to study the neighborhood of ratings involved in computing a user"s recommendations. Note that although our general formulation for rating influence is algorithm independent, due to space considerations, we present the approach for only item-based collaborative filtering. The definition for user-based algorithms is similar and will be presented in an expanded version of this paper.
Recall that an item-based approach to collaborative filtering relies on building item neighborhoods using the similarity of ratings by the same user. As described earlier, similarity is defined by the adjusted cosine measure (Equations (2) and (3)). A set of the top K neighbors is maintained for all items for space and computational efficiency. A prediction of item i for a user u is computed as the weighted deviation from the item"s mean rating as shown in Equation (4). The list of recommendations for a user is then the list of items sorted in descending order of their predicted values.
We first define impact(a, i, j), the impact a user a has in determining the similarity between two items i and j. This is the change in the similarity between i and j when a"s rating is removed, and is defined as impact(a, i, j) = |sim (i, j) − sim¯a(i, j)| P w∈Cij |sim (i, j) − sim ¯w(i, j)| where Cij = {u ∈ U | ∃ ru,i, ru,j ∈ R(u)} is the set of coraters 253 of items i and j (users who rate both i and j), R(u) is the set of ratings provided by user u, and sim¯a(i, j) is the similarity of i and j when the ratings of user a are removed sim¯a(i, j) = P v∈U\{a} (ru,i − ¯ru)(ru,j − ¯ru) qP u∈U\{a}(ru,i − ¯ru)2 qP u∈U\{a}(ru,j − ¯ru)2 , and adjusted for the number of raters sim¯a(i, j) = max(|Ui ∩ Uj| − 1, γ) γ · sim(i, j).
If all coraters of i and j rate them identically, we define the impact as impact(a, i, j) = 1 |Cij| since P w∈Cij |sim (i, j) − sim ¯w(i, j)| = 0.
The influence of each path (u, j, v, i) = [ru,j, rv,j, rv,i] in the prediction of pu,i is given by influence(u, j, v, i) = sim (i, j) P l∈Ni∩Iu sim (i, l) · impact(v, i, j) It follows that the sum of influences over all such paths, for a given set of endpoints, is 1.
The value of a rating in each role is computed from the influence depending on the evaluation measure employed.
Here we illustrate the approach using predictive accuracy as the evaluation metric.
In general, the goodness of a prediction decides whether the ratings involved must be credited or discredited for their role. For predictive accuracy, the error in prediction e = |pu,i − ru,i| is mapped to a comfort level using a mapping function M(e). Anecdotal evidence suggests that users are unable to discern errors less than 1.0 (for a rating scale of 1 to 5) [4], and so an error less than 1.0 is considered acceptable, but anything larger is not. We hence define M(e) as (1 − e) binned to an appropriate value in [−1, −0.5, 0.5, 1].
For each prediction pu,i, M(e) is attributed to all the paths that assisted the computation of pu,i, proportional to their influences. This tribute, M(e)∗influence(u, j, v, i), is in turn inherited by each of the ratings in the path [ru,j, rv,j, rv,i], with the credit/blame accumulating to the respective roles of ru,j as a scout, rv,j as a connector, and rv,i as a promoter. In other words, the scout value SF(ru,j), the connector value CF(rv,j) and the promoter value PF(rv,i) are all incremented by the tribute amount. Over a large number of predictions, scouts that have repeatedly resulted in big error rates have a big negative scout value, and vice versa (similarly with the other roles). Every rating is thus summarized by its triple [SF, CF, PF].
We now define the computation of the contribution of ratings to observed rank accuracy. For this computation, we must know the user"s preference order for a set of items for which predictions can be computed. We assume that we have a test set of the users" ratings of the items presented in the recommendation list. For every pair of items rated by a user in the test data, we check whether the predicted order is concordant with his preference. We say a pair (i, j) is concordant (with error ) whenever one of the following holds: • if (ru,i < ru,j) then (pu,i − pu,j < ); • if (ru,i > ru,j) then (pu,i − pu,j > ); or • if (ru,i = ru,j) then (|pu,i − pu,j| ≤ ).
Similarly, a pair (i, j) is discordant (with error ) if it is not concordant. Our experiments described below use an error tolerance of = 0.1.
All paths involved in the prediction of the two items in a concordant pair are credited, and the paths involved in a discordant pair are discredited. The credit assigned to a pair of items (i, j) in the recommendation list for user u is computed as c(i, j) = ( t T · 1 C+D if (i, j) are concordant − t T · 1 C+D if (i, j) are discordant (7) where t is the number of items in the user"s test set whose ratings could be predicted, T is the number of items rated by user u in the test set, C is the number of concordances and D is the number of discordances. The credit c is then divided among all paths responsible for predicting pu,i and pu,j proportional to their influences. We again add the role values obtained from all the experiments to form a triple [SF, CF, PF] for each rating.
After calculating the role values for individual ratings, we can also use these values to study neighborhoods and the system. Here we consider how we can use the role values to characterize the health of a neighborhood. Consider the list of top recommendations presented to a user at a specific point in time. The collaborative filtering algorithm traversed many paths in his neighborhood through his scouts and other connectors and promoters to make these recommendations. We call these ratings the recommender neighborhood of the user. The user implicitly chooses this neighborhood of ratings through the items he rates. Apart from the collaborative filtering algorithm, the health of this neighborhood completely influences a user"s satisfaction with the system. We can characterize a user"s recommender neighborhood by aggregating the individual role values of the ratings involved, weighted by the influence of individual ratings in determining his recommended list. Different sections of the user"s neighborhood wield varied influence on his recommendation list. For instance, ratings reachable through highly rated items have a bigger say in the recommended items.
Our aim is to study the system and classify users with respect to their positioning in a healthy or unhealthy neighborhood. A user can have a good set of scouts, but may be exposed to a neighborhood with bad connectors and promoters. He can have a good neighborhood, but his bad scouts may ensure the neighborhood"s potential is rendered useless. We expect that users with good scouts and good neighborhoods will be most satisfied with the system in the future.
A user"s neighborhood is characterized by a triple that represents the weighted sum of the role values of individual ratings involved in making recommendations. Consider a user u and his ordered list of recommendations L. An item i 254 in the list is weighted inversely, as K(i), depending on its position in the list. In our studies we use K(i) = p position(i).
Several paths of ratings [ru,j, rv,j, rv,i] are involved in predicting pu,i which ultimately decides its position in L, each with influence(u, j, v, i).
The recommender neighborhood of a user u is characterized by the triple, [SFN(u), CFN(u), PFN(u)] where SFN(u) = X i∈L P [ru,j ,rv,j ,rv,i] SF(ru,j)influence(u, j, v, i) K(i) ! CFN(u) and PFN(u) are defined similarly. This triple estimates the quality of u"s recommendations based on the past track record of the ratings involved in their respective roles.
As we have seen, we can assign role values to each rating when evaluating a collaborative filtering system. In this section, we demonstrate the use of this approach to our overall goal of defining an approach to monitor and manage the health of a recommender system through experiments done on the MovieLens million rating dataset. In particular, we discuss results relating to identifying good scouts, promoters, and connectors; the evolution of rating roles; and the characterization of user neighborhoods.
Our experiments use the MovieLens million rating dataset, which consists of ratings by 6040 users of 3952 movies. The ratings are in the range 1 to 5, and are labeled with the time the rating was given. As discussed before, we consider only the item-based algorithm here (with item neighborhoods of size 30) and, due to space considerations, only present role value results for rank accuracy.
Since we are interested in the evolution of the rating role values over time, the model of the recommender system is built by processing ratings in their arrival order. The timestamping provided by MovieLens is hence crucial for the analyses presented here. We make assessments of rating roles at intervals of 10,000 ratings and processed the first 200,000 ratings in the dataset (giving rise to 20 snapshots). We incrementally update the role values as the time ordered ratings are merged into the model. To keep the experiment computationally manageable, we define a test dataset for each user. As the time ordered ratings are merged into the model, we label a small randomly selected percentage (20%) as test data. At discrete epochs, i.e., after processing every 10,000 ratings, we compute the predictions for the ratings in the test data, and then compute the role values for the ratings used in the predictions. One potential criticism of this methodology is that the ratings in the test set are never evaluated for their roles. We overcome this concern by repeating the experiment, using different random seeds. The probability that every rating is considered for evaluation is then considerably high: 1 − 0.2n , where n is the number of times the experiment is repeated with different random seeds. The results here are based on n = 4 repetitions.
The item-based collaborative filtering algorithm"s performance was ordinary with respect to rank accuracy. Fig. 5 shows a plot of the precision and recall as ratings were merged in time order into the model. The recall was always high, but the average precision was just about 53%. 0
1
10000 30000 50000 70000 90000110000130000150000 Ratings merged into model Value Precision Recall Figure 5: Precision and recall for the item-based collaborative filtering algorithm.
The ratings of a user that serve as scouts are those that allow the user to receive recommendations. We claim that users with ratings that have respectable scout values will be happier with the system than those with ratings with low scout values. Note that the item-based algorithm discussed here produces recommendation lists with nearly half of the pairs in the list discordant from the user"s preference.
Whether all of these discordant pairs are observable by the user is unclear, however, certainly this suggests that there is a need to be able to direct users to items whose ratings would improve the lists.
The distribution of the scout values for most users" ratings are Gaussian with mean zero. Fig. 6 shows the frequency distribution of scout values for a sample user at a given snapshot. We observe that a large number of ratings never serve as scouts for their users. A relatable scenario is when Amazon"s recommender makes suggestions of books or items based on other items that were purchased as gifts.
With simple relevance feedback from the user, such ratings can be isolated as bad scouts and discounted from future predictions. Removing bad scouts was found to be worthwhile for individual users but the overall performance improvement was only marginal.
An obvious question is whether good scouts can be formed by merely rating popular movies as suggested by Rashid et al. [9]. They show that a mix of popularity and rating entropy identifies the best items to suggest to new users when evaluated using MAE. Following their intuition, we would expect to see a higher correlation between popularityentropy and good scouts. We measured the Pearson correlation coefficient between aggregated scout values for a movie with the popularity of a movie (number of times it is rated); and with its popularity*variance measure at different snapshots of the system. Note that the scout values were initially anti-correlated with popularity (Fig. 7), but became moderately correlated as the system evolved. Both popularity and popularity*variance performed similarly. A possible explanation is that there has been insufficient time for the popular movies to accumulate ratings. 255 -10 0 10 20 30 40 50 60 -0.08 -0.06 -0.04 -0.02 0 0.02 0.04 Scout Value Frequency Figure 6: Distribution of scout values for a sample user. -0.4 -0.2 0
Popularity Pop*Var Figure 7: Correlation between aggregated scout value and item popularity (computed at different intervals). -0.6 -0.4 -0.2 0
1
Figure 8: Correlation between aggregated promoter value and user prolificity (computed at different intervals).
Table 1: Movies forming the best scouts.
Best Scouts Conf. Pop.
Being John Malkovich (1999) 1.00 445 Star Wars: Episode IV - A New Hope (1977) 0.92 623 Princess Bride, The (1987) 0.85 477 Sixth Sense, The (1999) 0.85 617 Matrix, The (1999) 0.77 522 Ghostbusters (1984) 0.77 441 Casablanca (1942) 0.77 384 Insider, The (1999) 0.77 235 American Beauty (1999) 0.69 624 Terminator 2: Judgment Day (1991) 0.69 503 Fight Club (1999) 0.69 235 Shawshank Redemption, The (1994) 0.69 445 Run Lola Run (Lola rennt) (1998) 0.69 220 Terminator, The (1984) 0.62 450 Usual Suspects, The (1995) 0.62 326 Aliens (1986) 0.62 385 North by Northwest (1959) 0.62 245 Fugitive, The (1993) 0.62 402 End of Days (1999) 0.62 132 Raiders of the Lost Ark (1981) 0.54 540 Schindler"s List (1993) 0.54 453 Back to the Future (1985) 0.54 543 Toy Story (1995) 0.54 419 Alien (1979) 0.54 415 Abyss, The (1989) 0.54 345 2001: A Space Odyssey (1968) 0.54 358 Dogma (1999) 0.54 228 Little Mermaid, The (1989) 0.54 203 Table 2: Movies forming the worst scouts.
Worst scouts Conf. Pop.
Harold and Maude (1971) 0.46 141 Grifters, The (1990) 0.46 180 Sting, The (1973) 0.38 244 Godfather: Part III, The (1990) 0.38 154 Lawrence of Arabia (1962) 0.38 167 High Noon (1952) 0.38 84 Women on the Verge of a... (1988) 0.38 113 Grapes of Wrath, The (1940) 0.38 115 Duck Soup (1933) 0.38 131 Arsenic and Old Lace (1944) 0.38 138 Midnight Cowboy (1969) 0.38 137 To Kill a Mockingbird (1962) 0.31 195 Four Weddings and a Funeral (1994) 0.31 271 Good, The Bad and The Ugly, The (1966) 0.31 156 It"s a Wonderful Life (1946) 0.31 146 Player, The (1992) 0.31 220 Jackie Brown (1997) 0.31 118 Boat, The (Das Boot) (1981) 0.31 210 Manhattan (1979) 0.31 158 Truth About Cats & Dogs, The (1996) 0.31 143 Ghost (1990) 0.31 227 Lone Star (1996) 0.31 125 Big Chill, The (1983) 0.31 184 256 By studying the evolution of scout values, we can identify movies that consistently feature in good scouts over time.
We claim these movies will make viable scouts for other users. We found the aggregated scout values for all movies in intervals of 10,000 ratings each. A movie is said to induce a good scout if the movie was in the top 100 of the sorted list, and to induce a bad scout if it was in bottom 100 of the same list. Movies appearing consistently high over time are expected to remain up there in the future. The effective confidence in a movie m is Cm = Tm − Bm N (8) where Tm is the number of times it appeared in the top 100, Bm the number of times it appeared in the bottom 100, and N is the number of intervals considered. Using this measure, the top few movies expected to induce the best scouts are shown in Table 1. Movies that would be bad scout choices are shown in Table 2 with their associated confidences. The popularities of the movies are also displayed.
Although more popular movies appear in the list of good scouts, these tables show that a blind choice of scout based on popularity alone can be potentially dangerous.
Interestingly, the best scout-‘Being John Malkovich"-is about a puppeteer who discovers a portal into a movie star, a movie that has been described variously on amazon.com as ‘makes you feel giddy," ‘seriously weird," ‘comedy with depth," ‘silly," ‘strange," and ‘inventive." Indicating whether someone likes this movie or not goes a long way toward situating the user in a suitable neighborhood, with similar preferences.
On the other hand, several factors may have made a movie a bad scout, like the sharp variance in user preferences in the neighborhood of a movie. Two users may have the same opinion about Lawrence of Arabia, but they may differ sharply about how they felt about the other movies they saw. Bad scouts ensue when there is deviation in behavior around a common synchronization point.
Ratings that serve to promote items in a collaborative filtering system are critical to allowing a new item be recommended to users. So, inducing good promoters is important for cold-start recommendation. We note that the frequency distribution of promoter values for a sample movie"s ratings is also Gaussian (similar to Fig. 6). This indicates that the promotion of a movie is benefited most by the ratings of a few users, and are unaffected by the ratings of most users.
We find a strong correlation between a user"s number of ratings and his aggregated promoter value. Fig. 8 depicts the evolution of the Pearson correlation co-efficient between the prolificity of a user (number of ratings) versus his aggregated promoter value. We expect that conspicuous shills, by recommending wrong movies to users, will be discredited with negative aggregate promoter values and should be identifiable easily.
Given this observation, the obvious rule to follow when introducing a new movie is to have it rated directly by prolific users who posses high aggregated promoter values. A new movie is thus cast into the neighborhood of many other movies improving its visibility. Note, though, that a user may have long stopped using the system. Tracking promoter values consistently allows only the most active recent users to be considered.
Given the way scouts, connectors, and promoters are characterized, it follows that the movies that are part of the best scouts are also part of the best connectors. Similarly, the users that constitute the best promoters are also part of the best connectors. Good connectors are induced by ensuring a user with a high promoter value rates a movie with a high scout value. In our experiments, we find that a rating"s longest standing role is often as a connector. A rating with a poor connector value is often seen due to its user being a bad promoter, or its movie being a bad scout. Such ratings can be removed from the prediction process to bring marginal improvements to recommendations. In some selected experiments, we observed that removing a set of badly behaving connectors helped improve the system"s overall performance by 1.5%. The effect was even higher on a few select users who observed an improvement of above 10% in precision without much loss in recall.
One of the more significant contributions of our work is the ability to model the evolution of recommender systems, by studying the changing roles of ratings over time. The role and value of a rating can change depending on many factors like user behavior, redundancy, shilling effects or properties of the collaborative filtering algorithm used. Studying the dynamics of rating roles in terms of transitions between good, bad, and negligible values can provide insights into the functioning of the recommender system. We believe that a continuous visualization of these transitions will improve the ability to manage a recommender system.
We classify different rating states as good, bad, or negligible. Consider a user who has rated 100 movies in a particular interval, of which 20 are part of the test set. If a scout has a value greater than 0.005, it indicates that it is uniquely involved in at least 2 concordant predictions, which we will say is good. Thus, a threshold of 0.005 is chosen to bin a rating as good, bad or negligible in terms of its scout, connector and promoter value. For instance, a rating r, at time t with role value triple [0.1, 0.001, −0.01] is classified as [scout +, connector 0, promoter −], where + indicates good,
The positive credit held by a rating is a measure of its contribution to the betterment of the system, and the discredit is a measure of its contribution to the detriment of the system. Even though the positive roles (and the negative roles) make up a very small percentage of all ratings, their contribution supersedes their size. For example, even though only
79% of all positive credit in the system! Similarly, the bad scouts were just 1.4% of all ratings but hold 82% of all discredit. Note that good and bad scouts, together, comprise only 1.4% + 1.7% = 3.1% of the ratings, implying that the majority of the ratings are negligible role players as scouts (more on this later). Likewise, good connectors were 1.2% of the system, and hold 30% of all positive credit. The bad connectors (0.8% of the system) hold 36% of all discredit.
Good promoters (3% of the system) hold 46% of all credit, while bad promoters (2%) hold 50% of all discredit. This reiterates that a few ratings influence most of the system"s performance. Hence it is important to track transitions between them regardless of their small numbers. 257 Across different snapshots, a rating can remain in the same state or change. A good scout can become a bad scout, a good promoter can become a good connector, good and bad scouts can become vestigial, and so on. It is not practical to expect a recommender system to have no ratings in bad roles. However, it suffices to see ratings in bad roles either convert to good or vestigial roles. Similarly, observing a large number of good roles become bad ones is a sign of imminent failure of the system.
We employ the principle of non-overlapping episodes [6] to count such transitions. A sequence such as: [+, 0, 0] → [+, 0, 0] → [0, +, 0] → [0, 0, 0] is interpreted as the transitions [+, 0, 0] ; [0, +, 0] : 1 [+, 0, 0] ; [0, 0, 0] : 1 [0, +, 0] ; [0, 0, 0] : 1 instead of [+, 0, 0] ; [0, +, 0] : 2 [+, 0, 0] ; [0, 0, 0] : 2 [0, +, 0] ; [0, 0, 0] : 1.
See [6] for further details about this counting procedure.
Thus, a rating can be in one of 27 possible states, and there are about 272 possible transitions. We make a further simplification and utilize only 9 states, indicating whether the rating is a scout, promoter, or connector, and whether it has a positive, negative, or negligible role. Ratings that serve multiple purposes are counted using multiple episode instantiations but the states themselves are not duplicated beyond the 9 restricted states. In this model, a transition such as [+, 0, +] ; [0, +, 0] : 1 is counted as [scout+] ; [scout0] : 1 [scout+] ; [connector+] : 1 [scout+] ; [promoter0] : 1 [connector0] ; [scout0] : 1 [connector0] ; [scout+] : 1 [connector0] ; [promoter0] : 1 [promoter+] ; [scout0] : 1 [promoter+] ; [connector+] : 1 [promoter+] ; [promoter0] : 1 Of these, transitions like [pX] ; [q0] where p = q, X ∈ {+, 0, −} are considered uninteresting, and only the rest are counted.
Fig. 9 depicts the major transitions counted while processing the first 200,000 ratings from the MovieLens dataset.
Only transitions with frequency greater than or equal to 3% are shown. The percentages for each state indicates the number of ratings that were found to be in those states.
We consider transitions from any state to a good state as healthy, from any state to a bad state as unhealthy, and from any state to a vestigial state as decaying.
From Fig. 9, we can observe: • The bulk of the ratings have negligible values, irrespective of their role. The majority of the transitions involve both good and bad ratings becoming negligible.
Scout + (2%)  Scout(1.5%) Scout 0 (96.5%) Connector + (1.2%)  Connector(0.8%) Connector 0 (98%) Promoter + (3%)  Promoter(2%) Promoter 0 (95%) 84% 84% 81% 74% 10% 6% 11% 77% 8% 7% 8% 82% 4% 86% 4% 68% 15% 13% 5% 5% 77% 11% 7% 5% 4% 3% 3% 3% Healthy Unhealthy Decaying Figure 9: Transitions among rating roles. • The number of good ratings is comparable to the bad ratings, and ratings are seen to switch states often, except in the case of scouts (see below). • The negative and positive scout states are not reachable through any transition, indicating that these ratings must begin as such, and cannot be coerced into these roles. • Good promoters and good connectors have a much longer survival period than scouts. Transitions that recur to these states have frequencies of 10% and 15% when compared to just 4% for scouts. Good connectors are the slowest to decay whereas (good) scouts decay the fastest. • Healthy percentages are seen on transitions between promoters and connectors. As indicated earlier, there are hardly any transitions from promoters/connectors to scouts. This indicates that, over the long run, a user"s rating is more useful to others (movies or other users) than to the user himself. • The percentages of healthy transitions outweigh the unhealthy ones - this hints that the system is healthy, albeit only marginally.
Note that these results are conditioned by the static nature of the dataset, which is a set of ratings over a fixed window of time. However a diagram such as Fig. 9 is clearly useful for monitoring the health of a recommender system. For instance, acceptable limits can be imposed on different types of transitions and, if a transition fails to meet the threshold, the recommender system or a part of it can be brought under closer scrutiny. Furthermore, the role state transition diagram would also be the ideal place to study the effects of shilling, a topic we will consider in future research.
Earlier we saw that we can characterize the neighborhood of ratings involved in creating a recommendation list L for 258 a user. In our experiment, we consider lists of length 30, and sample the lists of about 5% of users through the evolution of the model (at intervals of 10,000 ratings each) and compute their neighborhood characteristics. To simplify our presentation, we consider the percentage of the sample that fall into one of the following categories:
(SFN(u) > 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0)
(SFN(u) > 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0)
(SFN(u) < 0) ∧ (CFN(u) > 0 ∧ PFN(u) > 0)
(SFN(u) < 0) ∧ (CFN(u) < 0 ∨ PFN(u) < 0) From our sample set of 561 users, we found that 476 users were inactive. Of the remaining 85 users, we found 26 users had good scouts and a good neighborhood, 6 had bad scouts and a good neighborhood, 29 had good scouts and a bad neighborhood, and 24 had bad scouts and a bad neighborhood. Thus, we conjecture that 59 users (29+24+6) are in danger of leaving the system.
As a remedy, users with bad scouts and a good neighborhood can be asked to reconsider rating of some movies hoping to improve the system"s recommendations. The system can be expected to deliver more if they engineer some good scouts. Users with good scouts and a bad neighborhood are harder to address; this situation might entail selectively removing some connector-promoter pairs that are causing the damage. Handling users with bad scouts and bad neighborhoods is a more difficult challenge.
Such a classification allows the use of different strategies to better a user"s experience with the system depending on his context. In future work, we intend to conduct field studies and study the improvement in performance of different strategies for different contexts.
To further recommender system acceptance and deployment, we require new tools and methodologies to manage an installed recommender and develop insights into the roles played by ratings. A fine-grained characterization in terms of rating roles such as scouts, promoters, and connectors, as done here, helps such an endeavor. Although we have presented results on only the item-based algorithm with list rank accuracy as the metric, the same approach outlined here applies to user-based algorithms and other metrics.
In future research, we plan to systematically study the many algorithmic parameters, tolerances, and cutoff thresholds employed here and reason about their effects on the downstream conclusions. We also aim to extend our formulation to other collaborative filtering algorithms, study the effect of shilling in altering rating roles, conduct field studies, and evaluate improvements in user experience by tweaking ratings based on their role values. Finally, we plan to develop the idea of mining the evolution of rating role patterns into a reporting and tracking system for all aspects of recommender system health.

We illustrate our problem with an anecdote from a supply chain research exercise: the 2003 and 2004 Trading Agent Competition (TAC) Supply Chain Management (SCM) game. TAC/SCM [1] defines a scenario where agents compete to maximize their profits as manufacturers in a supply chain. The agents procure components from the various suppliers and assemble finished goods for sale to customers, repeatedly over a simulated year.
As it happened, the specified negotiation behavior of suppliers provided a great incentive for agents to procure large quantities of components on day 0: the very beginning of the simulation. During the early rounds of the 2003 SCM competition, several agent developers discovered this, and the apparent success led to most agents performing the majority of their purchasing on day 0. Although jockeying for day-0 procurement turned out to be an interesting strategic issue in itself [19], the phenomenon detracted from other interesting problems, such as adapting production levels to varying demand (since component costs were already sunk), and dynamic management of production, sales, and inventory. Several participants noted that the predominance of day-0 procurement overshadowed other key research issues, such as factory scheduling [2] and optimizing bids for customer orders [13]. After the 2003 tournament, there was a general consensus in the TAC community that the rules should be changed to deter large day-0 procurement.
The task facing game organizers can be viewed as a problem in mechanism design. The designers have certain game features under their control, and a set of objectives regarding game outcomes.
Unlike most academic treatments of mechanism design, the objective is a behavioral feature (moderate day-0 procurement) rather than an allocation feature like economic efficiency, and the allowed mechanisms are restricted to those judged to require only an incremental modification of the current game. Replacing the supplychain negotiation procedures with a one-shot direct mechanism, for example, was not an option. We believe that such operational restrictions and idiosyncratic objectives are actually quite typical of practical mechanism design settings, where they are perhaps more commonly characterized as incentive engineering problems.
In response to the problem, the TAC/SCM designers adopted several rule changes intended to penalize large day-0 orders. These included modifications to supplier pricing policies and introduction of storage costs assessed on inventories of components and finished goods. Despite the changes, day-0 procurement was very high in the early rounds of the 2004 competition. In a drastic measure, the GameMaster imposed a fivefold increase of storage costs midway through the tournament. Even this did not stem the tide, and day-0 procurement in the final rounds actually increased (by some measures) from 2003 [9].
The apparent difficulty in identifying rule modifications that effect moderation in day-0 procurement is quite striking. Although the designs were widely discussed, predictions for the effects of various proposals were supported primarily by intuitive arguments or at best by back-of-the-envelope calculations. Much of the difficulty, of course, is anticipating the agents" (and their developers") responses without essentially running a gaming exercise for this purpose. The episode caused us to consider whether new ap306 proaches or tools could enable more systematic analysis of design options. Standard game-theoretic and mechanism design methods are clearly relevant, although the lack of an analytic description of the game seems to be an impediment. Under the assumption that the simulator itself is the only reliable source of outcome computation, we refer to our task as empirical mechanism design.
In the sequel, we develop some general methods for empirical mechanism design and apply them to the TAC/SCM redesign problem. Our analysis focuses on the setting of storage costs (taking other game modifications as fixed), since this is the most direct deterrent to early procurement adopted. Our results confirm the basic intuition that incentives for day-0 purchasing decrease as storage costs rise. We also confirm that the high day-0 procurement observed in the 2004 tournament is a rational response to the setting of storage costs used. Finally, we conclude from our data that it is very unlikely that any reasonable setting of storage costs would result in acceptable levels of day-0 procurement, so a different design approach would have been required to eliminate this problem.
Overall, we contribute a formal framework and a set of methods for tackling indirect mechanism design problems in settings where only a black-box description of players" utilities is available. Our methods incorporate estimation of sets of Nash equilibria and sample Nash equilibria, used in conjuction to support general claims about the structure of the mechanism designer"s utility, as well as a restricted probabilistic analysis to assess the likelihood of conclusions. We believe that most realistic problems are too complex to be amenable to exact analysis. Consequently, we advocate the approach of gathering evidence to provide indirect support of specific hypotheses.
A normal form game2 is denoted by [I, {Ri}, {ui(r)}], where I refers to the set of players and m = |I| is the number of players.
Ri is the set of strategies available to player i ∈ I, with R = R1 ×. . .×Rm representing the set of joint strategies of all players.
We designate the set of pure strategies available to player i by Ai, and denote the joint set of pure strategies of all players by A = A1 ×. . .×Am. It is often convenient to refer to a strategy of player i separately from that of the remaining players. To accommodate this, we use a−i to denote the joint strategy of all players other than player i.
Let Si be the set of all probability distributions (mixtures) over Ai and, similarly, S be the set of all distributions over A. An s ∈ S is called a mixed strategy profile. When the game is finite (i.e., A and I are both finite), the probability that a ∈ A is played under s is written s(a) = s(ai, a−i). When the distribution s is not correlated, we can simply say si(ai) when referring to the probability player i plays ai under s.
Next, we define the payoff (utility) function of each player i by ui : A1 ×· · ·×Am → R, where ui(ai, a−i) indicates the payoff to player i to playing pure strategy ai when the remaining players play a−i. We can extend this definition to mixed strategies by assuming that ui are von Neumann-Morgenstern (vNM) utilities as follows: ui(s) = Es[ui], where Es is the expectation taken with respect to the probability distribution of play induced by the players" mixed strategy s. 2 By employing the normal form, we model agents as playing a single action, with decisions taken simultaneously. This is appropriate for our current study, which treats strategies (agent programs) as atomic actions. We could capture finer-grained decisions about action over time in the extensive form. Although any extensive game can be recast in normal form, doing so may sacrifice compactness and blur relevant distinctions (e.g., subgame perfection).
Occasionally, we write ui(x, y) to mean that x ∈ Ai or Si and y ∈ A−i or S−i depending on context. We also express the set of utility functions of all players as u(·) = {u1(·), . . . , um(·)}.
We define a function, : R → R, interpreted as the maximum benefit any player can obtain by deviating from its strategy in the specified profile. (r) = max i∈I max ai∈Ai [ui(ai, r−i) − ui(r)], (1) where r belongs to some strategy set, R, of either pure or mixed strategies.
Faced with a game, an agent would ideally play its best strategy given those played by the other agents. A configuration where all agents play strategies that are best responses to the others constitutes a Nash equilibrium.
DEFINITION 1. A strategy profile r = (r1, . . . , rm) constitutes a Nash equilibrium of game [I, {Ri}, {ui(r)}] if for every i ∈ I, ri ∈ Ri, ui(ri, r−i) ≥ ui(ri, r−i).
When r ∈ A, the above defines a pure strategy Nash equilibrium; otherwise the definition describes a mixed strategy Nash equilibrium. We often appeal to the concept of an approximate, or -Nash equilibrium, where is the maximum benefit to any agent for deviating from the prescribed strategy. Thus, (r) as defined above (1) is such that profile r is an -Nash equilibrium iff (r) ≤ .
In this study we devote particular attention to games that exhibit symmetry with respect to payoffs, rendering agents strategically identical.
DEFINITION 2. A game [I, {Ri}, {ui(r)}] is symmetric if for all i, j ∈ I, (a) Ri = Rj and (b) ui(ri, r−i) = uj (rj, r−j) whenever ri = rj and r−i = r−j
We model the strategic interactions between the designer of the mechanism and its participants as a two-stage game. The designer moves first by selecting a value, θ, from a set of allowable mechanism settings, Θ. All the participant agents observe the mechanism parameter θ and move simultaneously thereafter. For example, the designer could be deciding between a first-price and second-price sealed-bid auction mechanisms, with the presumption that after the choice has been made, the bidders will participate with full awareness of the auction rules.
Since the participants play with full knowledge of the mechanism parameter, we define a game between them in the second stage as Γθ = [I, {Ri}, {ui(r, θ)}]. We refer to Γθ as a game induced by θ. Let N(θ) be the set of strategy profiles considered solutions of the game Γθ.3 Suppose that the goal of the designer is to optimize the value of some welfare function, W (r, θ), dependent on the mechanism parameter and resulting play, r. We define a pessimistic measure,
W ( ˆR, θ) = inf{W (r, θ) : r ∈ ˆR}, representing the worst-case welfare of the game induced by θ, assuming that agents play some joint strategy in ˆR. Typically we care about W (N(θ), θ), the worst-case outcome of playing some solution.4 On some problems we can gain considerable advantage by using an aggregation function to map the welfare outcome of a game 3 We generally adopt Nash equilibrium as the solution concept, and thus take N(θ) to be the set of equilibria. However, much of the methodology developed here could be employed with alternative criteria for deriving agent behavior from a game definition. 4 Again, alternatives are available. For example, if one has a probability distribution over the solution set N(θ), it would be natural to take the expectation of W (r, θ) instead. 307 specified in terms of agent strategies to an equivalent welfare outcome specified in terms of a lower-dimensional summary.
DEFINITION 3. A function φ : R1 × · · · × Rm → Rq is an aggregation function if m ≥ q and W (r, θ) = V (φ(r), θ) for some function V .
We overload the function symbol to apply to sets of strategy profiles: φ( ˆR) = {φ(r) : r ∈ ˆR}. For convenience of exposition, we write φ∗ (θ) to mean φ(N(θ)).
Using an aggregation function yields a more compact representation of strategy profiles. For example, suppose-as in our application below-that an agent"s strategy is defined by a numeric parameter. If all we care about is the total value played, we may take φ(a) = Pm i=1 ai. If we have chosen our aggregator carefully, we may also capture structure not obvious otherwise. For example, φ∗ (θ) could be decreasing in θ, whereas N(θ) might have a more complex structure.
Given a description of the solution correspondence N(θ) (equivalently, φ∗ (θ)), the designer faces a standard optimization problem. Alternatively, given a simulator that could produce an unbiased sample from the distribution of W (N(θ), θ) for any θ, the designer would be faced with another much appreciated problem in the literature: simulation optimization [12].
However, even for a game Γθ with known payoffs it may be computationally intractable to solve for Nash equilibria, particularly if the game has large or infinite strategy sets. Additionally, we wish to study games where the payoffs are not explicitly given, but must be determined from simulation or other experience with the game.5 Accordingly, we assume that we are given a (possibly noisy) data set of payoff realizations: Do = {(θ1 , a1 , U1 ), . . . , (θk , ak , Uk )}, where for every data point θi is the observed mechanism parameter setting, ai is the observed pure strategy profile of the participants, and Ui is the corresponding realization of agent payoffs. We may also have additional data generated by a (possibly noisy) simulator: Ds = {(θk+1 , ak+1 , Uk+1 ), . . . , (θk+l , ak+l , Uk+l )}. Let D = {Do, Ds} be the combined data set. (Either Do or Ds may be null for a particular problem.) In the remainder of this paper, we apply our modeling approach, together with several empirical game-theoretic methods, in order to answer questions regarding the design of the TAC/SCM scenario.
Since our data comes in the form of payoff experience and not as the value of an objective function for given settings of the control variable, we can no longer rely on the methods for optimizing functions using simulations. Indeed, a fundamental aspect of our design problem involves estimating the Nash equilibrium correspondence.
Furthermore, we cannot rely directly on the convergence results that abound in the simulation optimization literature, and must establish probabilistic analysis methods tailored for our problem setting.
We describe our empirical design analysis methods by presenting a detailed application to the TAC/SCM scenario introduced above.
Recall that during the 2004 tournament, the designers of the supplychain game chose to dramatically increase storage costs as a measure aimed at curbing day-0 procurement, to little avail. Here we systematically explore the relationship between storage costs and 5 This is often the case for real games of interest, where natural language or algorithmic descriptions may substitute for a formal specification of strategy and payoff functions. the aggregate quantity of components procured on day 0 in equilibrium. In doing so, we consider several questions raised during and after the tournament. First, does increasing storage costs actually reduce day-0 procurement? Second, was the excessive day-0 procurement that was observed during the 2004 tournament rational?
And third, could increasing storage costs sufficiently have reduced day-0 procurement to an acceptable level, and if so, what should the setting of storage costs have been? It is this third question that defines the mechanism design aspect of our analysis.6 To apply our methods, we must specify the agent strategy sets, the designer"s welfare function, the mechanism parameter space, and the source of data. We restrict the agent strategies to be a multiplier on the quantity of the day-0 requests by one of the finalists, Deep Maize, in the 2004 TAC/SCM tournament. We further restrict it to the set [0,1.5], since any strategy below 0 is illegal and strategies above 1.5 are extremely aggressive (thus unlikely to provide refuting deviations beyond those available from included strategies, and certainly not part of any desirable equilibrium). All other behavior is based on the behavior of Deep Maize and is identical for all agents. This choice can provide only an estimate of the actual tournament behavior of a typical agent. However, we believe that the general form of the results should be robust to changes in the full agent behavior.
We model the designer"s welfare function as a threshold on the sum of day-0 purchases. Let φ(a) = P6 i=1 ai be the aggregation function representing the sum of day-0 procurement of the six agents participating in a particular supply-chain game (for mixed strategy profiles s, we take expectation of φ with respect to the mixture). The designer"s welfare function W (N(θ), θ) is then given by I{sup{φ∗ (θ)} ≤ α}, where α is the maximum acceptable level of day-0 procurement and I is the indicator function. The designer selects a value θ of storage costs, expressed as an annual percentage of the baseline value of components in the inventory (charged daily), from the set Θ = R+ . Since the designer"s decision depends only on φ∗ (θ), we present all of our results in terms of the value of the aggregation function.
The objective of TAC/SCM agents is to maximize profits realized over a game instance. Thus, if we fix a strategy for each agent at the beginning of the simulation and record the corresponding profits at the end, we will have obtained a data point in the form (a, U(a)). If we also have fixed the parameter θ of the simulator, the resulting data point becomes part of our data set D. This data set, then, contains data only in the form of pure strategies of players and their corresponding payoffs, and, consequently, in order to formulate the designer"s problem as optimization, we must first determine or approximate the set of Nash equilibria of each game Γθ.
Thus, we need methods for approximating Nash equilibria for infinite games. Below, we describe the two methods we used in our study. The first has been explored empirically before, whereas the second is introduced here as the method specifically designed to approximate a set of Nash equilibria.
The first method for estimating Nash equilibria based on data uses supervised learning to approximate payoff functions of mech6 We do not address whether and how other measures (e.g., constraining procurement directly) could have achieved design objectives. Our approach takes as given some set of design options, in this case defined by the storage cost parameter. In principle our methods could be applied to a different or larger design space, though with corresponding complexity growth. 308 anism participants from a data set of game experience [17]. Once approximate payoff functions are available for all players, the Nash equilibria may be either found analytically or approximated using numerical techniques, depending on the learning model. In what follows, we estimate only a sample Nash equilibrium using this technique, although this restriction can be removed at the expense of additional computation time.
One advantage of this method is that it can be applied to any data set and does not require the use of a simulator. Thus, we can apply it when Ds = ∅. If a simulator is available, we can generate additional data to build confidence in our initial estimates.7 We tried the following methods for approximating payoff functions: quadratic regression (QR), locally weighted average (LWA), and locally weighted linear regression (LWLR). We also used control variates to reduce the variance of payoff estimates, as in our previous empirical game-theoretic analysis of TAC/SCM-03 [19].
The quadratic regression model makes it possible to compute equilibria of the learned game analytically. For the other methods we applied replicator dynamics [7] to a discrete approximation of the learned game. The expected total day-0 procurement in equilibrium was taken as the estimate of an outcome.
When we have access to a simulator, we can also use directed search through profile space to estimate the set of Nash equilibria, which we describe here after presenting some additional notation.
DEFINITION 4. A strategic neighbor of a pure strategy profile a is a profile that is identical to a in all but one strategy. We define Snb(a, D) as the set of all strategic neighbors of a available in the data set D. Similarly, we define Snb(a, ˜D) to be all strategic neighbors of a not in D. Finally, for any a ∈ Snb(a, D) we define the deviating agent as i(a, a ).
DEFINITION 5. The -bound, ˆ, of a pure strategy profile a is defined as maxa ∈Snb(a,D) max{ui(a,a )(a )−ui(a,a )(a), 0}. We say that a is a candidate δ-equilibrium for δ ≥ ˆ.
When Snb(a, ˜D) = ∅ (i.e., all strategic neighbors are represented in the data), a is confirmed as an ˆ-Nash equilibrium.
Our search method operates by exploring deviations from candidate equilibria. We refer to it as BestFirstSearch, as it selects with probability one a strategy profile a ∈ Snb(a, ˜D) that has the smallest ˆin D.
Finally we define an estimator for a set of Nash equilibria.
DEFINITION 6. For a set K, define Co(K) to be the convex hull of K. Let Bδ be the set of candidates at level δ. We define ˆφ∗ (θ) = Co({φ(a) : a ∈ Bδ}) for a fixed δ to be an estimator of φ∗ (θ).
In words, the estimate of a set of equilibrium outcomes is the convex hull of all aggregated strategy profiles with -bound below some fixed δ. This definition allows us to exploit structure arising from the aggregation function. If two profiles are close in terms of aggregation values, they may be likely to have similar -bounds.
In particular, if one is an equilibrium, the other may be as well. We present some theoretical support for this method of estimating the set of Nash equilibria below.
Since the game we are interested in is infinite, it is necessary to terminate BestFirstSearch before exploring the entire space of strat7 For example, we can use active learning techniques [5] to improve the quality of payoff function approximation. In this work, we instead concentrate on search in strategy profile space. egy profiles. We currently determine termination time in a somewhat ad-hoc manner, based on observations about the current set of candidate equilibria.8
Our data was collected by simulating TAC/SCM games on a local version of the 2004 TAC/SCM server, which has a configuration setting for the storage cost. Agent strategies in simulated games were selected from the set {0, 0.3, 0.6, . . . , 1.5} in order to have positive probability of generating strategic neighbors.9 A baseline data set Do was generated by sampling 10 randomly generated strategy profiles for each θ ∈ {0, 50, 100, 150, 200}. Between 5 and 10 games were run for each profile after discarding games that had various flaws.10 We used search to generate a simulated data set Ds, performing between 12 and 32 iterations of BestFirstSearch for each of the above settings of θ. Since simulation cost is extremely high (a game takes nearly 1 hour to run), we were able to run a total of 2670 games over the span of more than six months.
For comparison, to get the entire description of an empirical game defined by the restricted finite joint strategy space for each value of θ ∈ {0, 50, 100, 150, 200} would have required at least 23100 games (sampling each profile 10 times).
We applied the three learning methods described above to the baseline data set Do. Additionally, we generated an estimate of the Nash equilibrium correspondence, ˆφ∗ (θ), by applying Definition 6 with δ =2.5E6. The results are shown in Figure 1. As we can see, the correspondence ˆφ∗ (θ) has little predictive power based on Do, and reveals no interesting structure about the game. In contrast, all three learning methods suggest that total day-0 procurement is a decreasing function of storage costs. 0 1 2 3 4 5 6 7 8 9 10
Storage Cost TotalDay-0Procurement LWA LWLR QR BaselineMin BaselineMax Figure 1: Aggregate day-0 procurement estimates based on Do.
The correspondence ˆφ∗ (θ) is the interval between BaselineMin and BaselineMax. 8 Generally, search is terminated once the set of candidate equilibria is small enough to draw useful conclusions about the likely range of equilibrium strategies in the game. 9 Of course, we do not restrict our Nash equilibrium estimates to stay in this discrete subset of [0,1.5]. 10 For example, if we detected that any agent failed during the game (failures included crashes, network connectivity problems, and other obvious anomalies), the game would be thrown out. 309
To corroborate the initial evidence from the learning methods, we estimated ˆφ∗ (θ) (again, using δ =2.5E6) on the data set D = {Do, Ds}, where Ds is data generated through the application of BestFirstSearch. The results of this estimate are plotted against the results of the learning methods trained on Do 11 in Figure 2. First, we note that the addition of the search data narrows the range of potential equilibria substantially. Furthermore, the actual point predictions of the learning methods and those based on -bounds after search are reasonably close. Combining the evidence gathered from these two very different approaches to estimating the outcome correspondence yields a much more compelling picture of the relationship between storage costs and day-0 procurement than either method used in isolation. 0 1 2 3 4 5 6 7 8 9 10
Storage Cost TotayDay-0Procurement LWA LWLR QR SearchMin SearchMax Figure 2: Aggregate day-0 procurement estimates based on search in strategy profile space compared to function approximation techniques trained on Do. The correspondence ˆφ∗ (θ) for D = {Do, Ds} is the interval between SearchMin and SearchMax.
This evidence supports the initial intuition that day-0 procurement should be decreasing with storage costs. It also confirms that high levels of day-0 procurement are a rational response to the 2004 tournament setting of average storage cost, which corresponds to θ = 100. The minimum prediction for aggregate procurement at this level of storage costs given by any experimental methods is approximately 3. This is quite high, as it corresponds to an expected commitment of 1/3 of the total supplier capacity for the entire game. The maximum prediction is considerably higher at 4.5.
In the actual 2004 competition, aggregate day-0 procurement was equivalent to 5.71 on the scale used here [9]. Our predictions underestimate this outcome to some degree, but show that any rational outcome was likely to have high day-0 procurement.
We have reasonably strong evidence that the outcome correspondence is decreasing. However, the ultimate goal is to be able to either set the storage cost parameter to a value that would curb day-0 procurement in equilibrium or conclude that this is not possible.
To answer this question directly, suppose that we set a conservative threshold α = 2 on aggregate day-0 procurement.12 Linear 11 It is unclear how meaningful the results of learning would be if Ds were added to the training data set. Indeed, the additional data may actually increase the learning variance. 12 Recall that designer"s objective is to incentivize aggergate day-0 procurement that is below the threshold α. Our threshold here still represents a commitment of over 20% of the suppliers" capacity for extrapolation of the maximum of the outcome correspondence estimated from D yields θ = 320.
The data for θ = 320 were collected in the same way as for other storage cost settings, with 10 randomly generated profiles followed by 33 iterations of BestFirstSearch. Figure 3 shows the detailed -bounds for all profiles in terms of their corresponding values of φ.
Total Day-0 Procurement ε−boundFigure 3: Values of ˆ for profiles explored using search when θ = 320. Strategy profiles explored are presented in terms of the corresponding values of φ(a). The gray region corresponds to ˆφ∗ (320) with δ =2.5M.
The estimated set of aggregate day-0 outcomes is very close to that for θ = 200, indicating that there is little additional benefit to raising storage costs above 200. Observe, that even the lower bound of our estimated set of Nash equilibria is well above the target day-0 procurement of 2. Furthermore, payoffs to agents are almost always negative at θ = 320. Consequently, increasing the costs further would be undesirable even if day-0 procurement could eventually be curbed. Since we are reasonably confident that φ∗ (θ) is decreasing in θ, we also do not expect that setting θ somewhere between 200 and 320 will achieve the desired result.
We conclude that it is unlikely that day-0 procurement could ever be reduced to a desirable level using any reasonable setting of the storage cost parameter. That our predictions tend to underestimate tournament outcomes reinforces this conclusion. To achieve the desired reduction in day-0 procurement requires redesigning other aspects of the mechanism.
Our empirical analysis has produced evidence in support of the conclusion that no reasonable setting of storage cost was likely to sufficiently curb excessive day-0 procurement in TAC/SCM "04.
All of this evidence has been in the form of simple interpolation and extrapolation of estimates of the Nash equilibrium correspondence.
These estimates are based on simulating game instances, and are subject to sampling noise contributed by the various stochastic elements of the game. In this section, we develop and apply methods for evaluating the sensitivity of our -bound calculations to such stochastic effects.
Suppose that all agents have finite (and small) pure strategy sets,
A. Thus, it is feasible to sample the entire payoff matrix of the game. Additionally, suppose that noise is additive with zero-mean the entire game on average, so in practice we would probably want the threshold to be even lower. 310 and finite variance, that is, Ui(a) = ui(a) + ˜ξi(a), where Ui(a) is the observed payoff to i when a was played, ui(a) is the actual corresponding payoff, and ˜ξi(a) is a mean-zero normal random variable. We designate the known variance of ˜ξi(a) by σ2 i (a). Thus, we assume that ˜ξi(a) is normal with distribution N(0, σ2 i (a)).
We take ¯ui(a) to be the sample mean over all Ui(a) in D, and follow Chang and Huang [3] to assume that we have an improper prior over the actual payoffs ui(a) and sampling was independent for all i and a. We also rely on their result that ui(a)|¯ui(a) = ¯ui(a)−Zi(a)/[σi(a)/ p ni(a)] are independent with posterior distributions N(¯ui(a), σ2 i (a)/ni(a)), where ni(a) is the number of samples taken of payoffs to i for pure profile a, and Zi(a) ∼ N(0, 1).
We now derive a generic probabilistic bound that a profile a ∈ A is an -Nash equilibrium. If ui(·)|¯ui(·) are independent for all i ∈ I and a ∈ A, we have the following result (from this point on we omit conditioning on ¯ui(·) for brevity): PROPOSITION 1.
Pr „ max i∈I max b∈Ai ui(b, a−i) − ui(a) ≤ « = = Y i∈I Z R Y b∈Ai\ai Pr(ui(b, a−i) ≤ u + )fui(a)(u)du, (2) where fui(a)(u) is the pdf of N(¯ui(a), σi(a)).
The proofs of this and all subsequent results are in the Appendix.
The posterior distribution of the optimum mean of n samples, derived by Chang and Huang [3], is Pr (ui(a) ≤ c) = 1 − Φ "p ni(a)(¯ui(a) − c) σi(a) # , (3) where a ∈ A and Φ(·) is the N(0, 1) distribution function.
Combining the results (2) and (3), we obtain a probabilistic confidence bound that (a) ≤ γ for a given γ.
Now, we consider cases of incomplete data and use the results we have just obtained to construct an upper bound (restricted to profiles represented in data) on the distribution of sup{φ∗ (θ)} and inf{φ∗ (θ)} (assuming that both are attainable): Pr{sup{φ∗ (θ)} ≤ x} ≤D Pr{∃a ∈ D : φ(a) ≤ x ∧ a ∈ N(θ)} ≤ X a∈D:φ(a)≤x Pr{a ∈ N(θ)} = X a∈D:φ(a)≤x Pr{ (a) = 0}, where x is a real number and ≤D indicates that the upper bound accounts only for strategies that appear in the data set D. Since the events {∃a ∈ D : φ(a) ≤ x ∧ a ∈ N(θ)} and {inf{φ∗ (θ)} ≤ x} are equivalent, this also defines an upper bound on the probability of {inf{φ∗ (θ)} ≤ x}. The values thus derived comprise the Tables 1 and 2. φ∗ (θ) θ = 0 θ = 50 θ = 100 <2.7 0.000098 0 0.146 <3 0.158 0.0511 0.146 <3.9 0.536 0.163 1 <4.5 1 1 1 Table 1: Upper bounds on the distribution of inf{φ∗ (θ)} restricted to D for θ ∈ {0, 50, 100} when N(θ) is a set of Nash equilibria. φ∗ (θ) θ = 150 θ = 200 θ = 320 <2.7 0 0 0.00132 <3 0.0363 0.141 1 <3.9 1 1 1 <4.5 1 1 1 Table 2: Upper bounds on the distribution of inf{φ∗ (θ)} restricted to D for θ ∈ {150, 200, 320} when N(θ) is a set of Nash equilibria.
Tables 1 and 2 suggest that the existence of any equilibrium with φ(a) < 2.7 is unlikely for any θ that we have data for, although this judgment, as we mentioned, is only with respect to the profiles we have actually sampled. We can then accept this as another piece of evidence that the designer could not find a suitable setting of θ to achieve his objectives-indeed, the designer seems unlikely to achieve his objective even if he could persuade participants to play a desirable equilibrium! Table 1 also provides additional evidence that the agents in the
numbers of components at the beginning fo the game. If we look at the third column of this table, which corresponds to θ = 100, we can gather that no profile a in our data with φ(a) < 3 is very likely to be played in equilibrium.
The bounds above provide some general evidence, but ultimately we are interested in a concrete probabilistic assessment of our conclusion with respect to the data we have sampled. Particularly, we would like to say something about what happens for the settings of θ for which we have no data. To derive an approximate probabilistic bound on the probability that no θ ∈ Θ could have achieved the designer"s objective, let ∪J j=1Θj, be a partition of Θ, and assume that the function sup{φ∗ (θ)} satisfies the Lipschitz condition with Lipschitz constant Aj on each subset Θj.13 Since we have determined that raising the storage cost above 320 is undesirable due to secondary considerations, we restrict attention to Θ = [0, 320]. We now define each subset j to be the interval between two points for which we have produced data. Thus,
Θ = [0, 50] [ (50, 100] [ (100, 150] [ (150, 200] [ (200, 320], with j running between 1 and 5, corresponding to subintervals above. We will further denote each Θj by (aj , bj].14 Then, the following Proposition gives us an approximate upper bound15 on the probability that sup{φ∗ (θ)} ≤ α.
PROPOSITION 2.
Pr{ _ θ∈Θ sup{φ(θ)} ≤ α} ≤D 5X j=1 X y,z∈D:y+z≤cj 0 @ X a:φ(a)=z Pr{ (a) = 0} 1 A × × 0 @ X a:φ(a)=y Pr{ (a) = 0} 1 A , where cj = 2α + Aj(bj − aj) and ≤D indicates that the upper bound only accounts for strategies that appear in the data set D. 13 A function that satisfies the Lipschitz condition is called Lipschitz continuous. 14 The treatment for the interval [0,50] is identical. 15 It is approximate in a sense that we only take into account strategies that are present in the data. 311 Due to the fact that our bounds are approximate, we cannot use them as a conclusive probabilistic assessment. Instead, we take this as another piece of evidence to complement our findings.
Even if we can assume that a function that we approximate from data is Lipschitz continuous, we rarely actually know the Lipschitz constant for any subset of Θ. Thus, we are faced with a task of estimating it from data. Here, we tried three methods of doing this. The first one simply takes the highest slope that the function attains within the available data and uses this constant value for every subinterval. This produces the most conservative bound, and in many situations it is unlikely to be informative.
An alternative method is to take an upper bound on slope obtained within each subinterval using the available data. This produces a much less conservative upper bound on probabilities.
However, since the actual upper bound is generally greater for each subinterval, the resulting probabilistic bound may be deceiving.
A final method that we tried is a compromise between the two above. Instead of taking the conservative upper bound based on data over the entire function domain Θ, we take the average of upper bounds obtained at each Θj. The bound at an interval is then taken to be the maximum of the upper bound for this interval and the average upper bound for all intervals.
The results of evaluating the expression for Pr{ _ θ∈Θ sup{φ∗ (θ)} ≤ α} when α = 2 are presented in Table 3. In terms of our claims in maxj Aj Aj max{Aj ,ave(Aj)}
Table 3: Approximate upper bound on probability that some setting of θ ∈ [0, 320] will satisfy the designer objective with target α = 2. Different methods of approximating the upper bound on slope in each subinterval j are used. this work, the expression gives an upper bound on the probability that some setting of θ (i.e., storage cost) in the interval [0,320] will result in total day-0 procurement that is no greater in any equilibrium than the target specified by α and taken here to be 2. As we had suspected, the most conservative approach to estimating the upper bound on slope, presented in the first column of the table, provides us little information here. However, the other two estimation approaches, found in columns two and three of Table 3, suggest that we are indeed quite confident that no reasonable setting of θ ∈ [0, 320] would have done the job. Given the tremendous difficulty of the problem, this result is very strong.16 Still, we must be very cautious in drawing too heroic a conclusion based on this evidence. Certainly, we have not checked all the profiles but only a small proportion of them (infinitesimal, if we consider the entire continuous domain of θ and strategy sets). Nor can we expect ever to obtain enough evidence to make completely objective conclusions. Instead, the approach we advocate here is to collect as much evidence as is feasible given resource constraints, and make the most compelling judgment based on this evidence, if at all possible.
At this point, we explore abstractly whether a design parameter choice based on payoff data can be asymptotically reliable. 16 Since we did not have all the possible deviations for any profile available in the data, the true upper bounds may be even lower.
As a matter of convenience, we will use notation un,i(a) to refer to a payoff function of player i based on an average over n i.i.d. samples from the distribution of payoffs. We also assume that un,i(a) are independent for all a ∈ A and i ∈ I. We will use the notation Γn to refer to the game [I, R, {ui,n(·)}], whereas Γ will denote the underlying game, [I, R, {ui(·)}]. Similarly, we define n(r) to be (r) with respect to the game Γn.
In this section, we show that n(s) → (s) a.s. uniformly on the mixed strategy space for any finite game, and, furthermore, that all mixed strategy Nash equilibria in empirical games eventually become arbitrarily close to some Nash equilibrium strategies in the underlying game. We use these results to show that under certain conditions, the optimal choice of the design parameter based on empirical data converges almost surely to the actual optimum.
THEOREM 3. Suppose that |I| < ∞, |A| < ∞. Then n(s) → (s) a.s. uniformly on S.
Recall that N is a set of all Nash equilibria of Γ. If we define Nn,γ = {s ∈ S : n(s) ≤ γ}, we have the following corollary to Theorem 3: COROLLARY 4. For every γ > 0, there is M such that ∀n ≥ M, N ⊂ Nn,γ a.s.
PROOF. Since (s) = 0 for every s ∈ N, we can find M large enough such that Pr{supn≥M sups∈N n(s) < γ} = 1.
By the Corollary, for any game with a finite set of pure strategies and for any > 0, all Nash equilibria lie in the set of empirical -Nash equilibria if enough samples have been taken. As we now show, this provides some justification for our use of a set of profiles with a non-zero -bound as an estimate of the set of Nash equilibria.
First, suppose we conclude that for a particular setting of θ, sup{ˆφ∗ (θ)} ≤ α. Then, since for any fixed > 0, N(θ) ⊂ Nn, (θ) when n is large enough, sup{φ∗ (θ)} = sup s∈N (θ) φ(s) ≤ sup s∈Nn, (θ) φ(s) = sup{ˆφ∗ (θ)} ≤ α for any such n. Thus, since we defined the welfare function of the designer to be I{sup{φ∗ (θ)} ≤ α} in our domain of interest, the empirical choice of θ satisfies the designer"s objective, thereby maximizing his welfare function.
Alternatively, suppose we conclude that inf{ˆφ∗ (θ)} > α for every θ in the domain. Then, α < inf{ˆφ∗ (θ)} = inf s∈Nn, (θ) φ(s) ≤ inf s∈N (θ) φ(s) ≤ ≤ sup s∈N (θ) φ(s) = sup{φ∗ (θ)}, for every θ, and we can conclude that no setting of θ will satisfy the designer"s objective.
Now, we will show that when the number of samples is large enough, every Nash equilibrium of Γn is close to some Nash equilibrium of the underlying game. This result will lead us to consider convergence of optimizers based on empirical data to actual optimal mechanism parameter settings.
We first note that the function (s) is continuous in a finite game.
LEMMA 5. Let S be a mixed strategy set defined on a finite game. Then : S → R is continuous. 312 For the exposition that follows, we need a bit of additional notation. First, let (Z, d) be a metric space, and X, Y ⊂ Z and define directed Hausdorff distance from X to Y to be h(X, Y ) = sup x∈X inf y∈Y d(x, y).
Observe that U ⊂ X ⇒ h(U, Y ) ≤ h(X, Y ). Further, define BS(x, δ) to be an open ball in S ⊂ Z with center x ∈ S and radius δ. Now, let Nn denote all Nash equilibria of the game Γn and let Nδ = [ x∈N BS(x, δ), that is, the union of open balls of radius δ with centers at Nash equilibria of Γ. Note that h(Nδ, N) = δ.
We can then prove the following general result.
THEOREM 6. Suppose |I| < ∞ and |A| < ∞. Then almost surely h(Nn, N) converges to 0.
We will now show that in the special case when Θ and A are finite and each Γθ has a unique Nash equilibrium, the estimates ˆθ of optimal designer parameter converge to an actual optimizer almost surely.
Let ˆθ = arg maxθ∈Θ W (Nn(θ), θ), where n is the number of times each pure profile was sampled in Γθ for every θ, and let θ∗ = arg maxθ∈Θ W (N(θ), θ).
THEOREM 7. Suppose |N(θ)| = 1 for all θ ∈ Θ and suppose that Θ and A are finite. Let W (s, θ) be continuous at the unique s∗ (θ) ∈ N(θ) for each θ ∈ Θ. Then ˆθ is a consistent estimator of θ∗ if W (N(θ), θ) is defined as a supremum, infimum, or expectation over the set of Nash equilibria. In fact, ˆθ → θ∗ a.s. in each of these cases.
The shortcoming of the above result is that, within our framework, the designer has no way of knowing or ensuring that Γθ do, indeed, have unique equilibria. However, it does lend some theoretical justification for pursuing design in this manner, and, perhaps, will serve as a guide for more general results in the future.
The mechanism design literature in Economics has typically explored existence of a mechanism that implements a social choice function in equilibrium [10]. Additionally, there is an extensive literature on optimal auction design [10], of which the work by Roger Myerson [11] is, perhaps, the most relevant. In much of this work, analytical results are presented with respect to specific utility functions and accounting for constraints such as incentive compatibility and individual rationality.
Several related approaches to search for the best mechanism exist in the Computer Science literature. Conitzer and Sandholm [6] developed a search algorithm when all the relevant game parameters are common knowledge. When payoff functions of players are unknown, a search using simulations has been explored as an alternative. One approach in that direction, taken in [4] and [15], is to co-evolve the mechanism parameter and agent strategies, using some notion of social utility and agent payoffs as fitness criteria. An alternative to co-evolution explored in [16] was to optimize a well-defined welfare function of the designer using genetic programming. In this work the authors used a common learning strategy for all agents and defined an outcome of a game induced by a mechanism parameter as the outcome of joint agent learning.
Most recently, Phelps et al. [14] compared two mechanisms based on expected social utility with expectation taken over an empirical distribution of equilibria in games defined by heuristic strategies, as in [18].
In this work we spent considerable effort developing general tactics for empirical mechanism design. We defined a formal gametheoretic model of interaction between the designer and the participants of the mechanism as a two-stage game. We also described in some generality the methods for estimating a sample Nash equilibrium function when the data is extremely scarce, or a Nash equilibrium correspondence when more data is available. Our techniques are designed specifically to deal with problems in which both the mechanism parameter space and the agent strategy sets are infinite and only a relatively small data set can be acquired.
A difficult design issue in the TAC/SCM game which the TAC community has been eager to address provides us with a setting to test our methods. In applying empirical game analysis to the problem at hand, we are fully aware that our results are inherently inexact. Thus, we concentrate on collecting evidence about the structure of the Nash equilibrium correspondence. In the end, we can try to provide enough evidence to either prescribe a parameter setting, or suggest that no setting is possible that will satisfy the designer. In the case of TAC/SCM, our evidence suggests quite strongly that storage cost could not have been effectively adjusted in the 2004 tournament to curb excessive day-0 procurement without detrimental effects on overall profitability. The success of our analysis in this extremely complex environment with high simulation costs makes us optimistic that our methods can provide guidance in making mechanism design decisions in other challenging domains. The theoretical results confirm some intuitions behind the empirical mechanism design methods we have introduced, and increases our confidence that our framework can be effective in estimating the best mechanism parameter choice in relatively general settings.
Acknowledgments We thank Terence Kelly, Matthew Rudary, and Satinder Singh for helpful comments on earlier drafts of this work. This work was supported in part by NSF grant IIS-0205435 and the DARPA REAL strategic reasoning program.
[1] R. Arunachalam and N. M. Sadeh. The supply chain trading agent competition. Electronic Commerce Research and Applications, 4:63-81, 2005. [2] M. Benisch, A. Greenwald, V. Naroditskiy, and M. Tschantz.
A stochastic programming approach to scheduling in TAC SCM. In Fifth ACM Conference on Electronic Commerce, pages 152-159, New York, 2004. [3] Y.-P. Chang and W.-T. Huang. Generalized confidence intervals for the largest value of some functions of parameters under normality. Statistica Sinica, 10:1369-1383,
[4] D. Cliff. Evolution of market mechanism through a continuous space of auction-types. In Congress on Evolutionary Computation, 2002. [5] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129-145, 1996. [6] V. Conitzer and T. Sandholm. An algorithm for automatically designing deterministic mechanisms without payments. In 313 Third International Joint Conference on Autonomous Agents and Multi-Agent Systems, pages 128-135, 2004. [7] D. Friedman. Evolutionary games in economics.
Econometrica, 59(3):637-666, May 1991. [8] R. Keener. Statistical Theory: A Medley of Core Topics.
University of Michigan Department of Statistics, 2004. [9] C. Kiekintveld, Y. Vorobeychik, and M. P. Wellman. An analysis of the 2004 supply chain management trading agent competition. In IJCAI-05 Workshop on Trading Agent Design and Analysis, Edinburgh, 2005. [10] A. Mas-Colell, M. Whinston, and J. Green. Microeconomic Theory. Oxford University Press, 1995. [11] R. B. Myerson. Optimal auction design. Mathematics of Operations Research, 6(1):58-73, February 1981. [12] S. Olafsson and J. Kim. Simulation optimization. In E. Yucesan, C.-H. Chen, J. Snowdon, and J. Charnes, editors,
[13] D. Pardoe and P. Stone. TacTex-03: A supply chain management agent. SIGecom Exchanges, 4(3):19-28, 2004. [14] S. Phelps, S. Parsons, and P. McBurney. Automated agents versus virtual humans: an evolutionary game theoretic comparison of two double-auction market designs. In Workshop on Agent Mediated Electronic Commerce VI,
[15] S. Phelps, S. Parsons, P. McBurney, and E. Sklar.
Co-evolution of auction mechanisms and trading strategies: towards a novel approach to microeconomic design. In ECOMAS 2002 Workshop, 2002. [16] S. Phelps, S. Parsons, E. Sklar, and P. McBurney. Using genetic programming to optimise pricing rules for a double-auction market. In Workshop on Agents for Electronic Commerce, 2003. [17] Y. Vorobeychik, M. P. Wellman, and S. Singh. Learning payoff functions in infinite games. In Nineteenth International Joint Conference on Artificial Intelligence, pages 977-982, 2005. [18] W. E. Walsh, R. Das, G. Tesauro, and J. O. Kephart.
Analyzing complex strategic interactions in multi-agent systems. In AAAI-02 Workshop on Game Theoretic and Decision Theoretic Agents, 2002. [19] M. P. Wellman, J. J. Estelle, S. Singh, Y. Vorobeychik,
C. Kiekintveld, and V. Soni. Strategic interactions in a supply chain game. Computational Intelligence, 21(1):1-26,
February 2005.
APPENDIX A. PROOFS A.1 Proof of Proposition 1 Pr „ max i∈I max b∈Ai\ai ui(b, a−i) − ui(a) ≤ « = = Y i∈I Eui(a) » Pr( max b∈Ai\ai ui(b, a−i) − ui(a) ≤ |ui(a))  = = Y i∈I Z R Y b∈Ai\ai Pr(ui(b, a−i) ≤ u + )fui(a)(u)du.
A.2 Proof of Proposition 2 First, let us suppose that some function, f(x) defined on [ai, bi], satisfy Lipschitz condition on (ai, bi] with Lipschitz constant Ai.
Then the following claim holds: Claim: infx∈(ai,bi] f(x) ≥ 0.5(f(ai) + f(bi) − Ai(bi − ai).
To prove this claim, note that the intersection of lines at f(ai) and f(bi) with slopes −Ai and Ai respectively will determine the lower bound on the minimum of f(x) on [ai, bi] (which is a lower bound on infimum of f(x) on (ai, bj ]). The line at f(ai) is determined by f(ai) = −Aiai + cL and the line at f(bi) is determined by f(bi) = Aibi +cR. Thus, the intercepts are cL = f(ai)+Aiai and cR = f(bi) + Aibi respectively. Let x∗ be the point at which these lines intersect. Then, x∗ = − f(x∗ ) − cR A = f(x∗ ) − cL A .
By substituting the expressions for cR and cL, we get the desired result.
Now, subadditivity gives us Pr{ _ θ∈Θ sup{φ∗ (θ)} ≤ α} ≤ 5X j=1 Pr{ _ θ∈Θj sup{φ∗ (θ)} ≤ α}, and, by the claim,
Pr{ _ θ∈Θj sup{φ∗ (θ)} ≤ α} =
θ∈Θj sup{φ∗ (θ)} > α} ≤ Pr{sup{φ∗ (aj)} + sup{φ∗ (bj)} ≤ 2α + Aj(bj − aj )}.
Since we have a finite number of points in the data set for each θ, we can obtain the following expression: Pr{sup{φ∗ (aj)} + sup{φ∗ (bj)} ≤ cj } =D X y,z∈D:y+z≤cj Pr{sup{φ∗ (bj )} = y} Pr{sup{φ∗ (aj)} = z}.
We can now restrict attention to deriving an upper bound on Pr{sup{φ∗ (θ)} = y} for a fixed θ. To do this, observe that Pr{sup{φ∗ (θ)} = y} ≤D Pr{ _ a∈D:φ(a)=y (a) = 0} ≤ X a∈D:φ(a)=y Pr{ (a) = 0} by subadditivity and the fact that a profile a is a Nash equilibrium if and only if (a) = 0.
Putting everything together yields the desired result.
A.3 Proof of Theorem 3 First, we will need the following fact: Claim: Given a function fi(x) and a set X, | maxx∈X f1(x) − maxx∈X f2(x)| ≤ maxx∈X |f1(x) − f2(x)|.
To prove this claim, observe that | max x∈X f1(x) − max x∈X f2(x)| =  maxx f1(x) − maxx f2(x) if maxx f1(x) ≥ maxx f2(x) maxx f2(x) − maxx f1(x) if maxx f2(x) ≥ maxx f1(x) In the first case, max x∈X f1(x) − max x∈X f2(x) ≤ max x∈X (f1(x) − f2(x)) ≤ ≤ max x∈X |f1(x) − f2(x)|. 314 Similarly, in the second case, max x∈X f2(x) − max x∈X f1(x) ≤ max x∈X (f2(x) − f1(x)) ≤ ≤ max x∈X |f2(x) − f1(x)| = max x∈X |f1(x) − f2(x)|.
Thus, the claim holds.
By the Strong Law of Large Numbers, un,i(a) → ui(a) a.s. for all i ∈ I, a ∈ A. That is,
Pr{ lim n→∞ un,i(a) = ui(a)} = 1, or, equivalently [8], for any α > 0 and δ > 0, there is M(i, a) > 0 such that Pr{ sup n≥M(i,a) |un,i(a) − ui(a)| < δ 2|A| } ≥ 1 − α.
By taking M = maxi∈I maxa∈A M(i, a), we have Pr{max i∈I max a∈A sup n≥M |un,i(a) − ui(a)| < δ 2|A| } ≥ 1 − α.
Thus, by the claim, for any n ≥ M, sup n≥M | n(s) − (s)| ≤ max i∈I max ai∈Ai sup n≥M |un,i(ai, s−i) − ui(ai, s−i)|+ + sup n≥M max i∈I |un,i(s) − ui(s)| ≤ max i∈I max ai∈Ai X b∈A−i sup n≥M |un,i(ai, b) − ui(ai, b)|s−i(b)+ + max i∈I X b∈A sup n≥M |un,i(b) − ui(b)|s(b) ≤ max i∈I max ai∈Ai X b∈A−i sup n≥M |un,i(ai, b) − ui(ai, b)|+ + max i∈I X b∈A sup n≥M |un,i(b) − ui(b)| < max i∈I max ai∈Ai X b∈A−i ( δ 2|A| ) + max i∈I X b∈A ( δ 2|A| ) ≤ δ with probability at least 1 − α. Note that since s−i(a) and s(a) are bounded between 0 and 1, we were able to drop them from the expressions above to obtain a bound that will be valid independent of the particular choice of s. Furthermore, since the above result can be obtained for an arbitrary α > 0 and δ > 0, we have Pr{limn→∞ n(s) = (s)} = 1 uniformly on S.
A.4 Proof of Lemma 5 We prove the result using uniform continuity of ui(s) and preservation of continuity under maximum.
Claim: A function f : Rk → R defined by f(t) = Pk i=1 ziti, where zi are constants in R, is uniformly continuous in t.
The claim follows because |f(t)−f(t )| = | Pk i=1 zi(ti−ti)| ≤ Pk i=1 |zi||ti − ti|. An immediate result of this for our purposes is that ui(s) is uniformly continuous in s and ui(ai, s−i) is uniformly continuous in s−i.
Claim: Let f(a, b) be uniformly continuous in b ∈ B for every a ∈ A, with |A| < ∞. Then V (b) = maxa∈A f(a, b) is uniformly continuous in b.
To show this, take γ > 0 and let b, b ∈ B such that b − b < δ(a) ⇒ |f(a, b) − f(a, b )| < γ. Now take δ = mina∈A δ(a).
Then, whenever b − b < δ, |V (b) − V (b )| = | max a∈A f(a, b) − max a∈A f(a, b )| ≤ max a∈A |f(a, b) − f(a, b )| < γ.
Now, recall that (s) = maxi[maxai∈Ai ui(ai, s−i) − ui(s)]. By the claims above, maxai∈Ai ui(ai, s−i) is uniformly continuous in s−i and ui(s) is uniformly continuous in s. Since the difference of two uniformly continuous functions is uniformly continuous, and since this continuity is preserved under maximum by our second claim, we have the desired result.
A.5 Proof of Theorem 6 Choose δ > 0. First, we need to ascertain that the following claim holds: Claim: ¯ = mins∈S\Nδ (s) exists and ¯ > 0.
Since Nδ is an open subset of compact S, it follows that S \ Nδ is compact. As we had also proved in Lemma 5 that (s) is continuous, existence follows from the Weierstrass theorem. That ¯ > 0 is clear since (s) = 0 if and only if s is a Nash equilibrium of Γ.
Now, by Theorem 3, for any α > 0 there is M such that Pr{ sup n≥M sup s∈S | n(s) − (s)| < ¯} ≥ 1 − α.
Consequently, for any δ > 0,
Pr{ sup n≥M h(Nn, Nδ) < δ} ≥ Pr{∀n ≥ M Nn ⊂ Nδ} ≥ Pr{ sup n≥M sup s∈N (s) < ¯} ≥ Pr{ sup n≥M sup s∈S | n(s) − (s)| < ¯} ≥ 1 − α.
Since this holds for an arbitrary α > 0 and δ > 0, the desired result follows.
A.6 Proof of Theorem 7 Fix θ and choose δ > 0. Since W (s, θ) is continuous at s∗ (θ), given > 0 there is δ > 0 such that for every s that is within δ of s∗ (θ), |W (s , θ) − W (s∗ (θ), θ)| < . By Theorem 6, we can find M(θ) large enough such that all s ∈ Nn are within δ of s∗ (θ) for all n ≥ M(θ) with probability 1. Consequently, for any > 0 we can find M(θ) large enough such that with probability 1 we have supn≥M(θ) sups ∈Nn |W (s , θ) − W (s∗ (θ), θ)| < .
Let us assume without loss of generality that there is a unique optimal choice of θ. Now, since the set Θ is finite, there is also the second-best choice of θ (if there is only one θ ∈ Θ this discussion is moot anyway): θ∗∗ = arg max Θ\θ∗ W (s∗ (θ), θ).
Suppose w.l.o.g. that θ∗∗ is also unique and let ∆ = W (s∗ (θ∗ ), θ∗ ) − W (s∗ (θ∗∗ ), θ∗∗ ).

Combinatorial auctions have recently received a lot of attention. In a combinatorial auction, a set M of m nonidentical items are sold in a single auction to n competing bidders. The bidders have preferences regarding the bundles of items that they may receive. The preferences of bidder i are specified by a valuation function vi : 2M → R+ , where vi(S) denotes the value that bidder i attaches to winning the bundle of items S. We assume free disposal, i.e., that the vi"s are monotone non-decreasing. The usual goal of the auctioneer is to optimize the social welfare P i vi(Si), where the allocation S1...Sn must be a partition of the items.
Applications include many complex resource allocation problems and, in fact, combinatorial auctions may be viewed as the common abstraction of many complex resource allocation problems. Combinatorial auctions face both economic and computational difficulties and are a central problem in the recently active border of economic theory and computer science. A forthcoming book [11] addresses many of the issues involved in the design and implementation of combinatorial auctions.
The design of a combinatorial auction involves many considerations. In this paper we focus on just one central issue: the communication between bidders and the allocation mechanism - preference elicitation. Transferring all information about bidders" preferences requires an infeasible (exponential in m) amount of communication. Thus, direct revelation auctions in which bidders simply declare their preferences to the mechanism are only practical for very small auction sizes or for very limited families of bidder preferences. We have therefore seen a multitude of suggested iterative auctions in which the auction protocol repeatedly interacts with the different bidders, aiming to adaptively elicit enough information about the bidders" preferences as to be able to find a good (optimal or close to optimal) allocation.
Most of the suggested iterative auctions proceed by maintaining temporary prices for the bundles of items and repeatedly querying the bidders as to their preferences between the bundles under the current set of prices, and then updating the set of bundle prices according to the replies received (e.g., [22, 12, 17, 37, 3]). Effectively, such an iterative auction accesses the bidders" preferences by repeatedly making the following type of demand query to bidders: Query to bidder i: a vector of bundle prices p = {p(S)}S⊆M ; Answer: a bundle of items S ⊆ M that maximizes vi(S) − p(S)..
These types of queries are very natural in an economic setting as they capture the revealed preferences of the bidders. Some auctions, called item-price or linear-price auctions, specify a price pi for each item, and the price of any given bundle S is always linear, p(S) = P i∈S pi. Other auctions, called bundle-price auctions, allow specifying arbitrary (non-linear) prices p(S) for bundles. Another important differentiation between models of iterative auctions is 29 based on whether they use anonymous or non-anonymous prices: In some auctions the prices that are presented to the bidders are always the same (anonymous prices). In other auctions (non-anonymous), different bidders may face different (discriminatory) vectors of prices. In ascending-price auctions, forcing prices to be anonymous may be a significant restriction.
In this paper, we embark on a systematic analysis of the computational power of iterative auctions that are based on demand queries. We do not aim to present auctions for practical use but rather to understand the limitations and possibilities of these kinds of auctions. In the first part of this paper, our main question is what can be done using a polynomial number of these types of queries? That is, polynomial in the main parameters of the problem: n, m and the number of bits t needed for representing a single value vi(S).
Note that from an algorithmic point of view we are talking about sub-linear time algorithms: the input size here is really n(2m − 1) numbers - the descriptions of the valuation functions of all bidders. There are two aspects to computational efficiency in these settings: the first is the communication with the bidders, i.e., the number of queries made, and the second is the usual computational tractability. Our lower bounds will depend only on the number of queriesand hold independently of any computational assumptions like P = NP. Our upper bounds will always be computationally efficient both in terms of the number of queries and in terms of regular computation. As mentioned, this paper concentrates on the single aspect of preference elicitation and on its computational consequences and does not address issues of incentives. This strengthens our lower bounds, but means that the upper bounds require evaluation from this perspective also before being used in any real combinatorial auction.1 The second part of this paper studies the power of ascending -price auctions. Ascending auctions are iterative auctions where the published prices cannot decrease in time. In this work, we try to systematically analyze what do the differences between various models of ascending auctions mean.
We try to answer the following questions: (i) Which models of ascending auctions can find the optimal allocation, and for which classes of valuations? (ii) In cases where the optimal allocation cannot be determined by ascending auctions, how well can such auctions approximate the social welfare? (iii) How do the different models for ascending auctions compare?
Are some models computationally stronger than others?
Ascending auctions have been extensively studied in the literature (see the recent survey by Parkes [35]). Most of this work presented "upper bounds", i.e., proposed mechanisms with ascending prices and analyzed their properties. A result which is closer in spirit to ours, is by Gul and Stacchetti [17], who showed that no item-price ascending auction can always determine the VCG prices, even for substitutes valuations.2 Our framework is more general than the traditional line of research that concentrates on the final allocation and 1 We do observe however that some weak incentive property comes for free in demand-query auctions since myopic players will answer all demand queries truthfully. We also note that in some cases (but not always!) the incentives issues can be handled orthogonally to the preference elicitation issues, e.g., by using Vickrey-Clarke-Groves (VCG) prices (e.g., [4, 34]). 2 We further discuss this result in Section 5.3.
Iterative auctions Demand auctions Item-price auctions Anonymous price auctions Ascending auctions 1 2 3 4 5 6 97 8 10 Figure 1: The diagram classifies the following auctions according to their properties: (1) The adaptation [12] for Kelso & Crawford"s [22] auction. (2) The Proxy Auction [3] by Ausubel & Milgrom. (3) iBundle(3) by Parkes & Ungar [34]. (4) iBundle(2) by Parkes & Ungar [37]. (5) Our descending adaptation for the 2-approximation for submodular valuations by [25] (see Subsection 5.4). (6) Ausubel"s [4] auction for substitutes valuations. (7) The adaptation by Nisan & Segal [32] of the O( √ m) approximation by [26]. (8) The duplicate-item auction by [5]. (9) Auction for Read-Once formulae by [43]. (10) The AkBA Auction by Wurman & Wellman [42]. payments and in particular, on reaching "Walrasian equilibria" or "Competitive equilibria". A Walrasian equilibrium3 is known to exist in the case of Substitutes valuations, and is known to be impossible for any wider class of valuations [16]. This does not rule out other allocations by ascending auctions: in this paper we view the auctions as a computational process where the outcome - both the allocation and the payments - can be determined according to all the data elicited throughout the auction; This general framework strengthens our negative results.4 We find the study of ascending auctions appealing for various reasons. First, ascending auctions are widely used in many real-life settings from the FCC spectrum auctions [15] to almost any e-commerce website (e.g., [2, 1]). Actually, this is maybe the most straightforward way to sell items: ask the bidders what would they like to buy under certain prices, and increase the prices of over-demanded goods. Ascending auctions are also considered more intuitive for many bidders, and are believed to increase the trust of the bidders in the auctioneer, as they see the result gradually emerging from the bidders" responses. Ascending auctions also have other desired economic properties, e.g., they incur smaller information revelation (consider, for example, English auctions vs. second-price sealed bid auctions).
Many iterative combinatorial auction mechanisms rely on demand queries (see the survey in [35]). Figure 1 summa3 A Walrasian equilibrium is vector of item prices for which all the items are sold when each bidder receives a bundle in his demand set. 4 In few recent auction designs (e.g., [4, 28]) the payments are not necessarily the final prices of the auctions. 30 Valuation family Upper bound Reference Lower bound Reference General min(n, O( √ m)) [26], Section 4.2 min(n, m1/2− ) [32] Substitutes 1 [32] Submodular 2 [25], 1+ 1 2m , 1-1 e (*) [32],[23] Subadditive O(logm) [13] 2 [13] k-duplicates O(m1/k+1 ) [14] O(m1/k+1 ) [14] Procurement ln m [32] (log m)/2 [29, 32] Figure 2: The best approximation factors currently achievable by computationally-efficient combinatorial auctions, for several classes of valuations. All lower bounds in the table apply to all iterative auctions (except the one marked by *); all upper bounds in the table are achieved with item-price demand queries. rizes the basic classes of auctions implied by combinations of the above properties and classifies some of the auctions proposed in the literature according to this classification.
For our purposes, two families of these auctions serve as the main motivating starting points: the first is the ascending item-price auctions of [22, 17] that with computational efficiency find an optimal allocation among (gross) substitutes valuations, and the second is the ascending bundleprice auctions of [37, 3] that find an optimal allocation among general valuations - but not necessarily with computational efficiency. The main lower bound in this area, due to [32], states that indeed, due to inherent communication requirements, it is not possible for any iterative auction to find the optimal allocation among general valuations with sub-exponentially many queries. A similar exponential lower bound was shown in [32] also for even approximating the optimal allocation to within a factor of m1/2− . Several lower bounds and upper bounds for approximation are known for some natural classes of valuations - these are summarized in Figure 2.
In [32], the universal generality of demand queries is also shown: any non-deterministic communication protocol for finding an allocation that optimizes the social welfare can be converted into one that only uses demand queries (with bundle prices). In [41] this was generalized also to nondeterministic protocols for finding allocations that satisfy other natural types of economic requirements (e.g., approximate social efficiency, envy-freeness). However, in [33] it was demonstrated that this completeness of demand queries holds only in the nondeterministic setting, while in the usual deterministic setting, demand queries (even with bundle prices) may be exponentially weaker than general communication.
Bundle-price auctions are a generalization of (the more natural and intuitive) item-price auctions. It is known that indeed item-price auctions may be exponentially weaker: a nice example is the case of valuations that are a XOR of k bundles5 , where k is small (say, polynomial). Lahaie and Parkes [24] show an economically-efficient bundle-price auction that uses a polynomial number of queries whenever k is polynomial. In contrast, [7] show that there exist valuations that are XORs of k = √ m bundles such that any item-price auction that finds an optimal allocation between them requires exponentially many queries. These results are part of a recent line of research ([7, 43, 24, 40]) that study the preference elicitation problem in combinatorial auctions and its relation to the full elicitation problem (i.e., learn5 These are valuations where bidders have values for k specific packages, and the value of each bundle is the maximal value of one of these packages that it contains. ing the exact valuations of the bidders). These papers adapt methods from machine-learning theory to the combinatorialauction setting. The preference elicitation problem and the full elicitation problem relate to a well studied problem in microeconomics known as the integrability problem (see, e.g., [27]). This problem studies if and when one can derive the utility function of a consumer from her demand function.
Paper organization: Due to the relatively large number of results we present, we start with a survey of our new results in Section 2. After describing our formal model in Section 3, we present our results concerning the power of demand queries in Section 4. Then, we describe the power of item-price ascending auctions (Section 5) and bundle-price ascending auctions (Section 6). Readers who are mainly interested in the self-contained discussion of ascending auctions can skip Section 4.
Missing proofs from Section 4 can be found in part I of the full paper ([8]). Missing proofs from Sections 5 and 6 can be found in part II of the full paper ([9]).
Our systematic analysis is composed of the combination of a rather large number of results characterizing the power and limitations of various classes of auctions. In this section, we will present an exposition describing our new results. We first discuss the power of demand-query iterative auctions, and then we turn our attention to ascending auctions.
Figure 3 summarizes some of our main results.
Comparison of query types We first ask what other natural types of queries could we imagine iterative auctions using? Here is a list of such queries that are either natural, have been used in the literature, or that we found useful.
bidder reports his value v(S) for this bundle.
bundle A and an item j, the bidder reports how much he is willing to pay for j, given that he already owns A, i.e., v(j|A) = v(A ∪ {j}) − v(A).
presents a vector of item prices p1...pm; the bidder reports his demand under these prices, i.e., some set S that maximizes v(S) − P i∈S pi.6 6 A tie breaking rule should be specified. All of our results 31 Communication Constraint Can find an optimal allocation?
Upper bound for welfare approx.
Lower bound for welfare approx.
Item-Price Demand Queries Yes 1 1 Poly. Communication No [32] min(n, O(m1/2 )) [26] min(n, m1/2− ) [32] Poly. Item-Price Demand Queries No [32] min(n, O(m1/2 )) min(n, m1/2− ) [32] Poly. Value Queries No [32] O( m√ log m ) [19] O( m log m ) Anonymous Item-Price AA No - min(O(n), O(m1/2− )) Non-anonymous Item-Price AA No -Anonymous Bundle-Price AA No - min(O(n), O(m1/2− )) Non-anonymous Bundle-Price AA Yes [37] 1 1 Poly Number of Item-Price AA No min(n, O(m1/2  ))Figure 3: This paper studies the economic efficiency of auctions that follow certain communication constraints. For each class of auctions, the table shows whether the optimal allocation can be achieved, or else, how well can it be approximated (both upper bounds and lower bounds). New results are highlighted.
Abbreviations: Poly. (Polynomial number/size), AA (ascending auctions). - means that nothing is currently known except trivial solutions.
item prices p1...pm, and the bidder responds with his indirect-utility under these prices, that is, the highest utility he can achieve from a bundle under these prices: maxS⊆M (v(S) − P i∈S pi).7
of non-zero prices p1...pm, and the bidder reports the bundle that maximizes his value per unit of money, i.e., some set that maximizes v(S)P i∈S pi .8 Theorem: Each of these queries can be efficiently (i.e., in time polynomial in n, m, and the number of bits of precision t needed to represent a single value vi(S)) simulated by a sequence of demand queries with item prices.
In particular this shows that demand queries can elicit all information about a valuation by simulating all 2m −1 value queries. We also observe that value queries and marginalvalue queries can simulate each other in polynomial time and that demand queries and indirect-utility queries can also simulate each other in polynomial time. We prove that exponentially many value queries may be needed in order to simulate a single demand query. It is interesting to note that for the restricted class of substitutes valuations, demand queries may be simulated by polynomial number of value queries [6].
Welfare approximation The next question that we ask is how well can a computationally-efficient auction that uses only demand queries approximate the optimal allocation? Two separate obstacles are known: In [32], a lower bound of min(n, m1/2− ), for any fixed > 0, was shown for the approximation factor apply for any fixed tie breaking rule. 7 This is exactly the utility achieved by the bundle which would be returned in a demand query with the same prices.
This notion relates to the Indirect-utility function studied in the Microeconomic literature (see, e.g., [27]). 8 Note that when all the prices are 1, the bidder actually reports the bundle with the highest per-item price. We found this type of query useful, for example, in the design of the approximation algorithm described in Figure 5 in Section
obtained using any polynomial amount of communication.
A computational bound with the same value applies even for the case of single-minded bidders, but under the assumption of NP = ZPP [39]. As noted in [32], the computationallyefficient greedy algorithm of [26] can be adapted to become a polynomial-time iterative auction that achieves a nearly matching approximation factor of min(n, O( √ m)). This iterative auction may be implemented with bundle-price demand queries but, as far as we see, not as one with item prices. Since in a single bundle-price demand query an exponential number of prices can be presented, this algorithm can have an exponential communication cost. In Section
the same approximation factor with a polynomial number of queries (and thus with a polynomial communication).
Theorem: There exists a computationally-efficient iterative auction with item-price demand queries that finds an allocation that approximates the optimal welfare between arbitrary valuations to within a factor of min(n, O( √ m)).
One may then attempt obtaining such an approximation factor using iterative auctions that use only the weaker value queries. However, we show that this is impossible: Theorem: Any iterative auction that uses a polynomial (in n and m) number of value queries can not achieve an approximation factor that is better than O( m log m ).9 Note however that auctions with only value queries are not completely trivial in power: the bundling auctions of Holzman et al. [19] can easily be implemented by a polynomial number of value queries and can achieve an approximation factor of O( m√ log m ) by using O(log m) equi-sized bundles.
We do not know how to close the (tiny) gap between this upper bound and our lower bound.
Representing bundle-prices We then deal with a critical issue with bundle-price auctions that was side-stepped by our model, as well as by all previous works that used bundle-price auctions: how are 9 This was also proven independently by Shahar Dobzinski and Michael Schapira. 32 the bundle prices represented? For item-price auctions this is not an issue since a query needs only to specify a small number, m, of prices. In bundle-price auctions that situation is more difficult since there are exponentially many bundles that require pricing. Our basic model (like all previous work that used bundle prices, e.g., [37, 34, 3]), ignores this issue, and only requires that the prices be determined, somehow, by the protocol. A finer model would fix a specific language for denoting bundle prices, force the protocol to represent the bundle-prices in this language, and require that the representations of the bundle-prices also be polynomial.
What could such a language for denoting prices for all bundles look like? First note that specifying a price for each bundle is equivalent to specifying a valuation. Second, as noted in [31], most of the proposed bidding languages are really just languages for representing valuations, i.e., a syntactic representation of valuations - thus we could use any of them. This point of view opens up the general issue of which language should be used in bundle-price auctions and what are the implications of this choice.
Here we initiate this line of investigation. We consider bundle-price auctions where the prices must be given as a XOR-bid, i.e., the protocol must explicitly indicate the price of every bundle whose value is different than that of all of its proper subsets. Note that all bundle-price auctions that do not explicitly specify a bidding language must implicitly use this language or a weaker one, since without a specific language one would need to list prices for all bundles, perhaps except for trivial ones (those with value 0, or more generally, those with a value that is determined by one of their proper subsets.) We show that once the representation length of bundle prices is taken into account (using the XOR-language), bundle-price auctions are no more strictly stronger than item-price auctions. Define the cost of an iterative auction as the total length of the queries and answers used throughout the auction (in the worst case).
Theorem: For some class of valuations, bundle price auctions that use the XOR-language require an exponential cost for finding the optimal allocation. In contrast, item-price auctions can find the optimal allocation for this class within polynomial cost.10 This put doubts on the applicability of bundle-price auctions like [3, 37], and it may justify the use of hybrid pricing methods such as Ausubel, Cramton and Milgrom"s Clock-Proxy auction ([10]).
Demand queries and linear programs The winner determination problem in combinatorial auctions may be formulated as an integer program. In many cases solving the linear-program relaxation of this integer program is useful: for some restricted classes of valuations it finds the optimum of the integer program (e.g., substitute valuations [22, 17]) or helps approximating the optimum (e.g., by randomized rounding [13, 14]). However, the linear program has an exponential number of variables. Nisan and Segal [32] observed the surprising fact that despite the ex10 Our proof relies on the sophisticated known lower bounds for constant depth circuits. We were not able to find an elementary proof. ponential number of variables, this linear program may be solved within polynomial communication. The basic idea is to solve the dual program using the Ellipsoid method (see, e.g., [20]). The dual program has a polynomial number of variables, but an exponential number of constraints. The Ellipsoid algorithm runs in polynomial time even on such programs, provided that a separation oracle is given for the set of constraints. Surprisingly, such a separation oracle can be implemented using a single demand query (with item prices) to each of the bidders.
The treatment of [32] was somewhat ad-hoc to the problem at hand (the case of substitute valuations). Here we give a somewhat more general form of this important observation. Let us call the following class of linear programs generalized-winner-determination-relaxation (GWDR) LPs: Maximize X i∈N,S⊆M wi xi,S vi(S) s.t.
X i∈N, S|j∈S xi,S ≤ qj ∀j ∈ M X S⊆M xi,S ≤ di ∀i ∈ N xi,S ≥ 0 ∀i ∈ N, S ⊆ M The case where wi = 1, di = 1, qj = 1 (for every i, j) is the usual linear relaxation of the winner determination problem. More generally, wi may be viewed as the weight given to bidder i"s welfare, qj may be viewed as the quantity of units of good j, and di may be viewed as duplicity of the number of bidders of type i.
Theorem: Any GWDR linear program may be solved in polynomial time (in n, m, and the number of bits of precision t) using only demand queries with item prices.11
Ascending item-price auctions: It is well known that the item-price ascending auctions of Kelso and Crawford [22] and its variants [12, 16] find the optimal allocation as long as all players" valuations have the substitutes property. The obvious question is whether the optimal allocation can be found for a larger class of valuations.
Our main result here is a strong negative result: Theorem: There is a 2-item 2-player problem where no ascending item-price auction can find the optimal allocation.
This is in contrast to both the power of bundle-price ascending auctions and to the power of general item-price demand queries (see above), both of which can always find the optimal allocation and in fact even provide full preference elicitation. The same proof proves a similar impossibility result for other types of auctions (e.g., descending auctions, non-anonymous auctions). More extension of this result: • Eliciting some classes of valuations requires an exponential number of ascending item-price trajectories. 11 The produced optimal solution will have polynomial support and thus can be listed fully. 33 • At least k − 1 ascending item-price trajectories are needed to elicit XOR formulae with k terms. This result is in some sense tight, since we show that any k-term XOR formula can be fully elicited by k−1 nondeterministic (i.e., when some exogenous teacher instructs the auctioneer on how to increase the prices) ascending auctions.12 We also show that item-price ascending auctions and iterative auctions that are limited to a polynomial number of queries (of any kind, not necessarily ascending) are incomparable in their power: ascending auctions, with small enough increments, can elicit the preferences in cases where any polynomial number of queries cannot.
Motivated by several recent papers that studied the relation between eliciting and fully-eliciting the preferences in combinatorial auctions (e.g., [7, 24]), we explore the difference between these problems in the context of ascending auctions. We show that although a single ascending auction can determine the optimal allocation among any number of bidders with substitutes valuations, it cannot fully-elicit such a valuation even for a single bidder. While it was shown in [25] that the set of substitutes valuations has measure zero in the space of general valuations, its dimension is not known, and in particular it is still open whether a polynomial amount of information suffices to describe a substitutes valuation.
While our result may be a small step in that direction (a polynomial full elicitation may still be possible with other communication protocols), we note that our impossibility result also holds for valuations in the class OXS defined by [25], valuations that we are able to show have a compact representation.
We also give several results separating the power of different models for ascending combinatorial auctions that use item-prices: we prove, not surprisingly, that adaptive ascending auctions are more powerful than oblivious ascending auctions and that non-deterministic ascending auctions are more powerful than deterministic ascending auctions.
We also compare different kinds of non-anonymous auctions (e.g., simultaneous or sequential), and observe that anonymous bundle-price auctions and non-anonymous item-price auctions are incomparable in their power. Finally, motivated by Dutch auctions, we consider descending auctions, and how they compare to ascending ones; we show classes of valuations that can be elicited by ascending item-price auctions but not by descending item-price auctions, and vice versa.
Ascending bundle-price auctions: All known ascending bundle-price auctions that are able to find the optimal allocation between general valuations (with free disposal) use non-anonymous prices.
Anonymous ascending-price auctions (e.g., [42, 21, 37]) are only known to be able to find the optimal allocation among superadditive valuations or few other simple classes ([36]). We show that this is no mistake: Theorem: No ascending auction with anonymous prices can find the optimal allocation between general valuations. 12 Non-deterministic computation is widely used in CS and also in economics (e.g, a Walrasian equilibrium or [38]). In some settings, deterministic and non-deterministic models have equal power (e.g., computation with finite automata).
This bound is regardless of the running time, and it also holds for descending auctions and non-deterministic auctions.
We strengthen this result significantly by showing that anonymous ascending auctions cannot produce a better than O( √ m) approximation - the approximation ratio that can be achieved with a polynomial number of queries ([26, 32]) and, as mentioned, with a polynomial number of item-price demand queries. The same lower bound clearly holds for anonymous item-price ascending auctions since such auctions can be simulated by anonymous bundle-price ascending auctions. We currently do not have any lower bound on the approximation achievable by non-anonymous item-price ascending auctions.
Finally, we study the performance of the existing computationally-efficient ascending auctions. These protocols ([37, 3]) require exponential time in the worst case, and this is unavoidable as shown by [32]. However, we also observe that these auctions, as well as the whole class of similar ascending bundle-price auctions, require an exponential time even for simple additive valuations. This is avoidable and indeed the ascending item-price auctions of [22] can find the optimal allocation for these simple valuations with polynomial communication.
Our model aims to capture iterative auctions that operate on real-valued valuations. There is a slight technical difficulty here in bridging the gap between the discrete nature of an iterative auction, and the continuous nature of the valuations. This is exactly the same problem as in modeling a simple English auction. There are three standard formal ways to model it:
its trajectory in time. For example, the so-called Japanese auction is basically a continuous model of an English model.13
continuously valued. In this case we introduce a parameter and usually require the auction to produce results that are -close to optimal.
assume that all valuations are integer multiples of some small fixed quantity δ, e.g., 1 penny. All communication in this case is then naturally finite.
In this paper we use the latter formulation and assume that all values are multiples of some δ. Thus, in some parts of the paper we assume without loss of generality that δ = 1, hence all valuations are integral. Almost all (if not all) of our results can be translated to the other two models with little effort.
A single auctioneer is selling m indivisible non-homogeneous items in a single auction, and let M be the set of these 13 Another similar model is the moving knives model in the cake-cutting literature. 34 items and N be the set of bidders. Each one of the n bidders in the auction has a valuation function vi : 2m → {0, δ, 2δ, ..., L}, where for every bundle of items S ⊆ M, vi(S) denotes the value of bidder i for the bundle S and is a multiple of δ in the range 0...L. We will sometimes denote the number of bits needed to represent such values in the range 0...L by t = log L. We assume free disposal, i.e.,
S ⊆ T implies vi(S) ≤ vi(T) and that vi(∅) = 0 for all bidders.
We will mention the following classes of valuations: • A valuation is called sub-modular if for all sets of items A and B we have that v(A ∪ B) + v(A ∩ B) ≤ v(A) + v(B). • A valuation is called super-additive if for all disjoint sets of items A and B we have that v(A∪B) ≥ v(A)+ v(B). • A valuation is called a k-bundle XOR if it can be represented as a XOR combination of at most k atomic bids [30], i.e., if there are at most k bundles Si and prices pi such that for all S, v(S) = maxi|S⊇Si pi. Such valuations will be denoted by v = (S1 : p1) ⊕ (S2 : p2) ⊕ . . . ⊕ (Sk : pk).14
The auctioneer sets up a protocol (equivalently an algorithm), where at each stage of the protocol some information q - termed the query - is sent to some bidder i, and then bidder i replies with some reply that depends on the query as well as on his own valuation. In this paper, we assume that we have complete control over the bidders" behavior, and thus the protocol also defines a reply function ri(q, vi) that specifies bidder i"s reply to query q. The protocol may be adaptive: the query value as well as the queried bidder may depend on the replies received for past queries.
At the end of the protocol, an allocation S1...Sn must be declared, where Si ∩ Sj = ∅ for i = j.
We say that the auction finds an optimal allocation if it finds the allocation that maximizes the social welfareP i vi(Si). We say that it finds a c-approximation if P i vi(Si) ≥ P i vi(Ti)/c where T1...Tn is an optimal allocation. The running time of the auction on a given instance of the bidders" valuations is the total number of queries made on this instance. The running time of a protocol is the worst case cost over all instances. Note that we impose no computational limitations on the protocol or on the players.15 This of course only strengthens our hardness results. Yet, our positive results will not use this power and will be efficient also in the usual computational sense.
Our goal will be to design computationally-efficient protocols. We will deem a protocol computationally-efficient if its cost is polynomial in the relevant parameters: the number of bidders n, the number of items m, and t = log L, where L is the largest possible value of a bundle. However, when we discuss ascending-price auctions and their variants, a computationally-efficient protocol will be required to be 14 For example, v = (abcd : 5) ⊕ (ab : 3) ⊕ (c : 4) denotes the XOR valuation with the terms abcd, ab, c and prices 5, 3, 4 respectively. For this valuation, v(abcd) = 5, v(abd) = 3, v(abc) = 4. 15 The running time really measures communication costs and not computational running time. pseudo-polynomial, i.e., it should ask a number of queries which is polynomial in m, n and L δ . This is because that ascending auctions can usually not achieve such running times (consider even the English auction on a single item).16 Note that all of our results give concrete bounds, where the dependence on the parameters is given explicitly; we use the standard big-Oh notation just as a shorthand.
We say than an auction elicits some class V of valuations, if it determines the optimal allocation for any profile of valuations drawn from V ; We say that an auction fully elicits some class of valuations V , if it can fully learn any single valuation v ∈ V (i.e., learn v(S) for every S).
Most of the paper will be concerned with a common special case of iterative auctions that we term demand auctions. In such auctions, the queries that are sent to bidders are demand queries: the query specifies a price p(S) ∈ + for each bundle S. The reply of bidder i is simply the set most desired - demanded - under these prices. Formally, a set S that maximizes vi(S) − p(S). It may happen that more than one set S maximizes this value. In which case, ties are broken according to some fixed tie-breaking rule, e.g., the lexicographically first such set is returned. All of our results hold for any fixed tie-breaking rule.
Ascending auctions are iterative auctions with non-decreasing prices: Definition 1. In an ascending auction, the prices in the queries to the same bidder can only increase in time.
Formally, let p be a query made for bidder i, and q be a query made for bidder i at a later stage in the protocol. Then for all sets S, q(S) ≥ p(S). A similar variant, which we also study and that is also common in real life, is descending auctions, in which prices can only decrease in time.
Note that the term ascending auction refers to an auction with a single ascending trajectory of prices. It may be useful to define multi-trajectory ascending auctions, in which the prices maybe reset to zero a number of times (see, e.g., [4]).
We consider two main restrictions on the types of allowed demand queries: Definition 2. Item Prices: The prices in each query are given by prices pj for each item j. The price of a set S is additive: p(S) = P j∈S pj.
Definition 3. Anonymous prices: The prices seen by the bidders at any stage in the auction are the same, i.e. whenever a query is made to some bidder, the same query is also made to all other bidders (with the prices unchanged).
In auctions with non-anonymous (discriminatory) prices, each bidder i has personalized prices denoted by pi (S).17 In this paper, all auctions are anonymous unless otherwise specified.
Note that even though in our model valuations are integral (or multiples of some δ), we allow the demand query to 16 Most of the auctions we present may be adapted to run in time polynomial in log L, using a binary-search-like procedure, losing their ascending nature. 17 Note that a non-anonymous auction can clearly be simulated by n parallel anonymous auctions. 35 use arbitrary real numbers in +. That is, we assume that the increment we use in the ascending auctions may be significantly smaller than δ. All our hardness results hold for any , even for continuous price increments. A practical issue here is how will the query be specified: in the general case, an exponential number of prices needs to be sent in a single query. Formally, this is not a problem as the model does not limit the length of queries in any way - the protocol must simply define what the prices are in terms of the replies received for previous queries. We look into this issue further in Section 4.3.
In this section, we study the power of iterative auctions that use demand queries (not necessarily ascending). We start by comapring demand queries to other types of queries.
Then, we discuss how well can one approximate the optimal welfare using a polynomial number of demand queries. We also initiate the study of the representation of bundle-price demand queries, and finally, we show how demand queries help solving the linear-programming relaxation of combinatorial auctions in polynomial time.
In this section we compare the power of the various types of queries defined in Section 2. We will present computationally -efficient simulations of these query types using item-price demand queries. In Section 5.1 we show that these simulations can also be done using item-price ascending auctions.
Lemma 4.1. A value query can be simulated by m marginalvalue queries. A marginal-value query can be simulated by two value queries.
Lemma 4.2. A value query can be simulated by mt demand queries (where t = log L is the number of bits needed to represent a single bundle value).18 As a direct corollary we get that demand auctions can always fully elicit the bidders" valuations by simulating all possible 2m − 1 queries and thus elicit enough information for determining the optimal allocation. Note, however, that this elicitation may be computationally inefficient.
The next lemma shows that demand queries can be exponentially more powerful than value queries.
Lemma 4.3. An exponential number of value queries may be required for simulating a single demand query.
Indirect utility queries are, however, equivalent in power to demand queries: Lemma 4.4. An indirect-utility query can be simulated by mt + 1 demand queries. A demand query can be simulated by m + 1 indirect-utility queries.
Demand queries can also simulate relative-demand queries:19 18 Note that t bundle-price demand queries can easily simulate a value query by setting the prices of all the bundles except S (the bundle with the unknown value) to be L, and performing a binary search on the price of S. 19 Note: although in our model values are integral (our multiples of δ), we allow the query prices to be arbitrary real numV MV D IU RD V 1 2 exp exp exp MV m 1 exp exp exp D mt poly 1 mt+1 poly IU 1 2 m+1 1 poly RD - - - - 1 Figure 4: Each entry in the table specifies how many queries of this row are needed to simulate a query from the relevant column.
Abbreviations: V (value query), MV (marginal-value query), D (demand query), IU (Indirect-utility query),
RD (relative demand query).
Lemma 4.5. Relative-demand queries can be simulated by a polynomial number of demand queries.
According to our definition of relative-demand queries, they clearly cannot simulate even value queries. Figure 4 summarizes the relations between these query types.
Value and Demand Queries We know from [32] that iterative combinatorial auctions that only use a polynomial number of queries can not find an optimal allocation among general valuations and in fact can not even approximate it to within a factor better than min{n, m1/2− }. In this section we ask how well can this approximation be done using demand queries with item prices, or using the weaker value queries. We show that, using demand queries, the lower bound can be matched, while value queries can only do much worse.
Figure 5 describes a polynomial-time algorithm that achieves a min(n, O( √ m)) approximation ratio. This algorithm greedily picks the bundles that maximize the bidders" per-item value (using relative-demand queries, see Section 4.1). As a final step, it allocates all the items to a single bidder if it improves the social welfare (this can be checked using value queries). Since both value queries and relative-demand queries can be simulated by a polynomial number of demand queries with item prices (Lemmas 4.2 and 4.5), this algorithm can be implemented by a polynomial number of demand queries with item prices.20 Theorem 4.6. The auction described in Figure 5 can be implemented by a polynomial number of demand queries and achieves a min{n, 4 √ m}-approximation for the social welfare.
We now ask how well can the optimal welfare be approximated by a polynomial number of value queries. First we note that value queries are not completely powerless: In [19] it is shown that if the m items are split into k fixed bundles of size m/k each, and these fixed bundles are auctioned as though each was indivisible, then the social welfare bers, thus we may have bundles with arbitrarily close relative demands. In this sense the simulation above is only up to any given (and the number of queries is O(log L+log 1 )).
When the relative-demand query prices are given as rational numbers, exact simulation is implied when log is linear in the input length. 20 In the full paper [8], we observe that this algorithm can be implemented by two descending item-price auctions (where we allow removing items along the auction). 36 generated by such an auction is at least m√ k -approximation of that possible in the original auction. Notice that such an auction can be implemented by 2k − 1 value queries to each bidder - querying the value of each bundle of the fixed bundles. Thus, if we choose k = log m bundles we get an m√ log m -approximation while still using a polynomial number of queries.
The following lemma shows that not much more is possible using value queries: Lemma 4.7. Any iterative auction that uses only value queries and distinguishes between k-tuples of 0/1 valuations where the optimal allocation has value 1, and those where the optimal allocation has value k requires at least 2 m k queries.
Proof. Consider the following family of valuations: for every S, such that |S| > m/2, v(S) = 1, and there exists a single set T, such that for |S| ≤ m/2, v(S) = 1 iff T ⊆ S and v(S) = 0 otherwise. Now look at the behavior of the protocol when all valuations vi have T = {1...m}. Clearly in this case the value of the best allocation is 1 since no set of size m 2 or lower has non-zero value for any player. Fix the sequence of queries and answers received on this k-tuple of valuations.
Now consider the k-tuple of valuations chosen at random as follows: a partition of the m items into k sets T1...Tk each of size m k each is chosen uniformly at random among all such partitions. Now consider the k-tuple of valuations from our family that correspond to this partition - clearly Ti can be allocated to i, for each i, getting a total value of k.
Now look at the protocol when running on these valuations and compare its behavior to the original case. Note that the answer to a query S to player i differs between the case of Ti and the original case of T = {1...m} only if |S| ≤ m 2 and Ti ⊆ S. Since Ti is distributed uniformly among all sets of size exactly m k , we have that for any fixed query S to player i, where |S| ≤ m 2 ,
Pr[Ti ⊆ S] ≤ „ |S| m «|Ti| ≤ 2− m k Using the union-bound, if the original sequence of queries was of length less than 2 m k , then with positive probability none of the queries in the sequence would receive a different answer than for the original input tuple. This is forbidden since the protocol must distinguish between this case and the original case - which cannot happen if all queries receive the same answer. Hence there must have been at least 2 m k queries for the original tuple of valuations.
We conclude that a polynomial time protocol that uses only value queries cannot obtain a better than O( m log m ) approximation of the welfare: Theorem 4.8. An iterative auction that uses a polynomial number of value queries cannot achieve better than O( m log m )-approximation for the social welfare.
Proof. Immediate from Lemma 4.7: achieving any approximation ratio k which is asymptotically greater than m log m needs an exponential number of value queries.
An Approximation Algorithm: Initialization: Let T ← M be the current items for sale.
Let K ← N be the currently participating bidders.
Let S∗
n ← ∅ be the provisional allocation.
Repeat until T = ∅ or K = ∅: Ask each bidder i in K for the bundle Si that maximizes her per-item value, i.e., Si ∈ argmaxS⊆T vi(S) |S| .
Let i be the bidder with the maximal per-item value, i.e., i ∈ argmaxi∈K vi(Si) |Si| .
Set: s∗ i = si, K = K \ i, M = M \ Si Finally: Ask the bidders for their values vi(M) for the grand bundle. If allocating all the items to some bidder i improves the social welfare achieved so far (i.e., ∃i ∈ N such that vi(M) > P i∈N vi(S∗ i )), then allocate all items to this bidder i.
Figure 5: This algorithm achieves a min{n, 4 √  m}approximation for the social welfare, which is asymptotically the best worst-case approximation possible with polynomial communication. This algorithm can be implemented with a polynomial number of demand queries.
In this section we explicitly fix the language in which bundle prices are presented to the bidders in bundle-price auctions. This language requires the algorithm to explicitly list the price of every bundle with a non-trivial price.
Trivial in this context is a price that is equal to that of one of its proper subsets (which was listed explicitly). This representation is equivalent to the XOR-language for expressing valuations. Formally, each query q is given by an expression: q = (S1 : p1) ⊕ (S2 : p2) ⊕ ... ⊕ (Sl : pl). In this representation, the price demanded for every set S is simply p(S) = max{k=1...l|Sk⊆S}pk.
Definition 4. The length of the query q = (S1 : p1) ⊕ (S2 : p2) ⊕ ... ⊕ (Sl : pl) is l. The cost of an algorithm is the sum of the lengths of the queries asked during the operation of the algorithm on the worst case input.
Note that under this definition, bundle-price auctions are not necessarily stronger than item-price auctions. An itemprice query that prices each item for 1, is translated to an exponentially long bundle-price query that needs to specify the price |S| for each bundle S. But perhaps bundle-price auctions can still find optimal allocations whenever itemprice auction can, without directly simulating such queries?
We show that this is not the case: indeed, when the representation length is taken into account, bundle price auctions are sometimes seriously inferior to item price auctions.
Consider the following family of valuations: Each item is valued at 3, except that for some single set S, its value is a bit more: 3|S| + b, where b ∈ {0, 1, 2}. Note that an item price auction can easily find the optimal allocation between any two such valuations: Set the prices of each item to 3+ ; if the demand sets of the two players are both empty, then b = 0 for both valuations, and an arbitrary allocation is fine.
If one of them is empty and the other non-empty, allocate the non-empty demand set to its bidder, and the rest to the other. If both demand sets are non-empty then, unless they form an exact partition, we need to see which b is larger, which we can do by increasing the price of a single item in each demand set. 37 We will show that any bundle-price auction that uses only the XOR-language to describe bundle prices requires an exponential cost (which includes the sum of all description lengths of prices) to find an optimal allocation between any two such valuations.
Lemma 4.9. Every bundle-price auction that uses XORexpressions to denote bundle prices requires 2Ω( √ m) cost in order to find the optimal allocation among two valuations from the above family.
The complication in the proof stems from the fact that using XOR-expressions, the length of the price description depends on the number of bundles whose price is strictly larger than each of their subsets - this may be significantly smaller than the number of bundles that have a non-zero price. (The proof becomes easy if we require the protocol to explicitly name every bundle with non-zero price.) We do not know of any elementary proof for this lemma (although we believe that one can be found). Instead we reduce the problem to a well known lower bound in boolean circuit complexity [18] stating that boolean circuits of depth
2Ω( √ m) size.
Consider the following linear-programming relaxation for the generalized winner-determination problem in combinatorial auctions (the primal program): Maximize X i∈N,S⊆M wi xi,S vi(S) s.t.
X i∈N, S|j∈S xi,S ≤ qj ∀j ∈ M X S⊆M xi,S ≤ di ∀i ∈ N xi,S ≥ 0 ∀i ∈ N, S ⊆ M Note that the primal program has an exponential number of variables. Yet, we will be able to solve it in polynomial time using demand queries to the bidders. The solution will have a polynomial size support (non-zero values for xi,S), and thus we will be able to describe it in polynomial time.
Here is its dual: Minimize X j∈M qjpj + X i∈N diui s.t. ui + X j∈S pj ≥ wivi(S) ∀i ∈ N, S ⊆ M pi ≥ 0, uj ≥ 0 ∀i ∈ M, j ∈ N Notice that the dual problem has exactly n + m variables but an exponential number of constraints. Thus, the dual can be solved using the Ellipsoid method in polynomial time - if a separation oracle can be implemented in polynomial time. Recall that a separation oracle, when given a possible solution, either confirms that it is a feasible solution, or responds with a constraint that is violated by the possible solution.
We construct a separation oracle for solving the dual program, using a single demand query to each of the bidders.
Consider a possible solution (u, p) for the dual program. We can re-write the constraints of the dual program as: ui/wi ≥ vi(S) − X j∈S pj/wi Now a demand query to bidder i with prices pj/wi reveals exactly the set S that maximizes the RHS of the previous inequality. Thus, in order to check whether (u, p) is feasible it suffices to (1) query each bidder i for his demand Di under the prices pj/wi; (2) check only the n constraints ui + P j∈Di pj ≥ wivi(Di) (where vi(Di) can be simulated using a polynomial sequence of demand queries as shown in Lemma 4.2). If none of these is violated then we are assured that (u, p) is feasible; otherwise we get a violated constraint.
What is left to be shown is how the primal program can be solved. (Recall that the primal program has an exponential number of variables.) Since the Ellipsoid algorithm runs in polynomial time, it encounters only a polynomial number of constraints during its operation. Clearly, if all other constraints were removed from the dual program, it would still have the same solution (adding constraints can only decrease the space of feasible solutions). Now take the reduced dual where only the constraints encountered exist, and look at its dual. It will have the same solution as the original dual and hence of the original primal. However, look at the form of this dual of the reduced dual. It is just a version of the primal program with a polynomial number of variables - those corresponding to constraints that remained in the reduced dual. Thus, it can be solved in polynomial time, and this solution clearly solves the original primal program, setting all other variables to zero.
In this section we characterize the power of ascending item-price auctions. We first show that this power is not trivial: such auctions can in general elicit an exponential amount of information. On the other hand, we show that the optimal allocation cannot always be determined by a single ascending auction, and in some cases, nor by an exponential number of ascending-price trajectories. Finally, we separate the power of different models of ascending auctions.
Auctions We first show that if small enough increments are allowed, a single ascending trajectory of item-prices can elicit preferences that cannot be elicited with polynomial communication. As mentioned, all our hardness results hold for any increment, even infinitesimal.
Theorem 5.1. Some classes of valuations can be elicited by item-price ascending auctions, but cannot be elicited by a polynomial number of queries of any kind.
Proof. (sketch) Consider two bidders with v(S) = 1 if |S| > n 2 , v(S) = 0 if |S| < n 2 and every S such that |S| = n 2 has an unknown value from {0, 1}. Due to [32], determining the optimal allocation here requires exponential communication in the worst case. Nevertheless, we show (see [9]) that an item-price ascending auction can do it, as long as it can use exponentially small increments.
We now describe another positive result for the power of item-price ascending auctions. In section 4.1, we showed 38 v(ab) v(a) v(b) Bidder 1 2 α ∈ (0, 1) β ∈ (0, 1) Bidder 2 2 2 2 Figure 6: No item-price ascending auctions can determine the optimal allocation for this class of valuations. that a value query can be simulated with a (truly) polynomial number of item-price demand queries. Here, we show that every value query can be simulated by a (pseudo) polynomial number of ascending item-price demand queries. (In the next subsection, we show that we cannot always simulate even a pair of value queries using a single item-price ascending auction.) In the full paper (part II,[9]), we show that we can simulate other types of queries using item-price ascending auctions.
Proposition 5.2. A value query can be simulated by an item-price ascending auction. This simulation requires a polynomial number of queries.
Actually, the proof for Proposition 5.2 proves a stronger useful result regarding the information elicited by iterative auctions. It says that in any iterative auction in which the changes of prices are small enough in each stage (pseudocontinuous auctions), the value of all bundles demanded during the auction can be computed. The basic idea is that when the bidder moves from demanding some bundle Ti to demanding another bundle Ti+1, there is a point in which she is indifferent between these two bundles. Thus, knowing the value of some demanded bundle (e.g., the empty set) enables computing the values of all other demanded bundles.
We say that an auction is pseudo-continuous, if it only uses demand queries, and in each step, the price of at most one item is changed by (for some ∈ (0, δ]) with respect to the previous query.
Proposition 5.3. Consider any pseudo-continuous auction (not necessarily ascending), in which bidder i demands the empty set at least once along the auction. Then, the value of every bundle demanded by bidder i throughout the auction can be calculated at the end of the auction.
Auctions Although we observed that demand queries can solve any combinatorial auction problem, when the queries are restricted to be ascending, some classes of valuations cannot be elicited nor fully-elicited. An example for such class of valuations is given in Figure 6.
Theorem 5.4. There are classes of valuations that cannot be elicited nor fully elicited by any item-price ascending auction.
Proof. Let bidder 1 have the valuation described in the first row of Figure 6, where α and β are unknown values in (0, 1). First, we prove that this class cannot be fully elicited by a single ascending auction. Specifically, an ascending auction cannot reveal the values of both α and β.
As long as pa and pb are both below 1, the bidder will always demand the whole bundle ab: her utility from ab is strictly greater than the utility from either a or b separately.
For example, we show that u1(ab) > u1(a): u1(ab) = 2 − (pa + pb) = 1 − pa + 1 − pb > vA(a) − pa + 1 − pb > u1(a) Thus, in order to gain any information about α or β, the price of one of the items should become at least 1, w.l.o.g. pa ≥ 1. But then, the bundle a will not be demanded by bidder 1 throughout the auction, thus no information at all will be gained about α.
Now, assume that bidder 2 is known to have the valuation described in the second row of Figure 6. The optimal allocation depends on whether α is greater than β (in bidder 1"s valuation), and we proved that an ascending auction cannot determine this.
The proof of the theorem above shows that for an unknown value to be revealed, the price of one item should be greater than 1, and the other price should be smaller than
one of these values can be revealed. An immediate conclusion is that this impossibility result also holds for item-price descending auctions. Since no such trajectory exists, then the same conclusion even holds for non-deterministic itemprice auctions (in which exogenous data tells us how to increase the prices). Also note that since the hardness stems from the impossibility to fully-elicit a valuation of a single bidder, this result also holds for non-anonymous ascending item-price auctions.
Ascending Auctions According to Theorem 5.4, no ascending item-price auction can always elicit the preferences (we prove a similar result for bundle prices in section 6). But can two ascending trajectories do the job? Or a polynomial number of ascending trajectories? We give negative answers for such suggestions.
We define a k-trajectory ascending auction as a demandquery iterative auction in which the demand queries can be partitioned to k sets of queries, where the prices published in each set only increase in time. Note that we use a general definition; It allows the trajectories to run in parallel or sequentially, and to use information elicited in some trajectories for determining the future queries in other trajectories.
The power of multiple-trajectory auctions can be demonstrated by the negative result of Gul and Stacchetti [17] who showed that even for an auction among substitutes valuations, an anonymous ascending item-price auction cannot compute VCG prices for all players.21 Ausubel [4] overcame this impossibility result and designed auctions that do compute VCG prices by organizing the auction as a sequence of n + 1 ascending auctions. Here, we prove that one cannot elicit XOR valuations with k terms by less than k − 1 ascending trajectories. On the other hand, we show that an XOR formula can be fully elicited by k−1 non-deterministic ascending auctions (or by k−1 deterministic ascending auctions if the auctioneer knows the atomic bundles).22 21 A recent unpublished paper by Mishra and Parkes extends this result, and shows that non-anonymous prices with bundle-prices are necessary in order that an ascending auction will end up with a universal-competitive-equilibrium (that leads to VCG payments). 22 This result actually separates the power of deterministic 39 Proposition 5.5. XOR valuations with k terms cannot be elicited (or fully elicited) by any (k-2)-trajectory itemprice ascending auction, even when the atomic bundles are known to the elicitor. However, these valuations can be elicited (and fully elicited) by (k-1)-trajectory non-deterministic non-anonymous item-price ascending auctions.
Moreover, an exponential number of trajectories is required for eliciting some classes of valuations: Proposition 5.6. Elicitation and full-elicitation of some classes of valuations cannot be done by any k-trajectory itemprice ascending auction, where k = o(2m ).
Proof. (sketch) Consider the following class of valuations: For |S| < m 2 , v(S) = 0 and for |S| > m 2 , v(S) = 2; every bundle S of size m 2 has some unknown value in (0, 1).
We show ([9]) that a single item-price ascending auction can reveal the value of at most one bundle of size n 2 , and therefore an exponential number of ascending trajectories is needed in order to elicit such valuations.
We observe that the algorithm we presented in Section
ascending auctions (each item-price demand query can be considered as a separate ascending auction), and therefore a min(n, 4 √ m)-approximation can be achieved by a polynomial number of ascending auctions. We do not currently have a better upper bound or any lower bound.
Ascending Auctions Various models for ascending auctions have been suggested in the literature. In this section, we compare the power of the different models. As mentioned, all auctions are considered anonymous and deterministic, unless specified otherwise.
Ascending vs. Descending Auctions: We begin the discussion of the relation between ascending auctions and descending auctions with an example. The algorithm by Lehmann, Lehmann and Nisan [25] can be implemented by a simple item-price descending auction (see the full paper for details [9]). This algorithm guarantees at least half of the optimal efficiency for submodular valuations. However, we are not familiar with any ascending auction that guarantees a similar fraction of the efficiency. This raises a more general question: can ascending auctions solve any combinatorialauction problem that is solvable using a descending auction (and vice versa)? We give negative answers to these questions. The idea behind the proofs is that the information that the auctioneer can get for free at the beginning of each type of auction is different.23 and non-deterministic iterative auctions: our proof shows that a non-deterministic iterative auction can elicit the kterm XOR valuations with a polynomial number of demand queries, and [7] show that this elicitation must take an exponential number of demand queries. 23 In ascending auctions, the auctioneer can reveal the most valuable bundle (besides M) before she starts raising the prices, thus she can use this information for adaptively choose the subsequent queries. In descending auctions, one can easily find the bundle with the highest average per-item price, keeping all other bundles with non-positive utilities, and use this information in the adaptive price change.
Proposition 5.7. There are classes that cannot be elicited (fully elicited) by ascending item-price auctions, but can be elicited (resp. fully elicited) with a descending item-price auction.
Proposition 5.8. There are classes that cannot be elicited (fully elicited) by item-price descending auctions, but can be elicited (resp. fully elicited) by item-price ascending auctions.
Deterministic vs. Non-Deterministic Auctions: Nondeterministic ascending auctions can be viewed as auctions where some benevolent teacher that has complete information guides the auctioneer on how she should raise the prices.
That is, preference elicitation can be done by a non-deterministic ascending auction, if there is some ascending trajectory that elicits enough information for determining the optimal allocation (and verifying that it is indeed optimal). We show that non-deterministic ascending auctions are more powerful than deterministic ascending auctions: Proposition 5.9. Some classes can be elicited (fully elicited) by an item-price non-deterministic ascending auction, but cannot be elicited (resp. fully elicited) by item-price deterministic ascending auctions.
Anonymous vs. Non-Anonymous Auctions: As will be shown in Section 6, the power of anonymous and nonanonymous bundle-price ascending auctions differs significantly. Here, we show that a difference also exists for itemprice ascending auctions.
Proposition 5.10. Some classes cannot be elicited by anonymous item-price ascending auctions, but can be elicited by a non-anonymous item-price ascending auction.
Sequential vs. Simultaneous Auctions: A non-anonymous auction is called simultaneous if at each stage, the price of some item is raised by for every bidder. The auctioneer can use the information gathered until each stage, in all the personalized trajectories, to determine the next queries.
A non-anonymous auction is called sequential if the auctioneer performs an auction for each bidder separately, in sequential order. The auctioneer can determine the next query based on the information gathered in the trajectories completed so far and on the history of the current trajectory.
Proposition 5.11. There are classes that cannot be elicited by simultaneous non-anonymous item-price ascending auctions, but can be elicited by a sequential non-anonymous item-price ascending auction.
Adaptive vs. Oblivious Auctions: If the auctioneer determines the queries regardless of the bidders" responses (i.e., the queries are predefined) we say that the auction is oblivious. Otherwise, the auction is adaptive. We prove that an adaptive behaviour of the auctioneer may be beneficial.
Proposition 5.12. There are classes that cannot be elicited (fully elicited) using oblivious item-price ascending auctions, but can be elicited (resp. fully elicited) by an adaptive item-price ascending auction. 40
Preference elicitation and full elicitation are closely related problems. If full elicitation is easy (e.g., in polynomial time) then clearly elicitation is also easy (by a nonanonymous auction, simply by learning all the valuations separately24 ). On the other hand, there are examples where preference elicitation is considered easy but learning is hard (typically, elicitation requires smaller amount of information; some examples can be found in [7]).
The tatonnement algorithms by [22, 12, 16] end up with the optimal allocation for substitutes valuations.25 We prove that we cannot fully elicit substitutes valuations (or even their sub-class of OXS valuations defined in [25]), even for a single bidder, by an item-price ascending auction (although the optimal allocation can be found by an ascending auction for any number of bidders!).
Theorem 5.13. Substitute valuations cannot be fully elicited by ascending item-price auctions. Moreover, they cannot be fully elicited by any m 2 ascending trajectories (m > 3).
Whether substitutes valuations have a compact representation (i.e., polynomial in the number of goods) is an important open question. As a step in this direction, we show that its sub-class of OXS valuations does have a compact representation: every OXS valuation can be represented by at most m2 values.26 Lemma 5.14. Any OXS valuation can be represented by no more than m2 values.
AUCTIONS All the ascending auctions in the literature that are proved to find the optimal allocation for unrestricted valuations are non-anonymous bundle-price auctions (iBundle(3) by Parkes and Ungar [37] and the Proxy Auction by Ausubel and Milgrom [3]). Yet, several anonymous ascending auctions have been suggested (e.g., AkBA [42], [21] and iBundle(2) [37]). In this section, we prove that anonymous bundle-price ascending auctions achieve poor results in the worst-case.
We also show that the family of non-anonymous bundleprice ascending auctions can run exponentially slower than simple item-price ascending auctions.
Ascending Auctions We present a class of valuations that cannot be elicited by anonymous bundle-price ascending auctions. These valuations are described in Figure 7. The basic idea: for determining some unknown value of one bidder we must raise 24 Note that an anonymous ascending auction cannot necessarily elicit a class that can be fully elicited by an ascending auction. 25 Substitute valuations are defined, e.g., in [16]. Roughly speaking, a bidder with a substitute valuation will continue demand a certain item after the price of some other items was increased. For completeness, we present in the full paper [9] a proof for the efficiency of such auctions for substitutes valuations. 26 A unit-demand valuation is an XOR valuation in which all the atomic bundles are singletons. OXS valuations can be interpreted as an aggregation (OR) of any number of unit-demand bidders.
Bid. 1 v1(ac) = 2 v1(bd) = 2 v1(cd) = α ∈ (0, 1) Bid. 2 v2(ab) = 2 v2(cd) = 2 v2(bd) = β ∈ (0, 1) Figure 7: Anonymous ascending bundle-price auctions cannot determine the optimal allocation for this class of valuations. a price of a bundle that should be demanded by the other bidder in the future.
Theorem 6.1. Some classes of valuations cannot be elicited by anonymous bundle-price ascending auctions.
Proof. Consider a pair of XOR valuations as described in Figure 7. For finding the optimal allocation we must know which value is greater between α and β.27 However, we cannot learn the value of both α and β by a single ascending trajectory: assume w.l.o.g. that bidder 1 demands cd before bidder 2 demands bd (no information will be elicited if none of these happens). In this case, the price for bd must be greater than 1 (otherwise, bidder 1 prefers bd to cd). Thus, bidder 2 will never demand the bundle bd, and no information will be elicited about β.
The valuations described in the proof of Theorem 6.1 can be easily elicited by a non-anonymous item-price ascending auction. On the other hand, the valuations in Figure 6 can be easily elicited by an anonymous bundle-price ascending auction. We conclude that the power of these two families of ascending auctions is incomparable.
We strengthen the impossibility result above by showing that anonymous bundle-price auctions cannot even achieve better than a min{O(n), O( √ m)}-approximation for the social welfare. This approximation ratio can be achieved with polynomial communication, and specifically with a polynomial number of item-price demand queries.28 Theorem 6.2. An anonymous bundle-price ascending auction cannot guarantee better than a min{ n 2 , √ m 2 } approximation for the optimal welfare.
Proof. (Sketch) Assume we have n bidders and n2 items for sale, and that n is prime. We construct n2 distinct bundles with the following properties: for each bidder, we define a partition Si = (Si 1, ..., Si n) of the n2 items to n bundles, such that any two bundles from different partitions intersect.
In the full paper, part II [9] we show an explicit construction using the properties of linear functions over finite fields. The rest of the proof is independent of the specific construction.
Using these n2 bundles we construct a hard-to-elicit class. Every bidder has an atomic bid, in his XOR valuation, for each of these n2 bundles. A bidder i has a value of 2 for any bundle Si j in his partition. For all bundles in the other partitions, he has a value of either 0 or of 1 − δ, and these values are unknown to the auctioneer. Since every pair of bundles from different partitions intersect, only one bidder can receive a bundle with a value of 2. 27 If α > β, the optimal allocation will allocate cd to bidder
ac to bidder 1. Note that both bidders cannot gain a value of 2 in the same allocation, due to the intersections of the high-valued bundles. 28 Note that bundle-price queries may use exponential communication, thus the lower bound of [32] does not hold. 41 Non-anonymous Bundle-Price Economically-Efficient Ascending Auctions: Initialization: All prices are initialized to zero (non-anonymous bundle prices).
Repeat: - Each bidder submits a bundle that maximizes his utility under his current personalized prices. - The auctioneer calculates a provisional allocation that maximizes his revenue under the current prices. - The prices of bundles that were demanded by losing bidders are increased by .
Finally: Terminate when the provisional allocation assigns to each bidder the bundle he demanded.
Figure 8: Auctions from this family (denoted by NBEA auctions) are known to achieve the optimal welfare.
No bidder will demand a low-valued bundle, as long as the price of one of his high-valued bundles is below 1 (and thus gain him a utility greater than 1). Therefore, for eliciting any information about the low-valued bundles, the auctioneer should first arbitrarily choose a bidder (w.l.o.g bidder 1) and raise the prices of all the bundles (S1
n) to be greater than 1. Since the prices cannot decrease, the other bidders will clearly never demand these bundles in future stages. An adversary may choose the values such that the low values of all the bidders for the bundles not in bidder 1"s partition are zero (i.e., vi(S1 j ) = 0 for every i = 1 and every j), however, allocating each bidder a different bundle from bidder 1"s partition, might achieve a welfare of n+1−(n−1)δ (bidder 1"s valuation is 2, and 1 − δ for all other bidders); If these bundles were wrongly allocated, only a welfare of
for all other bidders). At this point, the auctioneer cannot have any information about the identity of the bundles with the non-zero values. Therefore, an adversary can choose the values of the bundles received by bidders 2, ..., n in the final allocation to be zero. We conclude that anonymous bundleprice auctions cannot guarantee a welfare greater than 2 for this class, where the optimal welfare can be arbitrarily close to n + 1.
The core of the auctions in [37, 3] is the scheme described in Figure 8 (in the spirit of [35]) for auctions with nonanonymous bundle prices. Auctions from this scheme end up with the optimal allocation for any class of valuations.
We denote this family of ascending auctions as NBEA auctions29 .
NBEA auctions can elicit k-term XOR valuations by a polynomial (in k) number of steps , although the elicitation of such valuations may require an exponential number of item-price queries ([7]), and item-price ascending auctions cannot do it at all (Theorem 5.4). Nevertheless, we show that NBEA auctions (and in particular, iBundle(3) and the proxy auction) are sometimes inferior to simple item-price demand auctions. This may justify the use of hybrid auctions that use both linear and non-linear prices (e.g., the clock-proxy auction [10]). We show that auctions from this 29 Non-anonymous Bundle-price economically Efficient Ascending auctions. For completeness, we give in the full paper [9] a simple proof for the efficiency (up to an ) of auctions of this scheme . family may use an exponential number of queries even for determining the optimal allocation among two bidders with additive valuations30 , where such valuations can be elicited by a simple item-price ascending auction. We actually prove this property for a wider class of auctions we call conservative auctions. We also observe that in conservative auctions, allowing the bidders to submit all the bundles in their demand sets ensures that the auction runs a polynomial number of steps - if L is not too high (but with exponential communication, of course).
An ascending auction is called conservative if it is nonanonymous, uses bundle prices initialized to zero and at every stage the auctioneer can only raise prices of bundles demanded by the bidders until this stage. In addition, each bidder can only receive bundles he demanded during the auction. Note that NBEA auctions are by definition conservative.
Proposition 6.3. If every bidder demands a single bundle in each step of the auction, conservative auctions may run for an exponential number of steps even for additive valuations. If the bidders are allowed to submit all the bundles in their demand sets in each step, then conservative auctions can run in a polynomial number of steps for any profile of valuations, as long as the maximal valuation L is polynomial in m, n and 1 δ .
Acknowledgments: The authors thank Moshe Babaioff, Shahar Dobzinski, Ron Lavi, Daniel Lehmann, Ahuva Mu"alem, David Parkes, Michael Schapira and Ilya Segal for helpful discussions. Supported by grants from the Israeli Academy of Sciences and the USAIsrael Binational Science Foundation.

Forecasting is a ubiquitous endeavor in human societies.
For decades, scientists have been developing and exploring various forecasting methods, which can be roughly divided into statistical and non-statistical approaches. Statistical approaches require not only the existence of enough historical data but also that past data contains valuable information about the future event. When these conditions can not be met, non-statistical approaches that rely on judgmental information about the future event could be better choices. One widely used non-statistical method is to elicit opinions from experts. Since experts are not generally in agreement, many belief aggregation methods have been proposed to combine expert opinions together and form a single prediction. These belief aggregation methods are called opinion pools, which have been extensively studied in statistics [20, 24, 38], and management sciences [8, 9, 30, 31], and applied in many domains such as group decision making [29] and risk analysis [12].
With the fast growth of the Internet, information markets have recently emerged as a promising non-statistical forecasting tool. Information markets (sometimes called prediction markets, idea markets, or event markets) are markets designed for aggregating information and making predictions about future events. To form the predictions, information markets tie payoffs of securities to outcomes of events. For example, in an information market to predict the result of a US professional National Football League (NFL) game, say New England vs Carolina, the security pays a certain amount of money per share to its holders if and only if New England wins the game. Otherwise, it pays off nothing. The security price before the game reflects the consensus expectation of market traders about the probability of New England winning the game. Such markets are becoming very popular. The Iowa Electronic Markets (IEM) [2] are real-money futures markets to predict economic and political events such as elections. The Hollywood Stock Exchange (HSX) [3] is a virtual (play-money) exchange for trading securities to forecast future box office proceeds of new movies, and the outcomes of entertainment awards, etc. TradeSports.com [7], a real-money betting exchange registered in Ireland, hosts markets for sports, political, entertainment, and financial events. The Foresight Exchange (FX) [4] allows traders to wager play money on unresolved scientific questions or other claims of public interest, and NewsFutures.com"s World News Exchange [1] has 58 popular sports and financial betting markets, also grounded in a play-money currency.
Despite the popularity of information markets, one of the most important questions to ask is: how accurately can information markets predict? Previous research in general shows that information markets are remarkably accurate.
The political election markets at IEM predict the election outcomes better than polls [16, 17, 18, 19]. Prices in HSX and FX have been found to give as accurate or more accurate predictions than judgment of individual experts [33, 34, 37]. However, information markets have not been calibrated against opinion pools, except for Servan-Schreiber et. al [36], in which the authors compare two information markets against arithmetic average of expert opinions. Since information markets, in nature, offer an adaptive and selforganized mechanism to aggregate opinions of market participants, it is interesting to compare them with existing opinion pooling methods, to evaluate the performance of information markets from another perspective. The comparison will provide beneficial guidance for practitioners to choose the most appropriate method for their needs.
This paper contributes to the literature in two ways: (1) As an initial attempt to compare information markets with opinion pools of multiple experts, it leads to a better understanding of information markets and their promise as an alternative institution for obtaining accurate forecasts; (2) In screening opinion pools to be used in the comparison, we cast insights into relative performances of different opinion pools. In terms of prediction accuracy, we compare two information markets with several linear and logarithmic opinion pools (LinOP and LogOP) at predicting the results of NFL games. Our results show that at the same time point ahead of the game, information markets provide as accurate predictions as our carefully selected opinion pools. In selecting the opinion pools to be used in our comparison, we find that arithmetic average is a robust and efficient pooling function; weighting expert assessments according to their past performances does not improve the prediction accuracy of opinion pools; and LogOP offers bolder predictions than LinOP. The remainder of the paper is organized as follows.
Section 2 reviews popular opinion pooling methods.
Section 3 introduces the basics of information markets. Data sets and our analysis methods are described in Section 4.
We present results and analysis in Section 5, followed by conclusions in Section 6.
Clemen and Winkler [12] classify opinion pooling methods into two broad categories: mathematical approaches and behavioral approaches. In mathematical approaches, the opinions of individual experts are expressed as subjective probability distributions over outcomes of an uncertain event. They are combined through various mathematical methods to form an aggregated probability distribution. Genest and Zidek [24] and French [20] provide comprehensive reviews of mathematical approaches. Mathematical approaches can be further distinguished into axiomatic approaches and Bayesian approaches. Axiomatic approaches apply prespecified functions that map expert opinions, expressed as a set of individual probability distributions, to a single aggregated probability distribution. These pooling functions are justified using axioms or certain desirable properties. Two of the most common pooling functions are the linear opinion pool (LinOP) and the logarithmic opinion pool (LogOP). Using LinOP, the aggregate probability distribution is a weighted arithmetic mean of individual probability distributions: p(θ) = n i=1 wipi(θ), (1) where pi(θ) is expert i"s probability distribution of uncertain event θ, p(θ) represents the aggregate probability distribution, wi"s are weights for experts, which are usually nonnegative and sum to 1, and n is the number of experts. Using LogOP, the aggregate probability distribution is a weighted geometric mean of individual probability distributions: p(θ) = k n i=1 pi(θ)wi , (2) where k is a normalization constant to ensure that the pooled opinion is a probability distribution. Other axiomatic pooling methods often are extensions of LinOP [22], LogOP [23], or both [13]. Winkler [39] and Morris [29, 30] establish the early framework of Bayesian aggregation methods. Bayesian approaches assume as if there is a decision maker who has a prior probability distribution over event θ and a likelihood function over expert opinions given the event. This decision maker takes expert opinions as evidence and updates its priors over the event and opinions according to Bayes rule. The resulted posterior probability distribution of θ is the pooled opinion.
Behavioral approaches have been widely studied in the field of group decision making and organizational behavior. The important assumption of behavioral approaches is that, through exchanging opinions or information, experts can eventually reach an equilibrium where further interaction won"t change their opinions. One of the best known behavioral approaches is the Delphi technique [28].
Typically, this method and its variants do not allow open discussion, but each expert has chance to judge opinions of other experts, and is given feedback. Experts then can reassess their opinions and repeat the process until a consensus or a smaller spread of opinions is achieved. Some other behavioral methods, such as the Nominal Group technique [14], promote open discussions in controlled environments.
Each approach has its pros and cons. Axiomatic approaches are easy to use. But they don"t have a normative basis to choose weights. In addition, several impossibility results (e.g., Genest [21]) show that no aggregation function can satisfy all desired properties of an opinion pool, unless the pooled opinion degenerates to a single individual opinion, which effectively implies a dictator. Bayesian approaches are nicely based on the normative Bayesian framework. However, it is sometimes frustratingly difficult to apply because it requires either (1) constructing an obscenely complex joint prior over the event and opinions (often impractical even in terms of storage / space complexity, not to mention from an elicitation standpoint) or (2) making strong assumptions about the prior, like conditional independence of experts. Behavior approaches allow experts to dynamically improve their information and revise their opinions during interactions, but many of them are not fixed or completely specified, and can"t guarantee convergence or repeatability. 59
Much of the enthusiasm for information markets stems from Hayek hypothesis [26] and efficient market hypothesis [15]. Hayek, in his classic critique of central planning in 1940"s, claims that the price system in a competitive market is a very efficient mechanism to aggregate dispersed information among market participants. The efficient market hypothesis further states that, in an efficient market, the price of a security almost instantly incorporates all available information. The market price summarizes all relevant information across traders, hence is the market participants" consensus expectation about the future value of the security.
Empirical evidence supports both hypotheses to a large extent [25, 27, 35]. Thus, when associating the value of a security with the outcome of an uncertain future event, market price, by revealing the consensus expectation of the security value, can indirectly predict the outcome of the event. This idea gives rise to information markets.
For example, if we want to predict which team will win the NFL game between New England and Carolina, an information market can trade a security $100 if New England defeats Carolina, whose payoff per share at the end of the game is specified as follow: $100 if New England wins the game; $0 otherwise.
The security price should roughly equal the expected payoff of the security in an efficient market. The time value of money usually can be ignored because durations of most information markets are short. Assuming exposure to risk is roughly equal for both outcomes, or that there are sufficient effectively risk-neutral speculators in the market, the price should not be biased by the risk attitudes of various players in the market. Thus, p = Pr(Patriots win) × 100 + [1 − Pr(Patriots win)] × 0, where p is the price of the security $100 if New England defeats Carolina and Pr(Patriots win) is the probability that New England will win the game. Observing the security price p before the game, we can derive Pr(Patriots win), which is the market participants" collective prediction about how likely it is that New England will win the game.
The above security is a winner-takes-all contract. It is used when the event to be predicted is a discrete random variable with disjoint outcomes (in this case binary). Its price predicts the probability that a specific outcome will be realized. When the outcome of a prediction problem can be any value in a continuous interval, we can design a security that pays its holder proportional to the realized value. This kind of security is what Wolfers and Zitzewitz [40] called an index contract. It predicts the expected value of a future outcome. Many other aspects of a future event such as median value of outcome can also be predicted in information markets by designing and trading different securities.
Wolfers and Zitzewitz [40] provide a summary of the main types of securities traded in information markets and what statistical properties they can predict. In practice, conceiving a security for a prediction problem is only one of the many decisions in designing an effective information market. Spann and Skiera [37] propose an initial framework for designing information markets.
Our data sets cover 210 NFL games held between September 28th, 2003 and December 28th, 2003. NFL games are very suitable for our purposes because: (1) two online exchanges and one online prediction contest already exist that provide data on both information markets and the opinions of self-identified experts for the same set of games; (2) the popularity of NFL games in the United States provides natural incentives for people to participate in information markets and/or the contest, which increases liquidity of information markets and improves the quality and number of opinions in the contest; (3) intense media coverage and analysis of the profiles and strengths of teams and individual players provide the public with much information so that participants of information markets and the contest can be viewed as knowledgeable regarding to the forecasting goal.
Information market data was acquired, by using a specially designed crawler program, from TradeSports.com"s Football-NFL markets [7] and NewsFutures.com"s Sports Exchange [1]. For each NFL game, both TradeSports and NewsFutures have a winner-takes-all information market to predict the game outcome. We introduce the design of the two markets according to Spann and Skiera"s three steps for designing an information market [37] as below. • Choice of forecasting goal: Markets at both TradeSports and NewsFutures aim at predicting which one of the two teams will win a NFL football game. They trade similar winner-takes-all securities that pay off
Small differences exist in how they deal with ties. In the case of a tie, TradeSports will unwind all trades that occurred and refund all exchange fees, but the security is worth 50 in NewsFutures. Since the probability of a tie is usually very low (much less the 1%), prices at both markets effectively represent the market participants" consensus assessment of the probability that the team will win. • Incentive for participation and information revelation: TradeSports and NewsFutures use different incentives for participation and information revelation.
TradeSports is a real-money exchange. A trader needs to open and fund an account with a minimum of $100 to participate in the market. Both profits and losses can occur as a result of trading activity. On the contrary, a trader can register at NewsFutures for free and get 2000 units of Sport Exchange virtual money at the time of registration. Traders at NewsFutures will not incur any real financial loss. They can accumulate virtual money by trading securities. The virtual money can then be used to bid for a few real prizes at NewsFutures" online shop. • Financial market design: Both markets at TradeSports and NewsFutures use the continuous double auction as their trading mechanism. TradeSports charges a small fee on each security transaction and expiry, while NewsFutures does not.
We can see that the main difference between two information markets is real money vs. virtual money. Servan-Schreiber 60 et. al [36] have compared the effect of money on the performance of the two information markets and concluded that the prediction accuracy of the two markets are at about the same level. Not intending to compare these two markets, we still use both markets in our analysis to ensure that our findings are not accidental.
We obtain the opinions of 1966 self-identified experts for NFL games from the ProbabilityFootball online contest [5], one of several ProbabilitySports contests [6]. The contest is free to enter. Participants of the contest are asked to enter their subjective probability that a team will win a game by noon on the day of the game. Importantly, the contest evaluates the participants" performance via the quadratic scoring rule: s = 100 − 400 × Prob Lose2 , (3) where s represents the score that a participant earns for the game, and Prob Lose is the probability that the participant assigns to the actual losing team. The quadratic score is one of a family of so-called proper scoring rules that have the property that an expert"s expected score is maximized when the expert reports probabilities truthfully. For example, for a game team A vs. team B, if a player assigns 0.5 to both team A and B, his/her score for the game is 0 no matter which team wins. If he/she assigns 0.8 to team A and 0.2 to team B, showing that he is confident in team A"s winning, he/she will score 84 points for the game if team A wins, and lose 156 points if team B wins. This quadratic scoring rule rewards bold predictions that are right, but penalizes bold predictions that turn out to be wrong. The top players, measured by accumulated scores over all games, win the prizes of the contest. The suggested strategy at the contest website is to make picks for each game that match, as closely as possible, the probabilities that each team will win. This strategy is correct if the participant seeks to maximize expected score. However, as prizes are awarded only to the top few winners, participants" goals are to maximize the probability of winning, not maximize expected score, resulting in a slightly different and more risk-seeking optimization.1 Still, as far as we are aware, this data offer the closest thing available to true subjective probability judgments from so many people over so many public events that have corresponding information markets.
In order to compare the prediction accuracy of information markets and that of opinion pools, we proceed to derive predictions from market data of TradeSports and NewsFutures, form pooled opinions using expert data from ProbabilityFootball contest, and specify the performance measures to be used.
For information markets, deriving predictions is straightforward. We can take the security price and divide it by
a team will win. To match the time when participants at the ProbabilityFootball contest are required to report their probability assessments, we derive predictions using the last trade price before noon on the day of the game. For more 1 Ideally, prizes would be awarded by lottery in proportion to accumulated score. than half of the games, this time is only about an hour earlier than the game starting time, while it is several hours earlier for other games. Two sets of market predictions are derived: • NF: Prediction equals NewsFutures" last trade price before noon of the game day divided by 100. • TS: Prediction equals TradeSports" last trade price before noon of the game day divided by 100.
We apply LinOP and LogOP to ProbabilityFootball data to obtain aggregate expert predictions. The reason that we do not consider other aggregation methods include: (1) data from ProbabilityFootball is only suitable for mathematical pooling methods-we can rule out behavioral approaches, (2) Bayesian aggregation requires us to make assumptions about the prior probability distribution of game outcomes and the likelihood function of expert opinions: given the large number of games and participants, making reasonable assumptions is difficult, and (3) for axiomatic approaches, previous research has shown that simpler aggregation methods often perform better than more complex methods [12].
Because the output of LogOP is indeterminate if there are probability assessments of both 0 and 1 (and because assessments of 0 and 1 are dictatorial using LogOP), we add a small number 0.01 to an expert opinion if it is 0, and subtract 0.01 from it if it is 1.
In pooling opinions, we consider two influencing factors: weights of experts and number of expert opinions to be pooled. For weights of experts, we experiment with equal weights and performance-based weights. The performancebased weights are determined according to previous accumulated score in the contest. The score for each game is calculated according to equation 3, the scoring rule used in the ProbabilityFootball contest. For the first week, since no previous scores are available, we choose equal weights. For later weeks, we calculate accumulated past scores for each player. Because the cumulative scores can be negative, we shift everyone"s score if needed to ensure the weights are non-negative. Thus, wi = cumulative scorei + shift n j=1(cumulative scorej + shift) . (4) where shift equals 0 if the smallest cumulative scorej is non-negative, and equals the absolute value of the smallest cumulative scorej otherwise. For simplicity, we call performance-weighted opinion pool as weighted, and equally weighted opinion pool as unweighted. We will use them interchangeably in the remaining of the paper.
As for the number of opinions used in an opinion pool, we form different opinion pools with different number of experts. Only the best performing experts are selected. For example, to form an opinion pool with 20 expert opinions, we choose the top 20 participants. Since there is no performance record for the first week, we use opinions of all participants in the first week. For week 2, we select opinions of
top 20. For week 3, 20 individuals whose cumulative scores of week 1 and 2 are among the top 20s are selected. Experts are chosen in a similar way for later weeks. Thus, the top
The possible opinion pools, varied in pooling functions, weighting methods, and number of expert opinions, are shown 61 Table 1: Pooled Expert Predictions # Symbol Description
of all experts.
LinOP of all experts.
with n experts.
LinOP with n experts.
of all experts.
LogOP of all experts.
with n experts.
LogOP with n experts. in Table 1. Lin represents linear, and Log represents Logarithmic. n is the number of expert opinions that are pooled, and All indicates that all opinions are combined.
We use u to symbolize unweighted (equally weighted) opinion pools. w is used for weighted (performance-weighted) opinion pools. Lin-All-u, the equally weighted LinOP with all participants, is basically the arithmetic mean of all participants" opinions. Log-All-u is simply the geometric mean of all opinions.
When a participant did not enter a prediction for a particular game, that participant was removed from the opinion pool for that game. This contrasts with the ProbabilityFootball average reported on the contest website and used by Servan-Schreiber et. al [36], where unreported predictions were converted to 0.5 probability predictions.
We use three common metrics to assess prediction accuracy of information markets and opinion pools. These measures have been used by Servan-Schreiber et. al [36] in evaluating the prediction accuracy of information markets.
where Prob Lose is the probability assigned to the eventual losing team. Absolute error simply measures the difference between a perfect prediction (1 for winning team) and the actual prediction. A prediction with lower absolute error is more accurate.
).
Quadratic score is the scoring function that is used in the ProbabilityFootball contest. It is a linear transformation of squared error, Prob Lose2 , which is one of the mostly used metrics in evaluating forecasting accuracy. Quadratic score can be negative. A prediction with higher quadratic score is more accurate.
where Prob W in is the probability assigned to the eventual winning team. The logarithmic score, like the quadratic score, is a proper scoring rule. A prediction with higher (less negative) logarithmic score is more accurate.
Depending on how many opinions are used, there can be numerous different opinion pools. We first examine the effect of number of opinions on prediction accuracy by forming opinion pools with the number of expert opinions varying from 1 to 960. In the ProbabilityFootball Competition, not all 1966 registered participants provide their probability assessments for every game. 960 is the smallest number of participants for all games. For each game, we sort experts according to their accumulated quadratic score in previous weeks. Predictions of the best performing n participants are picked to form an opinion pool with n experts.
Figure 1 shows the prediction accuracy of LinOP and LogOP in terms of mean values of the three performance measures across all 210 games. We can see the following trends in the figure.
opinion pools have similar levels of prediction accuracy, especially for LinOP.
general increases or keeps the same the level of prediction accuracy. When there are more than 200 experts, the prediction accuracy of LinOP is stable regarding the number of experts.
mean absolute error. But, using all other performance measures, LinOP outperforms LogOP.
the prediction accuracy at the beginning. But the curves (including the points with all experts) for mean quadratic score, and mean logarithmic score have slight bell-shapes, which represent a decrease in prediction accuracy when the number of experts is very large.
The curves for mean absolute error, on the other hand, show a consistent increase of accuracy.
The first and second trend above imply that when using LinOP, the simplest way, which has good prediction accuracy, is to average the opinions of all experts. Weighting does not seem to improve performance. Selecting experts according to past performance also does not help. It is a very interesting observation that even if many participants of the ProbabilityFootball contest do not provide accurate individual predictions (they have negative quadratic scores in the contest), including their opinions into the opinion pool still increases the prediction accuracy. One explanation of this phenomena could be that biases of individual judgment can offset with each other when opinions are diverse, which makes the pooled prediction more accurate.
The third trend presents a controversy. The relative prediction accuracy of LogOP and LinOP flips when using different accuracy measures. To investigate this disagreement, we plot the absolute error of Log-All-u and Lin-All-u for each game in Figure 2. When the absolute error of an opinion 62
Number of Expert Opinions MeanAbsoluteError Unweighted Linear Weighted Linear Unweighted Logarithmic Weighted Logarithmic Lin−All−u Lin−All−w Log−All−u Log−All−w (a) Mean Absolute Error
4 5 6 7 8 9 10 11 12 13 14 Number of Expert Opinions MeanQuadraticScore Unweighted Linear Weighted Linear Unweighted Logarithmic Weighted Logarithmic Lin−All−u Lin−All−w Log−All−u Log−All−w
−0.71 −0.7 −0.69 −0.68 −0.67 −0.66 −0.65 −0.64 −0.63 −0.62 Number of Expert Opinions MeanLogarithmicScore Unweighted Linear Weighted Linear Unweighted Logarithmic Weighted Logarithmic Lin−All−u Lin−All−w Log−All−u Log−All−w (b) Mean Quadratic Score (c) Mean Logarithmic Score Figure 1: Prediction Accuracy of Opinion Pools pool for a game is less than 0.5, it means that the team favored by the opinion pool wins the game. If it is greater than
has lower absolute error when it is less than 0.5, and greater absoluter error when it is greater than 0.5, which indicates that predictions of Log-All-u are bolder, more close to 0 or 1, than those of Lin-All-u. This is due to the nature of linear and logarithmic aggregating functions. Because quadratic score and logarithmic score penalize bold predictions that are wrong, LogOP is less accurate when measured in these terms.
Similar reasoning accounts for the fourth trend. When there are more than 500 experts, increasing number of experts used in LogOP improves the prediction accuracy measured by absolute error, but worsens the accuracy measured by the other two metrics. Examining expert opinions, we find that participants who rank lower are more frequent in offering extreme predictions (0 or 1) than those ranking high in the list. When we increase the number of experts in an opinion pool, we are incorporating more extreme predictions into it. The resulting LogOP is bolder, and hence has lower mean quadratic score and mean logarithmic score.
Opinion Pools Through the first screening of various opinion pools, we select Lin-All-u, Log-All-u, Log-All-w, and Log-200-u to compare with predictions from information markets. Lin-All-u as shown in Figure 1 can represent what LinOP can achieve.
However, the performance of LogOP is not consistent when evaluated using different metrics. Log-All-u and Log-All-w offer either the best or the worst predictions. Log-200-u, the LogOP with the 200 top performing experts, provides more stable predictions. We use all of the three to stand for the performance of LogOP in our later comparison.
If a prediction of the probability that a team will win a game, either from an opinion pool or an information market, is higher than 0.5, we say that the team is the predicted favorite for the game. Table 2 presents the number and percentage of games that predicted favorites actually win, out of a total of 210 games. All four opinion pools correctly predict a similar number and percentage of games as NF and TS. Since NF, TS, and the four opinion pools form their predictions using information available at noon of the game 63 Table 2: Number and Percentage of Games that Predicted Favorites Win NF TS Lin-All-u Log-All-u Log-All-w Log-200-u Number 142 137 144 144 143 141 Percentage 67.62% 65.24% 68.57% 68.57% 68.10% 67.14% Table 3: Mean of Prediction Accuracy Measures Absolute Error Quadratic Score Logarithmic Score NF
(0.0121) (4.6072) (0.0258) TS
(0.0118) (4.3982) (0.0241) Lin-All-u
(0.0126) (4.8088) (0.0268) Log-All-u
(0.0173) (6.6594) (0.0418) Log-All-w
(0.0168) (6.4440) (0.0398) Log-200-u
(0.0133) (5.0764) (0.0295) *Numbers in parentheses are standard errors. *Best value for each metric is shown in bold.
0
1 Absolute Error of Lin−All−u AbsoluteErrorofLog−All−u
Absolute Error Figure 2: Absolute Error: Lin-All-u vs. Log-All-u day, information markets and opinion pools have comparable potential at the same time point.
We then take a closer look at prediction accuracy of information markets and opinion pools using the three performance measures. Table 3 displays mean values of these measures over 210 games. Numbers in parentheses are standard errors, which estimate the standard deviation of the mean. To take into consideration of skewness of distributions, we also report median values of accuracy measures in Table 4. Judged by the mean values of accuracy measures in Table 3, all methods have similar accuracy levels, with NF and TS slightly better than the opinion pools. However, the median values of accuracy measures indicate that LogAll-u and Log-All-w opinion pools are more accurate than all other predictions.
We employ the randomization test [32] to study whether the differences in prediction accuracy presented in Table 3 and Table 4 are statistically significant. The basic idea of randomization test is that, by randomly swapping predictions of two methods numerous times, an empirical distribution for the difference of prediction accuracy can be constructed. Using this empirical distribution, we are then able to evaluate that at what confidence level the observed difference reflects a real difference. For example, the mean absolute error of NF is higher than that of Log-All-u by
difference is statistically significant, we shuﬄe predictions from two methods, randomly label half of predictions as NF and the other half as Log-All-u, and compute the difference of mean absolute error of the newly formed NF and Log-All-u data. The above procedure is repeated 10,000 times. The 10,000 differences of mean absolute error results in an empirical distribution of the difference. Comparing our observed difference, 0.0229, with this distribution, we find that the observed difference is greater than 75.37% of the empirical differences. This leads us to conclude that the difference of mean absolute error between NF and Log-All-u is not statistically significant, if we choose the level of significance to be 0.05.
Table 5 and Table 6 are results of randomization test for mean and median differences respectively. Each cell of the table is for two different prediction methods, represented by name of the row and name of the column. The first lines of table cells are results for absolute error. The second and third lines are dedicated to quadratic score and logarithmic score respectively. We can see that, in terms of mean values of accuracy measures, the differences of all methods are not statistically significant to any reasonable degree. When it 64 Table 4: Median of Prediction Accuracy Measures Absolute Error Quadratic Score Logarithmic Score NF 0.3800 42.2400 -0.4780 TS 0.4000 36.0000 -0.5108 Lin-All-u 0.3639 36.9755 -0.5057 Log-All-u 0.3417 53.2894 -0.4181 Log-All-w 0.3498 51.0486 -0.4305 Log-200-u 0.3996 36.1300 -0.5101 *Best value for each metric is shown in bold.
Table 5: Statistical Confidence of Mean Differences in Prediction Accuracy TS Lin-All-u Log-All-u Log-All-w Log-200-u NF
TS
Lin-All-u
Log-All-u
Log-All-w
*In each table cell, row 1 accounts for absolute error, row 2 for quadratic score, and row 3 for logarithmic score. comes to median values of prediction accuracy, Log-All-u outperforms Lin-All-u at a high confidence level.
These results indicate that differences in prediction accuracy between information markets and opinion pools are not statistically significant. This may seem to contradict the result of Servan-Schreiber et. al [36], in which NewsFutures"s information markets have been shown to provide statistically significantly more accurate predictions than the (unweighted) average of all ProbabilityFootball opinions. The discrepancy emerges in dealing with missing data. Not all
probability assessments for each game. When a participant does not provide a probability assessment for a game, the contest considers their prediction as 0.5.. This makes sense in the context of the contest, since 0.5 always yields 0 quadratic score. The ProbabilityFootball average reported on the contest website and used by Servan-Schreiber et. al includes these 0.5 estimates. Instead, we remove participants from games that they do not provide assessments, pooling only the available opinions together. Our treatment increases the prediction accuracy of Lin-All-u significantly.
With the fast growth of the Internet, information markets have recently emerged as an alternative tool for predicting future events. Previous research has shown that information markets give as accurate or more accurate predictions than individual experts and polls. However, information markets, as an adaptive mechanism to aggregate different opinions of market participants, have not been calibrated against many belief aggregation methods. In this paper, we compare prediction accuracy of information markets with linear and logarithmic opinion pools (LinOP and LogOP) using predictions from two markets and 1966 individuals regarding the outcomes of 210 American football games during the 2003 NFL season. In screening for representative opinion pools to compare with information markets, we investigate the effect of weights and number of experts on prediction accuracy.
Our results on both the comparison of information markets and opinion pools and the relative performance of different opinion pools are summarized as below.
information markets offer as accurate predictions as our selected opinion pools.
We have selected four opinion pools to represent the prediction accuracy level that LinOP and LogOP can achieve. With all four performance metrics, our two information markets obtain similar prediction accuracy as the four opinion pools. 65 Table 6: Statistical Confidence of Median Differences in Prediction Accuracy TS Lin-All-u Log-All-u Log-All-w Log-200-u NF
TS
Lin-All-u
Log-All-u
Log-All-w
*In each table cell, row 1 accounts for absolute error, row 2 for quadratic score, and row 3 for logarithmic score. *Confidence above 95% is shown in bold.
simple, robust, and efficient opinion pool.
Simply averaging across all experts seems resulting in better predictions than individual opinions and opinion pools with a few experts. It is quite robust in the sense that even if the included individual predictions are less accurate, averaging over all opinions still gives better (or equally good) predictions.
performance does not seem to significantly improve prediction accuracy of either LinOP or LogOP.
Comparing performance-weighted opinion pools with equally weighted opinion pools, we do not observe much difference in terms of prediction accuracy. Since we only use one performance-weighting method, calculating the weights according to past accumulated quadratic score that participants earned, this might due to the weighting method we chose.
LogOP yields predictions that are closer to the extremes, 0 or 1.
An information markets is a self-organizing mechanism for aggregating information and making predictions.
Compared with opinion pools, it is less constrained by space and time, and can eliminate the efforts to identify experts and decide belief aggregation methods. But the advantages do not compromise their prediction accuracy to any extent.
On the contrary, information markets can provide real-time predictions, which are hardly achievable through resorting to experts. In the future, we are interested in further exploring: • Performance comparison of information markets with other opinion pools and mathematical aggregation procedures.
In this paper, we only compare information markets with two simple opinion pools, linear and logarithmic.
It will be meaningful to investigate their relative prediction accuracy with other belief aggregation methods such as Bayesian approaches. There are also a number of theoretical expert algorithms with proven worst-case performance bounds [10] whose average-case or practical performance would be instructive to investigate. • Whether defining expertise more narrowly can improve predictions of opinion pools.
In our analysis, we broadly treat participants of the ProbabilityFootball contest as experts in all games. If we define expertise more narrowly, selecting experts in certain football teams to predict games involving these teams, will the predictions of opinion pools be more accurate? • The possibility of combining information markets with other forecasting methods to achieve better prediction accuracy.
Chen, Fine, and Huberman [11] use an information market to determine the risk attitude of participants, and then perform a nonlinear aggregation of their predictions based on their risk attitudes. The nonlinear aggregation mechanism is shown to outperform both the market and the best individual participants. It is worthy of more attention whether information markets, as an alternative forecasting method, can be used together with other methods to improve our predictions.
We thank Brian Galebach, the owner and operator of the ProbabilitySports and ProbabilityFootball websites, for providing us with such unique and valuable data. We thank Varsha Dani, Lance Fortnow, Omid Madani, Sumit Sang66 hai, and the anonymous reviewers for useful insights and pointers.
The authors acknowledge the support of The Penn State eBusiness Research Center.
[1] http://us.newsfutures.com [2] http://www.biz.uiowa.edu/iem/ [3] http://www.hsx.com/ [4] http://www.ideosphere.com/fx/ [5] http://www.probabilityfootball.com/ [6] http://www.probabilitysports.com/ [7] http://www.tradesports.com/ [8] A. H. Ashton and R. H. Ashton. Aggregating subjective forecasts: Some empirical results.
Management Science, 31:1499-1508, 1985. [9] R. P. Batchelor and P. Dua. Forecaster diversity and the benefits of combining forecasts. Management Science, 41:68-75, 1995. [10] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P.
Helmbold, R. E. Schapire, and M. K. Warmuth. How to use expert advice. Journal of the ACM, 44(3):427-485, 1997. [11] K. Chen, L. Fine, and B. Huberman. Predicting the future. Information System Frontier, 5(1):47-61, 2003. [12] R. T. Clemen and R. L. Winkler. Combining probability distributions from experts in risk analysis.
Risk Analysis, 19(2):187-203, 1999. [13] R. M. Cook. Experts in Uncertainty: Opinion and Subjective Probability in Science. Oxford University Press, New York, 1991. [14] A. L. Delbecq, A. H. Van de Ven, and D. H.
Gustafson. Group Techniques for Program Planners: A Guide to Nominal Group and Delphi Processes.
Scott Foresman and Company, Glenview, IL, 1975. [15] E. F. Fama. Efficient capital market: A review of theory and empirical work. Journal of Finance, 25:383-417, 1970. [16] R. Forsythe and F. Lundholm. Information aggregation in an experimental market. Econometrica, 58:309-47, 1990. [17] R. Forsythe, F. Nelson, G. R. Neumann, and J. Wright. Forecasting elections: A market alternative to polls. In T. R. Palfrey, editor, Contemporary Laboratory Experiments in Political Economy, pages 69-111. University of Michigan Press, Ann Arbor, MI,
[18] R. Forsythe, F. Nelson, G. R. Neumann, and J. Wright. Anatomy of an experimental political stock market. American Economic Review, 82(5):1142-1161,
[19] R. Forsythe, T. A. Rietz, and T. W. Ross. Wishes, expectations, and actions: A survey on price formation in election stock markets. Journal of Economic Behavior and Organization, 39:83-110, 1999. [20] S. French. Group consensus probability distributions: a critical survey. Bayesian Statistics, 2:183-202, 1985. [21] C. Genest. A conflict between two axioms for combining subjective distributions. Journal of the Royal Statistical Society, 46(3):403-405, 1984. [22] C. Genest. Pooling operators with the marginalization property. Canadian Journal of Statistics, 12(2):153-163, 1984. [23] C. Genest, K. J. McConway, and M. J. Schervish.
Characterization of externally Bayesian pooling operators. Annals of Statistics, 14(2):487-501, 1986. [24] C. Genest and J. V. Zidek. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, 1(1):114-148, 1986. [25] S. J. Grossman. An introduction to the theory of rational expectations under asymmetric information.
Review of Economic Studies, 48(4):541-559, 1981. [26] F. A. Hayek. The use of knowledge in society.
American Economic Review, 35(4):519-530, 1945. [27] J. C. Jackwerth and M. Rubinstein. Recovering probability distribution from options prices. Journal of Finance, 51(5):1611-1631, 1996. [28] H. A. Linstone and M. Turoff. The Delphi Method: Techniques and Applications. Addison-Wesley,
Reading, MA, 1975. [29] P. A. Morris. Decision analysis expert use.
Management Science, 20(9):1233-1241, 1974. [30] P. A. Morris. Combining expert judgments: A bayesian approach. Management Science, 23(7):679-693, 1977. [31] P. A. Morris. An axiomatic approach to expert resolution. Management Science, 29(1):24-32, 1983. [32] E. W. Noreen. Computer-Intensive Methods for Testing Hypotheses: An Introduction. Wiley and Sons,
Inc., New York, 1989. [33] D. M. Pennock, S. Lawrence, C. L. Giles, and F. A.
Nielsen. The real power of artificial markets. Science, 291:987-988, February 2002. [34] D. M. Pennock, S. Lawrence, F. A. Nielsen, and C. L.
Giles. Extracting collective probabilistic forecasts from web games. In Proceedings of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 174-183, San Francisco, CA, 2001. [35] C. Plott and S. Sunder. Rational expectations and the aggregation of diverse information in laboratory security markets. Econometrica, 56:1085-118, 1988. [36] E. Servan-Schreiber, J. Wolfers, D. M. Pennock, and B. Galebach. Prediction markets: Does money matter? Electronic Markets, 14(3):243-251, 2004. [37] M. Spann and B. Skiera. Internet-based virtual stock markets for business forecasting. Management Science, 49(10):1310-1326, 2003. [38] M. West. Bayesian aggregation. Journal of the Royal Statistical Society. Series A. General, 147(4):600-607,
[39] R. L. Winkler. The consensus of subjective probability distributions. Management Science, 15(2):B61-B75,
[40] J. Wolfers and E. Zitzewitz. Prediction markets.
Journal of Economic Perspectives, 18(2):107-126,

One key factor in the practicality of any preference aggregation rule is its communication burden. To successfully aggregate the agents" preferences, it is usually not necessary for all the agents to report all of their preference information.
Clever protocols that elicit the agents" preferences partially and sequentially have the potential to dramatically reduce the required communication. This has at least the following advantages: • It can make preference aggregation feasible in settings where the total amount of preference information is too large to communicate. • Even when communicating all the preference information is feasible, reducing the communication requirements lessens the burden placed on the agents. This is especially true when the agents, rather than knowing all their preferences in advance, need to invest effort (such as computation or information gathering) to determine their preferences [16]. • It preserves (some of) the agents" privacy.
Most of the work on reducing the communication burden in preference aggregation has focused on resource allocation settings such as combinatorial auctions, in which an auctioneer auctions off a number of (possibly distinct) items in a single event. Because in a combinatorial auction, bidders can have separate valuations for each of an exponential number of possible bundles of items, this is a setting in which reducing the communication burden is especially crucial. This can be accomplished by supplementing the auctioneer with an elicitor that incrementally elicits parts of the bidders" preferences on an as-needed basis, based on what the bidders have revealed about their preferences so far, as suggested by Conen and Sandholm [5]. For example, the elicitor can ask for a bidder"s value for a specific bundle (value queries), which of two bundles the bidder prefers (order queries), which bundle he ranks kth or what the rank of a given bundle is (rank queries), which bundle he would purchase given a particular vector of prices (demand queries), etc.-until (at least) the final allocation can be determined.
Experimentally, this yields drastic savings in preference revelation [11]. Furthermore, if the agents" valuation functions are drawn from certain natural subclasses, the elicitation problem can be solved using only polynomially many queries even in the worst case [23, 4, 13, 18, 14]. For a review of preference elicitation in combinatorial auctions, see [17].
Ascending combinatorial auctions are a well-known special form of preference elicitation, where the elicitor asks demand queries with increasing prices [15, 21, 1, 9]. Finally, resource 78 allocation problems have also been studied from a communication complexity viewpoint, thereby deriving lower bounds on the required communication. For example, Nisan and Segal show that exponential communication is required even to obtain a surplus greater than that obtained by auctioning off all objects as a single bundle [14]. Segal also studies social choice rules in general, and shows that for a large class of social choice rules, supporting budget sets must be revealed such that if every agent prefers the same outcome in her budget set, this proves the optimality of that outcome. Segal then uses this characterization to prove bounds on the communication required in resource allocation as well as matching settings [20].
In this paper, we will focus on the communication requirements of a generally applicable subclass of social choice rules, commonly known as voting rules. In a voting setting, there is a set of candidate outcomes over which the voters express their preferences by submitting a vote (typically, a ranking of the candidates), and the winner (that is, the chosen outcome) is determined based on these votes. The communication required by voting rules can be large either because the number of voters is large (such as, for example, in national elections), or because the number of candidates is large (for example, the agents can vote over allocations of a number of resources), or both. Prior work [8] has studied elicitation in voting, studying how computationally hard it is to decide whether a winner can be determined with the information elicited so far, as well as how hard it is to find the optimal sequence of queries given perfect suspicions about the voters" preferences. In addition, that paper discusses strategic (game-theoretic) issues introduced by elicitation.
In contrast, in this paper, we are concerned with the worst-case number of bits that must be communicated to execute a given voting rule, when nothing is known in advance about the voters" preferences. We determine the communication complexity of the common voting rules. For each rule, we first give an upper bound on the (deterministic) communication complexity by providing a communication protocol for it and analyzing how many bits need to be transmitted in this protocol. (Segal"s results [20] do not apply to most voting rules because most voting rules are not intersectionmonotonic (or even monotonic).1 ) For many of the voting rules under study, it turns out that one cannot do better than simply letting each voter immediately communicate all her (potentially relevant) information. However, for some rules (such as plurality with runoff, STV and cup) there is a straightforward multistage communication protocol that, with some analysis, can be shown to significantly outperform the immediate communication of all (potentially relevant) information. Finally, for some rules (such as the Condorcet and Bucklin rules), we need to introduce a more complex communication protocol to achieve the best possible upper 1 For two of the rules that we study that are intersectionmonotonic, namely the approval and Condorcet rules,
Segal"s results can in fact be used to give alternative proofs of our lower bounds. We only give direct proofs for these rules here because 1) these direct proofs are among the easier ones in this paper, 2) the alternative proofs are nontrivial even given Segal"s results, and 3) a space constraint applies.
However, we hope to also include the alternative proofs in a later version. bound. After obtaining the upper bounds, we show that they are tight by giving matching lower bounds on (even the nondeterministic) communication complexity of each voting rule. There are two exceptions: STV, for which our upper and lower bounds are apart by a factor log m; and maximin, for which our best deterministic upper bound is also a factor log m above the (nondeterministic) lower bound, although we give a nondeterministic upper bound that matches the lower bound.
In this section, we review the common voting rules that we study in this paper. A voting rule2 is a function mapping a vector of the n voters" votes (i.e. preferences over candidates) to one of the m candidates (the winner) in the candidate set C. In some cases (such as the Condorcet rule), the rule may also declare that no winner exists. We do not concern ourselves with what happens in case of a tie between candidates (our lower bounds hold regardless of how ties are broken, and the communication protocols used for our upper bounds do not attempt to break the ties). All of the rules that we study are rank-based rules, which means that a vote is defined as an ordering of the candidates (with the exception of the plurality rule, for which a vote is a single candidate, and the approval rule, for which a vote is a subset of the candidates).
We will consider the following voting rules. (For rules that define a score, the candidate with the highest score wins.) • scoring rules. Let α = α1, . . . , αm be a vector of integers such that α1 ≥ α2 . . . ≥ αm. For each voter, a candidate receives α1 points if it is ranked first by the voter, α2 if it is ranked second etc. The score sα of a candidate is the total number of points the candidate receives. The Borda rule is the scoring rule with α = m−1, m−2, . . . , 0 .
The plurality rule is the scoring rule with α = 1, 0, . . . , 0 . • single transferable vote (STV). The rule proceeds through a series of m − 1 rounds. In each round, the candidate with the lowest plurality score (that is, the least number of voters ranking it first among the remaining candidates) is eliminated (and each of the votes for that candidate transfer to the next remaining candidate in the order given in that vote). The winner is the last remaining candidate. • plurality with run-off. In this rule, a first round eliminates all candidates except the two with the highest plurality scores. Votes are transferred to these as in the STV rule, and a second round determines the winner from these two. • approval. Each voter labels each candidate as either approved or disapproved. The candidate approved by the greatest number of voters wins. • Condorcet. For any two candidates i and j, let N(i, j) be the number of voters who prefer i to j. If there is a candidate i that is preferred to any other candidate by a majority of the voters (that is, N(i, j) > N(j, i) for all j = i-that is, i wins every pairwise election), then candidate i wins. 2 The term voting protocol is often used to describe the same concept, but we seek to draw a sharp distinction between the rule mapping preferences to outcomes, and the communication/elicitation protocol used to implement this rule. 79 • maximin (aka. Simpson). The maximin score of i is s(i) = minj=i N(i, j)-that is, i"s worst performance in a pairwise election. The candidate with the highest maximin score wins. • Copeland. For any two distinct candidates i and j, let C(i, j) = 1 if N(i, j) > N(j, i), C(i, j) = 1/2 if N(i, j) = N(j, i) and C(i, j) = 0 if N(i, j) < N(j, i). The Copeland score of candidate i is s(i) = j=i C(i, j). • cup (sequential binary comparisons). The cup rule is defined by a balanced3 binary tree T with one leaf per candidate, and an assignment of candidates to leaves (each leaf gets one candidate). Each non-leaf node is assigned the winner of the pairwise election of the node"s children; the candidate assigned to the root wins. • Bucklin. For any candidate i and integer l, let B(i, l) be the number of voters that rank candidate i among the top l candidates. The winner is arg mini(min{l : B(i, l) > n/2}). That is, if we say that a voter approves her top l candidates, then we repeatedly increase l by 1 until some candidate is approved by more than half the voters, and this candidate is the winner. • ranked pairs. This rule determines an order on all the candidates, and the winner is the candidate at the top of this order. Sort all ordered pairs of candidates (i, j) by N(i, j), the number of voters who prefer i to j. Starting with the pair (i, j) with the highest N(i, j), we lock in the result of their pairwise election (i j). Then, we move to the next pair, and we lock the result of their pairwise election. We continue to lock every pairwise result that does not contradict the ordering established so far.
We emphasize that these definitions of voting rules do not concern themselves with how the votes are elicited from the voters; all the voting rules, including those that are suggestively defined in terms of rounds, are in actuality just functions mapping the vector of all the voters" votes to a winner. Nevertheless, there are always many different ways of eliciting the votes (or the relevant parts thereof) from the voters. For example, in the plurality with runoff rule, one way of eliciting the votes is to ask every voter to declare her entire ordering of the candidates up front. Alternatively, we can first ask every voter to declare only her most preferred candidate; then, we will know the two candidates in the runoff, and we can ask every voter which of these two candidates she prefers. Thus, we distinguish between the voting rule (the mapping from vectors of votes to outcomes) and the communication protocol (which determines how the relevant parts of the votes are actually elicited from the voters). The goal of this paper is to give efficient communication protocols for the voting rules just defined, and to prove that there do not exist any more efficient communication protocols.
It is interesting to note that the choice of the communication protocol may affect the strategic behavior of the voters.
Multistage communication protocols may reveal to the voters some information about how the other voters are voting (for example, in the two-stage communication protocol just given for plurality with runoff, in the second stage voters 3 Balanced means that the difference in depth between two leaves can be at most one. will know which two candidates have the highest plurality scores). In general, when the voters receive such information, it may give them incentives to vote differently than they would have in a single-stage communication protocol in which all voters declare their entire votes simultaneously.
Of course, even the single-stage communication protocol is not strategy-proof4 for any reasonable voting rule, by the Gibbard-Satterthwaite theorem [10, 19]. However, this does not mean that we should not be concerned about adding even more opportunities for strategic voting. In fact, many of the communication protocols introduced in this paper do introduce additional opportunities for strategic voting, but we do not have the space to discuss this here. (In prior work [8], we do give an example where an elicitation protocol for the approval voting rule introduces strategic voting, and give principles for designing elicitation protocols that do not introduce strategic problems.) Now that we have reviewed voting rules, we move on to a brief review of communication complexity theory.
COMPLEXITY THEORY In this section, we review the basic model of a communication problem and the lower-bounding technique of constructing a fooling set. (The basic model of a communication problem is due to Yao [22]; for an overview see Kushilevitz and Nisan [12].) Each player 1 ≤ i ≤ n knows (only) input xi. Together, they seek to compute f(x1, x2, . . . , xn). In a deterministic protocol for computing f, in each stage, one of the players announces (to all other players) a bit of information based on her own input and the bits announced so far.
Eventually, the communication terminates and all players know f(x1, x2, . . . , xn). The goal is to minimize the worst-case (over all input vectors) number of bits sent. The deterministic communication complexity of a problem is the worstcase number of bits sent in the best (correct) deterministic protocol for it. In a nondeterministic protocol, the next bit to be sent can be chosen nondeterministically. For the purposes of this paper, we will consider a nondeterministic protocol correct if for every input vector, there is some sequence of nondeterministic choices the players can make so that the players know the value of f when the protocol terminates. The nondeterministic communication complexity of a problem is the worst-case number of bits sent in the best (correct) nondeterministic protocol for it.
We are now ready to give the definition of a fooling set.
Definition 1. A fooling set is a set of input vectors {(x1 1, x1 2, . . . , x1 n), (x2 1, x2 2, . . . , x2 n), . . . , (xk
n) such that for any i, f(xi 1, xi 2, . . . , xi n) = f0 for some constant f0, but for any i = j, there exists some vector (r1, r2, . . . , rn) ∈ {i, j}n such that f(xr1
n ) = f0. (That is, we can mix the inputs from the two input vectors to obtain a vector with a different function value.) It is known that if a fooling set of size k exists, then log k is a lower bound on the communication complexity (even the nondeterministic communication complexity) [12]. 4 A strategy-proof protocol is one in which it is in the players" best interest to report their preferences truthfully. 80 For the purposes of this paper, f is the voting rule that maps the votes to the winning candidate, and xi is voter i"s vote (the information that the voting rule would require from the voter if there were no possibility of multistage communication-i.e. the most preferred candidate (plurality), the approved candidates (approval), or the ranking of all the candidates (all other protocols)). However, when we derive our lower bounds, f will only signify whether a distinguished candidate a wins. (That is, f is 1 if a wins, and
(because it implies that even finding out whether one given candidate wins is hard).5 Thus, a fooling set in our context is a set of vectors of votes so that a wins (does not win) with each of them; but for any two different vote vectors in the set, there is a way of taking some voters" votes from the first vector and the others" votes from the second vector, so that a does not win (wins).
To simplify the proofs of our lower bounds, we make assumptions such as the number of voters n is odd in many of these proofs. Therefore, technically, we do not prove the lower bound for (number of candidates, number of voters) pairs (m, n) that do not satisfy these assumptions (for example, if we make the above assumption, then we technically do not prove the lower bound for any pair (m, n) in which n is even). Nevertheless, we always prove the lower bound for a representative set of (m, n) pairs. For example, for every one of our lower bounds it is the case that for infinitely many values of m, there are infinitely many values of n such that the lower bound is proved for the pair (m, n).
We are now ready to present our results. For each voting rule, we first give a deterministic communication protocol for determining the winner to establish an upper bound.
Then, we give a lower bound on the nondeterministic communication complexity (even on the complexity of deciding whether a given candidate wins, which is an easier question). The lower bounds match the upper bounds in all but two cases: the STV rule (upper bound O(n(log m)2 ); lower bound Ω(n log m)) and the maximin rule (upper bound O(nm log m), although we do give a nondeterministic protocol that is O(nm); lower bound Ω(nm)).
When we discuss a voting rule in which the voters rank the candidates, we will represent a ranking in which candidate c1 is ranked first, c2 is ranked second, etc. as c1 c2 . . . cm. 5 One possible concern is that in the case where ties are possible, it may require much communication to verify whether a specific candidate a is among the winners, but little communication to produce one of the winners. However, all the fooling sets we use in the proofs have the property that if a wins, then a is the unique winner. Therefore, in these fooling sets, if one knows any one of the winners, then one knows whether a is a winner. Thus, computing one of the winners requires at least as much communication as verifying whether a is among the winners. In general, when a communication problem allows multiple correct answers for a given vector of inputs, this is known as computing a relation rather than a function [12]. However, as per the above, we can restrict our attention to a subset of the domain where the voting rule truly is a (single-valued) function, and hence lower bounding techniques for functions rather than relations will suffice.
Sometimes for the purposes of a proof the internal ranking of a subset of the candidates does not matter, and in this case we will not specify it. For example, if S = {c2, c3}, then c1 S c4 indicates that either the ranking c1 c2 c3 c4 or the ranking c1 c3 c2 c4 can be used for the proof.
We first give a universal upper bound.
Theorem 1. The deterministic communication complexity of any rank-based voting rule is O(nm log m).
Proof. This bound is achieved by simply having everyone communicate their entire ordering of the candidates (indicating the rank of an individual candidate requires only O(log m) bits, so each of the n voters can simply indicate the rank of each of the m candidates).
The next lemma will be useful in a few of our proofs.
Lemma 1. If m divides n, then log(n!)−m log((n/m)!) ≥ n(log m − 1)/2.
Proof. If n/m = 1 (that is, n = m), then this expression simplifies to log(n!). We have log(n!) = n i=1 log i ≥ n x=1 log(i)dx, which, using integration by parts, is equal to n log n − (n − 1) > n(log n − 1) = n(log m − 1) > n(log m − 1)/2. So, we can assume that n/m ≥ 2. We observe that log(n!) = n i=1 log i = n/m−1 i=0 m j=1 log(im+j) ≥ n/m−1 i=1 m j=1 log(im) = m n/m−1 i=1 log(im), and that m log((n/m)!) = m n/m i=1 log(i).
Therefore, log(n!) − m log((n/m)!) ≥ m n/m−1 i=1 log(im) − m n/m i=1 log(i) = m(( n/m−1 i=1 log(im/i))−log(n/m)) = m((n/m− 1) log m−log n+log m) = n log m−m log n. Now, using the fact that n/m ≥ 2, we have m log n = n(m/n) log m(n/m) = n(m/n)(log m + log(n/m)) ≤ n(1/2)(log m + log 2). Thus, log(n!) − m log((n/m)!) ≥ n log m − m log n ≥ n log m − n(1/2)(log m + log 2) = n(log m − 1)/2.
Theorem 2. The deterministic communication complexity of the plurality rule is O(n log m).
Proof. Indicating one of the candidates requires only O(log m) bits, so each voter can simply indicate her most preferred candidate.
Theorem 3. The nondeterministic communication complexity of the plurality rule is Ω(n log m) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size n ! (( n m )!)m where n = (n−1)/2. Taking the logarithm of this gives log(n !)− m log((n /m)!), so the result follows from Lemma 1. The fooling set will consist of all vectors of votes satisfying the following constraints: • For any 1 ≤ i ≤ n , voters 2i−1 and 2i vote the same. 81 • Every candidate receives equally many votes from the first 2n = n − 1 voters. • The last voter (voter n) votes for a.
Candidate a wins with each one of these vote vectors because of the extra vote for a from the last voter. Given that m divides n , let us see how many vote vectors there are in the fooling set. We need to distribute n voter pairs evenly over m candidates, for a total of n /m voter pairs per candidate; and there are precisely n ! (( n m )!)m ways of doing this.6 All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let i be a number such that the two vote vectors disagree on the candidate for which voters 2i − 1 and 2i vote.
Without loss of generality, suppose that in the first vote vector, these voters do not vote for a (but for some other candidate, b, instead). Now, construct a new vote vector by taking votes 2i − 1 and 2i from the first vote vector, and the remaining votes from the second vote vector. Then, b receives 2n /m + 2 votes in this newly constructed vote vector, whereas a receives at most 2n /m+1 votes. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 4. The deterministic communication complexity of the plurality with runoff rule is O(n log m).
Proof. First, let every voter indicate her most preferred candidate using log m bits. After this, the two candidates in the runoff are known, and each voter can indicate which one she prefers using a single additional bit.
Theorem 5. The nondeterministic communication complexity of the plurality with runoff rule is Ω(n log m) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size n ! (( n m )!)m where m = m/2 and n = (n − 2)/4. Taking the logarithm of this gives log(n !) − m log((n /m )!), so the result follows from Lemma 1. Divide the candidates into m pairs: (c1, d1), (c2, d2), . . . , (cm , dm ) where c1 = a and d1 = b.
The fooling set will consist of all vectors of votes satisfying the following constraints: • For any 1 ≤ i ≤ n , voters 4i − 3 and 4i − 2 rank the candidates ck(i) a C − {a, ck(i)}, for some candidate ck(i). (If ck(i) = a then the vote is simply a C − {a}.) • For any 1 ≤ i ≤ n , voters 4i − 1 and 4i rank the candidates dk(i) a C − {a, dk(i)} (that is, their most preferred candidate is the candidate that is paired with the candidate that the previous two voters vote for). 6 An intuitive proof of this is the following. We can count the number of permutations of n elements as follows. First, divide the elements into m buckets of size n /m, so that if x is placed in a lower-indexed bucket than y, then x will be indexed lower in the eventual permutation. Then, decide on the permutation within each bucket (for which there are (n /m)! choices per bucket). It follows that n ! equals the number of ways to divide n elements into m buckets of size n /m, times ((n /m)!)m . • Every candidate is ranked at the top of equally many of the first 4n = n − 2 votes. • Voter 4n +1 = n−1 ranks the candidates a C−{a}. • Voter 4n + 2 = n ranks the candidates b C − {b}.
Candidate a wins with each one of these vote vectors: because of the last two votes, candidates a and b are one vote ahead of all the other candidates and continue to the runoff, and at this point all the votes that had another candidate ranked at the top transfer to a, so that a wins the runoff.
Given that m divides n , let us see how many vote vectors there are in the fooling set. We need to distribute n groups of four voters evenly over the m pairs of candidates, and (as in the proof of Theorem 3) there are n ! (( n m )!)m ways of doing this. All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let i be a number such that ck(i) is not the same in both of these two vote vectors, that is, c1 k(i) (ck(i) in the first vote vector) is not equal to c2 k(i) (ck(i) in the second vote vector). Without loss of generality, suppose c1 k(i) = a. Now, construct a new vote vector by taking votes 4i − 3, 4i − 2, 4i − 1, 4i from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, c1 k(i) and d1 k(i) each receive 4n /m+2 votes in the first round, whereas a receives at most 4n /m+1 votes. So, a does not continue to the runoff in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 6. The nondeterministic communication complexity of the Borda rule is Ω(nm log m) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size (m !)n where m = m−2 and n = (n−2)/4. This will prove the theorem because m ! is Ω(m log m), so that log((m !)n ) = n log(m !) is Ω(nm log m). For every vector (π1, π2, . . . , πn ) consisting of n orderings of all candidates other than a and another fixed candidate b (technically, the orderings take the form of a one-to-one function πi : {1, 2, . . . , m } → C − {a, b} with πi(j) = c indicating that candidate c is the jth in the order represented by πi), let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voters 4i − 3 and 4i − 2 rank the candidates a b πi(1) πi(2) . . . πi(m ). • For 1 ≤ i ≤ n , let voters 4i − 1 and 4i rank the candidates πi(m ) πi(m − 1) . . . πi(1) b a. • Let voter 4n + 1 = n − 1 rank the candidates a b π0(1) π0(2) . . . π0(m ) (where π0 is an arbitrary order of the candidates other than a and b which is the same for every element of the fooling set). • Let voter 4n + 2 = n rank the candidates π0(m ) π0(m − 1) . . . π0(1) a b.
We observe that this fooling set has size (m !)n , and that candidate a wins in each vector of votes in the fooling set (to 82 see why, we observe that for any 1 ≤ i ≤ n , votes 4i−3 and 4i − 2 rank the candidates in the exact opposite way from votes 4i − 1 and 4i, which under the Borda rule means they cancel out; and the last two votes give one more point to a than to any other candidate-besides b who gets two fewer points than a). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (π1 1, π1 2, . . . , π1 n ), and let the second vote vector correspond to the vector (π2 1, π2 2, . . . , π2 n ). For some i, we must have π1 i = π2 i , so that for some candidate c /∈ {a, b}, (π1 i )−1 (c) < (π2 i )−1 (c) (that is, c is ranked higher in π1 i than in π2 i ). Now, construct a new vote vector by taking votes 4i−3 and 4i−2 from the first vote vector, and the remaining votes from the second vote vector. a"s Borda score remains unchanged. However, because c is ranked higher in π1 i than in π2 i , c receives at least 2 more points from votes 4i−3 and 4i − 2 in the newly constructed vote vector than it did in the second vote vector. It follows that c has a higher Borda score than a in the newly constructed vote vector. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 7. The nondeterministic communication complexity of the Copeland rule is Ω(nm log m) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size (m !)n where m = (m − 2)/2 and n = (n − 2)/2. This will prove the theorem because m ! is Ω(m log m), so that log((m !)n ) = n log(m !) is Ω(nm log m). We write the set of candidates as the following disjoint union: C = {a, b} ∪ L ∪ R where L = {l1, l2, . . . , lm } and R = {r1, r2, . . . , rm }. For every vector (π1, π2, . . . , πn ) consisting of n permutations of the integers 1 through m (πi : {1, 2, . . . , m } → {1, 2, . . . , m }), let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voter 2i − 1 rank the candidates a b lπi(1) rπi(1) lπi(2) rπi(2) . . . lπi(m ) rπi(m ). • For 1 ≤ i ≤ n , let voter 2i rank the candidates rπi(m ) lπi(m ) rπi(m −1) lπi(m −1) . . . rπi(1) lπi(1) b a. • Let voter n − 1 = 2n + 1 rank the candidates a b l1 r1 l2 r2 . . . lm rm . • Let voter n = 2n +2 rank the candidates rm lm rm −1 lm −1 . . . r1 l1 a b.
We observe that this fooling set has size (m !)n , and that candidate a wins in each vector of votes in the fooling set (every pair of candidates is tied in their pairwise election, with the exception that a defeats b, so that a wins the election by half a point). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (π1 1, π1 2, . . . , π1 n ), and let the second vote vector correspond to the vector (π2 1, π2 2, . . . , π2 n ). For some i, we must have π1 i = π2 i , so that for some j ∈ {1, 2, . . . , m }, we have (π1 i )−1 (j) < (π2 i )−1 (j).
Now, construct a new vote vector by taking vote 2i−1 from the first vote vector, and the remaining votes from the second vote vector. a"s Copeland score remains unchanged. Let us consider the score of lj. We first observe that the rank of lj in vote 2i − 1 in the newly constructed vote vector is at least 2 higher than it was in the second vote vector, because (π1 i )−1 (j) < (π2 i )−1 (j). Let D1 (lj) be the set of candidates in L ∪ R that voter 2i − 1 ranked lower than lj in the first vote vector (D1 (lj) = {c ∈ L ∪ R : lj 1 2i−1 c}), and let D2 (lj) be the set of candidates in L ∪ R that voter 2i − 1 ranked lower than lj in the second vote vector (D2 (lj) = {c ∈ L ∪ R : lj 2 2i−1 c}). Then, it follows that in the newly constructed vote vector, lj defeats all the candidates in D1 (lj) − D2 (lj) in their pairwise elections (because lj receives an extra vote in each one of these pairwise elections relative to the second vote vector), and loses to all the candidates in D2 (lj) − D1 (lj) (because lj loses a vote in each one of these pairwise elections relative to the second vote vector), and ties with everyone else. But |D1 (lj)|−|D2 (lj)| ≥ 2, and hence |D1 (lj) − D2 (lj)| − |D2 (lj) − D1 (lj)| ≥ 2. Hence, in the newly constructed vote vector, lj has at least two more pairwise wins than pairwise losses, and therefore has at least 1 more point than if lj had tied all its pairwise elections. Thus, lj has a higher Copeland score than a in the newly constructed vote vector. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 8. The nondeterministic communication complexity of the maximin rule is O(nm).
Proof. The nondeterministic protocol will guess which candidate w is the winner, and, for each other candidate c, which candidate o(c) is the candidate against whom c receives its lowest score in a pairwise election. Then, let every voter communicate the following: • for each candidate c = w, whether she prefers c to w; • for each candidate c = w, whether she prefers c to o(c).
We observe that this requires the communication of 2n(m− 1) bits. If the guesses were correct, then, letting N(d, e) be the number of voters preferring candidate d to candidate e, we should have N(c, o(c)) < N(w, c ) for any c = w, c = w, which will prove that w wins the election.
Theorem 9. The nondeterministic communication complexity of the maximin rule is Ω(nm) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size 2n m where m = m − 2 and n = (n − 1)/4. Let b be a candidate other than a. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si ⊆ C − {a, b}, let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voters 4i − 3 and 4i − 2 rank the candidates Si a C − (Si ∪ {a, b}) b. • For 1 ≤ i ≤ n , let voters 4i − 1 and 4i rank the candidates b C − (Si ∪ {a, b}) a Si. 83 • Let voter 4n + 1 = n rank the candidates a b C − {a, b}.
We observe that this fooling set has size (2m )n = 2n m , and that candidate a wins in each vector of votes in the fooling set (in every one of a"s pairwise elections, a is ranked higher than its opponent by 2n +1 = (n+1)/2 > n/2 votes).
All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1
n ), and let the second vote vector correspond to the vector (S2
n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let c be some candidate in S1 i − S2 i . Now, construct a new vote vector by taking votes 4i − 3 and 4i − 2 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is ranked higher than c by only 2n −1 voters, for the following reason. Whereas voters 4i−3 and 4i − 2 do not rank c higher than a in the second vote vector (because c /∈ S2 i ), voters 4i − 3 and 4i − 2 do rank c higher than a in the first vote vector (because c ∈ S1 i ).
Moreover, in every one of b"s pairwise elections, b is ranked higher than its opponent by at least 2n voters. So, a has a lower maximin score than b, therefore a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 10. The deterministic communication complexity of the STV rule is O(n(log m)2 ).
Proof. Consider the following communication protocol.
Let each voter first announce her most preferred candidate (O(n log m) communication). In the remaining rounds, we will keep track of each voter"s most preferred candidate among the remaining candidates, which will be enough to implement the rule. When candidate c is eliminated, let each of the voters whose most preferred candidate among the remaining candidates was c announce their most preferred candidate among the candidates remaining after c"s elimination. If candidate c was the ith candidate to be eliminated (that is, there were m − i + 1 candidates remaining before c"s elimination), it follows that at most n/(m − i + 1) voters had candidate c as their most preferred candidate among the remaining candidates, and thus the number of bits to be communicated after the elimination of the ith candidate is O((n/(m−i+1)) log m).7 Thus, the total communication in this communication protocol is O(n log m + m−1 i=1 (n/(m − i + 1)) log m). Of course, m−1 i=1 1/(m − i + 1) = m i=2 1/i, which is O(log m). Substituting into the previous expression, we find that the communication complexity is O(n(log m)2 ).
Theorem 11. The nondeterministic communication complexity of the STV rule is Ω(n log m) (even to decide whether a given candidate a wins).
Proof. We omit this proof because of space constraint. 7 Actually, O((n/(m − i + 1)) log(m − i + 1)) is also correct, but it will not improve the bound.
Theorem 12. The deterministic communication complexity of the approval rule is O(nm).
Proof. Approving or disapproving of a candidate requires only one bit of information, so every voter can simply approve or disapprove of every candidate for a total communication of nm bits.
Theorem 13. The nondeterministic communication complexity of the approval rule is Ω(nm) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size 2n m where m = m − 1 and n = (n − 1)/4. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si ⊆ C − {a}, let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voters 4i − 3 and 4i − 2 approve Si ∪ {a}. • For 1 ≤ i ≤ n , let voters 4i − 1 and 4i approve C − (Si ∪ {a}). • Let voter 4n + 1 = n approve {a}.
We observe that this fooling set has size (2m )n = 2n m , and that candidate a wins in each vector of votes in the fooling set (a is approved by 2n + 1 voters, whereas each other candidate is approved by only 2n voters). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1
n ), and let the second vote vector correspond to the vector (S2
n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let b be some candidate in S1 i − S2 i . Now, construct a new vote vector by taking votes 4i − 3 and 4i − 2 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is still approved by 2n + 1 votes.
However, b is approved by 2n + 2 votes, for the following reason. Whereas voters 4i−3 and 4i−2 do not approve b in the second vote vector (because b /∈ S2 i ), voters 4i − 3 and 4i − 2 do approve b in the first vote vector (because b ∈ S1 i ).
It follows that b"s score in the newly constructed vote vector is b"s score in the second vote vector (2n ), plus two. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Interestingly, an Ω(m) lower bound can be obtained even for the problem of finding a candidate that is approved by more than one voter [20].
Theorem 14. The deterministic communication complexity of the Condorcet rule is O(nm).
Proof. We maintain a set of active candidates S which is initialized to C. At each stage, we choose two of the active candidates (say, the two candidates with the lowest indices), and we let each voter communicate which of the two candidates she prefers. (Such a stage requires the communication of n bits, one per voter.) The candidate preferred by fewer 84 voters (the loser of the pairwise election) is removed from S. (If the pairwise election is tied, both candidates are removed.) After at most m − 1 iterations, only one candidate is left (or zero candidates are left, in which case there is no Condorcet winner). Let a be the remaining candidate. To find out whether candidate a is the Condorcet winner, let each voter communicate, for every candidate c = a, whether she prefers a to c. (This requires the communication of at most n(m − 1) bits.) This is enough to establish whether a won each of its pairwise elections (and thus, whether a is the Condorcet winner).
Theorem 15. The nondeterministic communication complexity of the Condorcet rule is Ω(nm) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size 2n m where m = m − 1 and n = (n − 1)/2. For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si ⊆ C − {a}, let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voter 2i − 1 rank the candidates Si a C − Si. • For 1 ≤ i ≤ n , let voter 2i rank the candidates C − Si a Si. • Let voter 2n +1 = n rank the candidates a C −{a}.
We observe that this fooling set has size (2m )n = 2n m , and that candidate a wins in each vector of votes in the fooling set (a wins each of its pairwise elections by a single vote). All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1
n ), and let the second vote vector correspond to the vector (S2
n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i .
Without loss of generality, suppose S1 i S2 i , and let b be some candidate in S1 i − S2 i . Now, construct a new vote vector by taking vote 2i − 1 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, b wins its pairwise election against a by one vote (vote 2i − 1 ranks b above a in the newly constructed vote vector because b ∈ S1 i , whereas in the second vote vector vote 2i − 1 ranked a above b because b /∈ S2 i ). So, a is not the Condorcet winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 16. The deterministic communication complexity of the cup rule is O(nm).
Proof. Consider the following simple communication protocol. First, let all the voters communicate, for every one of the matchups in the first round, which of its two candidates they prefer. After this, the matchups for the second round are known, so let all the voters communicate which candidate they prefer in each matchup in the second round-etc.
Because communicating which of two candidates is preferred requires only one bit per voter, and because there are only m − 1 matchups in total, this communication protocol requires O(nm) communication.
Theorem 17. The nondeterministic communication complexity of the cup rule is Ω(nm) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size 2n m where m = (m − 1)/2 and n = (n − 7)/2. Given that m + 1 is a power of 2, so that one candidate gets a bye (that is, does not face an opponent) in the first round, let a be the candidate with the bye. Of the m first-round matchups, let lj denote the one (left) candidate in the jth matchup, and let rj be the other (right) candidate. Let L = {lj : 1 ≤ j ≤ m } and R = {rj : 1 ≤ j ≤ m }, so that C = L ∪ R ∪ {a}. . . . . . . . . . l r l r l r a m"1 1 2 2 m" Figure 1: The schedule for the cup rule used in the proof of Theorem 17.
For every vector (S1, S2, . . . , Sn ) consisting of n subsets Si ⊆ R, let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voter 2i − 1 rank the candidates Si L a R − Si. • For 1 ≤ i ≤ n , let voter 2i rank the candidates R − Si L a Si. • Let voters 2n +1 = n−6, 2n +2 = n−5, 2n +3 = n−4 rank the candidates L a R. • Let voters 2n + 4 = n − 3, 2n + 5 = n − 2 rank the candidates a r1 l1 r2 l2 . . . rm lm . • Let voters 2n + 6 = n − 1, 2n + 7 = n rank the candidates rm lm rm −1 lm −1 . . . r1 l1 a.
We observe that this fooling set has size (2m )n = 2n m .
Also, candidate a wins in each vector of votes in the fooling set, for the following reasons. Each candidate rj defeats its opponent lj in the first round. (For any 1 ≤ i ≤ n , the net effect of votes 2i − 1 and 2i on the pairwise election between rj and lj is zero; votes n − 6, n − 5, n − 4 prefer lj to rj, but votes n − 3, n − 2, n − 1, n all prefer rj to lj.) Moreover, a defeats every rj in their pairwise election. (For any 1 ≤ i ≤ n , the net effect of votes 2i − 1 and 2i on the pairwise election between a and rj is zero; votes n − 1, n prefer rj to a, but votes n − 6, n − 5, n − 4, n − 3, n − 2 all prefer a to rj.) It follows that a will defeat all the candidates that it faces.
All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector 85 (S1
n ), and let the second vote vector correspond to the vector (S2
n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i . Without loss of generality, suppose S1 i S2 i , and let rj be some candidate in S1 i − S2 i . Now, construct a new vote vector by taking vote 2i from the first vote vector, and the remaining votes from the second vote vector. We note that, whereas in the second vote vector vote 2i preferred rj to lj (because rj ∈ R−S2 i ), in the newly constructed vote vector this is no longer the case (because rj ∈ S1 i ). It follows that, whereas in the second vote vector, rj defeated lj in the first round by one vote, in the newly constructed vote vector, lj defeats rj in the first round. Thus, at least one lj advances to the second round after defeating its opponent rj. Now, we observe that in the newly constructed vote vector, any lk wins its pairwise election against any rq with q = k. This is because among the first 2n votes, at least n − 1 prefer lk to rq; votes n − 6, n − 5, n − 4 prefer lk to rq; and, because q = k, either votes n − 3, n − 2 prefer lk to rq (if k < q), or votes n − 1, n prefer lk to rq (if k > q). Thus, at least n + 4 = (n + 1)/2 > n/2 votes prefer lk to rq. Moreover, any lk wins its pairwise election against a. This is because only votes n − 3 and n − 2 prefer a to lk. It follows that, after the first round, any surviving candidate lk can only lose a matchup against another surviving lk , so that one of the lk must win the election. So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 18. The deterministic communication complexity of the Bucklin rule is O(nm).
Proof. Let l be the minimum integer for which there is a candidate who is ranked among the top l candidates by more than half the votes. We will do a binary search for l.
At each point, we will have a lower bound lL which is smaller than l (initialized to 0), and an upper bound lH which is at least l (initialized to m). While lH − lL > 1, we continue by finding out whether (lH − l)/2 is smaller than l, after which we can update the bounds.
To find out whether a number k is smaller than l, we determine every voter"s k most preferred candidates.
Every voter can communicate which candidates are among her k most preferred candidates using m bits (for each candidate, indicate whether the candidate is among the top k or not), but because the binary search requires log m iterations, this gives us an upper bound of O((log m)nm), which is not strong enough. However, if lL < k < lH , and we already know a voter"s lL most preferred candidates, as well as her lH most preferred candidates, then the voter no longer needs to communicate whether the lL most preferred candidates are among her k most preferred candidates (because they must be), and she no longer needs to communicate whether the m−lH least preferred candidates are among her k most preferred candidates (because they cannot be). Thus the voter needs to communicate only m−lL −(m−lH ) = lH −lL bits in any given stage. Because each stage, lH − lL is (roughly) halved, each voter in total communicates only (roughly) m + m/2 + m/4 + . . . ≤ 2m bits.
Theorem 19. The nondeterministic communication complexity of the Bucklin rule is Ω(nm) (even to decide whether a given candidate a wins).
Proof. We will exhibit a fooling set of size 2n m where m = (m−1)/2 and n = n/2. We write the set of candidates as the following disjoint union: C = {a} ∪ L ∪ R where L = {l1, l2, . . . , lm } and R = {r1, r2, . . . , rm }. For any subset S ⊆ {1, 2, . . . , m }, let L(S) = {li : i ∈ S} and let R(S) = {ri : i ∈ S}. For every vector (S1, S2, . . . , Sn ) consisting of n sets Si ⊆ {1, 2, . . . , m }, let the following vector of votes be an element of the fooling set: • For 1 ≤ i ≤ n , let voter 2i − 1 rank the candidates L(Si) R − R(Si) a L − L(Si) R(Si). • For 1 ≤ i ≤ n , let voter 2i rank the candidates L − L(Si) R(Si) a L(Si) R − R(Si).
We observe that this fooling set has size (2m )n = 2n m , and that candidate a wins in each vector of votes in the fooling set, for the following reason. Each candidate in C − {a} is ranked among the top m candidates by exactly half the voters (which is not enough to win). Thus, we need to look at the voters" top m +1 candidates, and a is ranked m +1th by all voters. All that remains to show is that for any two distinct vectors of votes in the fooling set, we can let each of the voters vote according to one of these two vectors in such a way that a loses. Let the first vote vector correspond to the vector (S1
n ), and let the second vote vector correspond to the vector (S2
n ). For some i, we must have S1 i = S2 i , so that either S1 i S2 i or S2 i S1 i .
Without loss of generality, suppose S1 i S2 i , and let j be some integer in S1 i − S2 i . Now, construct a new vote vector by taking vote 2i − 1 from the first vote vector, and the remaining votes from the second vote vector. In this newly constructed vote vector, a is still ranked m + 1th by all votes. However, lj is ranked among the top m candidates by n + 1 = n/2 + 1 votes. This is because whereas vote 2i − 1 does not rank lj among the top m candidates in the second vote vector (because j /∈ S2 i , we have lj /∈ L(S2 i )), vote 2i − 1 does rank lj among the top m candidates in the first vote vector (because j ∈ S1 i , we have lj ∈ L(S1 i )). So, a is not the winner in the newly constructed vote vector, and hence we have a correct fooling set.
Theorem 20. The nondeterministic communication complexity of the ranked pairs rule is Ω(nm log m) (even to decide whether a given candidate a wins).
Proof. We omit this proof because of space constraint.
One key obstacle to using voting for preference aggregation is the communication burden that an election places on the voters. By lowering this burden, it may become feasible to conduct more elections over more issues. In the limit, this could lead to a shift from representational government to a system in which most issues are decided by referenda-a veritable e-democracy. In this paper, we analyzed the communication complexity of the common voting rules. Knowing which voting rules require little communication is especially important when the issue to be voted on is of low enough importance that the following is true: the parties involved are willing to accept a rule that tends 86 to produce outcomes that are slightly less representative of the voters" preferences, if this rule reduces the communication burden on the voters significantly. The following table summarizes the results we obtained.
Rule Lower bound Upper bound plurality Ω(n log m) O(n log m) plurality w/ runoff Ω(n log m) O(n log m) STV Ω(n log m) O(n(log m)2) Condorcet Ω(nm) O(nm) approval Ω(nm) O(nm) Bucklin Ω(nm) O(nm) cup Ω(nm) O(nm) maximin Ω(nm) O(nm) Borda Ω(nm log m) O(nm log m) Copeland Ω(nm log m) O(nm log m) ranked pairs Ω(nm log m) O(nm log m) Communication complexity of voting rules, sorted from low to high. All of the upper bounds are deterministic (with the exception of maximin, for which the best deterministic upper bound we proved is O(nm log m)). All of the lower bounds hold even for nondeterministic communication and even just for determining whether a given candidate a is the winner.
One area of future research is to study what happens when we restrict our attention to communication protocols that do not reveal any strategically useful information. This restriction may invalidate some of the upper bounds that we derived using multistage communication protocols. Also, all of our bounds are worst-case bounds. It may be possible to outperform these bounds when the distribution of votes has additional structure.
When deciding which voting rule to use for an election, there are many considerations to take into account. The voting rules that we studied in this paper are the most common ones that have survived the test of time. One way to select among these rules is to consider recent results on complexity. The table above shows that from a communication complexity perspective, plurality, plurality with runoff, and STV are preferable. However, plurality has the undesirable property that it is computationally easy to manipulate by voting strategically [3, 7]. Plurality with runoff is NP-hard to manipulate by a coalition of weighted voters, or by an individual that faces correlated uncertainty about the others" votes [7, 6]. STV is NP-hard to manipulate in those settings as well [7], but also by an individual with perfect knowledge of the others" votes (when the number of candidates is unbounded) [2]. Therefore, STV is more robust, although it may require slightly more worst-case communication as per the table above. Yet other selection criteria are the computational complexity of determining whether enough information has been elicited to declare a winner, and that of determining the optimal sequence of queries [8].
[1] Lawrence Ausubel and Paul Milgrom. Ascending auctions with package bidding. Frontiers of Theoretical Economics, 1, 2002. No. 1, Article 1. [2] John Bartholdi, III and James Orlin. Single transferable vote resists strategic voting. Social Choice and Welfare, 8(4):341-354, 1991. [3] John Bartholdi, III, Craig Tovey, and Michael Trick. The computational difficulty of manipulating an election. Social Choice and Welfare, 6(3):227-241, 1989. [4] Avrim Blum, Jeffrey Jackson, Tuomas Sandholm, and Martin Zinkevich. Preference elicitation and query learning.
Journal of Machine Learning Research, 5:649-667, 2004. [5] Wolfram Conen and Tuomas Sandholm. Preference elicitation in combinatorial auctions: Extended abstract. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 256-259, 2001. [6] Vincent Conitzer, Jerome Lang, and Tuomas Sandholm.
How many candidates are needed to make elections hard to manipulate? In Theoretical Aspects of Rationality and Knowledge (TARK), pages 201-214, 2003. [7] Vincent Conitzer and Tuomas Sandholm. Complexity of manipulating elections with few candidates. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 314-319, 2002. [8] Vincent Conitzer and Tuomas Sandholm. Vote elicitation: Complexity and strategy-proofness. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 392-397, 2002. [9] Sven de Vries, James Schummer, and Rakesh V. Vohra. On ascending auctions for heterogeneous objects, 2003. Draft. [10] Allan Gibbard. Manipulation of voting schemes.
Econometrica, 41:587-602, 1973. [11] Benoit Hudson and Tuomas Sandholm. Effectiveness of query types and policies for preference elicitation in combinatorial auctions. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 386-393, 2004. [12] E Kushilevitz and N Nisan. Communication Complexity.
Cambridge University Press, 1997. [13] Sebasti´en Lahaie and David Parkes. Applying learning algorithms to preference elicitation. In Proceedings of the ACM Conference on Electronic Commerce, 2004. [14] Noam Nisan and Ilya Segal. The communication requirements of efficient allocations and supporting prices.
Journal of Economic Theory, 2005. Forthcoming. [15] David Parkes. iBundle: An efficient ascending price bundle auction. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 148-157, 1999. [16] Tuomas Sandholm. An implementation of the contract net protocol based on marginal cost calculations. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 256-262, 1993. [17] Tuomas Sandholm and Craig Boutilier. Preference elicitation in combinatorial auctions. In Peter Cramton,
Yoav Shoham, and Richard Steinberg, editors,
Combinatorial Auctions, chapter 10. MIT Press, 2005. [18] Paolo Santi, Vincent Conitzer, and Tuomas Sandholm.
Towards a characterization of polynomial preference elicitation with value queries in combinatorial auctions. In Conference on Learning Theory (COLT), pages 1-16, 2004. [19] Mark Satterthwaite. Strategy-proofness and Arrow"s conditions: existence and correspondence theorems for voting procedures and social welfare functions. Journal of Economic Theory, 10:187-217, 1975. [20] Ilya Segal. The communication requirements of social choice rules and supporting budget sets, 2004. Draft. Presented at the DIMACS Workshop on Computational Issues in Auction Design, Rutgers University, New Jersey, USA. [21] Peter Wurman and Michael Wellman. AkBA: A progressive, anonymous-price combinatorial auction. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 21-29, 2000. [22] A. C. Yao. Some complexity questions related to distributed computing. In Proceedings of the 11th ACM symposium on theory of computing (STOC), pages 209-213, 1979. [23] Martin Zinkevich, Avrim Blum, and Tuomas Sandholm. On polynomial-time preference elicitation with value queries.

In multiagent systems with self-interested agents, the optimal action for one agent may depend on the actions taken by other agents. In such settings, the agents require tools from game theory to rationally decide on an action. Game theory offers various formal models of strategic settings-the best-known of which is a game in normal (or matrix) form, specifying a utility (payoff) for each agent for each combination of strategies that the agents choose-as well as solution concepts, which, given a game, specify which outcomes are reasonable (under various assumptions of rationality and common knowledge).
Probably the best-known (and certainly the most-studied) solution concept is that of Nash equilibrium. A Nash equilibrium specifies a strategy for each player, in such a way that no player has an incentive to (unilaterally) deviate from the prescribed strategy. Recently, numerous papers have studied computing Nash equilibria in various settings [9, 4, 12, 3, 13, 14], and the complexity of constructing a Nash equilibrium in normal form games has been labeled one of the two most important open problems on the boundary of P today [20].
The problem of computing solutions according to the perhaps more elementary solution concepts of dominance and iterated dominance has received much less attention. (After an early short paper on an easy case [11], the main computational study of these concepts has actually taken place in a paper in the game theory community [7].1 ) A strategy strictly dominates another strategy if it performs strictly 1 This is not to say that computer scientists have ignored 88 better against all vectors of opponent strategies, and weakly dominates it if it performs at least as well against all vectors of opponent strategies, and strictly better against at least one. The idea is that dominated strategies can be eliminated from consideration. In iterated dominance, the elimination proceeds in rounds, and becomes easier as more strategies are eliminated: in any given round, the dominating strategy no longer needs to perform better than or as well as the dominated strategy against opponent strategies that were eliminated in earlier rounds. Computing solutions according to (iterated) dominance is important for at least the following reasons: 1) it can be computationally easier than computing (for instance) a Nash equilibrium (and therefore it can be useful as a preprocessing step in computing a Nash equilibrium), and 2) (iterated) dominance requires a weaker rationality assumption on the players than (for instance) Nash equilibrium, and therefore solutions derived according to it are more likely to occur.
In this paper, we study some fundamental computational questions concerning dominance and iterated dominance, including how hard it is to check whether a given strategy can be eliminated by each of the variants of these notions. The rest of the paper is organized as follows. In Section 2, we briefly review definitions and basic properties of normal form games, strict and weak dominance, and iterated strict and weak dominance. In the remaining sections, we study computational aspects of dominance and iterated dominance. In Section 3, we study one-shot (not iterated) dominance. In Section 4, we study iterated dominance. In Section 5, we study dominance and iterated dominance when the dominating strategy can only place probability on a few pure strategies. Finally, in Section 6, we study dominance and iterated dominance in Bayesian games.
In this section, we briefly review normal form games, as well as dominance and iterated dominance (both strict and weak). An n-player normal form game is defined as follows.
Definition 1. A normal form game is given by a set of players {1, 2, . . . , n}; and, for each player i, a (finite) set of pure strategies Σi and a utility function ui : Σ1 × Σ2 × . . . × Σn → R (where ui(σ1, σ2, . . . , σn) denotes player i"s utility when each player j plays action σj).
The two main notions of dominance are defined as follows.
Definition 2. Player i"s strategy σi is said to be strictly dominated by player i"s strategy σi if for any vector of strategies σ−i for the other players, ui(σi, σ−i) > ui(σi, σ−i).
Player i"s strategy σi is said to be weakly dominated by player i"s strategy σi if for any vector of strategies σ−i for the other players, ui(σi, σ−i) ≥ ui(σi, σ−i), and for at least one vector of strategies σ−i for the other players, ui(σi, σ−i) > ui(σi, σ−i).
In this definition, it is sometimes allowed for the dominating strategy σi to be a mixed strategy, that is, a probability distribution over pure strategies. In this case, the utilities in dominance altogether. For example, simple dominance checks are sometimes used as a subroutine in searching for Nash equilibria [21]. the definition are the expected utilities.2 There are other notions of dominance, such as very weak dominance (in which no strict inequality is required, so two strategies can dominate each other), but we will not study them here. When we are looking at the dominance relations for player i, the other players (−i) can be thought of as a single player.3  Therefore, in the rest of the paper, when we study one-shot (not iterated) dominance, we will focus without loss of generality on two-player games.4 In two-player games, we will generally refer to the players as r (row) and c (column) rather than 1 and 2.
In iterated dominance, dominated strategies are removed from the game, and no longer have any effect on future dominance relations. Iterated dominance can eliminate more strategies than dominance, as follows. σr may originally not dominate σr because the latter performs better against σc; but then, once σc is removed because it is dominated by σc, σr dominates σr, and the latter can be removed. For example, in the following game, R can be removed first, after which D is dominated.
L R U 1, 1 0, 0 D 0, 1 1, 0 Either strict or weak dominance can be used in the definition of iterated dominance. We note that the process of iterated dominance is never helped by removing a dominated mixed strategy, for the following reason. If σi gives player i a higher utility than σi against mixed strategy σj for player j = i (and strategies σ−{i,j} for the other players), then for at least one pure strategy σj that σj places positive probability on, σi must perform better than σi against σj (and strategies σ−{i,j} for the other players). Thus, removing the mixed strategy σj does not introduce any new dominances.
More detailed discussions and examples can be found in standard texts on microeconomics or game theory [17, 5].
We are now ready to move on to the core of this paper.
In this section, we study the notion of one-shot (not iterated) dominance. As a first observation, checking whether a given strategy is strictly (weakly) dominated by some pure strategy is straightforward, by checking, for every pure strategy for that player, whether the latter strategy performs strictly better against all the opponent"s strategies (at least as well against all the opponent"s strategies, and strictly 2 The dominated strategy σi is, of course, also allowed to be mixed, but this has no technical implications for the paper: when we study one-shot dominance, we ask whether a given strategy is dominated, and it does not matter whether the given strategy is pure or mixed; when we study iterated dominance, there is no use in eliminating mixed strategies, as we will see shortly. 3 This player may have a very large strategy space (one pure strategy for every vector of pure strategies for the players that are being replaced). Nevertheless, this will not result in an increase in our representation size, because the original representation already had to specify utilities for each of these vectors. 4 We note that a restriction to two-player games would not be without loss of generality for iterated dominance. This is because for iterated dominance, we need to look at the dominated strategies of each individual player, so we cannot merge any players. 89 better against at least one).5 Next, we show that checking whether a given strategy is dominated by some mixed strategy can be done in polynomial time by solving a single linear program. (Similar linear programs have been given before [18]; we present the result here for completeness, and because we will build on the linear programs given below in Theorem 6.) Proposition 1. Given the row player"s utilities, a subset Dr of the row player"s pure strategies Σr, and a distinguished strategy σ∗ r for the row player, we can check in time polynomial in the size of the game (by solving a single linear program of polynomial size) whether there exists some mixed strategy σr, that places positive probability only on strategies in Dr and dominates σ∗ r , both for strict and for weak dominance.
Proof. Let pdr be the probability that σr places on dr ∈ Dr. We will solve a single linear program in each of our algorithms; linear programs can be solved in polynomial time [10]. For strict dominance, the question is whether the pdr can be set so that for every pure strategy for the column player σc ∈ Σc, dr∈Dr pdr ur(dr, σc) > ur(σ∗ r , σc). Because the inequality must be strict, we cannot solve this directly by linear programming. We proceed as follows. Because the game is finite, we may assume without loss of generality that all utilities are positive (if not, simply add a constant to all utilities.) Solve the following linear program: minimize dr∈Dr pdr such that for any σc ∈ Σc, dr∈Dr pdr ur(dr, σc) ≥ ur(σ∗ r , σc).
If σ∗ r is strictly dominated by some mixed strategy, this linear program has a solution with objective value < 1. (The dominating strategy is a feasible solution with objective value exactly 1. Because no constraint is binding for this solution, we can reduce one of the probabilities slightly without affecting feasibility, thereby obtaining a solution with objective value < 1.) Moreover, if this linear program has a solution with objective value < 1, there is a mixed strategy strictly dominating σ∗ r , which can be obtained by taking the LP solution and adding the remaining probability to any strategy (because all the utilities are positive, this will add to the left side of any inequality, so all inequalities will become strict).
For weak dominance, we can solve the following linear program: maximize σc∈Σc (( dr∈Dr pdr ur(dr, σc)) − ur(σ∗ r , σc)) such that for any σc ∈ Σc, dr∈Dr pdr ur(dr, σc) ≥ ur(σ∗ r , σc); dr∈Dr pdr = 1.
If σ∗ r is weakly dominated by some mixed strategy, then that mixed strategy is a feasible solution to this program with objective value > 0, because for at least one strategy σc ∈ Σc we have ( dr∈Dr pdr ur(dr, σc)) − ur(σ∗ r , σc) > 0. On the other hand, if this program has a solution with objective value > 0, then for at least one strategy σc ∈ Σc we 5 Recall that the assumption of a single opponent (that is, the assumption of two players) is without loss of generality for one-shot dominance. must have ( dr∈Dr pdr ur(dr, σc)) − ur(σ∗ r , σc) > 0, and thus the linear program"s solution is a weakly dominating mixed strategy.
We now move on to iterated dominance. It is well-known that iterated strict dominance is path-independent [6, 19]that is, if we remove dominated strategies until no more dominated strategies remain, in the end the remaining strategies for each player will be the same, regardless of the order in which strategies are removed. Because of this, to see whether a given strategy can be eliminated by iterated strict dominance, all that needs to be done is to repeatedly remove strategies that are strictly dominated, until no more dominated strategies remain. Because we can check in polynomial time whether any given strategy is dominated (whether or not dominance by mixed strategies is allowed, as described in Section 3), this whole procedure takes only polynomial time. In the case of iterated dominance by pure strategies with two players, Knuth et al. [11] slightly improve on (speed up) the straightforward implementation of this procedure by keeping track of, for each ordered pair of strategies for a player, the number of opponent strategies that prevent the first strategy from dominating the second.
Hereby the runtime for an m × n game is reduced from O((m + n)4 ) to O((m + n)3 ). (Actually, they only study very weak dominance (for which no strict inequalities are required), but the approach is easily extended.) In contrast, iterated weak dominance is known to be pathdependent.6 For example, in the following game, using iterated weak dominance we can eliminate M first, and then D, or R first, and then U.
L M R U 1, 1 0, 0 1, 0 D 1, 1 1, 0 0, 0 Therefore, while the procedure of removing weakly dominated strategies until no more weakly dominated strategies remain can certainly be executed in polynomial time, which strategies survive in the end depends on the order in which we remove the dominated strategies. We will investigate two questions for iterated weak dominance: whether a given strategy is eliminated in some path, and whether there is a path to a unique solution (one pure strategy left per player).
We will show that both of these problems are computationally hard.
Definition 3. Given a game in normal form and a distinguished strategy σ∗ , IWD-STRATEGY-ELIMINATION asks whether there is some path of iterated weak dominance that eliminates σ∗ . Given a game in normal form,
IWDUNIQUE-SOLUTION asks whether there is some path of iterated weak dominance that leads to a unique solution (one strategy left per player).
The following lemma shows a special case of normal form games in which allowing for weak dominance by mixed strategies (in addition to weak dominance by pure strategies) does 6 There is, however, a restriction of weak dominance called nice weak dominance which is path-independent [15, 16]. For an overview of path-independence results, see Apt [1]. 90 not help. We will prove the hardness results in this setting, so that they will hold whether or not dominance by mixed strategies is allowed.
Lemma 1. Suppose that all the utilities in a game are in {0, 1}. Then every pure strategy that is weakly dominated by a mixed strategy is also weakly dominated by a pure strategy.
Proof. Suppose pure strategy σ is weakly dominated by mixed strategy σ∗ . If σ gets a utility of 1 against some opponent strategy (or vector of opponent strategies if there are more than 2 players), then all the pure strategies that σ∗ places positive probability on must also get a utility of 1 against that opponent strategy (or else the expected utility would be smaller than 1). Moreover, at least one of the pure strategies that σ∗ places positive probability on must get a utility of 1 against an opponent strategy that σ gets
follows that this pure strategy weakly dominates σ.
We are now ready to prove the main results of this section.
Theorem 1. IWD-STRATEGY-ELIMINATION is NPcomplete, even with 2 players, and with 0 and 1 being the only utilities occurring in the matrix-whether or not dominance by mixed strategies is allowed.
Proof. The problem is in NP because given a sequence of strategies to be eliminated, we can easily check whether this is a valid sequence of eliminations (even when dominance by mixed strategies is allowed, using Proposition 1).
To show that the problem is NP-hard, we reduce an arbitrary satisfiability instance (given by a nonempty set of clauses C over a nonempty set of variables V , with corresponding literals L = {+v : v ∈ V } ∪ {−v : v ∈ V }) to the following IWD-STRATEGY-ELIMINATION instance. (In this instance, we will specify that certain strategies are uneliminable. A strategy σr can be made uneliminable, even when 0 and 1 are the only allowed utilities, by adding another strategy σr and another opponent strategy σc, so that:
a utility of 1 against σc. 2. σr and σr always give the row player the same utility. 3. σc is the only strategy that gives the column player a utility of 1 against σr, but otherwise σc always gives the column player utility 0. This makes it impossible to eliminate any of these three strategies. We will not explicitly specify the additional strategies to make the proof more legible.) In this proof, we will denote row player strategies by s, and column player strategies by t, to improve legibility. Let the row player"s pure strategy set be given as follows. For every variable v ∈ V , the row player has corresponding strategies s1 +v, s2 +v, s1 −v, s2 −v. Additionally, the row player has the following 2 strategies: s1
0, where s2
r (that is, it is the strategy we seek to eliminate). Finally, for every clause c ∈ C, the row player has corresponding strategies s1 c (uneliminable) and s2 c. Let the column player"s pure strategy set be given as follows. For every variable v ∈ V , the column player has a corresponding strategy tv. For every clause c ∈ C, the column player has a corresponding strategy tc, and additionally, for every literal l ∈ L that occurs in c, a strategy tc,l. For every variable v ∈ V , the column player has corresponding strategies t+v, t−v (both uneliminable).
Finally, the column player has three additional strategies: t1
0, and t1.
The utility function for the row player is given as follows: • ur(s1 +v, tv) = 0 for all v ∈ V ; • ur(s2 +v, tv) = 1 for all v ∈ V ; • ur(s1 −v, tv) = 1 for all v ∈ V ; • ur(s2 −v, tv) = 0 for all v ∈ V ; • ur(s1 +v, t1) = 1 for all v ∈ V ; • ur(s2 +v, t1) = 0 for all v ∈ V ; • ur(s1 −v, t1) = 0 for all v ∈ V ; • ur(s2 −v, t1) = 1 for all v ∈ V ; • ur(sb +v, t+v) = 1 for all v ∈ V and b ∈ {1, 2}; • ur(sb −v, t−v) = 1 for all v ∈ V and b ∈ {1, 2}; • ur(sl, t) = 0 otherwise for all l ∈ L and t ∈ S2; • ur(s1 0, tc) = 0 for all c ∈ C; • ur(s2 0, tc) = 1 for all c ∈ C; • ur(sb 0, t1 0) = 1 for all b ∈ {1, 2}; • ur(s1 0, t2 0) = 1; • ur(s2 0, t2 0) = 0; • ur(sb 0, t) = 0 otherwise for all b ∈ {1, 2} and t ∈ S2; • ur(sb c, t) = 0 otherwise for all c ∈ C and b ∈ {1, 2}; and the row player"s utility is 0 in every other case. The utility function for the column player is given as follows: • uc(s, tv) = 0 for all v ∈ V and s ∈ S1; • uc(s, t1) = 0 for all s ∈ S1; • uc(s2 l , tc) = 1 for all c ∈ C and l ∈ L where l ∈ c (literal l occurs in clause c); • uc(s2 l2 , tc,l1 ) = 1 for all c ∈ C and l1, l2 ∈ L, l1 = l2 where l2 ∈ c; • uc(s1 c, tc) = 1 for all c ∈ C; • uc(s2 c, tc) = 0 for all c ∈ C; • uc(sb c, tc,l) = 1 for all c ∈ C, l ∈ L, and b ∈ {1, 2}; • uc(s2, tc) = uc(s2, tc,l) = 0 otherwise for all c ∈ C and l ∈ L; and the column player"s utility is 0 in every other case. We now show that the two instances are equivalent.
First, suppose there is a solution to the satisfiability instance: that is, a truth-value assignment to the variables in V such that all clauses are satisfied. Then, consider the following sequence of eliminations in our game: 1. For every variable v that is set to true in the assignment, eliminate tv (which gives the column player utility 0 everywhere). 2.
Then, for every variable v that is set to true in the assignment, eliminate s2 +v using s1 +v (which is possible because tv has been eliminated, and because t1 has not been eliminated (yet)). 3. Now eliminate t1 (which gives the column player utility 0 everywhere). 4. Next, for every variable v that is set to false in the assignment, eliminate s2 −v using s1 −v (which is possible because t1 has been eliminated, and because tv has not been eliminated (yet)). 5. For every clause c which has the variable corresponding to one of its positive literals l = +v set to true in the assignment, eliminate tc using tc,l (which is possible because s2 l has been eliminated, and s2 c has not been eliminated (yet)). 6. For every clause c which has the variable corresponding to one of its negative literals l = −v set to false in the assignment, eliminate tc using tc,l 91 (which is possible because s2 l has been eliminated, and s2 c has not been eliminated (yet)). 7. Because the assignment satisfied the formula, all the tc have now been eliminated. Thus, we can eliminate s2
r using s1
solution to the IWD-STRATEGY-ELIMINATION instance.
Now suppose there is a solution to the IWD-STRATEGYELIMINATION instance. By Lemma 1, we can assume that all the dominances are by pure strategies. We first observe that only s1
r , because it is the only other strategy that gets the row player a utility of 1 against t1 0, and t1
better than s1
the tc strategies must be eliminated. For each c ∈ C, the strategy tc can only be eliminated by one of the strategies tc,l (with the same c), because these are the only other strategies that get the column player a utility of 1 against s1 c, and s1 c is uneliminable. But, in order for some tc,l to eliminate tc, s2 l must be eliminated first. Only s1 l can eliminate s2 l , because it is the only other strategy that gets the row player a utility of 1 against tl, and tl is uneliminable. We next show that for every v ∈ V only one of s2 +v, s2 −v can be eliminated. This is because in order for s1 +v to eliminate s2 +v, tv needs to have been eliminated and t1, not (so tv must be eliminated before t1); but in order for s1 −v to eliminate s2 −v, t1 needs to have been eliminated and tv, not (so t1 must be eliminated before tv). So, set v to true if s2 +v is eliminated, and to false otherwise Because by the above, for every clause c, one of the s2 l with l ∈ c must be eliminated, it follows that this is a satisfying assignment to the satisfiability instance.
Using Theorem 1, it is now (relatively) easy to show that IWD-UNIQUE-SOLUTION is also NP-complete under the same restrictions.
Theorem 2. IWD-UNIQUE-SOLUTION is NP-complete, even with 2 players, and with 0 and 1 being the only utilities occurring in the matrix-whether or not dominance by mixed strategies is allowed.
Proof. Again, the problem is in NP because we can nondeterministically choose the sequence of eliminations and verify whether it is correct. To show NP-hardness, we reduce an arbitrary IWD-STRATEGY-ELIMINATION instance to the following IWD-UNIQUE-SOLUTION instance. Let all the strategies for each player from the original instance remain part of the new instance, and let the utilities resulting from the players playing a pair of these strategies be the same. We add three additional strategies σ1 r , σ2 r , σ3 r for the row player, and three additional strategies σ1 c , σ2 c , σ3 c for the column player. Let the additional utilities be as follows: • ur(σr, σj c) = 1 for all σr /∈ {σ1 r , σ2 r , σ3 r } and j ∈ {2, 3}; • ur(σi r, σc) = 1 for all i ∈ {1, 2, 3} and σc /∈ {σ2 c , σ3 c }; • ur(σi r, σ2 c ) = 1 for all i ∈ {2, 3}; • ur(σ1 r , σ3 c ) = 1; • and the row player"s utility is 0 in all other cases involving a new strategy. • uc(σ3 r , σc) = 1 for all σc /∈ {σ1 c , σ2 c , σ3 c }; • uc(σ∗ r , σj c) = 1 for all j ∈ {2, 3} (σ∗ r is the strategy to be eliminated in the original instance); • uc(σi r, σ1 c ) = 1 for all i ∈ {1, 2}; • ur(σ1 r , σ2 c ) = 1; • ur(σ2 r , σ3 c ) = 1; • and the column player"s utility is 0 in all other cases involving a new strategy.
We proceed to show that the two instances are equivalent.
First suppose there exists a solution to the original IWDSTRATEGY-ELIMINATION instance. Then, perform the same sequence of eliminations to eliminate σ∗ r in the new IWD-UNIQUE-SOLUTION instance. (This is possible because at any stage, any weak dominance for the row player in the original instance is still a weak dominance in the new instance, because the two strategies" utilities for the row player are the same when the column player plays one of the new strategies; and the same is true for the column player.) Once σ∗ r is eliminated, let σ1 c eliminate σ2 c . (It performs better against σ2 r .) Then, let σ1 r eliminate all the other remaining strategies for the row player. (It always performs better against either σ1 c or σ3 c .) Finally, σ1 c is the unique best response against σ1 r among the column player"s remaining strategies, so let it eliminate all the other remaining strategies for the column player. Thus, there exists a solution to the IWD-UNIQUE-SOLUTION instance.
Now suppose there exists a solution to the IWD-UNIQUESOLUTION instance. By Lemma 1, we can assume that all the dominances are by pure strategies. We will show that none of the new strategies (σ1 r , σ2 r , σ3 r , σ1 c , σ2 c , σ3 c ) can either eliminate another strategy, or be eliminated before σ∗ r is eliminated. Thus, there must be a sequence of eliminations ending in the elimination of σ∗ r , which does not involve any of the new strategies, and is therefore a valid sequence of eliminations in the original game (because all original strategies perform the same against each new strategy). We now show that this is true by exhausting all possibilities for the first elimination before σ∗ r is eliminated that involves a new strategy. None of the σi r can be eliminated by a σr /∈ {σ1 r , σ2 r , σ3 r }, because the σi r perform better against σ1 c . σ1 r cannot eliminate any other strategy, because it always performs poorer against σ2 c . σ2 r and σ3 r are equivalent from the row player"s perspective (and thus cannot eliminate each other), and cannot eliminate any other strategy because they always perform poorer against σ3 c . None of the σj c can be eliminated by a σc /∈ {σ1 c , σ2 c , σ3 c }, because the σj c always perform better against either σ1 r or σ2 r . σ1 c cannot eliminate any other strategy, because it always performs poorer against either σ∗ r or σ3 r . σ2 c cannot eliminate any other strategy, because it always performs poorer against σ2 r or σ3 r . σ3 c cannot eliminate any other strategy, because it always performs poorer against σ1 r or σ3 r . Thus, there exists a solution to the IWDSTRATEGY-ELIMINATION instance.
A slightly weaker version of the part of Theorem 2 concerning dominance by pure strategies only is the main result of Gilboa et al. [7]. (Besides not proving the result for dominance by mixed strategies, the original result was weaker because it required utilities {0, 1, 2, 3, 4, 5, 6, 7, 8} rather than just {0, 1} (and because of this, our Lemma 1 cannot be applied to it to get the result for mixed strategies).)
MIXED STRATEGIES WITH SMALL SUPPORTS When showing that a strategy is dominated by a mixed strategy, there are several reasons to prefer exhibiting a 92 dominating strategy that places positive probability on as few pure strategies as possible. First, this will reduce the number of bits required to specify the dominating strategy (and thus the proof of dominance can be communicated quicker): if the dominating mixed strategy places positive probability on only k strategies, then it can be specified using k real numbers for the probabilities, plus k log m (where m is the number of strategies for the player under consideration) bits to indicate which strategies are used. Second, the proof of dominance will be cleaner: for a dominating mixed strategy, it is typically (always in the case of strict dominance) possible to spread some of the probability onto any unused pure strategy and still have a dominating strategy, but this obscures which pure strategies are the ones that are key in making the mixed strategy dominating. Third, because (by the previous) the argument for eliminating the dominated strategy is simpler and easier to understand, it is more likely to be accepted. Fourth, the level of risk neutrality required for the argument to work is reduced, at least in the extreme case where dominance by a single pure strategy can be exhibited (no risk neutrality is required here).
This motivates the following problem.
Definition 4 (MINIMUM-DOMINATING-SET). We are given the row player"s utilities of a game in normal form, a distinguished strategy σ∗ for the row player, a specification of whether the dominance should be strict or weak, and a number k. We are asked whether there exists a mixed strategy σ for the row player that places positive probability on at most k pure strategies, and dominates σ∗ in the required sense.
Unfortunately, this problem is NP-complete.
Theorem 3. MINIMUM-DOMINATING-SET is NPcomplete, both for strict and for weak dominance.
Proof. The problem is in NP because we can nondeterministically choose a set of at most k strategies to give positive probability, and decide whether we can dominate σ∗ with these k strategies as described in Proposition 1. To show NP-hardness, we reduce an arbitrary SET-COVER instance (given a set S, subsets S1, S2, . . . , Sr, and a number t, can all of S be covered by at most t of the subsets?) to the following MINIMUM-DOMINATING-SET instance.
For every element s ∈ S, there is a pure strategy σs for the column player. For every subset Si, there is a pure strategy σSi for the row player. Finally, there is the distinguished pure strategy σ∗ for the row player. The row player"s utilities are as follows: ur(σSi , σs) = t + 1 if s ∈ Si; ur(σSi , σs) = 0 if s /∈ Si; ur(σ∗ , σs) = 1 for all s ∈ S.
Finally, we let k = t. We now proceed to show that the two instances are equivalent.
First suppose there exists a solution to the SET-COVER instance. Without loss of generality, we can assume that there are exactly k subsets in the cover. Then, for every Si that is in the cover, let the dominating strategy σ place exactly 1 k probability on the corresponding pure strategy σSi . Now, if we let n(s) be the number of subsets in the cover containing s (we observe that that n(s) ≥ 1), then for every strategy σs for the column player, the row player"s expected utility for playing σ when the column player is playing σs is u(σ, σs) = n(s) k (k + 1) ≥ k+1 k > 1 = u(σ∗ , σs). So σ strictly (and thus also weakly) dominates σ∗ , and there exists a solution to the MINIMUM-DOMINATING-SET instance.
Now suppose there exists a solution to the MINIMUMDOMINATING-SET instance. Consider the (at most k) pure strategies of the form σSi on which the dominating mixed strategy σ places positive probability, and let T be the collection of the corresponding subsets Si. We claim that T is a cover. For suppose there is some s ∈ S that is not in any of the subsets in T . Then, if the column player plays σs, the row player (when playing σ) will always receive utility 0-as opposed to the utility of 1 the row player would receive for playing σ∗ , contradicting the fact that σ dominates σ∗ (whether this dominance is weak or strict). It follows that there exists a solution to the SET-COVER instance.
On the other hand, if we require that the dominating strategy only places positive probability on a very small number of pure strategies, then it once again becomes easy to check whether a strategy is dominated. Specifically, to find out whether player i"s strategy σ∗ is dominated by a strategy that places positive probability on only k pure strategies, we can simply check, for every subset of k of player i"s pure strategies, whether there is a strategy that places positive probability only on these k strategies and dominates σ∗ , using Proposition 1. This requires only O(|Σi|k ) such checks. Thus, if k is a constant, this constitutes a polynomial-time algorithm.
A natural question to ask next is whether iterated strict dominance remains computationally easy when dominating strategies are required to place positive probability on at most k pure strategies, where k is a small constant. (We have already shown in Section 4 that iterated weak dominance is hard even when k = 1, that is, only dominance by pure strategies is allowed.) Of course, if iterated strict dominance were path-independent under this restriction, computational easiness would follow as it did in Section 4.
However, it turns out that this is not the case.
Observation 1. If we restrict the dominating strategies to place positive probability on at most two pure strategies, iterated strict dominance becomes path-dependent.
Proof. Consider the following game: 7, 1 0, 0 0, 0 0, 0 7, 1 0, 0 3, 0 3, 0 0, 0 0, 0 0, 0 3, 1 1, 0 1, 0 1, 0 Let (i, j) denote the outcome in which the row player plays the ith row and the column player plays the jth column.
Because (1, 1), (2, 2), and (4, 3) are all Nash equilibria, none of the column player"s pure strategies will ever be eliminated, and neither will rows 1, 2, and 4. We now observe that randomizing uniformly over rows 1 and 2 dominates row 3, and randomizing uniformly over rows 3 and 4 dominates row
to dominate row 5 without randomizing over at least 3 pure strategies.
Indeed, iterated strict dominance turns out to be hard even when k = 3.
Theorem 4. If we restrict the dominating strategies to place positive probability on at most three pure strategies, it becomes NP-complete to decide whether a given strategy can be eliminated using iterated strict dominance. 93 Proof. The problem is in NP because given a sequence of strategies to be eliminated, we can check in polynomial time whether this is a valid sequence of eliminations (for any strategy to be eliminated, we can check, for every subset of three other strategies, whether there is a strategy placing positive probability on only these three strategies that dominates the strategy to be eliminated, using Proposition 1).
To show that the problem is NP-hard, we reduce an arbitrary satisfiability instance (given by a nonempty set of clauses C over a nonempty set of variables V , with corresponding literals L = {+v : v ∈ V } ∪ {−v : v ∈ V }) to the following two-player game.
For every variable v ∈ V , the row player has strategies s+v, s−v, s1 v, s2 v, s3 v, s4 v, and the column player has strategies t1 v, t2 v, t3 v, t4 v. For every clause c ∈ C, the row player has a strategy sc, and the column player has a strategy tc, as well as, for every literal l occurring in c, an additional strategy tl c. The row player has two additional strategies s1 and s2. (s2 is the strategy that we are seeking to eliminate.) Finally, the column player has one additional strategy t1.
The utility function for the row player is given as follows (where is some sufficiently small number): • ur(s+v, tj v) = 4 if j ∈ {1, 2}, for all v ∈ V ; • ur(s+v, tj v) = 1 if j ∈ {3, 4}, for all v ∈ V ; • ur(s−v, tj v) = 1 if j ∈ {1, 2}, for all v ∈ V ; • ur(s−v, tj v) = 4 if j ∈ {3, 4}, for all v ∈ V ; • ur(s+v, t) = ur(s−v, t) = 0 for all v ∈ V and t /∈ {t1 v, t2 v, t3 v, t4 v}; • ur(si v, ti v) = 13 for all v ∈ V and i ∈ {1, 2, 3, 4}; • ur(si v, t) = for all v ∈ V , i ∈ {1, 2, 3, 4}, and t = ti v; • ur(sc, tc) = 2 for all c ∈ C; • ur(sc, t) = 0 for all c ∈ C and t = tc; • ur(s1, t1) = 1 + ; • ur(s1, t) = for all t = t1; • ur(s2, t1) = 1; • ur(s2, tc) = 1 for all c ∈ C; • ur(s2, t) = 0 for all t /∈ {t1} ∪ {tc : c ∈ C}.
The utility function for the column player is given as follows: • uc(si v, ti v) = 1 for all v ∈ V and i ∈ {1, 2, 3, 4}; • uc(s, ti v) = 0 for all v ∈ V , i ∈ {1, 2, 3, 4}, and s = si v; • uc(sc, tc) = 1 for all c ∈ C; • uc(sl, tc) = 1 for all c ∈ C and l ∈ L occurring in c; • uc(s, tc) = 0 for all c ∈ C and s /∈ {sc} ∪ {sl : l ∈ c}; • uc(sc, tl c) = 1 + for all c ∈ C; • uc(sl , tl c) = 1 + for all c ∈ C and l = l occurring in c; • uc(s, tl c) = for all c ∈ C and s /∈ {sc} ∪ {sl : l ∈ c, l = l }; • uc(s2, t1) = 1; • uc(s, t1) = 0 for all s = s2.
We now show that the two instances are equivalent. First, suppose that there is a solution to the satisfiability instance.
Then, consider the following sequence of eliminations in our game: 1. For every variable v that is set to true in the satisfying assignment, eliminate s+v with the mixed strategy σr that places probability 1/3 on s−v, probability 1/3 on s1 v, and probability 1/3 on s2 v. (The expected utility of playing σr against t1 v or t2 v is 14/3 > 4; against t3 v or t4 v, it is 4/3 > 1; and against anything else it is 2 /3 > 0. Hence the dominance is valid.) 2. Similarly, for every variable v that is set to false in the satisfying assignment, eliminate s−v with the mixed strategy σr that places probability 1/3 on s+v, probability 1/3 on s3 v, and probability 1/3 on s4 v. (The expected utility of playing σr against t1 v or t2 v is 4/3 > 1; against t3 v or t4 v, it is 14/3 > 4; and against anything else it is 2 /3 > 0. Hence the dominance is valid.) 3. For every c ∈ C, eliminate tc with any tl c for which l was set to true in the satisfying assignment. (This is a valid dominance because tl c performs better than tc against any strategy other than sl, and we eliminated sl in step 1 or in step 2.) 4.
Finally, eliminate s2 with s1. (This is a valid dominance because s1 performs better than s2 against any strategy other than those in {tc : c ∈ C}, which we eliminated in step 3.) Hence, there is an elimination path that eliminates s2.
Now, suppose that there is an elimination path that eliminates s2. The strategy that eventually dominates s2 must place most of its probability on s1, because s1 is the only other strategy that performs well against t1, which cannot be eliminated before s2. But, s1 performs significantly worse than s2 against any strategy tc with c ∈ C, so it follows that all these strategies must be eliminated first. Each strategy tc can only be eliminated by a strategy that places most of its weight on the corresponding strategies tl c with l ∈ c, because they are the only other strategies that perform well against sc, which cannot be eliminated before tc. But, each strategy tl c performs significantly worse than tc against sl, so it follows that for every clause c, for one of the literals l occurring in it, sl must be eliminated first. Now, strategies of the form tj v will never be eliminated because they are the unique best responses to the corresponding strategies sj v (which are, in turn, the best responses to the corresponding tj v). As a result, if strategy s+v (respectively, s−v) is eliminated, then its opposite strategy s−v (respectively, s+v) can no longer be eliminated, for the following reason. There is no other pure strategy remaining that gets a significant utility against more than one of the strategies t1 v, t2 v, t3 v, t4 v, but s−v (respectively, s+v) gets significant utility against all 4, and therefore cannot be dominated by a mixed strategy placing positive probability on at most 3 strategies. It follows that for each v ∈ V , at most one of the strategies s+v, s−v is eliminated, in such a way that for every clause c, for one of the literals l occurring in it, sl must be eliminated. But then setting all the literals l such that sl is eliminated to true constitutes a solution to the satisfiability instance.
In the next section, we return to the setting where there is no restriction on the number of pure strategies on which a dominating mixed strategy can place positive probability.
BAYESIAN GAMES So far, we have focused on normal form games that are flatly represented (that is, every matrix entry is given ex94 plicitly). However, for many games, the flat representation is too large to write down explicitly, and instead, some representation that exploits the structure of the game needs to be used. Bayesian games, besides being of interest in their own right, can be thought of as a useful structured representation of normal form games, and we will study them in this section.
In a Bayesian game, each player first receives privately held preference information (the player"s type) from a distribution, which determines the utility that that player receives for every outcome of (that is, vector of actions played in) the game. After receiving this type, the player plays an action based on it.7 Definition 5. A Bayesian game is given by a set of players {1, 2, . . . , n}; and, for each player i, a (finite) set of actions Ai, a (finite) type space Θi with a probability distribution πi over it, and a utility function ui : Θi × A1 × A2 × . . . × An → R (where ui(θi, a1, a2, . . . , an) denotes player i"s utility when i"s type is θi and each player j plays action aj). A pure strategy in a Bayesian game is a mapping from types to actions, σi : Θi → Ai, where σi(θi) denotes the action that player i plays for type θi.
Any vector of pure strategies in a Bayesian game defines an (expected) utility for each player, and therefore we can translate a Bayesian game into a normal form game. In this normal form game, the notions of dominance and iterated dominance are defined as before. However, the normal form representation of the game is exponentially larger than the Bayesian representation, because each player i has |Ai||Θi| distinct pure strategies. Thus, any algorithm for Bayesian games that relies on expanding the game to its normal form will require exponential time. Specifically, our easiness results for normal form games do not directly transfer to this setting. In fact, it turns out that checking whether a strategy is dominated by a pure strategy is hard in Bayesian games.
Theorem 5. In a Bayesian game, it is NP-complete to decide whether a given pure strategy σr : Θr → Ar is dominated by some other pure strategy (both for strict and weak dominance), even when the row player"s distribution over types is uniform.
Proof. The problem is in NP because it is easy to verify whether a candidate dominating strategy is indeed a dominating strategy. To show that the problem is NP-hard, we reduce an arbitrary satisfiability instance (given by a set of clauses C using variables from V ) to the following Bayesian game. Let the row player"s action set be Ar = {t, f, 0} and let the column player"s action set be Ac = {ac : c ∈ C}.
Let the row player"s type set be Θr = {θv : v ∈ V }, with a distribution πr that is uniform. Let the row player"s utility function be as follows: • ur(θv, 0, ac) = 0 for all v ∈ V and c ∈ C; • ur(θv, b, ac) = |V | for all v ∈ V , c ∈ C, and b ∈ {t, f} such that setting v to b satisfies c; • ur(θv, b, ac) = −1 for all v ∈ V , c ∈ C, and b ∈ {t, f} such that setting v to b does not satisfy c. 7 In general, a player can also receive a signal about the other players" preferences, but we will not concern ourselves with that here.
Let the pure strategy to be dominated be the one that plays 0 for every type. We show that the strategy is dominated by a pure strategy if and only if there is a solution to the satisfiability instance.
First, suppose there is a solution to the satisfiability instance. Then, let σd r be given by: σd r (θv) = t if v is set to true in the solution to the satisfiability instance, and σd r (θv) = f otherwise. Then, against any action ac by the column player, there is at least one type θv such that either +v ∈ c and σd r (θv) = t, or −v ∈ c and σd r (θv) = f. Thus, the row player"s expected utility against action ac is at least |V | |V | − |V |−1 |V | = 1 |V | > 0. So, σd r is a dominating strategy.
Now, suppose there is a dominating pure strategy σd r .
This dominating strategy must play t or f for at least one type. Thus, against any ac by the column player, there must at least be some type θv for which ur(θv, σd r (θv), ac) > 0.
That is, there must be at least one variable v such that setting v to σd r (θv) satifies c. But then, setting each v to σd r (θv) must satisfy all the clauses. So a satisfying assignment exists.
However, it turns out that we can modify the linear programs from Proposition 1 to obtain a polynomial time algorithm for checking whether a strategy is dominated by a mixed strategy in Bayesian games.
Theorem 6. In a Bayesian game, it can be decided in polynomial time whether a given (possibly mixed) strategy σr is dominated by some other mixed strategy, using linear programming (both for strict and weak dominance).
Proof. We can modify the linear programs presented in Proposition 1 as follows. For strict dominance, again assuming without loss of generality that all the utilities in the game are positive, use the following linear program (in which pσr r (θr, ar) is the probability that σr, the strategy to be dominated, places on ar for type θr): minimize θr∈Θr ar∈Ar pr(ar) such that for any ac ∈ Ac, θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pr(θr, ar) ≥ θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pσr r (θr, ar); for any θr ∈ Θr, ar∈Ar pr(θr, ar) ≤ 1.
Assuming that π(θr) > 0 for all θr ∈ Θr, this program will return an objective value smaller than |Θr| if and only if σr is strictly dominated, by reasoning similar to that done in Proposition 1.
For weak dominance, use the following linear program: maximize ac∈Ac ( θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pr(θr, ar)− θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pσr r (θr, ar)) such that for any ac ∈ Ac, θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pr(θr, ar) ≥ θr∈Θr ar∈Ar π(θr)ur(θr, ar, ac)pσr r (θr, ar); for any θr ∈ Θr, ar∈Ar pr(θr, ar) = 1.
This program will return an objective value greater than
to that done in Proposition 1.
We now turn to iterated dominance in Bayesian games.
Na¨ıvely, one might argue that iterated dominance in Bayesian 95 games always requires an exponential number of steps when a significant fraction of the game"s pure strategies can be eliminated, because there are exponentially many pure strategies. However, this is not a very strong argument because oftentimes we can eliminate exponentially many pure strategies in one step. For example, if for some type θr ∈ Θr we have, for all ac ∈ Ac, that u(θr, a1 r, ac) > u(θr, a2 r, ac), then any pure strategy for the row player which plays action a2 r for type θr is dominated (by the strategy that plays action a1 r for type θr instead)-and there are exponentially many (|Ar||Θr|−1 ) such strategies. It is therefore conceivable that we need only polynomially many eliminations of collections of a player"s strategies. However, the following theorem shows that this is not the case, by giving an example where an exponential number of iterations (that is, alternations between the players in eliminating strategies) is required. (We emphasize that this is not a result about computational complexity.) Theorem 7. Even in symmetric 3-player Bayesian games, iterated dominance by pure strategies can require an exponential number of iterations (both for strict and weak dominance), even with only three actions per player.
Proof. Let each player i ∈ {1, 2, 3} have n + 1 types θ1 i , θ2 i , . . . , θn+1 i . Let each player i have 3 actions ai, bi, ci, and let the utility function of each player be defined as follows. (In the below, i + 1 and i + 2 are shorthand for i + 1(mod 3) and i + 2(mod 3) when used as player indices.
Also, −∞ can be replaced by a sufficiently negative number. Finally, δ and should be chosen to be very small (even compared to 2−(n+1) ), and should be more than twice as large as δ.) • ui(θ1 i ; ai, ci+1, ci+2) = −1; • ui(θ1 i ; ai, si+1, si+2) = 0 for si+1 = ci+1 or si+2 = ci+2; • ui(θ1 i ; bi, si+1, si+2) = − for si+1 = ai+1 and si+2 = ai+2; • ui(θ1 i ; bi, si+1, si+2) = −∞ for si+1 = ai+1 or si+2 = ai+2; • ui(θ1 i ; ci, si+1, si+2) = −∞ for all si+1, si+2; • ui(θj i ; ai, si+1, si+2) = −∞ for all si+1, si+2 when j > 1; • ui(θj i ; bi, si+1, si+2) = − for all si+1, si+2 when j > 1; • ui(θj i ; ci, si+1, ci+2) = δ − − 1/2 for all si+1 when j > 1; • ui(θj i ; ci, si+1, si+2) = δ− for all si+1 and si+2 = ci+2 when j > 1.
Let the distribution over each player"s types be given by p(θj i ) = 2−j (with the exception that p(θ2 i ) = 2−2 +2−(n+1) ).
We will be interested in eliminating strategies of the following form: play bi for type θ1 i , and play one of bi or ci otherwise. Because the utility function is the same for any type θj i with j > 1, these strategies are effectively defined by the total probability that they place on ci,8 which is t2 i (2−2 + 2−(n+1) ) + n+1 j=3 tj i 2−j where tj i = 1 if player i 8 Note that the strategies are still pure strategies; the probability placed on an action by a strategy here is simply the sum of the probabilities of the types for which the strategy chooses that action. plays ci for type θj i , and 0 otherwise. This probability is different for any two different strategies of the given form, and we have exponentially many different strategies of the given form. For any probability q which can be expressed as t2(2−2 + 2−(n+1) ) + n+1 j=3 tj2−j (with all tj ∈ {0, 1}), let σi(q) denote the (unique) strategy of the given form for player i which places a total probability of q on ci. Any strategy that plays ci for type θ1 i or ai for some type θj i with j > 1 can immediately be eliminated. We will show that, after that, we must eliminate the strategies σi(q) with high q first, slowly working down to those with lower q.
Claim 1: If σi+1(q ) and σi+2(q ) have not yet been eliminated, and q < q , then σi(q) cannot yet be eliminated.
Proof: First, we show that no strategy σi(q ) can eliminate σi(q). Against σi+1(q ), σi+2(q ), the utility of playing σi(p) is − + p · δ − p · q /2. Thus, when q = 0, it is best to set p as high as possible (and we note that σi+1(0) and σi+2(0) have not been eliminated), but when q > 0, it is best to set p as low as possible because δ < q /2. Thus, whether q > q or q < q , σi(q) will always do strictly better than σi(q ) against some remaining opponent strategies. Hence, no strategy σi(q ) can eliminate σi(q). The only other pure strategies that could dominate σi(q) are strategies that play ai for type θ1 i , and bi or ci for all other types. Let us take such a strategy and suppose that it plays c with probability p. Against σi+1(q ), σi+2(q ) (which have not yet been eliminated), the utility of playing this strategy is −(q )2 /2 − /2 + p · δ − p · q /2. On the other hand, playing σi(q) gives − + q · δ − q · q /2. Because q > q, we have −(q )2 /2 < −q · q /2, and because δ and are small, it follows that σi(q) receives a higher utility. Therefore, no strategy dominates σi(q), proving the claim.
Claim 2: If for all q > q, σi+1(q ) and σi+2(q ) have been eliminated, then σi(q) can be eliminated. Proof: Consider the strategy for player i that plays ai for type θ1 i , and bi for all other types (call this strategy σi); we claim σi dominates σi(q). First, if either of the other players k plays ak for θ1 k, then σi performs better than σi(q) (which receives −∞ in some cases). Because the strategies for player k that play ck for type θ1 k, or ak for some type θj k with j > 1, have already been eliminated, all that remains to check is that σi performs better than σi(q) whenever both of the other two players play strategies of the following form: play bk for type θ1 k, and play one of bk or ck otherwise. We note that among these strategies, there are none left that place probability greater than q on ck. Letting qk denote the probability with which player k plays ck, the expected utility of playing σi is −qi+1 · qi+2/2 − /2. On the other hand, the utility of playing σi(q) is − + q · δ − q · qi+2/2. Because qi+1 ≤ q, the difference between these two expressions is at least /2 − δ, which is positive. It follows that σi dominates σi(q).
From Claim 2, it follows that all strategies of the form σi(q) will eventually be eliminated. However, Claim 1 shows that we cannot go ahead and eliminate multiple such strategies for one player, unless at least one other player simultaneously keeps up in the eliminated strategies: every time a σi(q) is eliminated such that σi+1(q) and σi+2(q) have not yet been eliminated, we need to eliminate one of the latter two strategies before any σi(q ) with q > q can be eliminated-that is, we need to alternate between players.
Because there are exponentially many strategies of the form σi(q), it follows that iterated elimination will require exponentially many iterations to complete. 96 It follows that an efficient algorithm for iterated dominance (strict or weak) by pure strategies in Bayesian games, if it exists, must somehow be able to perform (at least part of) many iterations in a single step of the algorithm (because if each step only performed a single iteration, we would need exponentially many steps). Interestingly, Knuth et al. [11] argue that iterated dominance appears to be an inherently sequential problem (in light of their result that iterated very weak dominance is P-complete, that is, apparently not efficiently parallelizable), suggesting that aggregating many iterations may be difficult.
While the Nash equilibrium solution concept is studied more and more intensely in our community, the perhaps more elementary concept of (iterated) dominance has received much less attention. In this paper we studied various computational aspects of this concept.
We first studied both strict and weak dominance (not iterated), and showed that checking whether a given strategy is dominated by some mixed strategy can be done in polynomial time using a single linear program solve. We then moved on to iterated dominance. We showed that determining whether there is some path that eliminates a given strategy is NP-complete with iterated weak dominance. This allowed us to also show that determining whether there is a path that leads to a unique solution is NP-complete. Both of these results hold both with and without dominance by mixed strategies. (A weaker version of the second result (only without dominance by mixed strategies) was already known [7].) Iterated strict dominance, on the other hand, is path-independent (both with and without dominance by mixed strategies) and can therefore be done in polynomial time.
We then studied what happens when the dominating strategy is allowed to place positive probability on only a few pure strategies. First, we showed that finding the dominating strategy with minimum support size is NP-complete (both for strict and weak dominance). Then, we showed that iterated strict dominance becomes path-dependent when there is a limit on the support size of the dominating strategies, and that deciding whether a given strategy can be eliminated by iterated strict dominance under this restriction is NP-complete (even when the limit on the support size is 3).
Finally, we studied dominance and iterated dominance in Bayesian games, as an example of a concise representation language for normal form games that is interesting in its own right. We showed that, unlike in normal form games, deciding whether a given pure strategy is dominated by another pure strategy in a Bayesian game is NP-complete (both with strict and weak dominance); however, deciding whether a strategy is dominated by some mixed strategy can still be done in polynomial time with a single linear program solve (both with strict and weak dominance). Finally, we showed that iterated dominance using pure strategies can require an exponential number of iterations in a Bayesian game (both with strict and weak dominance).
There are various avenues for future research. First, there is the open question of whether it is possible to complete iterated dominance in Bayesian games in polynomial time (even though we showed that an exponential number of alternations between the players in eliminating strategies is sometimes required). Second, we can study computational aspects of (iterated) dominance in concise representations of normal form games other than Bayesian games-for example, in graphical games [9] or local-effect/action graph games [12, 2]. (How to efficiently perform iterated very weak dominance has already been studied for partially observable stochastic games [8].) Finally, we can ask whether some of the algorithms we described (such as the one for iterated strict dominance with mixed strategies) can be made faster.
[1] Krzysztof R. Apt. Uniform proofs of order independence for various strategy elimination procedures. Contributions to Theoretical Economics, 4(1), 2004. [2] Nivan A. R. Bhat and Kevin Leyton-Brown. Computing Nash equilibria of action-graph games. In UAI, 2004. [3] Ben Blum, Christian R. Shelton, and Daphne Koller. A continuation method for Nash equilibria in structured games. In IJCAI, 2003. [4] Vincent Conitzer and Tuomas Sandholm. Complexity results about Nash equilibria. In IJCAI, pages 765-771,
[5] Drew Fudenberg and Jean Tirole. Game Theory. MIT Press, 1991. [6] Itzhak Gilboa, Ehud Kalai, and Eitan Zemel. On the order of eliminating dominated strategies. Operations Research Letters, 9:85-89, 1990. [7] Itzhak Gilboa, Ehud Kalai, and Eitan Zemel. The complexity of eliminating dominated strategies.
Mathematics of Operation Research, 18:553-565, 1993. [8] Eric A. Hansen, Daniel S. Bernstein, and Shlomo Zilberstein. Dynamic programming for partially observable stochastic games. In AAAI, pages 709-715, 2004. [9] Michael Kearns, Michael Littman, and Satinder Singh.
Graphical models for game theory. In UAI, 2001. [10] Leonid Khachiyan. A polynomial algorithm in linear programming. Soviet Math. Doklady, 20:191-194, 1979. [11] Donald E. Knuth, Christos H. Papadimitriou, and John N Tsitsiklis. A note on strategy elimination in bimatrix games. Operations Research Letters, 7(3):103-107, 1988. [12] Kevin Leyton-Brown and Moshe Tennenholtz. Local-effect games. In IJCAI, 2003. [13] Richard Lipton, Evangelos Markakis, and Aranyak Mehta.
Playing large games using simple strategies. In ACM-EC, pages 36-41, 2003. [14] Michael Littman and Peter Stone. A polynomial-time Nash equilibrium algorithm for repeated games. In ACM-EC, pages 48-54, 2003. [15] Leslie M. Marx and Jeroen M. Swinkels. Order independence for iterated weak dominance. Games and Economic Behavior, 18:219-245, 1997. [16] Leslie M. Marx and Jeroen M. Swinkels. Corrigendum, order independence for iterated weak dominance. Games and Economic Behavior, 31:324-329, 2000. [17] Andreu Mas-Colell, Michael Whinston, and Jerry R. Green.
Microeconomic Theory. Oxford University Press, 1995. [18] Roger Myerson. Game Theory: Analysis of Conflict.

Endpoints wishing to communicate over a multi-hop network rely on intermediate nodes to forward packets from the sender to the receiver. In settings where the intermediate nodes are independent agents (such as individual nodes in ad-hoc and peer-topeer networks or autonomous systems on the Internet), this poses an incentive problem; the intermediate nodes may incur significant communication and computation costs in the forwarding of packets without deriving any direct benefit from doing so. Consequently, a rational (i.e., utility maximizing) intermediate node may choose to forward packets at a low priority or not forward the packets at all.
This rational behavior may lead to suboptimal system performance.
The endpoints can provide incentives, e.g., in the form of payments, to encourage the intermediate nodes to forward their packets. However, the actions of the intermediate nodes are often hidden from the endpoints. In many cases, the endpoints can only observe whether or not the packet has reached the destination, and cannot attribute failure to a specific node on the path. Even if some form of monitoring mechanism allows them to pinpoint the location of the failure, they may still be unable to attribute the cause of failure to either the deliberate action of the intermediate node, or to some external factors beyond the control of the intermediate node, such as network congestion, channel interference, or data corruption.
The problem of hidden action is hardly unique to networks. Also known as moral hazard, this problem has long been of interest in the economics literature concerning information asymmetry, incentive and contract theory, and agency theory. We follow this literature by formalizing the problem as a principal-agent model, where multiple agents making sequential hidden actions [17, 27].
Our results are threefold. First, we show that it is possible to design contracts to induce cooperation when intermediate nodes can choose to forward or drop packets, as well as when the nodes can choose to forward packets with different levels of quality of service. If the path and transit costs are known prior to transmission, the principal achieves first best solution, and can implement the contracts either directly with each intermediate node or recursively through the network (each node making a contract with the following node) without any loss in utility. Second, we find that introducing per-hop monitoring has no impact on the principal"s expected utility in equilibrium. For a principal who wishes to induce an equilibrium in which all intermediate nodes cooperate, its expected total payment is the same with or without monitoring.
However, monitoring provides a dominant strategy equilibrium, which is a stronger solution concept than the Nash equilibrium achievable in the absence of monitoring. Third, we show that in the absence of a priori information about transit costs on the packet forwarding path, it is possible to generalize existing mechanisms to overcome scenarios that involve both hidden-information and hidden-action.
In these scenarios, the principal pays a premium compared to scenarios with known transit costs.
We consider a principal-agent model, where the principal is a pair of communication endpoints who wish to communicate over a multi-hop network, and the agents are the intermediate nodes capable of forwarding packets between the endpoints. The principal (who in practice can be either the sender, the receiver, or 117 both) makes individual take-it-or-leave-it offers (contracts) to the agents. If the contracts are accepted, the agents choose their actions sequentially to maximize their expected payoffs based on the payment schedule of the contract. When necessary, agents can in turn make subsequent take-it-or-leave-it offers to their downstream agents.
We assume that all participants are risk neutral and that standard assumptions about the global observability of the final outcome and the enforceability of payments by guaranteeing parties hold.
For simplicity, we assume that each agent has only two possible actions; one involving significant effort and one involving little effort. We denote the action choice of agent i by ai ∈ {0, 1}, where ai = 0 and ai = 1 stand for the low-effort and high-effort actions, respectively. Each action is associated with a cost (to the agent) C(ai), and we assume: C(ai = 1) > C(ai = 0) At this stage, we assume that all nodes have the same C(ai) for presentation clarity, but we relax this assumption later. Without loss of generality we normalize the C(ai = 0) to be zero, and denote the high-effort cost by c, so C(ai = 0) = 0 and C(ai = 1) = c.
The utility of agent i, denoted by ui, is a function of the payment it receives from the principal (si), the action it takes (ai), and the cost it incurs (ci), as follows: ui(si, ci, ai) = si − aici The outcome is denoted by x ∈ {xG , xB }, where xG stands for the Good outcome in which the packet reaches the destination, and xB stands for the Bad outcome in which the packet is dropped before it reaches the destination. The outcome is a function of the vector of actions taken by the agents on the path, a = (a1, ..., an) ∈ {0, 1}n , and the loss rate on the channels, k.
The benefit of the sender from the outcome is denoted by w(x), where: w(xG ) = wG ; and w(xB ) = wB = 0 The utility of the sender is consequently: u(x, S) = w(x) − S where: S = Pn i=1 si A sender who wishes to induce an equilibrium in which all nodes engage in the high-effort action needs to satisfy two constraints for each agent i: (IR) Individual rationality (participation constraint)1 : the expected utility from participation should (weakly) exceed its reservation utility (which we normalize to 0). (IC) Incentive compatibility: the expected utility from exerting high-effort should (weakly) exceed its expected utility from exerting low-effort.
In some network scenarios, the topology and costs are common knowledge. That is, the sender knows in advance the path that its packet will take and the costs on that path. In other routing scenarios, the sender does not have this a priori information. We show that our model can be applied to both scenarios with known and unknown topologies and costs, and highlight the implications of each scenario in the context of contracts. We also distinguish between direct contracts, where the principal signs an individual contract 1We use the notion of ex ante individual rationality, in which the agents choose to participate before they know the state of the system.
S Dn1 Source Destination n intermediate nodes Figure 1: Multi-hop path from sender to destination.
Figure 2: Structure of the multihop routing game under known topology and transit costs. with each node, and recursive contracts, where each node enters a contractual relationship with its downstream node.
The remainder of this paper is organized as follows. In Section 3 we consider agents who decide whether to drop or forward packets with and without monitoring when the transit costs are common knowledge. In Section 4, we extend the model to scenarios with unknown transit costs. In Section 5, we distinguish between recursive and direct contracts and discuss their relationship. In Section 6, we show that the model applies to scenarios in which agents choose between different levels of quality of service. We consider Internet routing as a case study in Section 7. In Section 8 we present related work, and Section 9 concludes the paper.
In this section we analyze scenarios in which the principal knows in advance the nodes on the path to the destination and their costs, as shown in figure 1. We consider agents who decide whether to drop or forward packets, and distinguish between scenarios with and without monitoring.
In this scenario, the agents decide whether to drop (a = 0) or forward (a = 1) packets. The principal uses no monitoring to observe per-hop outcomes. Consequently, the principal makes the payment schedule to each agent contingent on the final outcome, x, as follows: si(x) = (sB i , sG i ) where: sB i = si(x = xB ) sG i = si(x = xG ) The timeline of this scenario is shown in figure 2. Given a perhop loss rate of k, we can express the probability that a packet is successfully delivered from node i to its successor i + 1 as: Pr(xG i→i+1|ai) = (1 − k)ai (1) where xG i→j denotes a successful transmission from node i to j.
PROPOSITION 3.1. Under the optimal contract that induces high-effort behavior from all intermediate nodes in the Nash Equi118 librium2 , the expected payment to each node is the same as its expected cost, with the following payment schedule: sB i = si(x = xB ) = 0 (2) sG i = si(x = xG ) = c (1 − k)n−i+1 (3) PROOF. The principal needs to satisfy the IC and IR constraints for each agent i, which can be expressed as follows: (IC)Pr(xG |aj≥i = 1)sG i + Pr(xB |aj≥i = 1)sB i − c ≥ Pr(xG |ai = 0, aj>i = 1)sG i + Pr(xB |ai = 0, aj>i = 1)sB i (4) This constraint says that the expected utility from forwarding is greater than or equal to its expected utility from dropping, if all subsequent nodes forward as well. (IR)Pr(xG S→i|aj<i = 1)(Pr(xG |aj≥i = 1)sG i + Pr(xB |aj≥i = 1)sB i − c) + Pr(xB S→i|aj<i = 1)sB i ≥ 0 (5) This constraint says that the expected utility from participating is greater than or equal to zero (reservation utility), if all other nodes forward.
The above constraints can be expressed as follows, based on Eq. 1: (IC) : (1 − k)n−i+1 sG i + (1 − (1 − k)n−i+1 )sB i − c ≥ sB i (IR) : (1−k)i ((1−k)n−i+1 sG i +(1−(1−k)n−i+1 )sB i −c)+ (1 − (1 − k)i )sB i ≥ 0 It is a standard result that both constraints bind at the optimal contract (see [23]). Solving the two equations, we obtain the solution that is presented in Eqs. 2 and 3.
We next prove that the expected payment to a node equals its expected cost in equilibrium. The expected cost of node i is its transit cost multiplied by the probability that it faces this cost (i.e., the probability that the packet reaches node i), which is: (1 − k)i c.
The expected payment that node i receives is: Pr(xG )sG i + Pr(xB )sB i = (1 − k)n+1 c (1 − k)n−i+1 = (1 − k)i c Note that the expected payment to a node decreases as the node gets closer to the destination due to the asymmetric distribution of risk. The closer the node is to the destination, the lower the probability that a packet will fail to reach the destination, resulting in the low payment being made to the node.
The expected payment by the principal is: E[S] = (1 − k)n+1 nX i=1 sG i + (1 − (1 − k)n+1 ) nX i=1 sB i = (1 − k)n+1 nX i=1 ci (1 − k)n−i+1 (6) The expected payment made by the principal depends not only on the total cost, but also the number of nodes on the path.
PROPOSITION 3.2. Given two paths with respective lengths of n1 and n2 hops, per-hop transit costs of c1 and c2, and per-hop loss rates of k1 and k2, such that: 2Since transit nodes perform actions sequentially, this is really a subgameperfect equilibrium (SPE), but we will refer to it as Nash equilibrium in the remainder of the paper.
Figure 3: Two paths of equal total costs but different lengths and individual costs. • c1n1 = c2n2 (equal total cost) • (1 − k1)n1+1 = (1 − k2)n2+1 (equal expected benefit) • n1 < n2 (path 1 is shorter than path 2) the expected total payment made by the principal is lower on the shorter path.
PROOF. The expected payment in path j is E[S]j = nj X i=1 cj (1 − kj )i = cj (1 − kj )
kj So, we have to show that: c1(1 − k1)
k1 > c2(1 − k2)
k2 Let M = c1n1 = c2n2 and N = (1 − k1)n1+1 = (1 − k2)n2+1 .
We have to show that MN 1 n1+1 (1 − N n1 n1+1 ) n1(1 − N 1 n1+1 ) < MN 1 n2+1 (1 − N n2 n2+1 ) n2(1 − N 1 n2+1 ) (7) Let f = N 1 n+1 (1 − N n n+1 ) n(1 − N 1 n+1 ) Then, it is enough to show that f is monotonically increasing in n ∂f ∂n = g(N, n) h(N, n) where: g(N, n) = −((ln(N)n − (n + 1)2 )(N 1 n+1 − N n+2 n+1 ) − (n + 1)2 (N + N 2 n+1 )) and h(N, n) = (n + 1)2 n2 (−1 + N 1 n+1 )2 but h(N, n) > 0 ∀N, n, therefore, it is enough to show that g(N, n) > 0. Because N ∈ (0, 1): (i) ln(N) < 0, and (ii) N 1 n+1 > N n+2 n+1 . Therefore, g(N, n) > 0 ∀N, n.
This means that, ceteris paribus, shorter paths should always be preferred over longer ones.
For example, consider the two topologies presented in Figure 3.
While the paths are of equal total cost, the total expected payment by the principal is different. Based on Eqs. 2 and 3, the expected total payment for the top path is: E[S] = Pr(xG )(sG A + sG B) = „ c1 (1 − k1)2 + c1
« (1 − k1)3 (8) 119 while the expect total payment for the bottom path is: E[S] = Pr(xG )(sG A + sG B + sG C ) = ( c2 (1 − k2)3 + c2 (1 − k2)2 + c2
)(1 − k2)4 For n1 = 2, c1 = 1.5, k1 = 0.5, n2 = 3, c2 = 1, k2 = 0.405, we have equal total cost and equal expected benefit, but E[S]1 =
Suppose the principal obtains per-hop monitoring information.3 Per-hop information broadens the set of mechanisms the principal can use. For example, the principal can make the payment schedule contingent on arrival to the next hop instead of arrival to the final destination. Can such information be of use to a principal wishing to induce an equilibrium in which all intermediate nodes forward the packet?
PROPOSITION 3.3. In the drop versus forward model, the principal derives the same expected utility whether it obtains per-hop monitoring information or not.
PROOF. The proof to this proposition is already implied in the findings of the previous section. We found that in the absence of per-hop information, the expected cost of each intermediate node equals its expected payment. In order to satisfy the IR constraint, it is essential to pay each intermediate node an expected amount of at least its expected cost; otherwise, the node would be better-off not participating. Therefore, no other payment scheme can reduce the expected payment from the principal to the intermediate nodes. In addition, if all nodes are incentivized to forward packets, the probability that the packet reaches the destination is the same in both scenarios, thus the expected benefit of the principal is the same.
Indeed, we have found that even in the absence of per-hop monitoring information, the principal achieves first-best solution.
To convince the reader that this is indeed the case, we provide an example of a mechanism that conditions payments on arrival to the next hop. This is possible only if per-hop monitoring information is provided. In the new mechanism, the principal makes the payment schedule contingent on whether the packet has reached the next hop or not. That is, the payment to node i is sG i if the packet has reached node i + 1, and sB i otherwise. We assume costless monitoring, giving us the best case scenario for the use of monitoring. As before, we consider a principal who wishes to induce an equilibrium in which all intermediate nodes forward the packet.
The expected utility of the principal is the difference between its expected benefit and its expected payment. Because the expected benefit when all nodes forward is the same under both scenarios, we only need to show that the expected total payment is identical as well. Under the monitoring mechanism, the principal has to satisfy the following constraints: (IC)Pr(xG i→i+1|ai = 1)sG + Pr(xB i→i+1|ai = 1)sB − c ≥ Pr(xG i→i+1|ai = 0)sG + Pr(xB i→i+1|ai = 0)sB (9) (IR)Pr(xG S→i|aj<i = 1)(Pr(xG i→i+1|ai = 1)sG + Pr(xB i→i+1|ai = 1)sB − c) ≥ 0 (10) 3For a recent proposal of an accountability framework that provides such monitoring information see [4].
These constraints can be expressed as follows: (IC) : (1 − k)sG + ksB − c ≥ s0 (IR) : (1 − k)i ((1 − k)sG + ksB − c) ≥ 0 The two constraints bind at the optimal contract as before, and we get the following payment schedule: sB = 0 sG = c
The expected total payment under this scenario is: E[S] = nX i=1 ((1 − k)i (sB + (i − 1)sG )) + (1 − k)n+1 nsG = (1 − k)n+1 nX i=1 ci (1 − k)n−i+1 as in the scenario without monitoring (see Equation 6.) While the expected total payment is the same with or without monitoring, there are some differences between the two scenarios.
First, the payment structure is different. If no per-hop monitoring is used, the payment to each node depends on its location (i). In contrast, monitoring provides us with n identical contracts.
Second, the solution concept used is different. If no monitoring is used, the strategy profile of ai = 1 ∀i is a Nash equilibrium, which means that no agent has an incentive to deviate unilaterally from the strategy profile. In contrast, with the use of monitoring, the action chosen by node i is independent of the other agents" forwarding behavior. Therefore, monitoring provides us with dominant strategy equilibrium, which is a stronger solution concept than Nash equilibrium. [15], [16] discuss the appropriateness of different solution concepts in the context of online environments.
In certain network settings, the transit costs of nodes along the forwarding path may not be common knowledge, i.e., there exists the problem of hidden information. In this section, we address the following questions:
behavior in the presence of both hidden-action and hiddeninformation?
the transit costs?
In hidden-information problems, the principal employs mechanisms to induce truthful revelation of private information from the agents. In the routing game, the principal wishes to extract transit cost information from the network routers in order to determine the lowest cost path (LCP) for a given source-destination pair. The network routers act strategically and declare transit costs to maximize their profit. Mechanisms that have been proposed in the literature for the routing game [24, 13] assume that once the transit costs have been obtained, and the LCP has been determined, the nodes on the LCP obediently forward all packets, and that there is no loss in the network, i.e., k = 0. In this section, we consider both hidden information and hidden action, and generalize these mechanisms to induce both truth revelation and high-effort action in equilibrium, where nodes transmit over a lossy communication channel, i.e., k ≥ 0.
In their seminal paper [24], Nisan and Ronen present a VCG mechanism that induces truthful revelation of transit costs by edges 120 Figure 4: Game structure for F P SS, where only hidden-information is considered.
Figure 5: Game structure for F P SS , where both hiddeninformation and hidden-action are considered. in a biconnected network, such that lowest cost paths can be chosen. Like all VCG mechanisms, it is a strategyproof mechanism, meaning that it induces truthful revelation in a dominant strategy equilibrium. In [13] (FPSS), Feigenbaum et al. slightly modify the model to have the routers as the selfish agents instead of the edges, and present a distributed algorithm that computes the VCG payments. The timeline of the FPSS game is presented in figure 4. Under FPSS, transit nodes keep track of the amount of traffic routed through them via counters, and payments are periodically transferred from the principals to the transit nodes based on the counter values. FPSS assumes that transit nodes are obedient in packet forwarding behavior, and will not update the counters without exerting high effort in packet forwarding.
In this section, we present FPSS , which generalizes FPSS to operate in an environment with lossy communication channels (i.e., k ≥ 0) and strategic behavior in terms of packet forwarding. We will show that FPSS induces an equilibrium in which all nodes truthfully reveal their transit costs and forward packets if they are on the LCP. Figure 5 presents the timeline of FPSS . In the first stage, the sender declares two payment functions, (sG i , sB i ), that will be paid upon success or failure of packet delivery. Given these payments, nodes have incentive to reveal their costs truthfully, and later to forward packets. Payments are transferred based on the final outcome.
In FPSS , each node i submits a bid bi, which is its reported transit cost. Node i is said to be truthful if bi = ci. We write b for the vector (b1, . . . , bn) of bids submitted by all transit nodes. Let Ii(b) be the indicator function for the LCP given the bid vector b such that Ii(b) = 
Following FPSS [13], the payment received by node i at equilibrium is: pi = biIi(b) + [ X r Ir(b|i ∞)br − X r Ir(b)br] = X r Ir(b|i ∞)br − X r=i Ir(b)br (11) where the expression b|i x means that (b|i x)j = cj for all j = i, and (b|i x)i = x.
In FPSS , we compute sB i and sG i as a function of pi, k, and n. First, we recognize that sB i must be less than or equal to zero in order for the true LCP to be chosen. Otherwise, strategic nodes may have an incentive to report extremely small costs to mislead the principal into believing that they are on the LCP. Then, these nodes can drop any packets they receive, incur zero transit cost, collect a payment of sB i > 0, and earn positive profit.
PROPOSITION 4.1. Let the payments of FPSS be: sB i = 0 sG i = pi (1 − k)n−i+1 Then, FPSS has a Nash equilibrium in which all nodes truthfully reveal their transit costs and all nodes on the LCP forward packets.
PROOF. In order to prove the proposition above, we have to show that nodes have no incentive to engage in the following misbehaviors:
If all nodes truthfully reveal their costs and forward packets, the expected utility of node i on the LCP is: E[u]i = Pr(xG S→i)(E[si] − ci) + Pr(xB S→i)sB i = (1 − k)i  (1 − k)n−i+1 sG i + (1 − (1 − k)n−i+1 )sB i − ci  + (1 − (1 − k)i )sB i = (1 − k)i (1 − k)n−i+1 pi (1 − k)n−i+1 − (1 − k)i ci = (1 − k)i (pi − ci) ≥ 0 (12) The last inequality is derived from the fact that FPSS is a truthful mechanism, thus pi ≥ ci. The expected utility of a node not on the LCP is 0.
A node that drops a packet receives sB i = 0, which is smaller than or equal to E[u]i for i ∈ LCP and equals E[u]i for i /∈ LCP.
Therefore, nodes cannot gain utility from misbehaviors (1) or (3).
We next show that nodes cannot gain utility from misbehavior (2).
(a) if it reports bi > ci: i. if bi < P r Ir(b|i ∞)br − P r=i Ir(b)br, it is still on the LCP, and since the payment is independent of bi, its utility does not change. ii. if bi > P r Ir(b|i ∞)br − P r=i Ir(b)br, it will not be on the LCP and obtain E[u]i = 0, which is less than its expected utility if truthfully revealing its cost. 121 (b) if it reports bi < ci, it is still on the LCP, and since the payment is independent of bi, its utility does not change.
(a) if it reports bi > ci, it remains out of the LCP, so its utility does not change. (b) if it reports bi < ci: i. if bi < P r Ir(b|i ∞)br − P r=i Ir(b)br, it joins the LCP, and gains an expected utility of E[u]i = (1 − k)i (pi − ct) However, if i /∈ LCP, it means that ci > X r Ir(c|i ∞)cr − X r=i Ir(c)cr But if all nodes truthfully reveal their costs, pi = X r Ir(c|i ∞)cr − X r=i Ir(c)cr < ci therefore, E[u]i < 0 ii. if bi > P r Ir(b|i ∞)br − P r=i Ir(b)br, it remains out of the LCP, so its utility does not change.
Therefore, there exists an equilibrium in which all nodes truthfully reveal their transit costs and forward the received packets.
We note that in the hidden information only context, FPSS induces truthful revelation as a dominant strategy equilibrium. In the current setting with both hidden information and hidden action,
FPSS achieves a Nash equilibrium in the absence of per-hop monitoring, and a dominant strategy equilibrium in the presence of per-hop monitoring, consistent with the results in section 3 where there is hidden action only. In particular, with per-hop monitoring, the principal declares the payments sB i and sG i to each node upon failure or success of delivery to the next node. Given the payments sB i = 0 and sG i = pi/(1 − k), it is a dominant strategy for the nodes to reveal costs truthfully and forward packets.
More generally, for any mechanism M that induces a bid vector b in equilibrium by making a payment of pi(b) to node i on the LCP, there exists a mechanism M that induces an equilibrium with the same bid vector and packet forwarding by making a payment of: sB i = 0 sG i = pi(b) (1 − k)n−i+1 .
A sketch of the proof would be as follows:
i (b) = IM i (b)∀i, since M uses the same choice metric.
k)i (pi(b) − ci) ≥ 0 if it forwards and 0 if it drops, and the expected utility of a non-LCP node is 0.
utility by deviating from bi under M , it can also increase its utility by deviating from bi in M, but this is in contradiction to bi being an equilibrium in M.
expected utility of 0 if they do.
In addition to the generalization of FPSS into FPSS , we can also consider the generalization of the first-price auction (FPA) mechanism, where the principal determines the LCP and pays each node on the LCP its bid, pi(b) = bi. First-price auctions achieve Nash equilibrium as opposed to dominant strategy equilibrium.
Therefore, we should expect the generalization of FPA to achieve Nash equilibrium with or without monitoring.
We make two additional comments concerning this class of mechanisms. First, we find that the expected total payment made by the principal under the proposed mechanisms is E[S] = nX i=1 (1 − k)i pi(b) and the expected benefit realized by the principal is E[w] = (1 − k)n+1 wG where Pn i=1 pi and wG are the expected payment and expected benefit, respectively, when only the hidden-information problem is considered. When hidden action is also taken into consideration, the generalized mechanism handles strategic forwarding behavior by conditioning payments upon the final outcome, and accounts for lossy communication channels by designing payments that reflect the distribution of risk. The difference between expected payment and benefit is not due to strategic forwarding behavior, but to lossy communications. Therefore, in a lossless network, we should not see any gap between expected benefits and payments, independent of strategic or non-strategic forwarding behavior.
Second, the loss to the principal due to unknown transit costs is also known as the price of frugality, and is an active field of research [2, 12]. This price greatly depends on the network topology and on the mechanism employed. While it is simple to characterize the principal"s loss in some special cases, it is not a trivial problem in general. For example, in topologies with parallel disjoint paths from source to destination, we can prove that under first-price auctions, the loss to the principal is the difference between the cost of the shortest path and the second-shortest path, and the loss is higher under the FPSS mechanism.
In this section, we distinguish between direct and recursive contracts. In direct contracts, the principal contracts directly with each node on the path and pays it directly. In recursive payment, the principal contracts with the first node on the path, which in turn contracts with the second, and so on, such that each node contracts with its downstream node and makes the payment based on the final result, as demonstrated in figure 6.
With direct payments, the principal needs to know the identity and cost of each node on the path and to have some communication channel with the node. With recursive payments, every node needs to communicate only with its downstream node. Several questions arise in this context: • What knowledge should the principal have in order to induce cooperative behavior through recursive contracts? • What should be the structure of recursive contracts that induce cooperative behavior? • What is the relation between the total expected payment under direct and recursive contracts? • Is it possible to design recursive contracts in scenarios of unknown transit costs? 122 Figure 6: Structure of the multihop routing game under known topology and recursive contracts.
In order to answer the questions outlined above, we look at the IR and IC constraints that the principal needs to satisfy when contracting with the first node on the path. When the principal designs a contract with the first node, he should take into account the incentives that the first node should provide to the second node, and so on all the way to the destination.
For example, consider the topology given in figure 3 (a). When the principal comes to design a contract with node A, he needs to consider the subsequent contract that A should sign with B, which should satisfy the following constrints. (IR) :Pr(xG A→B|aA = 1)(E[s|aB = 1] − c)+ Pr(xB A→B|aA = 1)sB A→B ≥ 0 (IC) :E[s|aB = 1] − c ≥ E[s|aB = 0] where: E[s|aB = 1] = Pr(xG B→D|aB = 1)sG A→B + Pr(xB B→D|aB = 1)sB A→B and E[s|aB = 0] = Pr(xG B→D|aB = 0)sG A→B + Pr(xB B→D|aB = 0)sB A→B These (binding) constraints yield the values of sB A→B and sG A→B: sB A→B = 0 sG A→B = c/(1 − k) Based on these values, S can express the constraints it should satisfy in a contract with A. (IR) :Pr(xG S→A|aS = 1)(E[sS→A − sA→B|ai = 1∀i] − c) + Pr(xB S→A|aS = 1)sB S→A ≥ 0 (IC) : E[sS→A − sA→B|ai = 1∀i] − c ≥ E[sS→A − sA→B|aA = 0, aB = 1] where: E[sS→A − sA→B|ai = 1∀i] = Pr(xG A→D|ai = 1∀i)(sG S→A − sG A→B) +Pr(xB A→D|ai = 1∀i)(sB S→A − sB A→B) and E[sS→A − sA→B|aA = 0, aB = 1] = Pr(xG A→D|aA = 0, aB = 1)(sG S→A − sG A→B) +Pr(xB A→D|aA = 0, aB = 1)(sB S→A − sB A→B) Solving for sB S→A and sG S→A, we get: sB S→A = 0 sG S→A = c(2 − k)
The expected total payment is E[S] = sG S→APr(xG S→D) + sB S→APr(xB S→D) = c(2 − k)(1 − k) (13) which is equal to the expected total payment under direct contracts (see Eq. 8).
PROPOSITION 5.1. The expected total payments by the principal under direct and recursive contracts are equal.
PROOF. In order to calculate the expected total payment, we have to find the payment to the first node on the path that will induce appropriate behavior. Because sB i = 0 in the drop / forward model, both constraints can be reduced to: Pr(xG i→R|aj = 1∀j)(sG i − sG i+1) − ci = 0 ⇔ (1 − k)n−i+1 (sG i − sG i+1) − ci = 0 which yields, for all 1 ≤ i ≤ n: sG i = ci (1 − k)n−i+1 + sG i+1 Thus, sG n = cn
sG n−1 = cn−1 (1 − k)2 + sG n = cn−1 (1 − k)2 + cn
· · · sG
c1 (1 − k)n + sG
nX i=1 ci (1 − k)i and the expected total payment is E[S] = (1 − k)n+1 sG
nX i=1 ci (1 − k)n−i+1 which equals the total expected payment in direct payments, as expressed in Eq. 6.
Because the payment is contingent on the final outcome, and the expected payment to a node equals its expected cost, nodes have no incentive to offer their downstream nodes lower payment than necessary, since if they do, their downstream nodes will not forward the packet.
What information should the principal posess in order to implement recursive contracts? Like in direct payments, the expected payment is not affected solely by the total payment on the path, but also by the topology. Therefore, while the principal only needs to communicate with the first node on the forwarding path and does not have to know the identities of the other nodes, it still needs to know the number of nodes on the path and their individual transit costs.
Finally, is it possible to design recursive contracts under unknown transit costs, and, if so, what should be the structure of such contracts? Suppose the principal has implemented the distributed algorithm that calculates the necessary payments, pi for truthful 123 revelation, would the following payment schedule to the first node induce cooperative behavior? sB
sG
nX i=1 pi (1 − k)i The answer is not clear. Unlike contracts in known transit costs, the expected payment to a node usually exceeds its expected cost.
Therefore, transit nodes may not have the appropriate incentive to follow the principal"s guarantee during the payment phase. For example, in FPSS , the principal guarantees to pay each node an expected payment of pi > ci. We assume that payments are enforceable if made by the same entity that pledge to pay. However, in the case of recursive contracts, the entity that pledges to pay in the cost discovery stage (the principal) is not the same as the entity that defines and executes the payments in the forwarding stage (the transit nodes). Transit nodes, who design the contracts in the second stage, know that their downstream nodes will forward the packet as long as the expected payment exceeds the expected cost, even if it is less than the promised amount. Thus, every node has incentive to offer lower payments than promised and keep the profit. Transit nodes, who know this is a plausible scenario, may no longer truthfully reveal their cost. Therefore, while recursive contracts under known transit costs are strategically equivalent to direct contracts, it is not clear whether this is the case under unknown transit costs.
LOW-QUALITY FORWARDING So far, we have considered the agents" strategy space to be limited to the drop (a = 0) and forward (a = 1) actions. In this section, we consider a variation of the model where the agents choose between providing a low-quality service (a = 0) and a high-quality service (a = 1).
This may correspond to a service-differentiated service model where packets are forwarded on a best-effort or a priority basis [6].
In contrast to drop versus forward, a packet may still reach the next hop (albeit with a lower probability) even if the low-effort action is taken.
As a second example, consider the practice of hot-potato routing in inter-domain routing of today"s Internet. Individual autonomous systems (AS"s) can either adopt hot-potato routing or early exit routing (a = 0), where a packet is handed off to the downstream AS at the first possible exit, or late exit routing (a = 1), where an AS carries the packet longer than it needs to, handing off the packet at an exit closer to the destination. In the absence of explicit incentives, it is not surprising that AS"s choose hot-potato routing to minimize their costs, even though it leads to suboptimal routes [28, 29].
In both examples, in the absence of contracts, a rational node would exert low-effort, resulting in lower performance.
Nevertheless, this behavior can be avoided with an appropriate design of contracts.
Formally, the probability that a packet successfully gets from node i to node i + 1 is: Pr(xG i→i+1|ai) = 1 − (k − qai) (14) where: q ∈ (0, 1] and k ∈ (q, 1] In the drop versus forward model, a low-effort action by any node results in a delivery failure. In contrast, a node in the high/low scenario may exert low-effort and hope to free-ride on the higheffort level exerted by the other agents.
PROPOSITION 6.1. In the high-quality versus low-quality forwarding model, where transit costs are common knowledge, the principal derives the same expected utility whether it obtains perhop monitoring information or not.
PROOF. The IC and IR constraints are the same as specified in the proof of proposition 3.1, but their values change, based on Eq. 14 to reflect the different model: (IC) : (1−k +q)n−i+1 sG i +(1−(1−k +q)n−i+1 )sB i −c ≥ (1 − k)(1 − k + q)n−i sG i + (1 − (1 − k)(1 − k + q)n−i )sB i (IR) : (1 − k + q)i ((1 − k + q)n−i+1 sG i +(1 − (1 − k + q)n−i+1 )sB i − c) + (1 − (1 − k + q)i )sB i ≥ 0 For this set of constraints, we obtain the following solution: sB i = (1 − k + q)i c(k − 1) q (15) sG i = (1 − k + q)i c(k − 1 + (1 − k + q)−n ) q (16) We observe that in this version, both the high and the low payments depend on i. If monitoring is used, we obtain the following constraints: (IC) : (1 − k + q)sG i + (k − q)sB i − c ≥ (1 − k)sG i + (k)sB i (IR) : (1 − k + q)i ((1 − k + q)sG i + (k − q)sB i − c) ≥ 0 and we get the solution: sB i = c(k − 1) q sG i = ck q The expected payment by the principal with or without forwarding is the same, and equals: E[S] = c(1 − k + q)(1 − (1 − k + q)n ) k − q (17) and this concludes the proof.
The payment structure in the high-quality versus low-quality forwarding model is different from that in the drop versus forward model. In particular, at the optimal contract, the low-outcome payment sB i is now less than zero. A negative payment means that the agent must pay the principal in the event that the packet fails to reach the destination. In some settings, it may be necessary to impose a limited liability constraint, i.e., si ≥ 0. This prevents the first-best solution from being achieved.
PROPOSITION 6.2. In the high-quality versus low-quality forwarding model, if negative payments are disallowed, the expected payment to each node exceeds its expected cost under the optimal contract.
PROOF. The proof is a direct outcome of the following statements, which are proved above:
and 16
cost
i = (1−k+q)i c(k−1) q < 0 Therefore, under any other contract the sender will have to compensate each node with an expected payment that is higher than its expected transit cost. 124 There is an additional difference between the two models. In drop versus forward, a principal either signs a contract with all n nodes along the path or with none. This is because a single node dropping the packet determines a failure. In contrast, in high versus low-quality forwarding, a success may occur under the low effort actions as well, and payments are used to increase the probability of success. Therefore, it may be possible for the principal to maximize its utility by contracting with only m of the n nodes along the path. While the expected outcome depends on m, it is independent of which specific m nodes are induced. At the same time, the individual expected payments decrease in i (see Eq. 16). Therefore, a principal who wishes to sign a contract with only m out of the n nodes should do so with the nodes that are closest to the destination; namely, nodes (n − m + 1, ..., n − 1, n).
Solving for the high-quality versus low-quality forwarding model with unknown transit costs is left for future work.
We can map different deployed and proposed Internet routing schemes to the various models we have considered in this work.
Border Gateway Protocol (BGP), the current inter-domain routing protocol in the Internet, computes routes based on path vectors.
Since the protocol reveals only the autonomous systems (AS"s) along a route but not the cost associated to them, the current BGP routing is best characterized by lack of a priori information about transit costs. In this case, the principal (e.g., a multi-homed site or a tier-1 AS) can implement one of the mechanisms proposed in Section 4 by contracting with individual nodes on the path. Such contracts involve paying some premium over the real cost, and it is not clear whether recursive contacts can be implemented in this scenario. In addition, the current protocol does not have the infrastructure to support implementation of direct contracts between endpoints and the network.
Recently, several new architectures have been proposed in the context of the Internet to provide the principal not only with a set of paths from which it can chose (like BGP does) but also with the performance along those paths and the network topology. One approach to obtain such information is through end-to-end probing [1]. Another approach is to have the edge networks perform measurements and discover the network topology [32]. Yet another approach is to delegate the task of obtaining topology and performance information to a third-party, like in the routing-as-a-service proposal [21]. These proposals are quite different in nature, but they are common in their attempt to provide more visibility and transparency into the network. If information about topology and transit costs is obtained, the scenario is mapped to the known transit costs model (Section 3). In this case, first-best contracts can be achieved through individual contracts with nodes along the path.
However, as we have shown in Section 5, as long as each agent can chose the next hop, the principal can gain full benefit by contracting with only the first hop (through the implementation of recursive contracts).
However, the various proposals for acquiring network topology and performance information do not deal with strategic behavior by the intermediate nodes. With the realization that the information collected may be used by the principal in subsequent contractual relationships, the intermediate nodes may behave strategically, misrepresenting their true costs to the entities that collect and aggregate such information. One recent approach that can alleviate this problem is to provide packet obituaries by having each packet to confirm its delivery or report its last successful AS hop [4].
Another approach is to have third parties like Keynote independently monitor the network performance.
The study of non-cooperative behavior in communication networks, and the design of incentives, has received significant attention in the context of wireless ad-hoc routing. [22] considers the problem of malicious behavior, where nodes respond positively to route requests but then fail to forward the actual packets. It proposes to mitigate it by detection and report mechanisms that will essentially help to route around the malicious nodes. However, rather than penalizing nodes that do not forward traffic, it bypasses the misbehaving nodes, thereby relieving their burden. Therefore, such a mechanism is not effective against selfish behavior.
In order to mitigate selfish behavior, some approaches [7, 8, 9] require reputation exchange between nodes, or simply first-hand observations [5]. Other approaches propose payment schemes [10, 20, 31] to encourage cooperation. [31] is the closest to our work in that it designs payment schemes in which the sender pays the intermediate nodes in order to prevent several types of selfish behavior.
In their approach, nodes are supposed to send receipts to a thirdparty entity. We show that this type of per-hop monitoring may not be needed.
In the context of Internet routing, [4] proposes an accountability framework that provide end hosts and service providers after-thefact audits on the fate of their packets. This proposal is part of a broader approach to provide end hosts with greater control over the path of their packets [3, 30]. If senders have transit cost information and can fully control the path of their packets, they can design contracts that yield them with first-best utility. The accountability framework proposed in [4] can serve two main goals: informing nodes of network conditions to help them make informed decision, and helping entities to establish whether individual ASs have performed their duties adequately. While such a framework can be used for the first task, we propose a different approach to the second problem without the need of per-hop auditing information.
Research in distributed algorithmic mechanism design (DAMD) has been applied to BGP routing [13, 14]. These works propose mechanisms to tackle the hidden-information problem, but ignore the problem of forwarding enforcement. Inducing desired behavior is also the objective in [26], which attempts to respond to the challenge of distributed AMD raised in [15]: if the same agents that seek to manipulate the system also run the mechanism, what prevents them from deviating from the mechanism"s proposed rules to maximize their own welfare? They start with the proposed mechanism presented in [13] and use mostly auditing mechanisms to prevent deviation from the algorithm.
The focus of this work is the design of a payment scheme that provides the appropriate incentives within the context of multi-hop routing. Like other works in this field, we assume that all the accounting services are done using out-of-band mechanisms.
Security issues within this context, such as node authentication or message encryption, are orthogonal to the problem presented in this paper, and can be found, for example, in [18, 19, 25].
The problem of information asymmetry and hidden-action (also known as moral hazard) is well studied in the economics literature [11, 17, 23, 27]. [17] identifies the problem of moral hazard in production teams, and shows that it is impossible to design a sharing rule which is efficient and budget-balanced. [27] shows that this task is made possible when production takes place sequentially.
DIRECTIONS In this paper we show that in a multi-hop routing setting, where the actions of the intermediate nodes are hidden from the source 125 and/or destination, it is possible to design payment schemes to induce cooperative behavior from the intermediate nodes. We conclude that monitoring per-hop outcomes may not improve the utility of the participants or the network performace. In addition, in scenarios of unknown transit costs, it is also possible to design mechanisms that induce cooperative behavior in equilibrium, but the sender pays a premium for extracting information from the transit nodes.
Our model and results suggest several natural and intriguing research avenues: • Consider manipulative or collusive behaviors which may arise under the proposed payment schemes. • Analyze the feasibility of recursive contracts under hiddeninformation of transit costs. • While the proposed payment schemes sustain cooperation in equilibrium, it is not a unique equilibrium. We plan to study under what mechanisms this strategy profile may emerge as a unique equilibrium (e.g., penalty by successor nodes). • Consider the effect of congestion and capacity constraints on the proposed mechanisms. Our preliminary results show that when several senders compete for a single transit node"s capacity, the sender with the highest demand pays a premium even if transit costs are common knowledge. The premium can be expressed as a function of the second-highest demand.
In addition, if congestion affects the probability of successful delivery, a sender with a lower cost alternate path may end up with a lower utility level than his rival with a higher cost alternate path. • Fully characterize the full-information Nash equilibrium in first price auctions, and use this characterization to derive its overcharging compared to truthful mechaisms.
We thank Hal Varian for his useful comments. This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI0133811.
[1] ANDERSEN, D. G., BALAKRISHNAN, H., KAASHOEK, M. F., AND MORRIS, R. Resilient Overlay Networks. In 18th ACM SOSP (2001). [2] ARCHER, A., AND TARDOS, E. Frugal path mechanisms. [3] ARGYRAKI, K., AND CHERITON, D. Loose Source Routing as a Mechanism for Traffic Policies. In Proceedings of SIGCOMM FDNA (August 2004). [4] ARGYRAKI, K., MANIATIS, P., CHERITON, D., AND SHENKER, S.
Providing Packet Obituaries. In Third Workshop on Hot Topics in Networks (HotNets) (November 2004). [5] BANSAL, S., AND BAKER, M. Observation-based cooperation enforcement in ad-hoc networks. Technical report, Stanford university (2003). [6] BLAKE, S., BLACK, D., CARLSON, M., DAVIES, E., WANG, Z.,
AND WEISS, W. An Architecture for Differentiated Service.
RFC 2475, 1998. [7] BUCHEGGER, S., AND BOUDEC, J.-Y. L. Performance Analysis of the CONFIDANT Protocol: Cooperation of Nodes - Fairness in Dynamic ad-hoc Networks. In IEEE/ACM Symposium on Mobile Ad Hoc Networking and Computing (MobiHOC) (2002). [8] BUCHEGGER, S., AND BOUDEC, J.-Y. L. Coping with False Accusations in Misbehavior Reputation Systems For Mobile ad-hoc Networks. In EPFL, Technical report (2003). [9] BUCHEGGER, S., AND BOUDEC, J.-Y. L. The effect of rumor spreading in reputation systems for mobile ad-hoc networks. In WiOpt"03: Modeling and Optimization in Mobile ad-hoc and Wireless Networks (2003). [10] BUTTYAN, L., AND HUBAUX, J. Stimulating Cooperation in Self-Organizing Mobile ad-hoc Networks. ACM/Kluwer Journal on Mobile Networks and Applications (MONET) (2003). [11] CAILLAUD, B., AND HERMALIN, B. Hidden Action and Incentives.
Teaching Notes. U.C. Berkeley. [12] ELKIND, E., SAHAI, A., AND STEIGLITZ, K. Frugality in path auctions, 2004. [13] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER,
S. A BGP-based Mechanism for Lowest-Cost Routing. In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [14] FEIGENBAUM, J., SAMI, R., AND SHENKER, S. Mechanism Design for Policy Routing. In Yale University, Technical Report (2003). [15] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions. In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [16] FRIEDMAN, E., AND SHENKER, S. Learning and implementation on the internet. In Manuscript. New Brunswick: Rutgers University,
Department of Economics (1997). [17] HOLMSTROM, B. Moral Hazard in Teams. Bell Journal of Economics 13 (1982), 324-340. [18] HU, Y., PERRIG, A., AND JOHNSON, D. Ariadne: A Secure On-Demand Routing Protocol for ad-hoc Networks. In Eighth Annual International Conference on Mobile Computing and Networking (Mobicom) (2002), pp. 12-23. [19] HU, Y., PERRIG, A., AND JOHNSON, D. SEAD: Secure Efficient Distance Vector Routing for Mobile ad-hoc Networks. In 4th IEEE Workshop on Mobile Computing Systems and Applications (WMCSA) (2002). [20] JAKOBSSON, M., HUBAUX, J.-P., AND BUTTYAN, L. A Micro-Payment Scheme Encouraging Collaboration in Multi-Hop Cellular Networks. In Financial Cryptography (2003). [21] LAKSHMINARAYANAN, K., STOICA, I., AND SHENKER, S.
Routing as a service. In UCB Technical Report No.
UCB/CSD-04-1327 (January 2004). [22] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. In Proceedings of MobiCom (2000), pp. 255-265. [23] MASS-COLELL, A., WHINSTON, M., AND GREEN, J.
Microeconomic Theory. Oxford University Press, 1995. [24] NISAN, N., AND RONEN, A. Algorithmic Mechanism Design. In Proceedings of the 31st Symposium on Theory of Computing (1999). [25] SANZGIRI, K., DAHILL, B., LEVINE, B., SHIELDS, C., AND BELDING-ROYER, E. A Secure Routing Protocol for ad-hoc Networks. In International Conference on Network Protocols (ICNP) (2002). [26] SHNEIDMAN, J., AND PARKES, D. C. Overcoming rational manipulation in mechanism implementation, 2004. [27] STRAUSZ, R. Moral Hazard in Sequential Teams. Departmental Working Paper. Free University of Berlin (1996). [28] TEIXEIRA, R., GRIFFIN, T., SHAIKH, A., AND VOELKER, G.
Network sensitivity to hot-potato disruptions. In Proceedings of ACM SIGCOMM (September 2004). [29] TEIXEIRA, R., SHAIKH, A., GRIFFIN, T., AND REXFORD,
J. Dynamics of hot-potato routing in IP networks. In Proceedings of ACM SIGMETRICS (June 2004). [30] YANG, X. NIRA: A New Internet Routing Architecture. In Proceedings of SIGCOMM FDNA (August 2003). [31] ZHONG, S., CHEN, J., AND YANG, Y. R. Sprite: A Simple,

The primary advantage of distributed shared clusters like the Grid [7] and PlanetLab [1] is their ability to pool together shared computational resources. This allows increased throughput because of statistical multiplexing and the bursty utilization pattern of typical users. Sharing nodes that are dispersed in the network allows lower delay because applications can store data close to users. Finally, sharing allows greater reliability because of redundancy in hosts and network connections.
However, resource allocation in these systems remains the major challenge. The problem is how to allocate a shared resource both fairly and efficiently (where efficiency is the ratio of the achieved social welfare to the social optimal) with the presence of strategic users who act in their own interests.
Several non-economic allocation algorithms have been proposed, but these typically assume that task values (i.e., their importance) are the same, or are inversely proportional to the resources required, or are set by an omniscient administrator. However, in many cases, task values vary significantly, are not correlated to resource requirements, and are difficult and time-consuming for an administrator to set.
Instead, we examine a market-based resource allocation system (others are described in [2, 4, 6, 21, 26, 27]) that allows users to express their preferences for resources through a bidding mechanism.
In particular, we consider a price-anticipating [12] scheme in which a user bids for a resource and receives the ratio of his bid to the sum of bids for that resource. This proportional scheme is simpler, more scalable, and more responsive [15] than auction-based schemes [6, 21, 26]. Previous work has analyzed price-anticipating schemes in the context of allocating network capacity for flows for users with unlimited budgets. In this work, we examine a price-anticipating scheme in the context of allocating computational capacity for users with private preferences and limited budgets, resulting in a qualitatively different game (as discussed in Section 6).
In this paper, we formulate the fixed budget resource allocation game and study the existence and performance of the Nash equilibria of this game. For evaluating the Nash equilibria, we consider both their efficiency, measuring how close the social welfare at equilibrium is to the social optimum, and fairness, measuring how different the users" utilities are. Although rarely considered in previous game theoretical study, we believe fairness is a critical metric for a resource allocation schemes because the perception of unfairness will cause some users to reject a system with more efficient, but less fair resource allocation in favor of one with less efficient, more fair resource allocation. We use both utility uniformity and envy-freeness to measure fairness.
Utility uniformity, which is common in Computer Science work, measures the closeness of utilities of different users.
Envyfreeness, which is more from the Economic perspective, measures the happiness of users with their own resources compared to the resources of others.
Our contributions are as follows: • We analyze the existence and performance of 127 Nash equilibria. Using analysis, we show that there is always a Nash equilibrium in the fixed budget game if the utility functions satisfy a fairly weak and natural condition of strong competitiveness. We also show the worst case performance bounds: for m players the efficiency at equilibrium is Ω(1/ √ m), the utility uniformity is ≥ 1/m, and the envyfreeness ≥ 2 √ 2−2 ≈ 0.83. Although these bounds are quite low, the simulations described below indicate these bounds are overly pessimistic. • We describe algorithms that allow strategic users to optimize their utility. As part of the fixed budget game analysis, we show that strategic users with linear utility functions can calculate their bids using a best response algorithm that quickly results in an allocation with high efficiency with little computational and communication overhead. We present variations of the best response algorithm for both finite and infinite parallelism tasks. In addition, we present a local greedy adjustment algorithm that converges more slowly than best response, but allows for non-linear or unformulatable utility functions. • We show that the price-anticipating resource allocation mechanism achieves a high degree of efficiency and fairness. Using simulation, we find that although the socially optimal allocation results in perfect efficiency, it also results in very poor fairness. Likewise, allocating according to only users" preference weights results in a high fairness, but a mediocre efficiency. Intuition would suggest that efficiency and fairness are exclusive. Surprisingly, the Nash equilibrium, reached by each user iteratively applying the best response algorithm to adapt his bids, achieves nearly the efficiency of the social optimum and nearly the fairness of the weight-proportional allocation: the efficiency is ≥ 0.90, the utility uniformity is ≥ 0.65, and the envyfreeness is ≥ 0.97, independent of the number of users in the system. In addition, the time to converge to the equilibrium is ≤ 5 iterations when all users use the best response strategy. The local adjustment algorithm performs similarly when there is sufficient competitiveness, but takes 25 to 90 iterations to stabilize.
As a result, we believe that shared distributed systems based on the fixed budget game can be highly decentralized, yet achieve a high degree of efficiency and fairness.
The rest of the paper is organized as follows. We describe the model in Section 2 and derive the performance at the Nash equilibria for the infinite parallelism model in Section 3. In Section 4, we describe algorithms for users to optimize their own utility in the fixed budget game. In Section 5, we describe our simulator and simulation results.
We describe related work in Section 6. We conclude by discussing some limit of our model and future work in Section 7.
Price-Anticipating Resource Allocation. We study the problem of allocating a set of divisible resources (or machines). Suppose that there are m users and n machines.
Each machine can be continuously divided for allocation to multiple users. An allocation scheme ω = (r1, . . . , rm), where ri = (ri1, · · · , rin) with rij representing the share of machine j allocated to user i, satisfies that for any 1 ≤ i ≤ m and 1 ≤ j ≤ n, rij ≥ 0 and Pm i=1 rij ≤ 1. Let Ω denote the set of all the allocation schemes.
We consider the price anticipating mechanism in which each user places a bid to each machine, and the price of the machine is determined by the total bids placed. Formally, suppose that user i submits a non-negative bid xij to machine j. The price of machine j is then set to Yj = Pn i=1 xij, the total bids placed on the machine j. Consequently, user i receives a fraction of rij = xij Yj of j. When Yj = 0, i.e. when there is no bid on a machine, the machine is not allocated to anyone. We call xi = (xi1, . . . , xin) the bidding vector of user i.
The additional consideration we have is that each user i has a budget constraint Xi. Therefore, user i"s total bids have to sum up to his budget, i.e.
Pn j=1 xij = Xi. The budget constraints come from the fact that the users do not have infinite budget.
Utility Functions. Each user i"s utility is represented by a function Ui of the fraction (ri1, . . . , rin) the user receives from each machine. Given the problem domain we consider, we assume that each user has different and relatively independent preferences for different machines. Therefore, the basic utility function we consider is the linear utility function: Ui(ri1, · · · , rin) = wi1ri1 +· · ·+winrin, where wij ≥ 0 is user i"s private preference, also called his weight, on machine j. For example, suppose machine 1 has a faster CPU but less memory than machine 2, and user 1 runs CPU bounded applications, while user 2 runs memory bounded applications. As a result, w11 > w12 and w21 < w22.
Our definition of utility functions corresponds to the user having enough jobs or enough parallelism within jobs to utilize all the machines. Consequently, the user"s goal is to grab as much of a resource as possible. We call this the infinite parallelism model. In practice, a user"s application may have an inherent limit on parallelization (e.g., some computations must be done sequentially) or there may be a system limit (e.g., the application"s data is being served from a file server with limited capacity). To model this, we also consider the more realistic finite parallelism model, where the user"s parallelism is bounded by ki, and the user"s utility Ui is the sum of the ki largest wijrij. In this model, the user only submits bids to up to ki machines. Our abstraction is to capture the essense of the problem and facilitate our analysis. In Section 7, we discuss the limit of the above definition of utility functions.
Best Response. As typically, we assume the users are selfish and strategic - they all act to maximize their own utility, defined by their utility functions. From the perspective of user i, if the total bids of the other users placed on each machine j is yj, then the best response of user i to the system is the solution of the following optimization problem: maximize Ui( xij xij +yj ) subject to Pn j=1 xij = Xi, and xij ≥ 0.
The difficulty of the above optimization problem depends on the formulation of Ui. We will show later how to solve it for the infinite parallelism model and provide a heuristic for finite parallelism model.
Nash Equilibrium. By the assumption that the user is selfish, each user"s bidding vector is the best response to the system. The question we are most interested in is whether there exists a collection of bidding vectors, one for each user, such that each user"s bidding vector is the best response to those of the other users. Such a state is known as the Nash equilibrium, a central concept in Game Theory. Formally, the bidding vectors x1, . . . , xm is a Nash equilibrium if for 128 any 1 ≤ i ≤ m, xi is the best response to the system, or, for any other bidding vector xi,
Ui(x1, . . . , xi, . . . , xm) ≥ Ui(x1, . . . , xi, . . . , xm) .
The Nash equilibrium is desirable because it is a stable state at which no one has incentive to change his strategy.
But a game may not have an equilibrium. Indeed, a Nash equilibrium may not exist in the price anticipating scheme we define above. This can be shown by a simple example of two players and two machines. For example, let U1(r1, r2) = r1 and U2(r1, r2) = r1 + r2. Then player 1 should never bid on machine 2 because it has no value to him. Now, player 2 has to put a positive bid on machine 2 to claim the machine, but there is no lower limit, resulting in the non-existence of the Nash equilibrium. We should note that even the mixed strategy equilibrium does not exist in this example. Clearly, this happens whenever there is a resource that is wanted by only one player. To rule out this case, we consider those strongly competitive games.1 Under the infinite parallelism model, a game is called strongly competitive if for any 1 ≤ j ≤ n, there exists an i = k such that wij, wkj > 0. Under such a condition, we have that (see [5] for a proof),
Theorem 1. There always exists a pure strategy Nash equilibrium in a strongly competitive game.
Given the existence of the Nash equilibrium, the next important question is the performance at the Nash equilibrium, which is often measured by its efficiency and fairness.
Efficiency (Price of Anarchy). For an allocation scheme ω ∈ Ω, denote by U(ω) = P i Ui(ri) the social welfare under ω. Let U∗ = maxω∈Ω U(ω) denote the optimal social welfare - the maximum possible aggregated user utilities. The efficiency at an allocation scheme ω is defined as π(ω) = U(ω) U∗ .
Let Ω0 denote the set of the allocation at the Nash equilibrium. When there exists Nash equilibrium, i.e. Ω0 = ∅, define the efficiency of a game Q to be π(Q) = minω∈Ω0 π(ω).
It is usually the case that π < 1, i.e. there is an efficiency loss at a Nash equilibrium. This is the price of anarchy [18] paid for not having central enforcement of the user"s good behavior. This price is interesting because central control results in the best possible outcome, but is not possible in most cases.
Fairness. While the definition of efficiency is standard, there are multiple ways to define fairness. We consider two metrics. One is by comparing the users" utilities. The utility uniformity τ(ω) of an allocation scheme ω is defined to be mini Ui(ω) maxi Ui(ω) , the ratio of the minimum utility and the maximum utility among the users. Such definition (or utility discrepancy defined similarly as maxi Ui(ω) mini Ui(ω) ) is used extensively in Computer Science literature. Under this definition, the utility uniformity τ(Q) of a game Q is defined to be τ(Q) = minω∈Ω0 τ(ω).
The other metric extensively studied in Economics is the concept of envy-freeness [25]. Unlike the utility uniformity metric, the enviness concerns how the user perceives the value of the share assigned to him, compared to the shares other users receive. Within such a framework, define the envy-freeness of an allocation scheme ω by ρ(ω) = mini,j Ui(ri) Ui(rj ) . 1Alternatives include adding a reservation price or limiting the lowest allowable bid to each machine. These alternatives, however, introduce the problem of coming up with the right price or limit.
When ρ(ω) ≥ 1, the scheme is known as an envy-free allocation scheme. Likewise, the envy-freeness ρ(Q) of a game Q is defined to be ρ(Q) = minω∈Ω0 ρ(ω).
In this section, we present some theoretical results regarding the performance at Nash equilibrium under the infinite parallelism model. We assume that the game is strongly competitive to guarantee the existence of equilibria. For a meaningful discussion of efficiency and fairness, we assume that the users are symmetric by requiring that Xi = 1 andPn j=1 wij = 1 for all the 1 ≤ i ≤ m. Or informally, we require all the users have the same budget, and they have the same utility when they own all the resources. This precludes the case when a user has an extremely high budget, resulting in very low efficiency or low fairness at equilibrium.
We first provide a characterization of the equilibria. By definition, the bidding vectors x1, . . . , xm is a Nash equilibrium if and only if each player"s strategy is the best response to the group"s bids. Since Ui is a linear function and the domain of each users bids {(xi1, . . . , xin)| P j xij = Xi , and xij ≥ 0} is a convex set, the optimality condition is that there exists λi > 0 such that ∂Ui ∂xij = wij Yj − xij Y 2 j  = λi if xij > 0, and < λi if xij = 0. (1) Or intuitively, at an equilibrium, each user has the same marginal value on machines where they place positive bids and has lower marginal values on those machines where they do not bid.
Under the infinite parallelism model, it is easy to compute the social optimum U∗ as it is achieved when we allocate each machine wholly to the person who has the maximum weight on the machine, i.e. U∗ = Pn j=1 max1≤i≤m wij.
We first show that even in the simplest nontrivial case when there are two users and two machines, the game has interesting properties. We start with two special cases to provide some intuition about the game. The weight matrices are shown in figure 1(a) and (b), which correspond respectively to the equal-weight and opposite-weight games.
Let x and y denote the respective bids of users 1 and 2 on machine 1. Denote by s = x + y and δ = (2 − s)/s.
Equal-weight game. In Figure 1, both users have equal valuations for the two machines. By the optimality condition, for the bid vectors to be in equilibrium, they need to satisfy the following equations according to (1) α y (x + y)2 = (1 − α)
(2 − x − y)2 α x (x + y)2 = (1 − α)
(2 − x − y)2 By simplifying the above equations, we obtain that δ =
equilibrium of the game where the two users have the same bidding vector. At the equilibrium, the utility of each user is 1/2, and the social welfare is 1. On the other hand, the social optimum is clearly 1. Thus, the equal-weight game is ideal as the efficiency, utility uniformity, and the envyfreeness are all 1. 129 m1 m2 u1 α 1 − α u2 α 1 − α m1 m2 u1 α 1 − α u2 1 − α α (a) equal weight game (b) opposite weight game Figure 1: Two special cases of two-player games.
Opposite-weight game. The situation is different for the opposite game in which the two users put the exact opposite weights on the two machines. Assume that α ≥ 1/2. Similarly, for the bid vectors to be at the equilibrium, they need to satisfy α y (x + y)2 = (1 − α)
(2 − x − y)2 (1 − α) x (x + y)2 = α
(2 − x − y)2 By simplifying the above equations, we have that each Nash equilibrium corresponds to a nonnegative root of the cubic equation f(δ) = δ3 − cδ2 + cδ − 1 = 0, where c = 1 2α(1−α) − 1.
Clearly, δ = 1 is a root of f(δ). When δ = 1, we have that x = α, y = 1 − α, which is the symmetric equilibrium that is consistent with our intuition - each user puts a bid proportional to his preference of the machine. At this equilibrium, U = 2 − 4α(1 − α), U∗ = 2α, and U/U∗ = (2α + 1 α ) − 2, which is minimized when α = √ 2 2 with the minimum value of 2 √
large enough, there exist two other roots, corresponding to less intuitive asymmetric equilibria.
Intuitively, the asymmetric equilibrium arises when user 1 values machine 1 a lot, but by placing even a relatively small bid on machine 1, he can get most of the machine because user 2 values machine 1 very little, and thus places an even smaller bid. In this case, user 1 gets most of machine 1 and almost half of machine 2.
The threshold is at when f (1) = 0, i.e. when c = 1 2α(1−α) =
√ 2 4 ≈ 0.854. Those asymmetric equilibria at δ = 1 are bad as they yield lower efficiency than the symmetric equilibrium. Let δ0 be the minimum root. When α → 0, c → +∞, and δ0 = 1/c + o(1/c) → 0.
Then, x, y → 1. Thus, U → 3/2, U∗ → 2, and U/U∗ → 0.75.
From the above simple game, we already observe that the Nash equilibrium may not be unique, which is different from many congestion games in which the Nash equilibrium is unique.
For the general two player game, we can show that 0.75 is actually the worst efficiency bound with a proof in [5].
Further, at the asymmetric equilibrium, the utility uniformity approaches 1/2 when α → 1. This is the worst possible for two player games because as we show in Section 3.2, a user"s utility at any Nash equilibrium is at least 1/m in the m-player game.
Another consequence is that the two player game is always envy-free. Suppose that the two user"s shares are r1 = (r11, . . . , r1n) and r2 = (r21, . . . , r2n) respectively. Then U1(r1) + U1(r2) = U1(r1 + r2) = U1(1, . . . , 1) = 1 because ri1 + ri2 = 1 for all 1 ≤ i ≤ n. Again by that U1(r1) ≥ 1/2, we have that U1(r1) ≥ U1(r2), i.e. any equilibrium allocation is envy-free.
Theorem 2. For a two player game, π(Q) ≥ 3/4, τ(Q) ≥
For large numbers of players, the loss in social welfare can be unfortunately large. The following example shows the worst case bound. Consider a system with m = n2 + n players and n machines. Of the players, there are n2 who have the same weights on all the machines, i.e. 1/n on each machine. The other n players have weight 1, each on a different machine and 0 (or a sufficiently small ) on all the other machines. Clearly, U∗ = n. The following allocation is an equilibrium: the first n2 players evenly distribute their money among all the machines, the other n player invest all of their money on their respective favorite machine. Hence, the total money on each machine is n + 1. At this equilibrium, each of the first n2 players receives 1 n 1/n n+1 = 1 n2(n+1) on each machine, resulting in a total utility of n3 · 1 n2(n+1) < 1.
The other n players each receives 1 n+1 on their favorite machine, resulting in a total utility of n · 1 n+1 < 1. Therefore, the total utility of the equilibrium is < 2, while the social optimum is n = Θ( √ m). This bound is the worst possible.
What about the utility uniformity of the multi-player allocation game? We next show that the utility uniformity of the m-player allocation game cannot exceed m.
Let (S1, . . . , Sn) be the current total bids on the n machines, excluding user i. User i can ensure a utility of 1/m by distributing his budget proportionally to the current bids.
That is, user i, by bidding sij = Xi/ Pn i=1 Si on machine j, obtains a resource level of: rij = sij sij + Sj = Sj/ Pn i=1 Si Sj/ Pn i=1 Si + Sj = 1
Pn i=1 Si , where Pn j=1 Sj = Pm j=1 Xj − Xi = m − 1.
Therefore, rij = 1 1+m−1 = 1 m . The total utility of user i is nX j=1 rijwij = (1/m) nX j=1 wij = 1/m .
Since each user"s utility cannot exceed 1, the minimal possible uniformity is 1/m.
While the utility uniformity can be small, the envy-freeness, on the other hand, is bounded by a constant of 2 √
Theorem 3. For the m-player game Q, π(Q) = Ω(1/ √ m), τ(Q) ≥ 1/m, and ρ(Q) ≥ 2 √
tight in the worst case.
In the previous section, we present the performance bounds of the game under the infinite parallelism model. However, the more interesting questions in practice are how the equilibrium can be reached and what is the performance at the Nash equilibrium for the typical distribution of utility functions. In particular, we would like to know if the intuitive strategy of each player constantly re-adjusting his bids according to the best response algorithm leads to the equilibrium. To answer these questions, we resort to simulations.
In this section, we present the algorithms that we use to compute or approximate the best response and the social optimum in our experiments. We consider both the infinite parallelism and finite parallelism model. 130
As we mentioned before, it is easy to compute the social optimum under the infinite parallelism model - we simply assign each machine to the user who likes it the most. We now present the algorithm for computing the best response.
Recall that for weights w1, . . . , wn, total bids y1, . . . , yn, and the budget X, the best response is to solve the following optimization problem maximize U = Pn j=1 wj xj xj +yj subject to Pn j=1 xj = X, and xj ≥ 0.
To compute the best response, we first sort wj yj in decreasing order. Without loss of generality, suppose that w1 y1 ≥ w2 y2 ≥ . . . wn yn .
Suppose that x∗ = (x∗ 1, . . . , x∗ n) is the optimum solution.
We show that if x∗ i = 0, then for any j > i, x∗ j = 0 too.
Suppose this were not true. Then ∂U ∂xj (x∗ ) = wj yj (x∗ j + yj)2 < wj yj y2 j = wj yj ≤ wi yi = ∂U ∂xi (x∗ ) .
Thus it contradicts with the optimality condition (1).
Suppose that k = max{i|x∗ i > 0}. Again, by the optimality condition, there exists λ such that wi yi (x∗ i +yi)2 = λ for 1 ≤ i ≤ k, and x∗ i = 0 for i > k. Equivalently, we have that: x∗ i = r wiyi λ − yi , for 1 ≤ i ≤ k, and x∗ i = 0 for i > k.
Replacing them in the equation Pn i=1 x∗ i = X, we can solve for λ = ( Pk i=1 √ wiyi)2 (X+ Pk i=1 yi)2 . Thus, x∗ i = √ wiyi Pk i=1 √ wiyi (X + kX i=1 yi) − yi .
The remaining question is how to determine k. It is the largest value such that x∗ k > 0. Thus, we obtain the following algorithm to compute the best response of a user:
yi in decreasing order.
√ wkyk Pk i=1 √ wiyi (X + kX i=1 yi) − yk ≥ 0.
xj = √ wjyj Pk i=1 √ wiyi (X + kX i=1 yi) − yj.
The computational complexity of this algorithm is O(n log n), dominated by the sorting. In practice, the best response can be computed infrequently (e.g. once a minute), so for a typically powerful modern host, this cost is negligible.
The best response algorithm must send and receive O(n) messages because each user must obtain the total bids from each host. In practice, this is more significant than the computational cost. Note that hosts only reveal to users the sum of the bids on them. As a result, hosts do not reveal the private preferences and even the individual bids of one user to another.
Recall that in the finite parallelism model, each user i only places bids on at most ki machines. Of course, the infinite parallelism model is just a special case of finite parallelism model in which ki = n for all the i"s. In the finite parallelism model, computing the social optimum is no longer trivial due to bounded parallelism. It can instead be computed by using the maximum matching algorithm.
Consider the weighted complete bipartite graph G = U × V , where U = {ui |1 ≤ i ≤ m , and 1 ≤ ≤ ki}, V = {1, 2, . . . , n} with edge weight wij assigned to the edge (ui , vj).
A matching of G is a set of edges with disjoint nodes, and the weight of a matching is the total weights of the edges in the matching. As a result, the following lemma holds.
Lemma 1. The social optimum is the same as the maximum weight matching of G.
Thus, we can use the maximum weight matching algorithm to compute the social optimum. The maximum weight matching is a classical network problem and can be solved in polynomial time [8, 9, 14]. We choose to implement the Hungarian algorithm [14, 19] because of its simplicity. There may exist a more efficient algorithm for computing the maximum matching by exploiting the special structure of G. This remains an interesting open question.
However, we do not know an efficient algorithm to compute the best response under the finite parallelism model.
Instead, we provide the following local search heuristic.
Suppose we again have n machines with weights w1, . . . , wn and total bids y1, . . . , yn. Let the user"s budget be X and the parallelism bound be k. Our goal is to compute an allocation of X to up to k machines to maximize the user"s utility.
For a subset of machines A, denote by x(A) the best response on A without parallelism bound and by U(A) the utility obtained by the best response algorithm. The local search works as follows:
algorithm (Sec 4.1) on A.
Intuitively, by the local search heuristic, we test if we can swap a machine in A for one not in A to improve the best response utility. If yes, we swap the machines and repeat the process. Otherwise, we have reached a local maxima and output that value. We suspect that the local maxima that this algorithm finds is also the global maximum (with respect to an individual user) and that this process stop after a few number of iterations, but we are unable to establish it. However, in our simulations, this algorithm quickly converges to a high (≥ .7) efficiency. 131
The above best response algorithms only work for the linear utility functions described earlier. In practice, utility functions may have more a complicated form, or even worse, a user may not have a formulation of his utility function. We do assume that the user still has a way to measure his utility, which is the minimum assumption necessary for any market-based resource allocation mechanism. In these situations, users can use a more general strategy, the local greedy adjustment method, which works as follows. A user finds the two machines that provide him with the highest and lowest marginal utility. He then moves a fixed small amount of money from the machine with low marginal utility to the machine with the higher one. This strategy aims to adjust the bids so that the marginal values at each machine being bid on are the same. This condition guarantees the allocation is the optimum when the utility function is concave. The tradeoff for local greedy adjustment is that it takes longer to stabilize than best-response.
While the analytic results provide us with worst-case analysis for the infinite parallelism model, in this section we employ simulations to study the properties of the Nash equilibria in more realistic scenarios and for the finite parallelism model. First, we determine whether the user bidding process converges, and if so, what the rate of convergence is.
Second, in cases of convergence, we look at the performance at equilibrium, using the efficiency and fairness metrics defined above.
Iterative Method. In our simulations, each user starts with an initial bid vector and then iteratively updates his bids until a convergence criterion (described below) is met.
The initial bid is set proportional to the user"s weights on the machines. We experiment with two update methods, the best response methods, as described in Section 4.1 and 4.2, and the local greedy adjustment method, as described in Section 4.3.
Convergence Criteria. Convergence time measures how quickly the system reaches equilibrium. It is particularly important in the highly dynamic environment of distributed shared clusters, in which the system"s conditions may change before reaching the equilibrium. Thus, a high convergence rate may be more significant than the efficiency at the equilibrium.
There are several different criteria for convergence. The strongest criterion is to require that there is only negligible change in the bids of each user. The problem with this criterion is that it is too strict: users may see negligible change in their utilities, but according to this definition the system has not converged. The less strict utility gap criterion requires there to be only negligible change in the users" utility. Given users" concern for utility, this is a more natural definition. Indeed, in practice, the user is probably not willing to re-allocate their bids dramatically for a small utility gain. Therefore, we use the utility gap criterion to measure convergence time for the best response update method, i.e. we consider that the system has converged if the utility gap of each user is smaller than (0.001 in our experiments).
However, this criterion does not work for the local greedy adjustment method because users of that method will experience constant fluctuations in utility as they move money around. For this method, we use the marginal utility gap criterion. We compare the highest and lowest utility margins on the machines. If the difference is negligible, then we consider the system to be converged.
In addition to convergence to the equilibrium, we also consider the criterion from the system provider"s view, the social welfare stabilization criterion. Under this criterion, a system has stabilized if the change in social welfare is ≤ . Individual users" utility may not have converged. This criterion is useful to evaluate how quickly the system as a whole reaches a particular efficiency level.
User preferences. We experiment with two models of user preferences, random distribution and correlated distribution. With random distribution, users" weights on the different machines are independently and identically distributed, according the uniform distribution. In practice, users" preferences are probably correlated based on factors like the hosts" location and the types of applications that users run. To capture these correlations, we associate with each user and machine a resource profile vector where each dimension of the vector represents one resource (e.g., CPU, memory, and network bandwidth). For a user i with a profile pi = (pi1, . . . , pi ), pik represents user i"s need for resource k. For machine j with profile qj = (qj1, . . . , qj ), qjk represents machine j"s strength with respect to resource k. Then, wij is the dot product of user i"s and machine j"s resource profiles, i.e. wij = pi · qj = P k=1 pikqjk. By using these profiles, we compress the parameter space and introduce correlations between users and machines.
In the following simulations, we fix the number of machines to 100 and vary the number of users from 5 to 250 (but we only report the results for the range of 5 − 150 users since the results remain similar for a larger number of users). Sections 5.1 and 5.2 present the simulation results when we apply the infinite parallelism and finite parallelism models, respectively. If the system converges, we report the number of iterations until convergence. A convergence time of 200 iterations indicates non-convergence, in which case we report the efficiency and fairness values at the point we terminate the simulation.
In this section, we apply the infinite parallelism model, which assumes that users can use an unlimited number of machines. We present the efficiency and fairness at the equilibrium, compared to two baseline allocation methods: social optimum and weight-proportional, in which users distribute their bids proportionally to their weights on the machines (which may seem a reasonable distribution method intuitively).
We present results for the two user preference models.
With uniform preferences, users" weights for the different machines are independently and identically distributed according to the uniform distribution, U ∼ (0, 1) (and are normalized thereafter). In correlated preferences, each user"s and each machine"s resource profile vector has three dimensions, and their values are also taken from the uniform distribution, U ∼ (0, 1).
Convergence Time. Figure 2 shows the convergence time, efficiency and fairness of the infinite parallelism model under uniform (left) and correlated (right) preferences. Plots (a) and (b) show the convergence and stabilization time of the best-response and local greedy adjustment methods. 132 0 50 100 150 200
Convergencetime(#iterations) Number of Users Uniform preferences (a) Best-Response Greedy (convergence) Greedy (stabilization) 0 50 100 150 200
Number of Users Correlated preferences (b) Best-response Greedy (convergence) Greedy (stabilization) 0
1
Efficiency Number of Users (c) Nash equilibrium Weight-proportional Social Optimum 0
1
Number of Users (d) Nash equilibrium Weight-proportional Social optimum 0
1
Utilityuniformity Number of Users (e) Nash equilibrium Weight-proportional Social optimum 0
1
Number of Users (f) Nash equilibrium Weight proportional Social optimum 0
1
Envy-freeness Number of Users (g) Nash equilibrium Weight proportional Social optimum 0
1
Number of Users (h) Nash equilibrium Weight proportional Social optimum Figure 2: Efficiency, utility uniformity, enviness and convergence time as a function of the number of users under the infinite parallelism model, with uniform and correlated preferences. n = 100. 133 0
1
Efficiency Iteration number Best-Response Greedy Figure 3: Efficiency level over time under the infinite parallelism model. number of users = 40. n = 100.
The best-response algorithm converges within a few number of iterations for any number of users. In contrast, the local greedy adjustment algorithm does not converge even within
but does converge for a larger number of users. We believe that for small numbers of users, there are dependency cycles among the users that prevent the system from converging because one user"s decisions affects another user, whose decisions affect another user, etc. Regardless, the local greedy adjustment method stabilizes within 100 iterations.
Figure 3 presents the efficiency over time for a system with 40 users. It demonstrates that while both adjustment methods reach the same social welfare, the best-response algorithm is faster.
In the remainder of this paper, we will refer to the (Nash) equilibrium, independent of the adjustment method used to reach it.
Efficiency. Figure 2 (c) and (d) present the efficiency as a function of the number of users. We present the efficiency at equilibrium, and use the social optimum and the weightproportional static allocation methods for comparison.
Social optimum provides an efficient allocation by definition.
For both user preference models, the efficiency at the equilibrium is approximately 0.9, independent of the number of users, which is only slightly worse than the social optimum.
The efficiency at the equilibrium is ≈ 50% improvement over the weight-proportional allocation method for uniform preferences, and ≈ 30% improvement for correlated preferences.
Fairness. Figure 2(e) and (f) present the utility uniformity as a function of the number of users, and figures (g) and (h) present the envy-freeness. While the social optimum yields perfect efficiency, it has poor fairness. The weightproportional method achieves the highest fairness among the three allocation methods, but the fairness at the equilibrium is close.
The utility uniformity is slightly better at the equilibrium under uniform preferences (> 0.7) than under correlated preferences (> 0.6), since when users" preferences are more aligned, users" happiness is more likely going to be at the expense of each other. Although utility uniformity decreases in the number of users, it remains reasonable even for a large number of users, and flattens out at some point. At the social optimum, utility uniformity can be infinitely poor, as some users may be allocated no resources at all. The same is true with respect to envy-freeness. The difference between uniform and correlated preferences is best demonstrated in the social optimum results. When the number of users is small, it may be possible to satisfy all users to some extent if their preferences are not aligned, but if they are aligned, even with a very small number of users, some users get no resources, thus both utility uniformity and envy-freeness go to zero. As the number of users increases, it becomes almost impossible to satisfy all users independent of the existence of correlation.
These results demonstrate the tradeoff between the different allocation methods. The efficiency at the equilibrium is lower than the social optimum, but it performs much better with respect to fairness. The equilibrium allocation is completely envy-free under uniform preferences and almost envy-free under correlated preferences.
0 50 100 150 200
Convergencetime(#iterations) Number of Users
Figure 4: Convergence time under the finite parallelism model. n = 100.
1
Efficiency Iteration number 5-machines/user (40 users) 20-machines/user (10 users) Figure 5: Efficiency level over time under the finite parallelism model with local search algorithm. n = 100.
We also consider the finite parallelism model and use the local search algorithm, as described in Section 4.2, to adjust user"s bids. We again experimented with both the uniform and correlated preferences distributions and did not find significant differences in the results so we present the simulation results for only the uniform distribution.
In our experiments, the local search algorithm stops quickly - it usually discovers a local maximum within two iterations. As mentioned before, we cannot prove that a local maximum is the global maximum, but our experiments indicate that the local search heuristic leads to high efficiency. 134 Convergence time. Let ∆ denote the parallelism bound that limits the maximum number of machines each user can bid on. We experiment with ∆ = 5 and ∆ = 20. In both cases, we use 100 machines and vary the number of users.
Figure 4 shows that the system does not always converge, but if it does, the convergence happens quickly. The nonconvergence occurs when the number of users is between 20 and 40 for ∆ = 5, between 5 and 10 for ∆ = 20. We believe that the non-convergence is caused by moderate competition. No competition allows the system to equilibrate quickly because users do not have to change their bids in reaction to changes in others" bids. High competition also allows convergence because each user"s decision has only a small impact on other users, so the system is more stable and can gradually reach convergence. However, when there is moderate competition, one user"s decisions may cause dramatic changes in another"s decisions and cause large fluctuations in bids. In both cases of non-convergence, the ratio of competitors per machine, δ = m×∆/n for m users and n machines, is in the interval [1, 2]. Although the system does not converge in these bad ranges, the system nontheless achieves and maintains a high level of overall efficiency after a few iterations (as shown in Figure 5).
Performance. In Figure 6, we present the efficiency, utility uniformity, and envy-freeness at the Nash equilibrium for the finite parallelism model. When the system does not converge, we measure performance by taking the minimum value we observe after running for many iterations.
When ∆ = 5, there is a performance drop, in particular with respect to the fairness metrics, in the range between
number of users, the system converges and achieves a lower level of utility uniformity, but a high degree of efficiency and envy-freeness, similar to those under the infinite parallelism model. As described above, this is due the competition ratio falling into the head-to-head range. When the parallelism bound is large (∆ = 20), the performance is closer to the infinite parallelism model, and we do not observe this drop in performance.
There are two main groups of related work in resource allocation: those that incorporate an economic mechanism, and those that do not.
One non-economic approach is scheduling (surveyed by Pinedo [20]). Examples of this approach are queuing in first-come, first-served (FCFS) order, queueing using the resource consumption of tasks (e.g., [28]), and scheduling using combinatorial optimization [19]. These all assume that the values and resource consumption of tasks are reported accurately, which does not apply in the presence of strategic users. We view scheduling and resource allocation as two separate functions. Resource allocation divides a resource among different users while scheduling takes a given allocation and orders a user"s jobs.
Examples of the economic approach are Spawn [26]), work by Stoica, et al. [24]., the Millennium resource allocator [4], work by Wellman, et al. [27], Bellagio [2]), and Tycoon [15]).
Spawn and the work by Wellman, et al. uses a reservation abstraction similar to the way airline seats are allocated.
Unfortunately, reservations have a high latency to acquire resources, unlike the price-anticipating scheme we consider.
The tradeoff of the price-anticipating schemes is that users have uncertainty about exactly how much of the resources they will receive.
Bellagio[3] uses the SHARE centralized allocator. SHARE allocates resources using a centralized combinatorial auction that allows users to express preferences with complementarities. Solving the NP-complete combinatorial auction problem provides an optimally efficient allocation. The priceanticipating scheme that we consider does not explicitly operate on complementarities, thereby possibly losing some efficiency, but it also avoids the complexity and overhead of combinatorial auctions.
There have been several analyses [10, 11, 12, 13, 23] of variations of price-anticipating allocation schemes in the context of allocation of network capacity for flows. Their methodology follows the study of congestion (potential) games [17, 22] by relating the Nash equilibrium to the solution of a (usually convex) global optimization problem. But those techniques no longer apply to our game because we model users as having fixed budgets and private preferences for machines. For example, unlike those games, there may exist multiple Nash equilibria in our game. Milchtaich [16] studied congestion games with private preferences but the technique in [16] is specific to the congestion game.
This work studies the performance of a market-based mechanism for distributed shared clusters using both analyatical and simulation methods. We show that despite the worst case bounds, the system can reach a high performance level at the Nash equilibrium in terms of both efficiency and fairness metrics. In addition, with a few exceptions under the finite parallelism model, the system reaches equilibrium quickly by using the best response algorithm and, when the number of users is not too small, by the greedy local adjustment method.
While our work indicates that the price-anticipating scheme may work well for resource allocation for shared clusters, there are many interesting directions for future work. One direction is to consider more realistic utility functions. For example, we assume that there is no parallelization cost, and there is no performance degradation when multiple users share the same machine. In practice, both assumptions may not be correct. For examples, the user must copy code and data to a machine before running his application there, and there is overhead for multiplexing resources on a single machine. When the job size is large enough and the degree of multiplexing is sufficiently low, we can probably ignore those effects, but those costs should be taken into account for a more realistic modeling. Another assumption is that users have infinite work, so the more resources they can acquire, the better. In practice, users have finite work. One approach to address this is to model the user"s utility according to the time to finish a task rather than the amount of resources he receives.
Another direction is to study the dynamic properties of the system when the users" needs change over time, according to some statistical model. In addition to the usual questions concerning repeated games, it would also be important to understand how users should allocate their budgets wisely over time to accomodate future needs. 135 0
1
Number of Users (a) Limit: 5 machines/user Efficiency Utility uniformity Envy-freeness 0
1
Number of Users (b) Limit: 20 machines/user Efficiency Utility uniformity Envy-freeness Figure 6: Efficiency, utility uniformity and envy-freeness under the finite parallelism model. n = 100.
We thank Bernardo Huberman, Lars Rasmusson, Eytan Adar and Moshe Babaioff for fruitful discussions. We also thank the anonymous reviewers for their useful comments.
[1] http://planet-lab.org. [2] A. AuYoung, B. N. Chun, A. C. Snoeren, and A. Vahdat.
Resource Allocation in Federated Distributed Computing Infrastructures. In Proceedings of the 1st Workshop on Operating System and Architectural Support for the On-demand IT InfraStructure, 2004. [3] B. Chun, C. Ng, J. Albrecht, D. C. Parkes, and A. Vahdat.
Computational Resource Exchanges for Distributed Resource Allocation. 2004. [4] B. N. Chun and D. E. Culler. Market-based Proportional Resource Sharing for Clusters. Technical Report CSD-1092,
University of California at Berkeley, Computer Science Division, January 2000. [5] M. Feldman, K. Lai, and L. Zhang. A Price-anticipating Resource Allocation Mechanism for Distributed Shared Clusters. Technical report, arXiv, 2005. http://arxiv.org/abs/cs.DC/0502019. [6] D. Ferguson, Y. Yemimi, and C. Nikolaou. Microeconomic Algorithms for Load Balancing in Distributed Computer Systems. In International Conference on Distributed Computer Systems, pages 491-499, 1988. [7] I. Foster and C. Kesselman. Globus: A Metacomputing Infrastructure Toolkit. The International Journal of Supercomputer Applications and High Performance Computing, 11(2):115-128, Summer 1997. [8] M. L. Fredman and R. E. Tarjan. Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms.
Journal of the ACM, 34(3):596-615, 1987. [9] H. N. Gabow. Data Structures for Weighted Matching and Nearest Common Ancestors with Linking. In Proceedings of 1st Annual ACM-SIAM Symposium on Discrete algorithms, pages 434-443, 1990. [10] B. Hajek and S. Yang. Strategic Buyers in a Sum Bid Game for Flat Networks. Manuscript, http: //tesla.csl.uiuc.edu/~hajek/Papers/HajekYang.pdf,
[11] R. Johari and J. N. Tsitsiklis. Efficiency Loss in a Network Resource Allocation Game. Mathematics of Operations Research, 2004. [12] F. P. Kelly. Charging and Rate Control for Elastic Traffic.
European Transactions on Telecommunications, 8:33-37,
[13] F. P. Kelly and A. K. Maulloo. Rate Control in Communication Networks: Shadow Prices, Proportional Fairness and Stability. Operational Research Society, 49:237-252, 1998. [14] H. W. Kuhn. The Hungarian Method for the Assignment Problem. Naval Res. Logis. Quart., 2:83-97, 1955. [15] K. Lai, L. Rasmusson, S. Sorkin, L. Zhang, and B. A.
Huberman. Tycoon: an Implemention of a Distributed Market-Based Resource Allocation System. Manuscript, http://www.hpl.hp.com/research/tycoon/papers_and_ presentations, 2004. [16] I. Milchtaich. Congestion Games with Player-Specific Payoff Functions. Games and Economic Behavior, 13:111-124, 1996. [17] D. Monderer and L. S. Sharpley. Potential Games. Games and Economic Behavior, 14:124-143, 1996. [18] C. Papadimitriou. Algorithms, Games, and the Internet. In Proceedings of 33rd STOC, 2001. [19] C. H. Papadimitriou and K. Steiglitz. Combinatorial Optimization. Dover Publications, Inc., 1982. [20] M. Pinedo. Scheduling. Prentice Hall, 2002. [21] O. Regev and N. Nisan. The Popcorn Market: Online Markets for Computational Resources. In Proceedings of 1st International Conference on Information and Computation Economies, pages 148-157, 1998. [22] R. W. Rosenthal. A Class of Games Possessing Pure-Strategy Nash Equilibria. Internation Journal of Game Theory, 2:65-67, 1973. [23] S. Sanghavi and B. Hajek. Optimal Allocation of a Divisible Good to Strategic Buyers. Manuscript, http: //tesla.csl.uiuc.edu/~hajek/Papers/OptDivisible.pdf,
[24] I. Stoica, H. Abdel-Wahab, and A. Pothen. A Microeconomic Scheduler for Parallel Computers. In Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing, pages 122-135, April 1995. [25] H. R. Varian. Equity, Envy, and Efficiency. Journal of Economic Theory, 9:63-91, 1974. [26] C. A. Waldspurger, T. Hogg, B. A. Huberman, J. O.
Kephart, and S. Stornetta. Spawn: A Distributed Computational Economy. IEEE Transactions on Software Engineering, 18(2):103-117, February 1992. [27] M. P. Wellman, W. E. Walsh, P. R. Wurman, and J. K.
MacKie-Mason. Auction Protocols for Decentralized Scheduling. Games and Economic Behavior, 35:271-303,

The research area of optimal mechanism design looks at designing a mechanism to produce the most desirable outcome for the entity running the mechanism. This problem is well studied for the auction design problem where the optimal mechanism is the one that brings the seller the most profit. Here, the classical approach is to design such a mechanism given the prior distribution from which the bidders" preferences are drawn (See e.g., [12, 4]).
Recently Goldberg et al. [9] introduced the use of worst-case competitive analysis (See e.g., [3]) to analyze the performance of auctions that have no knowledge of the prior distribution. The goal of such work is to design an auction that achieves a large constant fraction of the profit attainable by an optimal mechanism that knows the prior distribution in advance. Positive results in this direction are fueled by the observation that in auctions for a number of identical units, much of the distribution from which the bidders are drawn can be deduced on the fly by the auction as it is being run [9, 14, 2].
The performance of an auction in such a worst-case competitive analysis is measured by its competitive ratio, the ratio between a benchmark performance and the auction"s performance on the input distribution that maximizes this ratio. The holy grail of the worstcase competitive analysis of auctions is the auction that achieves the optimal competitive ratio (as small as possible). Since [9] this search has led to improved understanding of the nature of the optimal auction, the techniques for on-the-fly pricing in these scenarios, and the competitive ratio of the optimal auction [5, 7, 8]. In this paper we continue this line of research by improving in all of these directions. Furthermore, we give evidence corroborating the conjecture that the form of the optimal auction is independent of the benchmark used in the auction"s competitive analysis. This result further validates the use of competitive analysis in gauging auction performance.
We consider the single item, multi-unit, unit-demand auction problem. In such an auction there are many units of a single item available for sale to bidders who each desire only one unit. Each bidder has a valuation representing how much the item is worth to him.
The auction is performed by soliciting a sealed bid from each of the bidders and deciding on the allocation of units to bidders and the prices to be paid by the bidders. The bidders are assumed to bid so as to maximize their personal utility, the difference between their valuation and the price they pay. To handle the problem of designing and analyzing auctions where bidders may falsely declare their valuations to get a better deal, we will adopt the solution concept of truthful mechanism design (see, e.g., [9, 15, 13]). In a truthful auction, revealing one"s true valuation as one"s bid is an optimal strategy for each bidder regardless of the bids of the other bidders.
In this paper, we will restrict our attention to truthful (a.k.a., incentive compatible or strategyproof) auctions.
A particularly interesting special case of the auction problem is the unlimited supply case. In this case the number of units for sale is at least the number of bidders in the auction. This is natural for the sale of digital goods where there is negligible cost for duplicating 175 and distributing the good. Pay-per-view television and downloadable audio files are examples of such goods.
The competitive framework introduced in [9] and further refined in [5] uses the profit of the optimal omniscient single priced mechanism that sells at least two units as the benchmark for competitive analysis. The assumption that two or more units are sold is necessary because in the worst case it is impossible to obtain a constant fraction of the profit of the optimal mechanism when it sells only one unit [9]. In this framework for competitive analysis, an auction is said to be β-competitive if it achieves a profit that is within a factor of β ≥ 1 of the benchmark profit on every input. The optimal auction is the one which is β-competitive with the minimum value of β.
Previous to this work, the best known auction for the unlimited supply case had a competitive ratio of 3.39 [7] and the best lower bound known was 2.42 [8]. For the limited supply case, auctions can achieve substantially better competitive ratios. When there are only two units for sale, the optimal auction gives a competitive ratio of 2, which matches the lower bound for two units. When there are three units for sale, the best previously known auction had a competitive ratio of 2.3, compared with a lower bound of 13/6 ≈
The results of this paper are as follows: • We give the auction for three units that is optimally competitive against the profit of the omniscient single priced mechanism that sells at least two units. This auction achieves a competitive ratio of 13/6, matching the lower bound from [8] (Section 3). • We show that the form of the optimal auction is independent of the benchmark used in competitive analysis. In doing so, we give an optimal three bidder auction for generalized benchmarks (Section 4). • We give a general technique for converting a limited supply auction into an unlimited supply auction where it is possible to use the competitive ratio of the limited supply auction to obtain a bound on the competitive ratio of the unlimited supply auction. We refer to auctions derived from this framework as aggregation auctions (Section 5). • We improve on the best known competitive ratio by proving that the aggregation auction constructed from our optimal three-unit auction is 3.25-competitive (Section 5.1). • Assuming that the conjecture that the optimal -unit auction has a competitive ratio that matches the lower bound proved in [8], we show that this optimal auction for ≥ 3 on some inputs will occasionally offer prices that are higher than any bid in that input (Section 6). For the three-unit case where we have shown that the lower bound of [8] is tight, this observation led to our construction of the optimal three-unit auction.
We consider single-round, sealed-bid auctions for a set of identical units of an item to bidders who each desire one unit. As mentioned in the introduction, we adopt the game-theoretic solution concept of truthful mechanism design. A useful simplification of the problem of designing truthful auctions is obtained through the following algorithmic characterization [9]. Related formulations to this one have appeared in numerous places in recent literature (e.g., [1, 14, 5, 10]).
DEFINITION 1. Given a bid vector of n bids, b = (b1, . . . , bn), let b-i denote the vector of with bi replaced with a ‘?", i.e., b-i = (b1, . . . , bi−1, ?, bi+1, . . . , bn).
DEFINITION 2. Let f be a function from bid vectors (with a ‘?") to prices (non-negative real numbers). The deterministic bidindependent auction defined by f, BIf , works as follows. For each bidder i:
price ti or reject it.
A randomized bid-independent auction is a distribution over deterministic bid-independent auctions.
The proof of the following theorem can be found, for example, in [5].
THEOREM 1. An auction is truthful if and only if it is equivalent to a bid-independent auction.
Given this equivalence, we will use the the terminology bidindependent and truthful interchangeably.
For a randomized bid-independent auction, f(b-i) is a random variable. We denote the probability density of f(b-i) at z by ρb-i (z).
We denote the profit of a truthful auction A on input b as A(b).
The expected profit of the auction, E[A(b)], is the sum of the expected payments made by each bidder, which we denote by pi(b) for bidder i. Clearly, the expected payment of each bid satisfies pi(b) = bi 0 xρb-i (x)dx.
We now review the competitive framework from [5]. In order to evaluate the performance of auctions with respect to the goal of profit maximization, we introduce the optimal single price omniscient auction F and the related omniscient auction F(2) .
DEFINITION 3. Give a vector b = (b1, . . . , bn), let b(i) represent the i-th largest value in b.
The optimal single price omniscient auction, F, is defined as follows. Auction F on input b determines the value k such that kb(k) is maximized. All bidders with bi ≥ b(k) win at price b(k); all remaining bidders lose. The profit of F on input b is thus F(b) = max1≤k≤n kb(k).
In the competitive framework of [5] and subsequent papers, the performance of a truthful auction is gauged in comparison to F(2) , the optimal singled priced auction that sells at least two units. The profit of F(2) is max2≤k≤n kb(k) There are a number of reasons to choose this benchmark for comparison, interested readers should see [5] or [6] for a more detailed discussion.
Let A be a truthful auction. We say that A is β-competitive against F(2) (or just β-competitive) if for all bid vectors b, the expected profit of A on b satisfies E[A(b)] ≥ F(2) (b) β .
In Section 4 we generalize this framework to other profit benchmarks. 176
A symmetric auction is one where the auction outcome is unchanged when the input bids arrive in a different permutation.
Goldberg et al. [8] show that a symmetric auction achieves the optimal competitive ratio. This is natural as the profit benchmark we consider is symmetric, and it allows us to consider only symmetric auctions when looking for the one with the optimal competitive ratio.
An auction defined by bid-independent function f is scale invariant if, for all i and all z, Pr[f(b-i) ≥ z] = Pr[f(cb-i) ≥ cz].
It is conjectured that the assumption of scale invariance is without loss of generality. Thus, we are motivated to consider symmetric scale-invariant auctions. When specifying a symmetric scaleinvariant auction we can assume that f is only a function of the relative magnitudes of the n − 1 bids in b-i and that one of the bids, bj = 1. It will be convenient to specify such auctions via the density function of f(b-i), ρb-i (z). It is enough to specify such a density function of the form ρ1,z1,...,zn−1 (z) with 1 ≤ zi ≤ zi+1.
Following [8], throughout the remainder of this paper we will be making the assumption that n = , i.e., the number of bidders is equal to the number of units for sale. This is without loss of generality as (a) any lower bound that applies to the n = case also extends to the case where n ≥ [8], and (b) there is a reduction from the unlimited supply auction problem to the limited supply auction problem that takes an unlimited supply auction that is β-competitive with F(2) and constructs a limited supply auction parameterized by that is β-competitive with F(2, ) , the optimal omniscient auction that sells between 2 and units [6].
Henceforth, we will assume that we are in the unlimited supply case, and we will examine lower bounds for limited supply problems by placing a restriction on the number of bidders in the auction.
Frequently in this paper, we will refer to the best known lower bound on the competitive ratio of truthful auctions: THEOREM 2. [8] The competitive ratio of any auction on n bidders is at least
n i=2 −1 n i−1 i i − 1 n − 1 i − 1 .
DEFINITION 4. Let Υn denote the n-bidder auction that achieves the optimal competitive ratio.
This bound is derived by analyzing the performance of any auction on the following distribution B. In each random bid vector B, each bid Bi is drawn i.i.d. from the distribution such that Pr[Bi ≥ s] ≤ 1/s for all s ∈ S.
In the two-bidder case, this lower bound is 2. This is achieved by Υ2 which is the 1-unit Vickrey auction.1 In the three-bidder case, this lower bound is 13/6. In the next section, we define the auction Υ3 which matches this lower bound. In the four-bidder case, this lower bound is 96/215. In the limit as the number of bidders grows, this lower bound approaches a number which is approximately 2.42.
It is conjectured that this lower bound is tight for any number of bidders and the optimal auction, Υn, matches it. 1 The 1-unit Vickrey auction sells to the highest bidder at the second highest bid value.
In this section we review the truthful profit extraction mechanism ProfitExtractR. This mechanism is a special case of a general cost-sharing schema due to Moulin and Shenker [11].
The goal of profit extraction is, given bids b, to extract a target value R of profit from some subset of the bidders.
ProfitExtractR: Given bids b, find the largest k such that the highest k bidders can equally share the cost R. Charge each of these bidders R/k. If no subset of bidders can cover the cost, the mechanism has no winners.
Important properties of this auction are as follows: • ProfitExtractR is truthful. • If R ≤ F(b), ProfitExtractR(b) = R; otherwise it has no winners and no revenue.
We will use this profit extraction mechanism in Section 5 with the following intuition. Such a profit extractor makes it possible to treat this subset of bidders as a single bid with value F(S). Note that given a single bid, b, a truthful mechanism might offer it price t and if t ≤ b then the bidder wins and pays t; otherwise the bidder pays nothing (and loses). Likewise, a mechanism can offer the set of bidders S a target revenue R. If R ≤ F(2) (S), then ProfitExtractR raises R from S; otherwise, the it raises no revenue from S.
BIDDERS In this section we define the optimal auction for three bidders,
Υ3, and prove that it indeed matches the known lower bound of 13/6. We follow the definition and proof with a discussion of how this auction was derived.
DEFINITION 5. Υ3 is scale-invariant and symmetric and given by the bid-independent function with density function ρ1,x(z) = ⎧ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ For x ≤ 3/2
z with probability density g(z) for z > 3/2 For x > 3/2⎧ ⎪⎨ ⎪⎩
x 3/2 zg(z)dz x with probability x 3/2 (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2/13 (x−1)3 .
THEOREM 3. The Υ3 auction has a competitive ratio of 13/6 ≈
6 13 F(2) on every input with non-identical bids.
PROOF. Consider the bids 1, x, y, with 1 < x < y. There are three cases.
CASE 1 (x < y ≤ 3/2): F(2) = 3. The auction must raise expected revenue of at least 18/13 on these bids. The bidder with valuation x will pay 1 with 9/13, and the bidder with valuation y will pay 1 with probability 9/13. Therefore Υ3 raises 18/13 on these bids.
CASE 2 (x ≤ 3/2 < y): F(2) = 3. The auction must raise expected revenue of at least 18/13 on these bids. The bidder with 177 valuation x will pay 9/13 − y 3/2 zg(z)dz in expectation. The bidder with valuation y will pay 9/13 + y 3/2 zg(z)dz in expectation.
Therefore Υ3 raises 18/13 on these bids.
CASE 3 (3/2 < x ≤ y): F(2) = 2x. The auction must raise expected revenue of at least 12x/13 on these bids. Consider the revenue raised from all three bidders: E[Υ3(b)] = p(1, x, y) + p(x, 1, y) + p(y, 1, x) = 0 + 9/13 − y 3/2 zg(z)dz + 9/13 − x 3/2 zg(z)dz + x x 3/2 (z + 1)g(z)dz + y x zg(z)dz = 18/13 + (x − 2) x 3/2 zg(z)dz + x x 3/2 g(z)dz = 12x/13.
The final equation comes from substituting in g(x) = 2/13 (x−1)3 and expanding the integrals. Note that the fraction of F(2) raised on every input is identical. If any of the inequalities 1 ≤ x ≤ y are not strict, the same proof applies giving a lower bound on the auction"s profit; however, this bound may no longer be tight.
Motivation for Υ3 In this section, we will conjecture that a particular input distribution is worst-case, and show, as a consequence, that all inputs are worstcase in the optimal auction. By applying this consequence, we will derive an optimal auction for three bidders.
A truthful, randomized auction on n bidders can be represented by a randomized function f : Rn−1 × n → R that maps masked bid vectors to prices in R. By normalization, we can assume that the lowest possible bid is 1. Recall that ρb-i (z) = Pr[f(b-i) = z].
The optimal auction for the finite auction problem can be found by the following optimization problem in which the variables are ρb-i (z): maximize r subject to n i=1 bi z=1 zρb-i (z)dz ≥ rF(2) (b) ∞ z=1 ρb-i (z)dz = 1 ρb-i (z) ≥ 0 This set of integral inequalities is difficult to maximize over.
However, by guessing which constraints are tight and which are slack at the optimum, we will be able to derive a set of differential equations for which any feasible solution is an optimal auction.
As we discuss in Section 2.4, in [8], the authors define a distribution and use it to find a lower bound on the competitive ratio of the optimal auction. For two bidders, this bid distribution is the worst-case input distribution. We guess (and later verify) that this distribution is the worst-case input distribution for three bidders as well. Since this distribution has full support over the set of all bid vectors and a worst-case distribution puts positive probability only on worst-case inputs, we can therefore assume that all but a measure zero set of inputs is worst-case for the optimal auction. In the optimal two-bidder auction, all inputs with non-identical bids are worst-case, so we will assume the same for three bidders.
The guess that these constraints are tight allows us to transform the optimization problem into a feasibility problem constrained by differential equations. If the solution to these equations has value matching the lower bound obtained from the worst-case distribution, then this solution is the optimal auction and that our conjectured choice of worst-case distribution is correct.
In Section 6 we show that the optimal auction must sometimes place probability mass on sale prices above the highest bid. This motivates considering symmetric scale-invariant auctions for three bidders with probability density function, ρ1,x(z), of the following form: ρ1,x(z) = ⎧ ⎪⎨ ⎪⎩
x with discrete probability b(x) z with probability density g(z) for z > x In this auction, the sale price for the first bidder is either one of the latter two bids, or higher than either bid with a probability density which is independent of the input.
The feasibility problem which arises from the linear optimization problem by assuming the constraints are tight is as follows: a(y) + a(x) + xb(x) + y x zg(z)dz = r max(3, 2x) ∀x < y a(x) + b(x) + ∞ x g(z)dz = 1 a(x) ≥ 0 b(x) ≥ 0 g(z) ≥ 0 Solving this feasibility problem gives the auction Υ3 proposed above. The proof of its optimality validates its proposed form.
Finding a simple restriction on the form of n-bidder auctions for n > 3 under which the optimal auction can be found analytically as above remains an open problem.
In this section, we widen our focus beyond auctions that compete with F(2) to consider other benchmarks for an auction"s profit. We will show that, for three bidders, the form of the optimal auction is essentially independent of the benchmark profit used. This results strongly corroborates the worst-case competitive analysis of auctions by showing that our techniques allow us to derive auctions which are competitive against a broad variety of reasonable benchmarks rather than simply against F(2) .
Previous work in competitive analysis of auctions has focused on the question of designing the auction with the best competitive ratio against F(2) , the profit of the optimal omniscient single-priced mechanism that sells at least two items. However, it is reasonable to consider other benchmarks. For instance, one might wish to compete against V∗ , the profit of the k-Vickrey auction with optimal-inhindsight choice of k.2 Alternatively, if an auction is being used as a subroutine in a larger mechanism, one might wish to choose the auction which is optimally competitive with a benchmark specific to that purpose.
Recall that F(2) (b) = max2≥k≥n kb(k). We can generalize this definition to Gs, parameterized by s = (s2, . . . , sn) and defined as: Gs(b) = max 2≤k≤n skb(k).
When considering Gs we assume without loss of generality that si < si+1 as otherwise the constraint imposed by si+1 is irrelevant.
Note that F(2) is the special case of Gs with si = i, and that V∗ = Gs with si = i − 1. 2 Recall that the k-Vickrey auction sells a unit to each of the highest k bidders at a price equal to the k + 1st highest bid, b(k+1), achieving a profit of kb(k+1). 178 Competing with Gs We will now design a three-bidder auction Υs,t
optimal competitive ratio against Gs,t. As before, we will first find a lower bound on the competitive ratio and then design an auction to meet that bound.
We can lower bound the competitive ratio of Υs,t
worst-case distribution from [8] that we used against F(2) .
Evaluating the performance of any auction competing against Gs,t on this distribution will yield the following theorem. We denote the optimal auction for three bidders against Gs,t as Υs,t
THEOREM 4. The optimal three-bidder auction, Υs,t
competing against Gs,t(b) = max(sb(2), tb(3)) has a competitive ratio of at least s2 +t2 2t .
The proof can be found in the appendix.
Similarly, we can find the optimal auction against Gs,t using the same technique we used to solve for the three bidder auction with the best competitive ratio against F(2) .
DEFINITION 6. Υs,t
by the bid-independent function with density function ρ1,x(z) = ⎧ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ For x ≤ t s
s2+t2 z with probability density g(z) for z > t s For x > t s⎧ ⎪⎪⎨ ⎪⎪⎩
s2+t2 − x t s zg(z)dz x with probability x t s (z + 1)g(z)dz z with probability density g(z) for z > x where g(x) = 2(t−s)2 /(s2 +t2 ) (x−1)3 .
THEOREM 5. Υs,t
+t2 2t -competitive with Gs,t.
This auction, like Υ3, can be derived by reducing the optimization problem to a feasibility problem, guessing that the optimal solution has the same form as Υs,t
because it matches the lower bound found above. Note that the form of Υs,t
each price is scaled depending on the values of s and t.
That our auction for three bidders matches the lower bound computed by the input distribution used in [8] is strong evidence that this input distribution is the worst-case input distribution for any number of bidders and any generalized profit benchmark.
Furthermore, we strongly suspect that for any number of bidders, the form of the optimal auction will be independent of the benchmark used.
We have seen that optimal auctions for small cases of the limitedsupply model can be found analytically. In this section, we will construct a schema for turning limited supply auctions into unlimited supply auctions with a good competitive ratio.
As discussed in Section 2.5, the existence of a profit extractor, ProfitExtractR, allows an auction to treat a set of bids S as a single bid with value F(S). Given n bidders and an auction, Am, for m < n bidders, we can convert the m-bidder auction into an n-bidder auction by randomly partitioning the bidders into m subsets and then treating each subset as a single bidder (via ProfitExtractR) and running the m-bidder auction.
DEFINITION 7. Given a truthful m-bidder auction, Am, the m-aggregation auction for Am, AggAm , works as follows:
resulting in bid vectors b(1) , . . . , b(m) .
).
Let B be the vector of aggregate bids, and B−j be the aggregate bids for all bins other than j.
bid-independent function for Am.
.
Since Am and ProfitExtractR are truthful, Tj is computed independently of any bid in bin j and thus the price offered any bidder in b(j) is independent of his bid; therefore,
THEOREM 6. If Am is truthful, the m-aggregation auction for Am, AggAm , is truthful.
Note that this schema yields a new way of understanding the Random Sampling Profit Extraction (RSPE) auction [5] as the simplest case of an aggregation auction. It is the 2-aggregation auction for Υ2, the 1-unit Vickrey auction.
To analyze AggAm , consider throwing k balls into m labeled bins. Let k represent a configuration of balls in bins, so that ki is equal to the number of balls in bin i, and k(i) is equal to the number of balls in the ith largest bin. Let Km,k represent the set of all possible configurations of k balls in m bins. We write the multinomial coefficient of k as k k . The probability that a particular configuration k arises by throwing balls into bins uniformly at random is k k m−k .
THEOREM 7. Let Am be an auction with competitive ratio β.
Then the m-aggregation auction for Am, AggAm , raises the following fraction of the optimal revenue F(2) (b): E AggAm (b) F(2) ≥ min k≥2 k∈Km,k F(2) (k) k k βkmk PROOF. By definition, F(2) sells to k ≥ 2 bidders at a single price p. Let kj be the number of such bidders in b(j) . Clearly,
F(b(j) ) ≥ pkj. Therefore,
F(2) (F(b(1) ), . . . , F(b(n) )) F(2)(b) ≥ F(2) (pk1, . . . , pkn) pk = F(2) (k1, . . . , kn) k The inequality follows from the monotonicity of F(2) , and the equality from the homogeneity of F(2) .
ProfitExtractTj will raise Tj if Tj ≤ Bj , and no profit otherwise. Thus, E AggAm (b) ≥ E F(2) (B)/β . The theorem follows by rewriting this expectation as a sum over all k in Km,k.
We apply the aggregation auction schema to Υ3, our optimal auction for three bidders, to achieve an auction with competitive ratio 3.25. This improves on the previously best known auction which is 3.39-competitive [7].
THEOREM 8. The aggregation auction for Υ3 has competitive ratio 3.25. 179 PROOF. By theorem 7,
E AggΥ3 (b) F(2)(b) ≥ min k≥2 k i=1 k−i j=1 F(2) (i, j, k − i − j) k i,j,k−i−j βk3k For k = 2 and k = 3, E AggΥ3 (b) = 2 3 k/β. As k increases,
E AggΥ3 (b) /F(2) increases as well. Since we do not expect to find a closed-form formula for the revenue, we lower bound F(2) (b) by 3b(3). Using large deviation bounds, one can show that this lower bound is greater than 2 3 k/β for large-enough k, and the remainder can be shown by explicit calculation.
Plugging in β = 13/6, the competitive ratio is 13/4. As k increases, the competitive ratio approaches 13/6.
Note that the above bound on the competitive ratio of AggΥ3 is tight. To see this, consider the bid vector with two very large and non-identical bids of h and h + with the remaining bids 1.
Given that the competitive ratio of Υ3 is tight on this example, the expected revenue of this auction on this input will be exactly 13/4.
In this section we show that Υ3 is not the optimal auction to use in an aggregation auction. One can do better by choosing the auction that is optimally competitive against a specially tailored benchmark.
To see why this might be the case, notice (Table 1) that the fraction of F(2) (b) raised for when there are k = 2 and k = 3 winning bidders in F(2) (b) is substantially smaller than the fraction of F(2) (b) raised when there are more winners. This occurs because the expected ratio between F(2) (B) and F(2) (b) is lower in this case while the competitive ratio of Υ3 is constant. If we chose a three bidder auction that performed better when F(2) has smaller numbers of winners, our aggregation auction would perform better in the worst case.
One approach is to compete against a different benchmark that puts more weight than F(2) on solutions with a small number of winners. Recall that F(2) is the instance of Gs,t with s = 2 and t = 3. By using the auction that competes optimally against Gs,t with s > 2, while holding t = 3, we will raise a higher fraction of revenue on smaller numbers of winning bidders and a lower fraction of revenue on large numbers of winning bidders. We can numerically optimize the values of s and t in Gs,t(b) in order to achieve the best competitive ratio for the aggregation auction. In fact, this will allow us to improve our competitive ratio slightly.
THEOREM 9. For an optimal choice of s and t, the aggregation auction for Υs,t
The proof follows the outline of Theorem 7 and 8 with the optimal choice of s = 2.162 (while t is held constant at 3).
There are a number of ways we might attempt to use this aggregation auction schema to continue to push the competitive ratio down. In this section, we give a brief discussion of several attempts.
for m > 3 If the aggregation auction for Υ2 has a competitive ratio of 4 and the aggregation auction for Υ3 has a competitive ratio of 3.25, can we improve the competitive ratio by aggregating Υ4 or Υm for larger m? We conjecture in the negative: for m > 3, the aggregation auction for Υm has a larger competitive ratio than the aggregation auction for Υ3. The primary difficulty in proving this k m = 2 m = 3 m = 4 m = 5 m = 6 m = 7
Table 1: E A(b)/F(2) (b) for AggΥm as a function of k, the optimal number of winners in F(2) (b). The lowest value for each column is printed in bold. conjecture lies in the difficulty of finding a closed-form solution for the formula of Theorem 7. We can, however, evaluate this formula numerically for different values of m and k, assuming that the competitive ratio for Υm matches the lower bound for m given by Theorem 2. Table 1 shows, for each value of m and k, the fraction of F(2) raised by the aggregation auction for AggΥm when there are k winning bidders, assuming the lower bound of Theorem 2 is tight.
As can be seen in Table 1, when m > 3, the worst-case value of k is no longer 2 and 3, but instead an increasing function of m. An aggregation auction for Υm outperforms the aggregation auction for Υ3 when there are two or three winning bidders, while the aggregation auction for Υ3 outperforms the other when there are at least six winning bidders. Thus, for instance, an auction which randomizes between aggregation auctions for Υ3 and Υ4 will have a worst-case which is better than that of either auction alone. Larger combinations of auctions will allow more room to optimize the worst-case. However, we suspect that no convex combination of aggregation auctions will have a competitive ratio lower than 3. Furthermore, note that we cannot yet claim the existence of a good auction via this technique as the optimal auction Υn for n > 3 is not known and it is only conjectured that the bound given by Theorem 2 and represented in Table 1 is correct for Υn.
AUCTIONS In this section, we define a class of auctions that never offer a sale price which is higher than any bid in the input and prove a lower bound on the competitive ratio of these auctions. As this 180 lower bound is stronger than the lower bound of Theorem 2 for n ≥ 3, it shows that the optimal auction must occasionally charge a sales price higher than any bid in the input. Specifically, this result partially explains the form of the optimal three bidder auction.
DEFINITION 8. We say an auction BIf is conservative if its bidindependent function f satisfies f(b-i) ≤ max(b-i).
We can now state our lower bound for conservative auctions.
THEOREM 10. Let A be a conservative auction for n bidders.
Then the competitive ratio of A is at least 3n−2 n .
COROLLARY 1. The competitive ratio of any conservative auction for an arbitrary number of bidders is at least three.
For a two-bidder auction, this restriction does not prevent optimality. Υ2, the 1-unit Vickrey auction, is conservative. For larger numbers of bidders, however, the restriction to conservative auctions does affect the competitive ratio. For the three-bidder case,
Υ3 has competitive ratio 2.17, while the best conservative auction is no better than 2.33-competitive.
The k-Vickrey auction and the Random Sampling Optimal Price auction [9] are conservative auctions. The Random Sampling Profit Extraction auction [5] and the CORE auction [7], on the other hand, use the ProfitExtractR mechanism as a subroutine and thus sometimes offer a sale price which is higher than the highest input bid value.
In [8], the authors define a restricted auction as one on which, for any input, the sale prices are drawn from the set of input bid values. The class of conservative auctions can be viewed as a generalization of the class of restricted auctions and therefore our result below gives lower bounds on the performance of a broader class of auctions.
We will prove Theorem 10 with the aid of the following lemma: LEMMA 1. Let A be a conservative auction with competitive ratio 1/r for n bidders. Let h n. Let h0 = 1 and hk = kh otherwise. Then, for all k and H ≥ kh, Pr[f(1, 1, . . . , 1, H) ≤ hk] ≥ nr−1 n−1 + k(3nr−2r−n n−1 ).
PROOF. The lemma is proved by strong induction on k. First some notation that will be convenient. For any particular k and H we will be considering the bid vector b = (1, . . . , 1, hk, H) and placing bounds on ρb-i (z). Since we can assume without loss of generality that the auction is symmetric, we will notate b-1 as b with any one of the 1-valued bids masked. Similarly we notate b-hk (resp. b-H ) as b with the hk-valued bid (resp. H-valued bid) masked. We will also let p1(b), phk (b), and pH (b) represent the expected payment of a 1-valued, hk-valued, and H-valued bidder in A on b, respectively (note by symmetry the expected payment for all 1-valued bidders is the same).
Base case (k = 0, hk = 1): A must raise revenue of at least rn on b = (1, . . . , 1, 1, H): rn ≤ pH (b) + (n − 1)p1(b) = 1 + (n − 1) 1 0 xρb-1 (x)dx ≤ 1 + (n − 1) 1 0 ρb-1 (x)dx The second inequality follows from the conservatism of the underlying auction. The base case follows trivially from the final inequality.
Inductive case (k > 0, hk = kh): Let b = (1, . . . , 1, hk, H).
First, we will find an upper bound on pH(b) pH (b) = 1 0 xρb-H (x)dx + k i=1 hi hi−1 xρb-H (x)dx (1) ≤ 1 + k i=1 hi hi hi−1 ρb-H (x)dx ≤ 1 + 3nr − 2r − n n − 1 k−1 i=1 ih + kh 1 − nr − 1 n − 1 − (k − 1) 3nr − 2r − n n − 1 (2) = kh n(1 − r) n − 1 + (k − 1) 2 3nr − 2r − n n − 1 + 1. (3) Equation (1) follows from the conservatism of A and (2) is from invoking the strong inductive hypothesis with H = kh and the observation that the maximum possible revenue will be found by placing exactly enough probability at each multiple of h to satisfy the constraints of the inductive hypothesis and placing the remaining probability at kh. Next, we will find a lower bound on phk (b) by considering the revenue raised by the bids b. Recall that A must obtain a profit of at least rF(2) (b) = 2rkh. Given upper-bounds on the profit from the H-valued, equation bid (3), and the 1-valued bids, the profit from the hk-valued bid must be at least: phk (b) ≥ 2rkh − (n − 2)p1(b) − pH(b) ≥ kh 2r − n(1 − r) n − 1 + (k − 1) 2 3nr − 2r − n n − 1 − O(n). (4) In order to lower bound Pr[f(b-hk ) ≤ kh], consider the auction that minimizes it and is consistent with the lower bounds obtained by the strong inductive hypothesis on Pr[f(b-hk ) ≤ ih]. To minimize the constraints implied by the strong inductive hypothesis, we place the minimal amount of probability mass required each price level. This gives ρhk (b) with nr−1 n−1 probability at 1 and exactly 3nr−2r−n n−1 at each hi for 1 ≤ i < k. Thus, the profit from offering prices at most hk−1 is nr−1 n−1 −kh(k−1)3nr−2r−n n−1 . In order to satisfy our lower bound, (4), on phk (b), it must put at least 3nr−2r−n n−1 on hk.
Therefore, the probability that the sale price will be no more than kh on masked bid vector on bid vector b = (1, . . . , 1, kh, H) must be at least nr−1 n−1 + k(3nr−2r−n n−1 ).
Given Lemma 1, Theorem 10 is simple to prove.
PROOF. Let A be a conservative auction. Suppose 3nr−2r−n n−1 = > 0. Let k = 1/ , H ≥ kh, and h n. By Lemma 1,
Pr[f(1, . . . , 1, kh, H) ≤ hk] ≥ nr−1 n−1 + k > 1. But this is a contradiction, so 3nr−2r−n n−1 ≤ 0. Thus, r ≤ n 3n−2 . The theorem follows.
We have found the optimal auction for the three-unit limitedsupply case, and shown that its structure is essentially independent of the benchmark used in its competitive analysis. We have then used this auction to derive the best known auction for the unlimited supply case.
Our work leaves many interesting open questions. We found that the lower bound of [8] is matched by an auction for three bidders, 181 even when competing against generalized benchmarks. The most interesting open question from our work is whether the lower bound from Theorem 2 can be matched by an auction for more than three bidders. We conjecture that it can.
Second, we consider whether our techniques can be extended to find optimal auctions for greater numbers of bidders. The use of our analytic solution method requires knowledge of a restricted class of auctions which is large enough to contain an optimal auction but small enough that the optimal auction in this class can be found explicitly through analytic methods. No class of auctions which meets these criteria is known even for the four bidder case.
Also, when the number of bidders is greater than three, it might be the case that the optimal auction is not expressible in terms of elementary functions.
Another interesting set of open questions concerns aggregation auctions. As we show, the aggregation auction for Υ3 outperforms the aggregation auction for Υ2 and it appears that the aggregation auction for Υ3 is better than Υm for m > 3. We leave verification of this conjecture for future work. We also show that Υ3 is not the best three-bidder auction for use in an aggregation auction, but the auction that beats it is able to reduce the competitive ratio of the overall auction only a little bit. It would be interesting to know whether for any m there is an m-aggregation auction that substantially improves on the competitive ratio of AggΥm .
Finally, we remark that very little is known about the structure of the optimal competitive auction. In our auction Υ3, the sales price for a given bidder is restricted either to be one of the other bid values or to be higher than all other bid values. The optimal auction for two bidders, the 1-unit Vickrey auction, also falls within this class of auctions, as its sales prices are restricted to bid values.
We conjecture that an optimal auction for any number of bidders lies within this class. Our paper provides partial evidence for this conjecture: the lower bound of Section 6 on conservative auctions shows that the optimal auction must offer sales prices higher than any bid value if the lower bound of Theorem 2 is tight, as is conjectured. It remains to show that optimal auctions otherwise only offer sales prices at bid values.
The authors wish to thank Yoav Shoham and Noga Alon for helpful discussions.
[1] A. Archer and E. Tardos. Truthful mechanisms for one-parameter agents. In Proc. of the 42nd IEEE Symposium on Foundations of Computer Science, 2001. [2] S. Baliga and R. Vohra. Market research and market design.
Advances in Theoretical Economics, 3, 2003. [3] A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis. Cambridge University Press, 1998. [4] J. Bulow and J. Roberts. The Simple Economics of Optimal Auctions. The Journal of Political Economy, 97:1060-90,
[5] A. Fiat, A. V. Goldberg, J. D. Hartline, and A. R. Karlin.
Competitive generalized auctions. In Proc. 34th ACM Symposium on the Theory of Computing, pages 72-81.
ACM, 2002. [6] A. Goldberg, J. Hartline, A. Karlin, M. Saks, and A. Wright.
Competitive auctions and digital goods. Games and Economic Behavior, 2002. Submitted for publication. An earlier version available as InterTrust Technical Report at URL http://www.star-lab.com/tr/tr-99-01.html. [7] A. V. Goldberg and J. D. Hartline. Competitiveness via consensus. In Proc. 14th Symposium on Discrete Algorithms, pages 215-222. ACM/SIAM, 2003. [8] A. V. Goldberg, J. D. Hartline, A. R. Karlin, and M. E. Saks.
A lower bound on the competitive ratio of truthful auctions.
In Proc. 21st Symposium on Theoretical Aspects of Computer Science, pages 644-655. Springer, 2004. [9] A. V. Goldberg, J. D. Hartline, and A. Wright. Competitive auctions and digital goods. In Proc. 12th Symposium on Discrete Algorithms, pages 735-744. ACM/SIAM, 2001. [10] D. Lehmann, L. I. O"Callaghan, and Y. Shoham. Truth Revelation in Approximately Efficient Combinatorial Auctions. In Proc. of 1st ACM Conf. on E-Commerce, pages 96-102. ACM Press, New York, 1999. [11] H. Moulin and S. Shenker. Strategyproof Sharing of Submodular Costs: Budget Balance Versus Efficiency.
Economic Theory, 18:511-533, 2001. [12] R. Myerson. Optimal Auction Design. Mathematics of Operations Research, 6:58-73, 1981. [13] N. Nisan and A. Ronen. Algorithmic Mechanism Design. In Proc. of 31st Symp. on Theory of Computing, pages 129-140. ACM Press, New York, 1999. [14] I. Segal. Optimal pricing mechanisms with unknown demand. American Economic Review, 16:50929, 2003. [15] W. Vickrey. Counterspeculation, Auctions, and Competitive Sealed Tenders. J. of Finance, 16:8-37, 1961.
APPENDIX A. PROOF OF THEOREM 4 We wish to prove that Υs,t
against Gs,t, has competitive ratio at least s2 +t2 2t . Our proof follows the outline of the proof of Lemma 5 and Theorem 1 from [8]; however, our case is simpler because we only looking for a bound when n = 3. Define the random bid vector B = (B1, B2, B3) with Pr[Bi > z] = 1/z. We compute EB[Gs,t(B)] by integrating Pr[Gs,t(B) > z]. Then we use the fact that no auction can have expected profit greater than 3 on B to find a lower bound on the competitive ratio against Gs,t for any auction.
For the input distribution B defined above, let B(i) be the ith largest bid. Define the disjoint events H2 = B(2) ≥ z/s ∧ B(3) < z/t, and H3 = B(3) ≥ z/t. Intuitively, H3 corresponds to the event that all three bidders win in Gs,t, while H2 corresponds to the event that only the top two bidders win. Gs,t(B) will be greater than z if either event occurs: Pr[Gs,t(B) > z] = Pr[H2] + Pr[H3] (5) = 3 s z 2
t z + t z 3 (6) Using the identity defined for non-negative continuous random variables that E[X] = ∞ 0 Pr[X > x] dx, we have EB[Gs,t(B)] = t + ∞ t 3 s z 2

A combinatorial auction (CA) [5] provides an efficient means of allocating multiple distinguishable items amongst bidders whose perceived valuations for combinations of items differ. Such auctions are gaining in popularity and there is a proliferation in their usage across various industries such as telecoms, B2B procurement and transportation [11, 19].
Revenue is the most obvious optimization criterion for such auctions, but another desirable attribute is solution robustness. In terms of combinatorial auctions, a robust solution is one that can withstand bid withdrawal (a break) by making changes easily to form a repair solution of adequate revenue. A brittle solution to a CA is one in which an unacceptable loss in revenue is unavoidable if a winning bid is withdrawn. In such situations the bid-taker may be left with a set of items deemed to be of low value by all other bidders. These bidders may associate a higher value for these items if they were combined with items already awarded to others, hence the bid-taker is left in an undesirable local optimum in which a form of backtracking is required to reallocate the items in a manner that results in sufficient revenue. We have called this the Bid-taker"s Exposure Problem that bears similarities to the Exposure Problem faced by bidders seeking multiple items in separate single-unit auctions but holding little or no value for a subset of those items.
However, reallocating items may be regarded as disruptive to a solution in many real-life scenarios. Consider a scenario where procurement for a business is conducted using a CA. It would be highly undesirable to retract contracts from a group of suppliers because of the failure of a third party. A robust solution that is tolerant of such breaks is preferable. Robustness may be regarded as a preventative measure protecting against future uncertainty by sacrificing revenue in place of solution stability and reparability. We assume a probabilistic approach whereby the bid-taker has knowledge of the reliability of bidders from which the likelihood of an incomplete transaction may be inferred.
Repair solutions are required for bids that are seen as brittle (i.e. likely to break). Repairs may also be required for sets of bids deemed brittle. We propose the use of the Weighted Super 183 Solutions (WSS) framework [13] for constraint programming, that is ideal for establishing such robust solutions. As we shall see, this framework can enforce constraints on solutions so that possible breakages are reparable.
This paper is organized as follows. Section 2 presents the Winner Determination Problem (WDP) for combinatorial auctions, outlines some possible reasons for bid withdrawal and shows how simply maximizing expected revenue can lead to intolerable revenue losses for risk-averse bid-takers. This motivates the use of robust solutions and Section 3 introduces a constraint programming (CP) framework, Weighted Super Solutions [13], that finds such solutions. We then propose an auction model in Section 4 that enhances reparability by introducing mandatory mutual bid bonds, that may be seen as a form of leveled commitment contract [26, 27].
Section 5 presents an extensive empirical evaluation of the approach presented in this paper, in the context of a number of well-known combinatorial auction distributions, with very encouraging results.
Section 6 discusses possible extensions and questions raised by our research that deserve future work. Finally, in Section 7 a number of concluding remarks are made.
Before presenting the technical details of our solution to the Bid-taker"s Exposure Problem, we shall present a brief survey of combinatorial auctions and existing techniques for handling bid withdrawal.
Combinatorial auctions involve a single bid-taker allocating multiple distinguishable items amongst a group of bidders. The bidtaker has a set of m items for sale, M = {1, 2, . . . , m}, and bidders submit a set of bids, B = {B1, B2, . . . , Bn}. A bid is a tuple Bj = Sj, pj where Sj ⊆ M is a subset of the items for sale and pj ≥ 0 is a price. The WDP for a CA is to label all bids as either winning or losing so as to maximize the revenue from winning bids without allocating any item to more than one bid. The following is the integer programming formulation for the WDP: max n j=1 pjxj s.t. j|i∈Sj xj ≤ 1, ∀i ∈ {1 . . . m}, xj ∈ {0, 1}.
This problem is NP-complete [23] and inapproximable [25], and is otherwise known as the Set Packing Problem. The above problem formulation assumes the notion of free disposal. This means that the optimal solution need not necessarily sell all of the items. If the auction rules stipulate that all items must be sold, the problem becomes a Set Partition Problem [5]. The WDP has been extensively studied in recent years. The fastest search algorithms that find optimal solutions (e.g. CABOB [25]) can, in practice, solve very large problems involving thousands of bids very quickly.
We assume an auction protocol with a three stage process involving the submission of bids, winner determination, and finally a transaction phase. We are interested in bid withdrawals that occur between the announcement of winning bids and the end of the transaction phase. All bids are valid until the transaction is complete, so we anticipate an expedient transaction process1 . 1 In some instances the transaction period may be so lengthy that consideration of non-winning bids as still being valid may not be fair. Breaks that occur during a lengthy transaction phase are more difficult to remedy and may require a subsequent auction. For example, if the item is a service contract for a given period of time and the break occurs after partial fulfilment of this contract, the other An example of a winning bid withdrawal occurred in an FCC spectrum auction [32]. Withdrawals, or breaks, may occur for various reasons. Bid withdrawal may be instigated by the bid-taker when Quality of Service agreements are broken or payment deadlines are not met. We refer to bid withdrawal by the bid-taker as item withdrawal in this paper to distinguish between the actions of a bidder and the bid-taker. Harstad and Rothkopf [8] outlined several possibilities for breaks in single item auctions that include:
but before the transaction that reduces the value of an item;
profitability, a problem known as the Winner"s Curse.
Kastner et al. [15] examined how to handle perturbations given a solution whilst minimizing necessary changes to that solution.
These perturbations may include bid withdrawals, change of valuation/items of a bid or the submission of a new bid. They looked at the problem of finding incremental solutions to restructure a supply chain whose formation is determined using combinatorial auctions [30]. Following a perturbation in the optimal solution they proceed to impose involuntary item withdrawals from winning bidders. They formulated an incremental integer linear program (ILP) that sought to maximize the valuation of the repair solution whilst preserving the previous solution as much as possible.
When a bid is withdrawn there may be constraints on how the solution can be repaired. If the bid-taker was freely able to revoke the awarding of items to other bidders then the solution could be repaired easily by reassigning all the items to the optimal solution without the withdrawn bid. Alternatively, the bidder who reneged upon a bid may have all his other bids disqualified and the items could be reassigned based on the optimum solution without that bidder present. However, the bid-taker is often unable to freely reassign the items already awarded to other bidders. When items cannot be withdrawn from winning bidders, following the failure of another bidder to honor his bid, repair solutions are restricted to the set of bids whose items only include those in the bid(s) that were reneged upon. We are free to award items to any of the previously unsuccessful bids when finding a repair solution.
When faced with uncertainty over the reliability of bidders a possible approach is to maximize expected revenue. This approach does not make allowances for risk-averse bid-takers who may view a small possibility of very low revenue as unacceptable.
Consider the example in Table 1, and the optimal expected revenue in the situation where a single bid may be withdrawn. There are three submitted bids for items A and B, the third being a combination bid for the pair of items at a value of 190. The optimal solution has a value of 200, with the first and second bids as winners. When we consider the probabilities of failure, in the fourth column, the problem of which solution to choose becomes more difficult.
Computing the expected revenue for the solution with the first and second bids winning the items, denoted 1, 1, 0 , gives: (200×0.9×0.9)+(2×100×0.9×0.1)+(190×0.1×0.1) = 181.90. bidders" valuations for the item may have decreased in a non-linear fashion. 184 Table 1: Example Combinatorial Auction.
Items Bids A B AB Withdrawal prob x1 100 0 0 0.1 x2 0 100 0 0.1 x3 0 0 190 0.1 If a single bid is withdrawn there is probability of 0.18 of a revenue of 100, given the fact that we cannot withdraw an item from the other winning bidder. The expected revenue for 0, 0, 1 is: (190 × 0.9) + (200 × 0.1) = 191.00.
We can therefore surmise that the second solution is preferable to the first based on expected revenue.
Determining the maximum expected revenue in the presence of such uncertainty becomes computationally infeasible however, as the number of brittle bids grows. A WDP needs to be solved for all possible combinations of bids that may fail. The possible loss in revenue for breaks is also not tightly bounded using this approach, therefore a large loss may be possible for a small number of breaks.
Consider the previous example where the bid amount for x3 becomes 175. The expected revenue of 1, 1, 0 (181.75) becomes greater than that of 0, 0, 1 (177.50). There are some bid-takers who may prefer the latter solution because the revenue is never less than 175, but the former solution returns revenue of only 100 with probability 0.18. A risk-averse bid-taker may not tolerate such a possibility, preferring to sacrifice revenue for reduced risk.
If we modify our repair search so that a solution of at least a given revenue is guaranteed, the search for a repair solution becomes a satisfiability test rather than an optimization problem. The approaches described above are in contrast to that which we propose in the next section. Our approach can be seen as preventative in that we find an initial allocation of items to bidders which is robust to bid withdrawal. Possible losses in revenue are bounded by a fixed percentage of the true optimal allocation. Perturbations to the original solution are also limited so as to minimize disruption.
We regard this as the ideal approach for real-world combinatorial auctions.
DEFINITION 1 (ROBUST SOLUTION FOR A CA). A robust solution for a combinatorial auction is one where any subset of successful bids whose probability of withdrawal is greater than or equal to α can be repaired by reassigning items at a cost of at most β to other previously losing bids to form a repair solution.
Constraints on acceptable revenue, e.g. being a minimum percentage of the optimum, are defined in the problem model and are thus satisfied by all solutions. The maximum cost of repair, β, may be a fixed value that may be thought of as a fund for compensating winning bidders whose items are withdrawn from them when creating a repair solution. Alternatively, β may be a function of the bids that were withdrawn. Section 4 will give an example of such a mechanism. In the following section we describe an ideal constraint-based framework for the establishment of such robust solutions.
In constraint programming [4] (CP), a constraint satisfaction problem (CSP) is modeled as a set of n variables X = {x1, . . . , xn}, a set of domains D = {D(x1), . . . , D(xn)}, where D(xi) is the set of finite possible values for variable xi and a set C = {C1, . . . , Cm} of constraints, each restricting the assignments of some subset of the variables in X. Constraint satisfaction involves finding values for each of the problem variables such that all constraints are satisfied. Its main advantages are its declarative nature and flexibility in tackling problems with arbitrary side constraints.
Constraint optimization seeks to find a solution to a CSP that optimizes some objective function. A common technique for solving constraint optimization problems is to use branch-and-bound techniques that avoid exploring sub-trees that are known not to contain a better solution than the best found so far. An initial bound can be determined by finding a solution that satisfies all constraints in C or by using some heuristic methods.
A classical super solution (SS) is a solution to a CSP in which, if a small number of variables lose their values, repair solutions are guaranteed with only a few changes, thus providing solution robustness [9, 10]. It is a generalization of both fault tolerance in CP [31] and supermodels in propositional satisfiability (SAT) [7].
An (a,b)-super solution is one in which if at most a variables lose their values, a repair solution can be found by changing at most b other variables [10].
Super solutions for combinatorial auctions minimize the number of bids whose status needs to be changed when forming a repair solution [12]. Only a particular set of variables in the solution may be subject to change and these are said to be members of the breakset. For each combination of brittle assignments in the break-set, a repair-set is required that comprises the set of variables whose values must change to provide another solution. The cardinality of the repair set is used to measure the cost of repair. In reality, changing some variable assignments in a repair solution incurs a lower cost than others thereby motivating the use of a different metric for determining the legality of repair sets.
The Weighted Super Solution (WSS) framework [13] considers the cost of repair required, rather than simply the number of assignments modified, to form an alternative solution. For CAs this may be a measure of the compensation penalties paid to winning bidders to break existing agreements. Robust solutions are particularly desirable for applications where unreliability is a problem and potential breakages may incur severe penalties. Weighted super solutions offer a means of expressing which variables are easily re-assigned and those that incur a heavy cost [13]. Hebrard et al. [9] describe how some variables may fail (such as machines in a job-shop problem) and others may not. A WSS generalizes this approach so that there is a probability of failure associated with each assignment and sets of variables whose assignments have probabilities of failure greater than or equal to a threshold value, α, require repair solutions.
A WSS measures the cost of repairing, or reassigning, other variables using inertia as a metric. Inertia is a measure of a variable"s aversion to change and depends on its current assignment, future assignment and the breakage variable(s).
It may be desirable to reassign items to different bidders in order to find a repair solution of satisfactory revenue. Compensation may have to be paid to bidders who lose items during the formation of a repair solution. The inertia of a bid reflects the cost of changing its state. For winning bids this may reflect the necessary compensation penalty for the bid-taker to break the agreement (if such breaches are permitted), whereas for previously losing bids this is a free operation. The total amount of compensation payable to bidders may depend upon other factors, such as the cause of the break. There is a limit to how much these overall repair costs should be, and this is given by the value β. This value may not be known in advance and 185 Algorithm 1: WSS(int level, double α, double β):Boolean begin if level > number of variables then return true choose unassigned variable x foreach value v in the domain of x do assign x : v if problem is consistent then foreach combination of brittle assignments, A do if ¬reparable(A, β) then return false; if WSS(level+1) then return true unassign x return false end may depend upon the break. Therefore, β may be viewed as the fund used to compensate winning bidders for the unilateral withdrawal of their bids by the bid-taker. In summary, an (α,β)-WSS allows any set of variables whose probability of breaking is greater than or equal to α be repaired with changes to the original robust solution with a cost of at most β.
The depth-first search for a WSS (see pseudo-code description in Algorithm 1) maintains arc-consistency [24] at each node of the tree. As search progresses, the reparability of each previous assignment is verified at each node by extending a partial repair solution to the same depth as the current partial solution. This may be thought of as maintaining concurrent search trees for repairs.
A repair solution is provided for every possible set of break variables, A. The WSS algorithm attempts to extend the current partial assignment by choosing a variable and assigning it a value.
Backtracking may then occur for one of two reasons: we cannot extend the assignment to satisfy the given constraints, or the current partial assignment cannot be associated with a repair solution whose cost of repair is less than β should a break occur. The procedure reparable searches for partial repair solutions using backtracking and attempts to extend the last repair found, just as in (1,b)super solutions [9]; the differences being that a repair is provided for a set of breakage variables rather than a single variable and the cost of repair is considered. A summation operator is used to determine the overall cost of repair. If a fixed bound upon the size of any potential break-set can be formed, the WSS algorithm is NPcomplete. For a more detailed description of the WSS search algorithm, the reader is referred to [13], since a complete description of the algorithm is beyond the scope of this paper.
EXAMPLE 1. We shall step through the example given in Table 1 when searching for a WSS. Each bid is represented by a single variable with domain values of 0 and 1, the former representing bid-failure and the latter bid-success. The probability of failure of the variables are 0.1 when they are assigned to 1 and 0.0 otherwise. The problem is initially solved using an ILP solver such as lp_solve [3] or CPLEX, and the optimal revenue is found to be
value for a robust solution and its repairs. The bid-taker wishes to have a robust solution so that if a single winning bid is withdrawn, a repair solution can be formed without withdrawing items from any other winning bidder. This example may be seen as searching for a (0.1,0)-weighted super solution, β is 0 because no funds are available to compensate the withdrawal of items from winning bidders. The bid-taker is willing to compromise on revenue, but only by 5%, say, of the optimal value.
Bids 1 and 3 cannot both succeed, since they both require item A, so a constraint is added precluding the assignment in which both variables take the value 1. Similarly, bids 2 and 3 cannot both win so another constraint is added between these two variables.
Therefore, in this example the set of CSP variables is V = {x1, x2, x3}, whose domains are all {0, 1}. The constraints are x1 + x3 ≤ 1, x2 + x3 ≤ 1 and xi∈V aixi ≥ 190, where ai reflects the relevant bid-amounts for the respective bid variables. In order to find a robust solution of optimal revenue we seek to maximize the sum of these amounts, max xi∈V aixi.
When all variables are set to 0 (see Figure 1(a) branch 3), this is not a solution because the minimum revenue of 190 has not been met, so we try assigning bid3 to 1 (branch 4). This is a valid solution but this variable is brittle because there is a 10% chance that this bid may be withdrawn (see Table 1). Therefore we need to determine if a repair can be formed should it break. The search for a repair begins at the first node, see Figure 1(b). Notice that value 1 has been removed from bid3 because this search tree is simulating the withdrawal of this bid. When bid1 is set to 0 (branch 4.1), the maximum revenue solution in the remaining subtree has revenue of only 100, therefore search is discontinued at that node of the tree.
Bid1 and bid2 are both assigned to 1 (branches 4.2 and 4.4) and the total cost of both these changes is still 0 because no compensation needs to be paid for bids that change from losing to winning.
With bid3 now losing (branch 4.5), this gives a repair solution of
our search in Figure 1(a) however, because we are seeking a robust solution of optimal revenue.
When bid1 is assigned to 1 (branch 6) we seek a partial repair for this variable breaking (branch 5 is not considered since it offers insufficient revenue). The repair search sets bid1 to 0 in a separate search tree, (not shown), and control is returned to the search for a WSS. Bid2 is set to 0 (branch 7), but this solution would not produce sufficient revenue so bid2 is then set to 1 (branch 8). We then attempt to extend the repair for bid1 (not shown). This fails because the repair for bid1 cannot assign bid2 to 0 because the cost of repairing such an assignment would be ∞, given that the auction rules do not permit the withdrawal of items from winning bids. A repair for bid1 breaking is therefore not possible because items have already been awarded to bid2. A repair solution with bid2 assigned to 1 does not produce sufficient revenue when bid1 is assigned to 0. The inability to withdraw items from winning bids implies that 1, 1, 0 is an irreparable solution when the minimum tolerable revenue is greater than 100. The italicized comments and dashed line in Figure 1(a) illustrate the search path for a WSS if both of these bids were deemed reparable.
Section 4 introduces an alternative auction model that will allow the bid-taker to receive compensation for breakages and in turn use this payment to compensate other bidders for withdrawal of items from winning bids. This will enable the reallocation of items and permit the establishment of 1, 1, 0 as a second WSS for this example.
BACKTRACKING MECHANISM Some auction solutions are inherently brittle and it may be impossible to find a robust solution. If we can alter the rules of an auction so that the bid-taker can retract items from winning bidders, then the reparability of solutions to such auctions may be improved. In this section we propose an auction model that permits bid and item withdrawal by the bidders and bid-taker, respectively.
We propose a model that incorporates mutual bid bonds to enable solution reparability for the bid-taker, a form of insurance against 186 0 0 0 0
1
Insufficient revenue Find repair solution for bid 3 breakage Find partial repair for bid 1 breakage Insufficient revenue (a) Extend partial repair for bid 1 breakage (b) Find partial repair for bid 2 breakage Bid 1 Bid 2 Bid 3 Find repair solutions for bid 1 & 2 breakages [0] [190] [100] [100] [200] 1 2
5 6
9 Insufficient revenue (a) Search for WSS. 0 0 0 0
1
Insufficient revenue Insufficient revenue Bid 1 Bid 2 Bid 3 inertia=0 inertia=0 inertia=0
(b) Search for a repair for bid 3 breakage.
Figure 1: Search Tree for a WSS without item withdrawal. the winner"s curse for the bidder whilst also compensating bidders in the case of item withdrawal from winning bids. We propose that such Winner"s Curse & Bid-taker"s Exposure insurance comprise a fixed percentage, κ, of the bid amount for all bids. Such mutual bid bonds are mandatory for each bid in our model2 . The conditions attached to the bid bonds are that the bid-taker be allowed to annul winning bids (item withdrawal) when repairing breaks elsewhere in the solution. In the interests of fairness, compensation is paid to bidders from whom items are withdrawn and is equivalent to the penalty that would have been imposed on the bidder should he have withdrawn the bid.
Combinatorial auctions impose a heavy computational burden on the bidder so it is important that the hedging of risk should be a simple and transparent operation for the bidder so as not to further increase this burden unnecessarily. We also contend that it is imperative that the bidder knows the potential penalty for withdrawal in advance of bid submission. This information is essential for bidders when determining how aggressive they should be in their bidding strategy. Bid bonds are commonplace in procurement for construction projects. Usually they are mandatory for all bids, are a fixed percentage, κ, of the bid amount and are unidirectional in that item withdrawal by the bid-taker is not permitted. Mutual bid bonds may be seen as a form of leveled commitment contract in which both parties may break the contract for the same fixed penalty. Such contracts permit unilateral decommitment for prespecified penalties. Sandholm et al. showed that this can increase the expected payoffs of all parties and enables deals that would be impossible under full commitment [26, 28, 29].
In practice a bid bond typically ranges between 5 and 20% of the 2 Making the insurance optional may be beneficial in some instances. If a bidder does not agree to the insurance, it may be inferred that he may have accurately determined the valuation for the items and therefore less likely to fall victim to the winner"s curse.
The probability of such a bid being withdrawn may be less, so a repair solution may be deemed unnecessary for this bid. On the other hand it decreases the reparability of solutions. bid amount [14, 18]. If the decommitment penalties are the same for both parties in all bids, κ does not influence the reparability of a given set of bids. It merely influences the levels of penalties and compensation transacted by agents. Low values of κ incur low bid withdrawal penalties and simulate a dictatorial bid-taker who does not adequately compensate bidders for item withdrawal. Andersson and Sandholm [1] found that myopic agents reach a higher social welfare quicker if they act selfishly rather than cooperatively when penalties in leveled commitment contracts are low. Increased levels of bid withdrawal are likely when the penalties are low also.
High values of κ tend towards full-commitment and reduce the advantages of such Winner"s Curse & Bid-taker"s Exposure insurance. The penalties paid are used to fund a reassignment of items to form a repair solution of sufficient revenue by compensating previously successful bidders for withdrawal of the items from them.
EXAMPLE 2. Consider the example given in Table 1 once more, where the bids also comprise a mutual bid bond of 5% of the bid amount. If a bid is withdrawn, the bidder forfeits this amount and the bid-taker can then compensate winning bidders whose items are withdrawn when trying to form a repair solution later. The search for repair solutions for breaks to bid1 and bid2 appear in Figures 2(a) and 2(b), respectively3 .
When bid1 breaks, there is a compensation penalty paid to the bid-taker equal to 5 that can be used to fund a reassignment of the items. We therefore set β to 5 and this becomes the maximum expenditure allowed to withdraw items from winning bidders. β may also be viewed as the size of the fund available to facilitate backtracking by the bid-taker. When we extend the partial repair for bid1 so that bid2 loses an item (branch 8.1), the overall cost of repair increases to 5, due to this item withdrawal by the bid-taker, 3 The actual implementation of WSS search checks previous solutions to see if they can repair breaks before searching for a new repair solution. 0, 0, 1 is a solution that has already been found so the search for a repair in this example is not strictly necessary but is described for pedagogical reasons. 187 0 0 0 1 1 Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=5 =5 inertia=0 =5 inertia=5 =5 1
(a) Search for a repair for bid 1 breakage. 0 0 0 1 1 Bid 1 Bid 2 Bid 3 Insufficient revenue inertia=10 =10 inertia=10 =10 inertia=10 =10 1
(b) Search for a repair for bid 2 breakage.
Figure 2: Repair Search Tree for breaks 1 and 2, κ = 0.05. and is just within the limit given by β. In Figure 1(a) the search path follows the dashed line and sets bid3 to be 0 (branch 9). The repair solutions for bids 1 and 2 can be extended further by assigning bid3 to 1 (branches 9.2 and 9.4). Therefore, 1, 1, 0 may be considered a robust solution. Recall, that previously this was not the case.
Using mutual bid bonds thus increases reparability and allows a robust solution of revenue 200 as opposed to 190, as was previously the case.
We have used the Combinatorial Auction Test Suite (CATS) [16] to generate sample auction data. We generated 100 instances of problems in which there are 20 items for sale and 100-2000 bids that may be dominated in some instances4 . Such dominated bids can participate in repair solutions although they do not feature in optimal solutions. CATS uses economically motivated bidding patterns to generate auction data in various scenarios. To motivate the research presented in this paper we use sensitivity analysis to examine the brittleness of optimal solutions and hence determine the types of auctions most likely to benefit from a robust solution. We then establish robust solutions for CAs using the WSS framework.
We have performed sensitivity analysis of the following four distributions: airport take-off/landing slots (matching), electronic components (arbitrary), property/spectrum-rights (regions) and transportation (paths). These distributions were chosen because they describe a broad array of bidding patterns in different application domains.
The method used is as follows. We first of all determined the optimal solution using lp_solve, a mixed integer linear program solver [3]. We then simulated a single bid withdrawal and re-solved the problem with the other winning bids remaining fixed, i.e. there were no involuntary dropouts. The optimal repair solution was then determined. This process is repeated for all winning bids in the overall optimal solution, thus assuming that all bids are brittle.
Figure 3 shows the average revenue of such repair solutions as a percentage of the optimum. Also shown is the average worst-case scenario over 100 auctions. We also implemented an auction rule that disallows bids from the reneging bidder participate in a repair5 .
Figure 3(a) illustrates how the paths distribution is inherently the most robust distribution since when any winning bid is withdrawn the solution can be repaired to achieve over 98.5% of the 4 The CATS flags included int prices with the bid alpha parameter set to 1000. 5 We assumed that all bids in a given XOR bid with the same dummy item were from the same bidder. optimal revenue on average for auctions with more than 250 bids.
There are some cases however when such withdrawals result in solutions whose revenue is significantly lower than optimum. Even in auctions with as many as 2000 bids there are occasions when a single bid withdrawal can result in a drop in revenue of over 5%, although the average worst-case drop in revenue is only 1%.
Figure 3(b) shows how the matching distribution is more brittle on average than paths and also has an inferior worst-case revenue on average. This trend continues as the regions-npv (Figure 3(c)) and arbitrary-npv (Figure 3(d)) distributions are more brittle still. These distributions are clearly sensitive to bid withdrawal when no other winning bids in the solution may be involuntarily withdrawn by the bid-taker.
In this section we focus upon both the arbitrary-npv and regions-npv distributions because the sensitivity analysis indicated that these types of auctions produce optimal solutions that tend to be most brittle, and therefore stand to benefit most from solution robustness. We ignore the auctions with 2000 bids because the sensitivity analysis has indicated that these auctions are inherently robust with a very low average drop in revenue following a bid withdrawal. They would also be very computationally expensive, given the extra complexity of finding robust solutions.
A pure CP approach needs to be augmented with global constraints that incorporate operations research techniques to increase pruning sufficiently so that thousands of bids may be examined.
Global constraints exploit special-purpose filtering algorithms to improve performance [21]. There are a number of ways to speed up the search for a weighted super solution in a CA, although this is not the main focus of our current work. Polynomial matching algorithms may be used in auctions whose bid length is short, such as those for airport landing/take-off slots for example. The integer programming formulation of the WDP stipulates that a bid either loses or wins. If we relax this constraint so that bids can partially win, this corresponds to the linear relaxation of the problem and is solvable in polynomial time. At each node of the search tree we can quickly solve the linear relaxation of the remaining problem in the subtree below the current node to establish an upper bound on remaining revenue. If this upper bound plus revenue in the parent tree is less than the current lower bound on revenue, search at that node can cease. The (continuous) LP relaxation thus provides a vital speed-up in the search for weighted super solutions, which we have exploited in our implementation. The LP formulation is as follows: max xi∈V aixi 188 100 95 90 85 80 75
Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (a) paths 100 95 90 85 80 75
Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (b) matching 100 95 90 85 80 75
Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (c) regions-npv 100 95 90 85 80 75
Revenue(%ofoptimum) Bids Average Repair Solution Revenue Worst-case Repair Solution Revenue (d) arbitrary-npv Figure 3: Sensitivity of bid distributions to single bid withdrawal. s.t. j|i∈Sj xj ≤ 1, ∀i ∈ {1 . . . m}, xj ≥ 0, xj ∈ R.
Additional techniques, that are outlined in [25], can aid the scalability of a CP approach but our main aim in these experiments is to examine the robustness of various auction distributions and consider the tradeoff between robustness and revenue. The WSS solver we have developed is an extension of the super solution solver presented in [9, 10]. This solver is, in turn, based upon the EFC constraint solver [2].
Combinatorial auctions are easily modeled as a constraint optimization problems. We have chosen the branch-on-bids formulation because in tests it worked faster than a branch-on-items formulation for the arbitrary-npv and regions-npv distributions. All variables are binary and our search mechanism uses a reverse lexicographic value ordering heuristic. This complements our dynamic variable ordering heuristic that selects the most promising unassigned variable as the next one in the search tree. We use the product of the solution of the LP relaxation and the degree of a variable to determine the likelihood of its participation in a robust solution. High values in the LP solution are a strong indication of variables most likely to form a high revenue solution whilst the a variable"s degree reflects the number of other bids that overlap in terms of desired items. Bids for large numbers of items tend to be more robust, which is why we weight our robust solution search in this manner. We found this heuristic to be slightly more effective than the LP solution alone. As the number of bids in the auction increases however, there is an increase in the inherent robustness of solutions so the degree of a variable loses significance as the auction size increases.
Our experiments simulate three different constraints on repair solutions. The first is that no winning bids are withdrawn by the bid-taker and a repair solution must return a revenue of at least 90% of the optimal overall solution. Secondly, we relaxed the revenue constraint to 85% of optimum. Thirdly, we allowed backtracking by the bid-taker on winning bids using mutual bid bonds but maintaining the revenue constraint at 90% of optimum.
Prior to finding a robust solution we solved the WDP optimally using lp_solve [3]. We then set the minimum tolerable revenue for a solution to be 90% (then 85%) of the revenue of this optimal solution. We assumed that all bids were brittle, thus a repair solution is required for every bid in the solution. Initially we assume that no backtracking was permitted on assignments of items to other winning bids given a bid withdrawal elsewhere in the solution. Table 2 shows the percentage of optimal solutions that are robust for minimum revenue constraints for repair solutions of 90% and 85% of optimal revenue. Relaxing the revenue constraint on repair solutions to 85% of the optimum revenue greatly increases the number of optimal solutions that are robust. We also conducted experiments on the same auctions in which backtracking by the bid-taker is permitted using mutual bid bonds. This significantly improves the reparability of optimal solutions whilst still maintaining repair solutions of 90% of optimum. An interesting feature of the arbitrary-npv distribution is that optimal solutions can become more brittle as the number of bids increases. The reason for this is that optimal solutions for larger auctions have more winning bids. Some of the optimal solutions for the smallest auctions with 100 bids have only one winning bidder. If this bid is withdrawn it is usually easy to find a new repair solution within 90% of the previous optimal revenue. Also, repair solutions for bids that contain a small number of items may be made difficult by the fact that a reduced number of bids cover only a subset of those items. A mitigating factor is that such bids form a smaller percentage of the revenue of the optimal solution on average.
We also implemented a rule stipulating that any losing bids from 189 Table 2: Optimal Solutions that are Inherently Robust (%). #Bids Min Revenue 100 250 500 1000 2000 arbitrary-npv repair ≥ 90% 21 5 3 37 93 repair ≥ 85% 26 15 40 87 100 MBB & repair ≥ 90% 41 35 60 94 ≥ 93 regions-npv repair ≥ 90% 30 33 61 91 98 repair ≥ 85% 50 71 95 100 100 MBB & repair ≥ 90% 60 78 96 99 ≥ 98 Table 3: Occurrence of Robust Solutions (%). #Bids Min Revenue 100 250 500 1000 arbitrary-npv repair ≥ 90% 58 39 51 98 repair ≥ 85% 86 88 94 99 MBB & repair ≥ 90% 78 86 98 100 regions-npv repair ≥ 90% 61 70 97 100 repair ≥ 85% 89 99 99 100 MBB & repair ≥ 90% 83 96 100 100 a withdrawing bidder cannot participate in a repair solution. This acts as a disincentive for strategic withdrawal and was also used previously in the sensitivity analysis. In some auctions, a robust solution may not exist. Table 3 shows the percentage of auctions that support robust solutions for the arbitrary-npv and regions -npv distributions. It is clear that finding robust solutions for the former distribution is particularly difficult for auctions with 250 and
difficulty was previously alluded to by the low percentage of optimal solutions that were robust for these auctions. Relaxing the revenue constraint helps increase the percentage of auctions in which robust solutions are achievable to 88% and 94%, respectively. This improves the reparability of all solutions thereby increasing the average revenue of the optimal robust solution. It is somewhat counterintuitive to expect a reduction in reparability of auction solutions as the number of bids increases because there tends to be an increased number of solutions above a revenue threshold in larger auctions.
The MBB auction model performs very well however, and ensures that robust solutions are achievable for such inherently brittle auctions without sacrificing over 10% of optimal revenue to achieve repair solutions.
Figure 4 shows the average revenue of the optimal robust solution as a percentage of the overall optimum. Repair solutions found for a WSS provide a lower bound on possible revenue following a bid withdrawal. Note that in some instances it is possible for a repair solution to have higher revenue than the original solution.
When backtracking on winning bids by the bid-taker is disallowed, this can only happen when the repair solution includes two or more bids that were not in the original. Otherwise the repair bids would participate in the optimal robust solution in place of the bid that was withdrawn. A WSS guarantees minimum levels of revenue for repair solutions but this is not to say that repair solutions cannot be improved upon. It is possible to use an incremental algorithm to 100 98 96 94 92
Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (a) regions-npv 100 98 96 94 92
Revenue(%ofoptimum) Bids Repair Revenue: Min 90% Optimal Repair Revenue: Min 85% Optimal MBB: Repair Revenue: Min 90% Optimal (b) arbitrary-npv Figure 4: Revenue of optimal robust solutions. determine an optimal repair solution following a break, whilst safe in the knowledge that in advance of any possible bid withdrawal we can establish a lower bound on the revenue of a repair. Kastner et al. have provided such an incremental ILP formulation [15].
Mutual bid bonds facilitate backtracking by the bid-taker on already assigned items. This improves the reparability of all possible solutions thus increasing the revenue of the optimal robust solution on average. Figure 4 shows the increase in revenue of robust solutions in such instances. The revenues of repair solutions are bounded by at least 90% of the optimum in our experiments thereby allowing a direct comparison with robust solutions already found using the same revenue constraint but not providing for backtracking. It is immediately obvious that such a mechanism can significantly increase revenue whilst still maintaining solution robustness.
Table 4 shows the number of winning bids participating in optimal and optimal robust solutions given the three different constraints on repairing solutions listed at the beginning of this section. As the number of bids increases, more of the optimal overall solutions are robust. This leads to a convergence in the number of winning bids. The numbers in brackets are derived from the sensitivity analysis of optimal solutions that reveals the fact that almost all optimal solutions for auctions of 2000 bids are robust. We can therefore infer that the average number of winning bids in revenuemaximizing robust solutions converges towards that of the optimal overall solutions.
A notable side-effect of robust solutions is that fewer bids participate in the solutions. It can be clearly seen from Table 4 that when revenue constraints on repair solutions are tight, there are fewer winning bids in the optimal robust solution on average. This is particularly pronounced for smaller auctions in both distributions.
This can win benefits for the bid-taker such as reduced overheads in dealing with fewer suppliers. Although MBBs aid solution repara190 Table 4: Number of winning bids. #Bids Solution 100 250 500 1000 2000 arbitrary-npv Optimal 3.31 5.60 7.17 9.31 10.63 Repair ≥ 90% 1.40 2.18 6.10 9.03 (≈ 10.63) Repair ≥ 85% 1.65 3.81 6.78 9.31 (10.63) MBB (≥ 90%) 2.33 5.49 7.33 9.34 (≈ 10.63) regions-npv Optimal 4.34 7.05 9.10 10.67 12.76 Repair ≥ 90% 3.03 5.76 8.67 10.63 (≈ 12.76) Repair ≥ 85% 3.45 6.75 9.07 (10.67) (12.76) MBB (≥ 90%) 3.90 6.86 9.10 10.68 (≈ 12.76) bility, the number of bids in the solutions increases on average.
This is to be expected because a greater fraction of these solutions are in fact optimal, as we saw in Table 2.
Bidding strategies can become complex in non-incentive-compatible mechanisms where winner determination is no longer necessarily optimal. The perceived reparability of a bid may influence the bid amount, with reparable bids reaching a lower equilibrium point and perceived irreparable bids being more aggressive.
Penalty payments for bid withdrawal also create an incentive for more aggressive bidding by providing a form of insurance against the winner"s curse [8]. If a winning bidder"s revised valuation for a set of items drops by more than the penalty for withdrawal of the bid, then it is in his best interests to forfeit the item(s) and pay the penalty. Should the auction rules state that the bid-taker will refuse to sell the items to any of the remaining bidders in the event of a withdrawal, then insurance against potential losses will stimulate more aggressive bidding. However, in our case we are seeking to repair the solution with the given bids. A side-effect of such a policy is to offset the increased aggressiveness by incentivizing reduced valuations in expectation that another bidder"s successful bid is withdrawn. Harstad and Rothkopf [8] examined the conditions required to ensure an equilibrium position in which bidding was at least as aggressive as if no bid withdrawal was permitted, given this countervailing incentive to under-estimate a valuation. Three major results arose from their study of bid withdrawal in a single item auction:
sufficiently small probabilities of an award to the second highest bidder in the event of a bid withdrawal;
the number of bidders is large enough;
bidding is more aggressive with withdrawal if the variability of the estimating distribution is sufficiently large.
It is important that mutual bid bonds do not result in depressed bidding in equilibrium. An analysis of the resultant behavior of bidders must incorporate the possibility of a bidder winning an item and having it withdrawn in order for the bid-taker to formulate a repair solution after a break elsewhere. Harstad and Rothkopf have analyzed bidder aggressiveness [8] using a strictly game-theoretic model in which the only reason for bid withdrawal is the winner"s curse. They assumed all bidders were risk-neutral, but surmised that it is entirely possible for the bid-taker to collect a risk premium from risk-averse bidders with the offer of such insurance.
Combinatorial auctions with mutual bid bonds add an extra incentive to bid aggressively because of the possibility of being compensated for having a winning bid withdrawn by a bid-taker. This is militated against by the increased probability of not having items withdrawn in a repair solution. We leave an in-depth analysis of the sufficient conditions for more aggressive bidding for future work.
Whilst the WSS framework provides ample flexibility and expressiveness, scalability becomes a problem for larger auctions.
Although solutions to larger auctions tend to be naturally more robust, some bid-takers in such auctions may require robustness. A possible extension of our work in this paper may be to examine the feasibility of reformulating integer linear programs so that the solutions are robust. Hebrard et al. [10] examined reformulation of CSPs for finding super solutions. Alternatively, it may be possible to use a top-down approach by looking at the k-best solutions sequentially, in terms of revenue, and performing sensitivity analysis upon each solution until a robust one is found. In procurement settings the principle of free disposal is often discounted and all items must be sold. This reduces the number of potential solutions and thereby reduces the reparability of each solution. The impact of such a constraint on revenue of robust solutions is also left for future work.
There is another interesting direction this work may take, namely robust mechanism design. Porter et al. introduced the notion of fault tolerant mechanism design in which agents have private information regarding costs for task completion, but also their probabilities of failure [20]. When the bid-taker has combinatorial valuations for task completions it may be desirable to assign the same task to multiple agents to ensure solution robustness. It is desirable to minimize such potentially redundant task assignments but not to the detriment of completed task valuations. This problem could be modeled using the WSS framework in a similar manner to that of combinatorial auctions.
In the case where no robust solutions are found, it is possible to optimize robustness, instead of revenue, by finding a solution of at least a given revenue that minimizes the probability of an irreparable break. In this manner the least brittle solution of adequate revenue may be chosen.
Fairness is often cited as a reason for choosing the optimal solution in terms of revenue only [22]. Robust solutions militate against bids deemed brittle, therefore bidders must earn a reputation for being reliable to relax the reparability constraint attached to their bids. This may be seen as being fair to long-standing business partners whose reliability is unquestioned. Internet-based auctions are often seen as unwelcome price-gouging exercises by suppliers in many sectors [6, 17]. Traditional business partnerships are being severed by increased competition amongst suppliers. Quality of Service can suffer because of the increased focus on short-term profitability to the detriment of the bid-taker in the long-term.
Robust solutions can provide a means of selectively discriminating against distrusted bidders in a measured manner. As combinatorial auction deployment moves from large value auctions with a small pool of trusted bidders (e.g. spectrum-rights sales) towards lower value auctions with potentially unknown bidders (e.g.
Supply Chain Management [30]), solution robustness becomes more relevant. As well as being used to ensure that the bid-taker is not left vulnerable to bid withdrawal, it may also be used to cement relationships with preferred, possibly incumbent, suppliers. 191 We have shown that it is possible to attain robust solutions for CAs with only a small loss in revenue. We have also illustrated how such solutions tend to have fewer winning bids than overall optimal solutions, thereby reducing any overheads associated with dealing with more bidders. We have also demonstrated that introducing mutual bid bonds, a form of leveled commitment contract, can significantly increase the revenue of optimal robust solutions by improving reparability. We contend that robust solutions using such a mechanism can allow a bid-taker to offer the possibility of bid withdrawal to bidders whilst remaining confident about postrepair revenue and also facilitating increased bidder aggressiveness.
[1] Martin Andersson and Tuomas Sandholm. Leveled commitment contracts with myopic and strategic agents.
Journal of Economic Dynamics and Control, 25:615-640,
Economics. [2] Fahiem Bacchus and George Katsirelos. EFC solver. www.cs.toronto.edu/˜gkatsi/efc/efc.html. [3] Michael Berkelaar, Kjell Eikland, and Peter Notebaert. lp solve version 5.0.10.0. http://groups.yahoo.com/group/lp_solve/. [4] Rina Dechter. Constraint Processing. Morgan Kaufmann,
[5] Sven DeVries and Rakesh Vohra. Combinatorial auctions: A survey. INFORMS Journal on Computing, pages 284-309,
[6] Jim Ericson. Reverse auctions: Bad idea. Line 56, Sept 2001. [7] Matthew L. Ginsberg, Andrew J. Parkes, and Amitabha Roy.
Supermodels and Robustness. In Proceedings of AAAI-98, pages 334-339, Madison, WI, 1998. [8] Ronald M. Harstad and Michael H. Rothkopf. Withdrawable bids as winner"s curse insurance. Operations Research, 43(6):982-994, November-December 1995. [9] Emmanuel Hebrard, Brahim Hnich, and Toby Walsh. Robust solutions for constraint satisfaction and optimization. In Proceedings of the European Conference on Artificial Intelligence, pages 186-190, 2004. [10] Emmanuel Hebrard, Brahim Hnich, and Toby Walsh. Super solutions in constraint programming. In Proceedings of CP-AI-OR 2004, pages 157-172, 2004. [11] Gail Hohner, John Rich, Ed Ng, Grant Reid, Andrew J.
Davenport, Jayant R. Kalagnanam, Ho Soo Lee, and Chae An. Combinatorial and quantity-discount procurement auctions benefit Mars Incorporated and its suppliers.
Interfaces, 33(1):23-35, 2003. [12] Alan Holland and Barry O"Sullivan. Super solutions for combinatorial auctions. In Ercim-Colognet Constraints Workshop (CSCLP 04). Springer LNAI, Lausanne,
Switzerland, 2004. [13] Alan Holland and Barry O"Sullivan. Weighted super solutions for constraint programs, December 2004. Technical Report: No. UCC-CS-2004-12-02. [14] Selective Insurance. Business insurance. http://www.selectiveinsurance.com/psApps /Business/Ins/bonds.asp?bc=13.16.127. [15] Ryan Kastner, Christina Hsieh, Miodrag Potkonjak, and Majid Sarrafzadeh. On the sensitivity of incremental algorithms for combinatorial auctions. In WECWIS, pages 81-88, June 2002. [16] Kevin Leyton-Brown, Mark Pearson, and Yoav Shoham.
Towards a universal test suite for combinatorial auction algorithms. In ACM Conference on Electronic Commerce, pages 66-76, 2000. [17] Associated General Contractors of America. Associated general contractors of America white paper on reverse auctions for procurement of construction. http://www.agc.org/content/public/pdf /Member_Resources/ ReverseAuctionWhitePaper.pdf, 2003. [18] National Society of Professional Engineers. A basic guide to surety bonds. http://www.nspe.org/pracdiv /76-02surebond.asp. [19] Martin Pesendorfer and Estelle Cantillon. Combination bidding in multi-unit auctions. Harvard Business School Working Draft, 2003. [20] Ryan Porter, Amir Ronen, Yoav Shoham, and Moshe Tennenholtz. Mechanism design with execution uncertainty.
In Proceedings of UAI-02, pages 414-421, 2002. [21] Jean-Charles R´egin. Global constraints and filtering algorithms. In Constraint and Integer ProgrammingTowards a Unified Methodology, chapter 4, pages 89-129.
Kluwer Academic Publishers, 2004. [22] Michael H. Rothkopf and Aleksandar Peke˘c. Combinatorial auction design. Management Science, 4(11):1485-1503,
November 2003. [23] Michael H. Rothkopf, Aleksandar Peke˘c, and Ronald M.
Harstad. Computationally manageable combinatorial auctions. Management Science, 44(8):1131-1147, 1998. [24] Daniel Sabin and Eugene C. Freuder. Contradicting conventional wisdom in constraint satisfaction. In A. Cohn, editor, Proceedings of ECAI-94, pages 125-129, 1994. [25] Tuomas Sandholm. Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 135(1-2):1-54, 2002. [26] Tuomas Sandholm and Victor Lesser. Leveled Commitment Contracts and Strategic Breach. Games and Economic Behavior, 35:212-270, January 2001. [27] Tuomas Sandholm and Victor Lesser. Leveled commitment contracting: A backtracking instrument for multiagent systems. AI Magazine, 23(3):89-100, 2002. [28] Tuomas Sandholm, Sandeep Sikka, and Samphel Norden.
Algorithms for optimizing leveled commitment contracts. In Proceedings of the IJCAI-99, pages 535-541. Morgan Kaufmann Publishers Inc., 1999. [29] Tuomas Sandholm and Yunhong Zhou. Surplus equivalence of leveled commitment contracts. Artificial Intelligence, 142:239-264, 2002. [30] William E. Walsh, Michael P. Wellman, and Fredrik Ygge.

Agents can often benefit by coordinating their actions.
Coalitional games capture these opportunities of coordination by explicitly modeling the ability of the agents to take joint actions as primitives. As an abstraction, coalitional games assign a payoff to each group of agents in the game.
This payoff is intended to reflect the payoff the group of agents can secure for themselves regardless of the actions of the agents not in the group. These choices of primitives are in contrast to those of non-cooperative games, of which agents are modeled independently, and their payoffs depend critically on the actions chosen by the other agents.
Coalitional games have appeared in the context of e-commerce. In [7], Kleinberg et al. use coalitional games to study recommendation systems. In their model, each individual knows about a certain set of items, is interested in learning about all items, and benefits from finding out about them.
The payoffs to groups of agents are the total number of distinct items known by its members. Given this coalitional game setting, Kleinberg et al. compute the value of the private information of the agents is worth to the system using the solution concept of the Shapley value (definition can be found in section 2). These values can then be used to determine how much each agent should receive for participating in the system.
As another example, consider the economics behind supply chain formation. The increased use of the Internet as a medium for conducting business has decreased the costs for companies to coordinate their actions, and therefore coalitional game is a good model for studying the supply chain problem. Suppose that each manufacturer purchases his raw materials from some set of suppliers, and that the suppliers offer higher discount with more purchases. The decrease in communication costs will let manufacturers find others interested in the same set of suppliers cheaper, and facilitates formation of coalitions to bargain with the suppliers.
Depending on the set of suppliers and how much from each supplier each coalition purchases, we can assign payoffs to the coalitions depending on the discount it receives. The resulting game can be analyzed using coalitional game theory, and we can answer questions such as the stability of coalitions, and how to fairly divide the benefits among the participating manufacturers. A similar problem, combinatorial coalition formation, has previously been studied in [8].
Representation To capture the coalitional games described above and perform computations on them, we must first find a representation for these games. The na¨ıve solution is to enumerate the payoffs to each set of agents, therefore requiring space 193 exponential in the number of agents in the game. For the two applications described, the number of agents in the system can easily exceed a hundred; this na¨ıve approach will not be scalable to such problems. Therefore, it is critical to find good representation schemes for coalitional games.
We believe that the quality of a representation scheme should be evaluated by four criteria.
Expressivity: the breadth of the class of coalitional games covered by the representation.
Conciseness: the space requirement of the representation.
Efficiency: the efficiency of the algorithms we can develop for the representation.
Simplicity: the ease of use of the representation by users of the system.
The ideal representation should be fully expressive, i.e., it should be able to represent any coalitional games, use as little space as possible, have efficient algorithms for computation, and be easy to use. The goal of this paper is to develop a representation scheme that has properties close to the ideal representation.
Unfortunately, given that the number of degrees of freedom of coalitional games is O(2n ), not all games can be represented concisely using a single scheme due to information theoretic constraints. For any given class of games, one may be able to develop a representation scheme that is tailored and more compact than a general scheme. For example, for the recommendation system game, a highly compact representation would be one that simply states which agents know of which products, and let the algorithms that operate on the representation to compute the values of coalitions appropriately. For some problems, however, there may not be efficient algorithms for customized representations. By having a general representation and efficient algorithms that go with it, the representation will be useful as a prototyping tool for studying new economic situations.
The question of coalitional game representation has only been sparsely explored in the past [2, 3, 4]. In [4], Deng and Papadimitriou focused on the complexity of different solution concepts on coalitional games defined on graphs.
While the representation is compact, it is not fully expressive. In [2], Conitzer and Sandholm looked into the problem of determining the emptiness of the core in superadditive games. They developed a compact representation scheme for such games, but again the representation is not fully expressive either. In [3], Conitzer and Sandholm developed a fully expressive representation scheme based on decomposition. Our work extends and generalizes the representation schemes in [3, 4] through decomposing the game into a set of rules that assign marginal contributions to groups of agents.
We will give a more detailed review of these papers in section
• We develop the marginal contribution networks representation, a fully expressive representation scheme whose size scales according to the complexity of the interactions among the agents. We believe that the representation is also simple and intuitive. • We develop an algorithm for computing the Shapley value of coalitional games under this representation that runs in time linear in the size of the input. • Under the graphical interpretation of the representation, we develop an algorithm for determining the whether a payoff vector is in the core and the emptiness of the core in time exponential only in the treewidth of the graph.
In this section, we will briefly review the basics of coalitional game theory and its two primary solution concepts, the Shapley value and the core.1 We will also review previous work on coalitional game representation in more detail.
Throughout this paper, we will assume that the payoff to a group of agents can be freely distributed among its members. This assumption is often known as the transferable utility assumption.
We can represent a coalition game with transferable utility by the pair N, v , where • N is the set of agents; and • v : 2N → R is a function that maps each group of agents S ⊆ N to a real-valued payoff.
This representation is known as the characteristic form. As there are exponentially many subsets, it will take space exponential in the number of agents to describe a coalitional game.
An outcome in a coalitional game specifies the utilities the agents receive. A solution concept assigns to each coalitional game a set of reasonable outcomes. Different solution concepts attempt to capture in some way outcomes that are stable and/or fair. Two of the best known solution concepts are the Shapley value and the core.
The Shapley value is a normative solution concept. It prescribes a fair way to divide the gains from cooperation when the grand coalition (i.e., N) is formed. The division of payoff to agent i is the average marginal contribution of agent i over all possible permutations of the agents.
Formally, let φi(v) denote the Shapley value of i under characteristic function v, then2 φi(v) = S⊂N s!(n − s − 1)! n! (v(S ∪ {i}) − v(S)) (1) The Shapley value is a solution concept that satisfies many nice properties, and has been studied extensively in the economic and game theoretic literature. It has a very useful axiomatic characterization.
Efficiency (EFF) A total of v(N) is distributed to the agents, i.e., i∈N φi(v) = v(N).
Symmetry (SYM) If agents i and j are interchangeable, then φi(v) = φj(v). 1 The materials and terminology are based on the textbooks by Mas-Colell et al. [9] and Osborne and Rubinstein [11]. 2 As a notational convenience, we will use the lower-case letter to represent the cardinality of a set denoted by the corresponding upper-case letter. 194 Dummy (DUM) If agent i is a dummy player, i.e., his marginal contribution to all groups S are the same, φi(v) = v({i}).
Additivity (ADD) For any two coalitional games v and w defined over the same set of agents N, φi(v + w) = φi(v) + φi(w) for all i ∈ N, where the game v + w is defined as (v + w)(S) = v(S) + w(S) for all S ⊆ N.
We will refer to these axioms later in our proof of correctness of the algorithm for computing the Shapley value under our representation in section 4.
The core is another major solution concept for coalitional games. It is a descriptive solution concept that focuses on outcomes that are stable. Stability under core means that no set of players can jointly deviate to improve their payoffs.
Formally, let x(S) denote i∈S xi. An outcome x ∈ Rn is in the core if ∀S ⊆ N x(S) ≥ v(S) (2) The core was one of the first proposed solution concepts for coalitional games, and had been studied in detail. An important question for a given coalitional game is whether the core is empty. In other words, whether there is any outcome that is stable relative to group deviation. For a game to have a non-empty core, it must satisfy the property of balancedness, defined as follows. Let 1S ∈ Rn denote the characteristic vector of S given by (1S)i =
Let (λS)S⊆N be a set of weights such that each λS is in the range between 0 and 1. This set of weights, (λS)S⊆N , is a balanced collection if for all i ∈ N,
S⊆N λS(1S)i = 1 A game is balanced if for all balanced collections of weights,
S⊆N λSv(S) ≤ v(N) (3) By the Bondereva-Shapley theorem, the core of a coalitional game is non-empty if and only if the game is balanced. Therefore, we can use linear programming to determine whether the core of a game is empty. maximize λ∈R2n S⊆N λSv(S) subject to S⊆N λS1S = 1 ∀i ∈ N λS ≥ 0 ∀S ⊆ N (4) If the optimal value of (4) is greater than the value of the grand coalition, then the core is empty. Unfortunately, this program has an exponential number of variables in the number of players in the game, and hence an algorithm that operates directly on this program would be infeasible in practice.
In section 5.4, we will describe an algorithm that answers the question of emptiness of core that works on the dual of this program instead.
Deng and Papadimitriou looked into the complexity of various solution concepts on coalitional games played on weighted graphs in [4]. In their representation, the set of agents are the nodes of the graph, and the value of a set of agents S is the sum of the weights of the edges spanned by them. Notice that this representation is concise since the space required to specify such a game is O(n2 ). However, this representation is not general; it will not be able to represent interactions among three or more agents. For example, it will not be able to represent the majority game, where a group of agents S will have value of 1 if and only if s > n/2.
On the other hand, there is an efficient algorithm for computing the Shapley value of the game, and for determining whether the core is empty under the restriction of positive edge weights. However, in the unrestricted case, determining whether the core is non-empty is coNP-complete.
Conitzer and Sandholm in [2] considered coalitional games that are superadditive. They described a concise representation scheme that only states the value of a coalition if the value is strictly superadditive. More precisely, the semantics of the representation is that for a group of agents S, v(S) = max {T1,T2,...,Tn}∈Π i v(Ti) where Π is the set of all possible partitions of S. The value v(S) is only explicitly specified for S if v(S) is greater than all partitioning of S other than the trivial partition ({S}).
While this representation can represent all games that are superadditive, there are coalitional games that it cannot represent. For example, it will not be able to represent any games with substitutability among the agents. An example of a game that cannot be represented is the unit game, where v(S) = 1 as long as S = ∅. Under this representation, the authors showed that determining whether the core is non-empty is coNP-complete. In fact, even determining the value of a group of agents is NP-complete.
In a more recent paper, Conitzer and Sandholm described a representation that decomposes a coalitional game into a number of subgames whose sum add up to the original game [3]. The payoffs in these subgames are then represented by their respective characteristic functions. This scheme is fully general as the characteristic form is a special case of this representation. For any given game, there may be multiple ways to decompose the game, and the decomposition may influence the computational complexity. For computing the Shapley value, the authors showed that the complexity is linear in the input description; in particular, if the largest subgame (as measured by number of agents) is of size n and the number of subgames is m, then their algorithm runs in O(m2n ) time, where the input size will also be O(m2n ).
On the other hand, the problem of determining whether a certain outcome is in the core is coNP-complete.
In this section, we will describe the Marginal Contribution Networks representation scheme. We will show that the idea is flexible, and we can easily extend it to increase its conciseness. We will also show how we can use this scheme to represent the recommendation game from the introduction.
Finally, we will show that this scheme is fully expressive, and generalizes the representation schemes in [3, 4].
The basic idea behind marginal contribution networks (MC-nets) is to represent coalitional games using sets of rules. The rules in MC-nets have the following syntactic 195 form: Pattern → value A rule is said to apply to a group of agents S if S meets the requirement of the Pattern. In the basic scheme, these patterns are conjunctions of agents, and S meets the requirement of the given pattern if S is a superset of it. The value of a group of agents is defined to be the sum over the values of all rules that apply to the group. For example, if the set of rules are {a ∧ b} → 5 {b} → 2 then v({a}) = 0, v({b}) = 2, and v({a, b}) = 5 + 2 = 7.
MC-nets is a very flexible representation scheme, and can be extended in different ways. One simple way to extend it and increase its conciseness is to allow a wider class of patterns in the rules. A pattern that we will use throughout the remainder of the paper is one that applies only in the absence of certain agents. This is useful for expressing concepts such as substitutability or default values. Formally, we express such patterns by {p1 ∧ p2 ∧ . . . ∧ pm ∧ ¬n1 ∧ ¬n2 ∧ . . . ∧ ¬nn} which has the semantics that such rule will apply to a group S only if {pi}m i=1 ∈ S and {nj}n j=1 /∈ S. We will call the {pi}m i=1 in the above pattern the positive literals, and {nj}n j=1 the negative literals. Note that if the pattern of a rule consists solely of negative literals, we will consider that the empty set of agents will also satisfy such pattern, and hence v(∅) may be non-zero in the presence of negative literals.
To demonstrate the increase in conciseness of representation, consider the unit game described in section 2.2. To represent such a game without using negative literals, we will need 2n rules for n players: we need a rule of value 1 for each individual agent, a rule of value −1 for each pair of agents to counter the double-counting, a rule of value 1 for each triplet of agents, etc., similar to the inclusion-exclusion principle. On the other hand, using negative literals, we only need n rules: value 1 for the first agent, value 1 for the second agent in the absence of the first agent, value 1 for the third agent in the absence of the first two agents, etc. The representational savings can be exponential in the number of agents.
Given a game represented as a MC-net, we can interpret the set of rules that make up the game as a graph. We call this graph the agent graph. The nodes in the graph will represent the agents in the game, and for each rule in the MCnet, we connect all the agents in the rule together and assign a value to the clique formed by the set of agents. Notice that to accommodate negative literals, we will need to annotate the clique appropriately. This alternative view of MC-nets will be useful in our algorithm for Core-Membership in section 5.
We would like to end our discussion of the representation scheme by mentioning a trade-off between the expressiveness of patterns and the space required to represent them.
To represent a coalitional game in characteristic form, one would need to specify all 2n − 1 values. There is no overhead on top of that since there is a natural ordering of the groups. For MC-nets, however, specification of the rules requires specifying both the patterns and the values. The patterns, if not represented compactly, may end up overwhelming the savings from having fewer values to specify.
The space required for the patterns also leads to a tradeoff between the expressiveness of the allowed patterns and the simplicity of representing them. However, we believe that for most naturally arising games, there should be sufficient structure in the problem such that our representation achieves a net saving over the characteristic form.
As an example, we will use MC-net to represent the recommendation game discussed in the introduction. For each product, as the benefit of knowing about the product will count only once for each group, we need to capture substitutability among the agents. This can be captured by a scaled unit game. Suppose the value of the knowledge about product i is vi, and there are ni agents, denoted by {xj i }, who know about the product, the game for product i can then be represented as the following rules: {x1 i } → vi {x2 i ∧ ¬x1 i } → vi ... {xni i ∧ ¬xni−1 i ∧ · · · ∧ ¬x1 i } → vi The entire game can then be built up from the sets of rules of each product. The space requirement will be O(mn∗ ), where m is the number of products in the system, and n∗ is the maximum number of agents who knows of the same product.
We will discuss the expressiveness and conciseness of our representation scheme and compare it with the previous works in this subsection.
Proposition 1. Marginal contribution networks constitute a fully expressive representation scheme.
Proof. Consider an arbitrary coalitional game N, v in characteristic form representation. We can construct a set of rules to describe this game by starting from the singleton sets and building up the set of rules. For any singleton set {i}, we create a rule {i} → v(i). For any pair of agents {i, j}, we create a rule {i ∧ j} → v({i, j}) − v({i}) − v({j}. We can continue to build up rules in a manner similar to the inclusion-exclusion principle. Since the game is arbitrary,
MC-nets are fully expressive.
Using the construction outlined in the proof, we can show that our representation scheme can simulate the multi-issue representation scheme of [3] in almost the same amount of space.
Proposition 2. Marginal contribution networks use at most a linear factor (in the number of agents) more space than multi-issue representation for any game.
Proof. Given a game in multi-issue representation, we start by describing each of the subgames, which are represented in characteristic form in [3], with a set of rules. 196 We then build up the grand game by including all the rules from the subgames. Note that our representation may require a space larger by a linear factor due to the need to describe the patterns for each rule. On the other hand, our approach may have fewer than exponential number of rules for each subgame, depending on the structure of these subgames, and therefore may be more concise than multi-issue representation.
On the other hand, there are games that require exponentially more space to represent under the multi-issue scheme compared to our scheme.
Proposition 3. Marginal contribution networks are exponentially more concise than multi-issue representation for certain games.
Proof. Consider a unit game over all the agents N. As explained in 3.1, this game can be represented in linear space using MC-nets with negative literals. However, as there is no decomposition of this game into smaller subgames, it will require space O(2n ) to represent this game under the multiissue representation.
Under the agent graph interpretation of MC-nets, we can see that MC-nets is a generalization of the graphical representation in [4], namely from weighted graphs to weighted hypergraphs.
Proposition 4. Marginal contribution networks can represent any games in graphical form (under [4]) in the same amount of space.
Proof. Given a game in graphical form, G, for each edge (i, j) with weight wij in the graph, we create a rule {i, j} → wij. Clearly this takes exactly the same space as the size of G, and by the additive semantics of the rules, it represents the same game as G.
Given a MC-net, we have a simple algorithm to compute the Shapley value of the game. Considering each rule as a separate game, we start by computing the Shapley value of the agents for each rule. For each agent, we then sum up the Shapley values of that agent over all the rules. We first show that this final summing process correctly computes the Shapley value of the agents.
Proposition 5. The Shapley value of an agent in a marginal contribution network is equal to the sum of the Shapley values of that agent over each rule.
Proof. For any group S, under the MC-nets representation, v(S) is defined to be the sum over the values of all the rules that apply to S. Therefore, considering each rule as a game, by the (ADD) axiom discussed in section 2, the Shapley value of the game created from aggregating all the rules is equal to the sum of the Shapley values over the rules.
The remaining question is how to compute the Shapley values of the rules. We can separate the analysis into two cases, one for rules with only positive literals and one for rules with mixed literals.
For rules that have only positive literals, the Shapley value of the agents is v/m, where v is the value of the rule and m is the number of agents in the rule. This is a direct consequence of the (SYM) axiom of the Shapley value, as the agents in a rule are indistinguishable from each other.
For rules that have both positive and negative literals, we can consider the positive and the negative literals separately.
For a given positive literal i, the rule will apply only if i occurs in a given permutation after the rest of the positive literals but before any of the negative literals. Formally, let φi denote the Shapley value of i, p denote the cardinality of the positive set, and n denote the cardinality of the negative set, then φi = (p − 1)!n! (p + n)! v = v p p+n n For a given negative literal j, j will be responsible for cancelling the application of the rule if all positive literals come before the negative literals in the ordering, and j is the first among the negative literals. Therefore, φj = p!(n − 1)! (p + n)! (−v) = −v n p+n p By the (SYM) axiom, all positive literals will have the value of φi and all negative literals will have the value of φj.
Note that the sum over all agents in rules with mixed literals is 0. This is to be expected as these rules contribute
effect on the grand coalition may appear odd at first. But this is because the presence of such rules is to define the values of coalitions smaller than the grand coalition.
In terms of computational complexity, given that the Shapley value of any agent in a given rule can be computed in time linear in the pattern of the rule, the total running time of the algorithm for computing the Shapley value of the game is linear in the size of the input.
QUESTIONS There are a few different but related computational problems associated with the solution concept of the core. We will focus on the following two problems: Definition 1. (Core-Membership) Given a coalitional game and a payoff vector x, determine if x is in the core.
Definition 2. (Core-Non-Emptiness) Given a coalitional game, determine if the core is non-empty.
In the rest of the section, we will first show that these two problems are coNP-complete and coNP-hard respectively, and discuss some complexity considerations about these problems. We will then review the main ideas of tree decomposition as it will be used extensively in our algorithm for Core-Membership. Next, we will present the algorithm for Core-Membership, and show that the algorithm runs in polynomial time for graphs of bounded treewidth. We end by extending this algorithm to answer the question of CoreNon-Emptiness in polynomial time for graphs of bounded treewidth.
The hardness of Core-Membership and Core-NonEmptiness follows directly from the hardness results of games over weighted graphs in [4]. 197 Proposition 6. Core-Membership for games represented as marginal contribution networks is coNP-complete.
Proof. Core-Membership in MC-nets is in the class of coNP since any set of agents S of which v(S) > x(S) will serve as a certificate to show that x does not belong to the core. As for its hardness, given any instance of CoreMembership for a game in graphical form of [4], we can encode the game in exactly the same space using MC-net due to Proposition 4. Since Core-Membership for games in graphical form is coNP-complete, Core-Membership in MC-nets is coNP-hard.
Proposition 7. Core-Non-Emptiness for games represented as marginal contribution networks is coNP-hard.
Proof. The same argument for hardness between games in graphical frm and MC-nets holds for the problem of CoreNon-Emptiness.
We do not know of a certificate to show that Core-NonEmptiness is in the class of coNP as of now. Note that the obvious certificate of a balanced set of weights based on the Bondereva-Shapley theorem is exponential in size. In [4], Deng and Papadimitriou showed the coNP-completeness of Core-Non-Emptiness via a combinatorial characterization, namely that the core is non-empty if and only if there is no negative cut in the graph. In MC-nets, however, there need not be a negative hypercut in the graph for the core to be empty, as demonstrated by the following game (N = {1, 2, 3, 4}): v(S) =   
3/4 if S = {1, 2}, {1, 3}, {1, 4}, or {2, 3, 4}
(5) Applying the Bondereva-Shapley theorem, if we let λ12 = λ13 = λ14 = 1/3, and λ234 = 2/3, this set of weights demonstrates that the game is not balanced, and hence the core is empty. On the other hand, this game can be represented with MC-nets as follows (weights on hyperedges): w({1, 2}) = w({1, 3}) = w({1, 4}) = 3/4 w({1, 2, 3}) = w({1, 2, 4}) = w({1, 3, 4}) = −6/4 w({2, 3, 4}) = 3/4 w({1, 2, 3, 4}) = 10/4 No matter how the set is partitioned, the sum over the weights of the hyperedges in the cut is always non-negative.
To overcome the computational hardness of these problems, we have developed algorithms that are based on tree decomposition techniques. For Core-Membership, our algorithm runs in time exponential only in the treewidth of the agent graph. Thus, for graphs of small treewidth, such as trees, we have a tractable solution to determine if a payoff vector is in the core. By using this procedure as a separation oracle, i.e., a procedure for returning the inequality violated by a candidate solution, to solving a linear program that is related to Core-Non-Emptiness using the ellipsoid method, we can obtain a polynomial time algorithm for Core-Non-Emptiness for graphs of bounded treewidth.
As our algorithm for Core-Membership relies heavily on tree decomposition, we will first briefly review the main ideas in tree decomposition and treewidth.3 Definition 3. A tree decomposition of a graph G = (V, E) is a pair (X, T), where T = (I, F) is a tree and X = {Xi | i ∈ I} is a family of subsets of V , one for each node of T, such that • i∈I Xi = V ; • For all edges (v, w) ∈ E, there exists an i ∈ I with v ∈ Xi and w ∈ Xi; and • (Running Intersection Property) For all i, j, k ∈ I: if j is on the path from i to k in T, then Xi ∩ Xk ⊆ Xj.
The treewidth of a tree decomposition is defined as the maximum cardinality over all sets in X, less one. The treewidth of a graph is defined as the minimum treewidth over all tree decompositions of the graph.
Given a tree decomposition, we can convert it into a nice tree decomposition of the same treewidth, and of size linear in that of T.
Definition 4. A tree decomposition T is nice if T is rooted and has four types of nodes: Leaf nodes i are leaves of T with |Xi| = 1.
Introduce nodes i have one child j such that Xi = Xj ∪ {v} of some v ∈ V .
Forget nodes i have one child j such that Xi = Xj \ {v} for some v ∈ Xj.
Join nodes i have two children j and k with Xi = Xj = Xk.
An example of a (partial) nice tree decomposition together with a classification of the different types of nodes is in Figure 1. In the following section, we will refer to nodes in the tree decomposition as nodes, and nodes in the agent graph as agents.
Our algorithm for Core-Membership takes as an input a nice tree decomposition T of the agent graph and a payoff vector x. By definition, if x belongs to the core, then for all groups S ⊆ N, x(S) ≥ v(S). Therefore, the difference x(S)−v(S) measures how close the group S is to violating the core condition. We call this difference the excess of group S.
Definition 5. The excess of a coalition S, e(S), is defined as x(S) − v(S).
A brute-force approach to determine if a payoff vector belongs to the core will have to check that the excesses of all groups are non-negative. However, this approach ignores the structure in the agent graph that will allow an algorithm to infer that certain groups have non-negative excesses due to 3 This is based largely on the materials from a survey paper by Bodlaender [1]. 198 i j k l nm Introduce Node: Xj = {1, 4} Xk = {1, 4} Forget Node: Xl = {1, 4} Introduce Node: Xm = {1, 2, 4} Xn = {4} Leaf Node: Join Node: Xi = {1, 3, 4} Join Node: Figure 1: Example of a (partial) nice tree decomposition the excesses computed elsewhere in the graph. Tree decomposition is the key to take advantage of such inferences in a structured way.
For now, let us focus on rules with positive literals.
Suppose we have already checked that the excesses of all sets R ⊆ U are non-negative, and we would like to check if the addition of an agent i to the set U will create a group with negative excess. A na¨ıve solution will be to compute the excesses of all sets that include i. The excess of the group (R ∪ {i}) for any group R can be computed as follows e(R ∪ {i}) = e(R) + xi − v(c) (6) where c is the cut between R and i, and v(c) is the sum of the weights of the edges in the cut.
However, suppose that from the tree decomposition, we know that i is only connected to a subset of U, say S, which we will call the entry set to U. Ideally, because i does not share any edges with members of ¯U = (U \ S), we would hope that an algorithm can take advantage of this structure by checking only sets that are subsets of (S ∪ {i}). This computational saving may be possible since (xi −v(c)) in the update equation of (6) does not depend on ¯U. However, we cannot simply ignore ¯U as members of ¯U may still influence the excesses of groups that include agent i through group S. Specifically, if there exists a group T ⊃ S such that e(T) < e(S), then even when e(S ∪ {i}) has non-negative excess, e(T ∪{i}) may have negative excess. In other words, the excess available at S may have been drained away due to T. This motivates the definition of the reserve of a group.
Definition 6. The reserve of a coalition S relative to a coalition U is the minimum excess over all coalitions between S and U, i.e., all T : S ⊆ T ⊆ U. We denote this value by r(S, U). We will refer to the group T that has the minimum excess as arg r(S, U). We will also call U the limiting set of the reserve and S the base set of the reserve.
Our algorithm works by keeping track of the reserves of all non-empty subsets that can be formed by the agents of a node at each of the nodes of the tree decomposition. Starting from the leaves of the tree and working towards the root, at each node i, our algorithm computes the reserves of all groups S ⊆ Xi, limited by the set of agents in the subtree rooted at i, Ti, except those in (Xi\S). The agents in (Xi\S) are excluded to ensure that S is an entry set. Specifically,
S is the entry set to ((Ti \ Xi) ∪ S).
To accomodate for negative literals, we will need to make two adjustments. Firstly, the cut between an agent m and a set S at node i now refers to the cut among agent m, set S, and set ¬(Xi \ S), and its value must be computed accordingly. Also, when an agent m is introduced to a group at an introduce node, we will also need to consider the change in the reserves of groups that do not include m due to possible cut involving ¬m and the group.
As an example of the reserve values we keep track of at a tree node, consider node i of the tree in Figure 1. At node i, we will keep track of the following: r({1}, {1, 2, . . .}) r({3}, {2, 3, . . .}) r({4}, {2, 4, . . .}) r({1, 3}, {1, 2, 3, . . .}) r({1, 4}, {1, 2, 4, . . .}) r({3, 4}, {2, 3, 4, . . .}) r({1, 3, 4}, {1, 2, 3, 4, . . .} where the dots . . . refer to the agents rooted under node m.
For notational use, we will use ri(S) to denote r(S, U) at node i where U is the set of agents in the subtree rooted at node i excluding agents in (Xi \ S). We sometimes refer to these values as the r-values of a node. The details of the r-value computations are in Algorithm 1.
To determine if the payoff vector x is in the core, during the r-value computation at each node, we can check if all of the r-values are non-negative. If this is so for all nodes in the tree, the payoff vector x is in the core. The correctness of the algorithm is due to the following proposition.
Proposition 8. The payoff vector x is not in the core if and only if the r-values at some node i for some group S is negative.
Proof. (⇐) If the reserve at some node i for some group S is negative, then there exists a coalition T for which e(T) = x(T) − v(T) < 0, hence x is not in the core. (⇒) Suppose x is not in the core, then there exists some group R∗ such that e(R∗ ) < 0. Let Xroot be the set of nodes at the root. Consider any set S ∈ Xroot, rroot(S) will have the base set of S and the limiting set of ((N \ Xroot) ∪ S).
The union over all of these ranges includes all sets U for which U ∩ Xroot = ∅. Therefore, if R∗ is not disjoint from Xroot, the r-value for some group in the root is negative.
If R∗ is disjoint from U, consider the forest {Ti} resulting from removal of all tree nodes that include agents in Xroot. 199 Algorithm 1 Subprocedures for Core Membership Leaf-Node(i) 1: ri(Xi) ← e(Xi) Introduce-Node(i) 2: j ← child of i 3: m ← Xi \ Xj {the introduced node} 4: for all S ⊆ Xj, S = ∅ do 5: C ← all hyperedges in the cut of m, S, and ¬(Xi \ S) 6: ri(S ∪ {x}) ← rj(S) + xm − v(C) 7: C ← all hyperedges in the cut of ¬m, S, and ¬(Xi \S) 8: ri(S) ← rj(S) − v(C) 9: end for 10: r({m}) ← e({m}) Forget-Node(i) 11: j ← child of i 12: m ← Xj \ Xi {the forgotten node} 13: for all S ⊆ Xi, S = ∅ do 14: ri(S) = min(rj(S), rj(S ∪ {m})) 15: end for Join-Node(i) 16: {j, k} ← {left, right} child of i 17: for all S ⊆ Xi, S = ∅ do 18: ri(S) ← rj(S) + rk(S) − e(S) 19: end for By the running intersection property, the sets of nodes in the trees Ti"s are disjoint. Thus, if the set R∗ = i Si for some Si ∈ Ti, e(R∗ ) = i e(Si) < 0 implies some group S∗ i has negative excess as well. Therefore, we only need to check the r-values of the nodes on the individual trees in the forest.
But for each tree in the forest, we can apply the same argument restricted to the agents in the tree. In the base case, we have the leaf nodes of the original tree decomposition, say, for agent i. If R∗ = {i}, then r({i}) = e({i}) < 0.
Therefore, by induction, if e(R∗ ) < 0, some reserve at some node would be negative.
We will next explain the intuition behind the correctness of the computations for the r-values in the tree nodes. A detailed proof of correctness of these computations can be found in the appendix under Lemmas 1 and 2.
Proposition 9. The procedure in Algorithm 1 correctly compute the r-values at each of the tree nodes.
Proof. (Sketch) We can perform a case analysis over the four types of tree nodes in a nice tree decomposition.
Leaf nodes (i) The only reserve value to be computed is ri(Xi), which equals r(Xi, Xi), and therefore it is just the excess of group Xi.
Forget nodes (i with child j) Let m be the forgotten node.
For any subset S ⊆ Xi, arg ri(S) must be chosen between the groups of S and S ∪ {m}, and hence we choose between the lower of the two from the r-values at node j.
Introduce nodes (i with child j) Let m be the introduced node. For any subset T ⊆ Xi that includes m, let S denote (T \ {m}). By the running intersection property, there are no rules that involve m and agents of the subtree rooted at node i except those involving m and agents in Xi. As both the base set and the limiting set of the r-values of node j and node i differ by {m}, for any group V that lies between the base set and the limiting set of node i, the excess of group V will differ by a constant amount from the corresponding group (V \ {m}) at node j. Therefore, the set arg ri(T) equals the set arg rj(S) ∪ {m}, and ri(T) = rj(S) + xm − v(cut), where v(cut) is the value of the rules in the cut between m and S. For any subset S ⊂ Xi that does not include m, we need to consider the values of rules that include ¬m as a literal in the pattern. Also, when computing the reserve, the payoff xm will not contribute to group S. Therefore, together with the running intersection property as argued above, we can show that ri(S) = rj(S) − v(cut).
Join nodes (i with left child j and right child k) For any given set S ⊆ Xi, consider the r-values of that set at j and k. If arg rj(S) or arg rk(S) includes agents not in S, then argrj(S) and argrk(S) will be disjoint from each other due to the running intersection property. Therefore, we can decompose arg ri(S) into three sets, (arg rj(S) \ S) on the left, S in the middle, and (arg rk(S) \ S) on the right. The reserve rj(S) will cover the excesses on the left and in the middle, whereas the reserve rk(S) will cover those on the right and in the middle, and so the excesses in the middle is double-counted. We adjust for the double-counting by subtracting the excesses in the middle from the sum of the two reserves rj(S) and rk(S).
Finally, note that each step in the computation of the rvalues of each node i takes time at most exponential in the size of Xi, hence the algorithm runs in time exponential only in the treewidth of the graph.
We can extend the algorithm for Core-Membership into an algorithm for Core-Non-Emptiness. As described in section 2, whether the core is empty can be checked using the optimization program based on the balancedness condition (3). Unfortunately, that program has an exponential number of variables. On the other hand, the dual of the program has only n variables, and can be written as follows: minimize x∈Rn n i=1 xi subject to x(S) ≥ v(S), ∀S ⊆ N (7) By strong duality, optimal value of (7) is equal to optimal value of (4), the primal program described in section
optimal value of (7) is greater than v(N), the core is empty.
We can solve the dual program using the ellipsoid method with Core-Membership as a separation oracle, i.e., a procedure for returning a constraint that is violated. Note that a simple extension to the Core-Membership algorithm will allow us to keep track of the set T for which e(T) < 0 during the r-values computation, and hence we can return the inequality about T as the constraint violated. Therefore,
Core-Non-Emptiness can run in time polynomial in the running time of Core-Membership, which in turn runs in 200 time exponential only in the treewidth of the graph. Note that when the core is not empty, this program will return an outcome in the core.
We have developed a fully expressive representation scheme for coalitional games of which the size depends on the complexity of the interactions among the agents. Our focus on general representation is in contrast to the approach taken in [3, 4]. We have also developed an efficient algorithm for the computation of the Shapley values for this representation. While Core-Membership for MC-nets is coNP-complete, we have developed an algorithm for CoreMembership that runs in time exponential only in the treewidth of the agent graph. We have also extended the algorithm to solve Core-Non-Emptiness. Other than the algorithm for Core-Non-Emptiness in [4] under the restriction of non-negative edge weights, and that in [2] for superadditive games when the value of the grand coalition is given, we are not aware of any explicit description of algorithms for core-related problems in the literature.
The work in this paper is related to a number of areas in computer science, especially in artificial intelligence. For example, the graphical interpretation of MC-nets is closely related to Markov random fields (MRFs) of the Bayes nets community. They both address the issue of of conciseness of representation by using the combinatorial structure of weighted hypergraphs. In fact, Kearns et al. first apply these idea to games theory by introducing a representation scheme derived from Bayes net to represent non-cooperative games [6]. The representational issues faced in coalitional games are closely related to the problem of expressing valuations in combinatorial auctions [5, 10]. The OR-bid language, for example, is strongly related to superadditivity.
The question of the representation power of different patterns is also related to Boolean expression complexity [12].
We believe that with a better understanding of the relationships among these related areas, we may be able to develop more efficient representations and algorithms for coalitional games.
Finally, we would like to end with some ideas for extending the work in this paper. One direction to increase the conciseness of MC-nets is to allow the definition of equivalent classes of agents, similar to the idea of extending Bayes nets to probabilistic relational models. The concept of symmetry is prevalent in games, and the use of classes of agents will allow us to capture symmetry naturally and concisely.
This will also address the problem of unpleasing assymetric representations of symmetric games in our representation.
Along the line of exploiting symmetry, as the agents within the same class are symmetric with respect to each other, we can extend the idea above by allowing functional description of marginal contributions. More concretely, we can specify the value of a rule as dependent on the number of agents of each relevant class. The use of functions will allow concise description of marginal diminishing returns (MDRs).
Without the use of functions, the space needed to describe MDRs among n agents in MC-nets is O(n). With the use of functions, the space required can be reduced to O(1).
Another idea to extend MC-nets is to augment the semantics to allow constructs that specify certain rules cannot be applied simultaneously. This is useful in situations where a certain agent represents a type of exhaustible resource, and therefore rules that depend on the presence of the agent should not apply simultaneously. For example, if agent i in the system stands for coal, we can either use it as fuel for a power plant or as input to a steel mill for making steel, but not for both at the same time. Currently, to represent such situations, we have to specify rules to cancel out the effects of applications of different rules. The augmented semantics can simplify the representation by specifying when rules cannot be applied together.
The authors would like to thank Chris Luhrs, Bob McGrew, Eugene Nudelman, and Qixiang Sun for fruitful discussions, and the anonymous reviewers for their helpful comments on the paper.
[1] H. L. Bodlaender. Treewidth: Algorithmic techniques and results. In Proc. 22nd Symp. on Mathematical Foundation of Copmuter Science, pages 19-36.
Springer-Verlag LNCS 1295, 1997. [2] V. Conitzer and T. Sandholm. Complexity of determining nonemptiness of the core. In Proc. 18th Int. Joint Conf. on Artificial Intelligence, pages 613-618, 2003. [3] V. Conitzer and T. Sandholm. Computing Shapley values, manipulating value division schemes, and checking core membership in multi-issue domains. In Proc. 19th Nat. Conf. on Artificial Intelligence, pages 219-225, 2004. [4] X. Deng and C. H. Papadimitriou. On the complexity of cooperative solution concepts. Math. Oper. Res., 19:257-266, May 1994. [5] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.
Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches. In Proc. 16th Int. Joint Conf. on Artificial Intelligence, pages 548-553, 1999. [6] M. Kearns, M. L. Littman, and S. Singh. Graphical models for game theory. In Proc. 17th Conf. on Uncertainty in Artificial Intelligence, pages 253-260,
[7] J. Kleinberg, C. H. Papadimitriou, and P. Raghavan.
On the value of private information. In Proc. 8th Conf. on Theoretical Aspects of Rationality and Knowledge, pages 249-257, 2001. [8] C. Li and K. Sycara. Algoirthms for combinatorial coalition formation and payoff division in an electronic marketplace. Technical report, Robotics Insititute,
Carnegie Mellon University, November 2001. [9] A. Mas-Colell, M. D. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, New York, 1995. [10] N. Nisan. Bidding and allocation in combinatorial auctions. In Proc. 2nd ACM Conf. on Electronic Commerce, pages 1-12, 2000. [11] M. J. Osborne and A. Rubinstein. A Course in Game Theory. The MIT Press, Cambridge, Massachusetts,
[12] I. Wegener. The Complexity of Boolean Functions.
John Wiley & Sons, New York, October 1987. 201 APPENDIX We will formally show the correctness of the r-value computation in Algorithm 1 of introduce nodes and join nodes.
Lemma 1. The procedure for computing the r-values of introduce nodes in Algorithm 1 is correct.
Proof. Let node m be the newly introduced agent at i.
Let U denote the set of agents in the subtree rooted at i.
By the running intersection property, all interactions (the hyperedges) between m and U must be in node i. For all S ⊆ Xi : m ∈ S, let R denote (U \ Xi) ∪ S), and Q denote (R \ {m}). ri(S) = r(S, R) = min T :S⊆T ⊆R e(T) = min T :S⊆T ⊆R x(T) − v(T) = min T :S⊆T ⊆R x(T \ {m}) + xm − v(T \ {m}) − v(cut) = min T :S\{m}⊆T ⊆Q e(T ) + xm − v(cut) = rj(S) + xm − v(cut) The argument for sets S ⊆ Xi : m /∈ S is symmetric except xm will not contribute to the reserve due to the absence of m.
Lemma 2. The procedure for computing the r-values of join nodes in Algorithm 1 is correct.

In recent years, with the rapid development of the Internet, many protocols and algorithms have been proposed to make the Internet more efficient and reliable. The Internet is a complex distributed system where a multitude of heterogeneous agents cooperate to achieve some common goals, and the existing protocols and algorithms often assume that all agents will follow the prescribed rules without deviation. However, in some settings where the agents are selfish instead of altruistic, it is more reasonable to assume these agents are rational - maximize their own profits - according to the neoclassic economics, and new models are needed to cope with the selfish behavior of such agents.
Towards this end, Nisan and Ronen [14] proposed the framework of algorithmic mechanism design and applied VCG mechanisms to some fundamental problems in computer science, including shortest paths, minimum spanning trees, and scheduling on unrelated machines. The VCG mechanisms [5, 11, 21] are applicable to mechanism design problems whose outputs optimize the utilitarian objective function, which is simply the sum of all agents" valuations. Unfortunately, some objective functions are not utilitarian; even for those problems with a utilitarian objective function, sometimes it is impossible to find the optimal output in polynomial time unless P=NP. Some mechanisms other than VCG mechanism are needed to address these issues.
Archer and Tardos [2] studied a scheduling problem where it is NP-Hard to find the optimal output. They pointed out that a certain monotonicity property of the output work load is a necessary and sufficient condition for the existence of a truthful mechanism for their scheduling problem. Auletta et al. [3] studied a similar scheduling problem. They provided a family of deterministic truthful (2 + )-approximation mechanisms for any fixed number of machines and several (1 + )-truthful mechanisms for some NP-hard restrictions of their scheduling problem. Lehmann et al. [12] studied the single-minded combinatorial auction and gave a√ m-approximation truthful mechanism, where m is the number of goods. They also pointed out that a certain monotonicity in the allocation rule can lead to a truthful mechanism. The work of Mu"alem and Nisan [13] is the closest in spirit to our work. They characterized all truthful mechanisms based on a certain monotonicity property in a single-minded auction setting. They also showed how to used MAX and IF-THEN-ELSE to combine outputs from subproblems. As shown in this paper, the MAX and IF-THEN-ELSE combinations are special cases of the composition-based techniques that we present in this paper for computing the payments in polynomial time under mild assumptions.
More generally, we study how to design truthful mechanisms for binary demand games where the allocation of an agent is either selected or not selected. We also assume that the valuations 213 of agents are uncorrelated, i.e., the valuation of an agent only depends on its own allocation and type. Recall that a mechanism M = (O, P) consists of two parts, an allocation rule O and a payment scheme P. Previously, it is often assumed that there is an objective function g and an allocation rule O, that either optimizes g exactly or approximately. In contrast to the VCG mechanisms, we do not require that the allocation should optimize the objective function. In fact, we do not even require the existence of an objective function. Given any allocation rule O for a binary demand game, we showed that a truthful mechanism M = (O, P) exists for the game if and only if O satisfies a certain monotonicity property. The monotonicity property only guarantees the existence of a payment scheme P such that (O, P) is truthful. We complement this existence theorem with a general framework to design such a payment scheme P. Furthermore, we present general techniques to compute the payment when the output is a composition of the outputs of subgames through the operators or and and; through round-based combinations; or through intermediate results, which may be themselves computed from other subproblems.
The remainder of the paper is organized as follows. In Section 2, we discuss preliminaries and previous works, define binary demand games and discuss the basic assumptions about binary demand games. In Section 3, we show that O satisfying a certain monotonicity property is a necessary and sufficient condition for the existence of a truthful mechanism M = (O, P). A framework is then proposed in Section 4 to compute the payment P in polynomial time for several types of allocation rules O. In Section 5, we provide several examples to demonstrate the effectiveness of our general framework. We conclude our paper in Section 6 with some possible future directions.
As usually done in the literatures about the designing of algorithms or protocols with inputs from individual agents, we adopt the assumption in neoclassic economics that all agents are rational, i.e., they respond to well-defined incentives and will deviate from the protocol only if the deviation improves their gain.
A standard model for mechanism design is as follows. There are n agents 1, . . . , n and each agent i has some private information ti, called its type, only known to itself. For example, the type ti can be the cost that agent i incurs for forwarding a packet in a network or can be a payment that the agent is willing to pay for a good in an auction. The agents" types define the type vector t = (t1, t2, . . . , tn). Each agent i has a set of strategies Ai from which it can choose. For each input vector a = (a1, . . . , an) where agent i plays strategy ai ∈ Ai, the mechanism M = (O, P) computes an output o = O(a) and a payment vector p(a) = (p1(a), . . . , pn(a)). Here the payment pi(·) is the money given to agent i and depends on the strategies used by the agents. A game is defined as G = (S, M), where S is the setting for the game G. Here, S consists the parameters of the game that are set before the game starts and do not depend on the players" strategies. For example, in a unicast routing game [14], the setting consists of the topology of the network, the source node and the destination node.
Throughout this paper, unless explicitly mentioned otherwise, the setting S of the game is fixed and we are only interested in how to design P for a given allocation rule O.
A valuation function v(ti, o) assigns a monetary amount to agent i for each possible output o. Everything about a game S, M , including the setting S, the allocation rule O and the payment scheme P, is public knowledge except the agent i"s actual type ti, which is private information to agent i. Let ui(ti, o) denote the utility of agent i at the outcome of the game o, given its preferences ti. Here, following a common assumption in the literature, we assume the utility for agent i is quasi-linear, i.e., ui(ti, o) = v(ti, o) + Pi(a).
Let a|i ai = (a1, · · · , ai−1, ai, ai+1, · · · , an), i.e., each agent j = i plays an action aj except that the agent i plays ai. Let a−i = (a1, · · · , ai−1, ai+1, · · · , an) denote the actions of all agents except i. Sometimes, we write (a−i, bi) as a|i bi. An action ai is called dominant for i if it (weakly) maximizes the utility of i for all possible strategies b−i of other agents, i.e., ui(ti, O(b−i, ai)) ≥ ui(ti, O(b−i, ai)) for all ai = ai and b−i.
A direct-revelation mechanism is a mechanism in which the only actions available to each agent are to report its private type either truthfully or falsely to the mechanism. An incentive compatible (IC) mechanism is a direct-revelation mechanism in which if an agent reports its type ti truthfully, then it will maximize its utility. Then, in a direct-revelation mechanism satisfying IC, the payment scheme should satisfy the property that, for each agent i, v(ti, O(t)) + pi(t) ≥ v(ti, O(t|i ti)) + pi(t|i ti). Another common requirement in the literature for mechanism design is so called individual rationality or voluntary participation: the agent"s utility of participating in the output of the mechanism is not less than the utility of the agent of not participating. A direct-revelation mechanism is strategproof if it satisfies both IC and IR properties.
Arguably the most important positive result in mechanism design is the generalized Vickrey-Clarke-Groves (VCG) mechanism by Vickrey [21], Clarke [5], and Groves [11]. The VCG mechanism applies to (affine) maximization problems where the objective function is utilitarian g(o, t) = P i v(ti, o) (i.e., the sum of all agents" valuations) and the set of possible outputs is assumed to be finite. A direct revelation mechanism M = (O(t), P(t)) belongs to the VCG family if (1) the allocation O(t) maximizesP i v(ti, o), and (2) the payment to agent i is pi(t) = P j=i vj(tj, O(t))+ hi (t−i), where hi () is an arbitrary function of t−i. Under mild assumptions, VCG mechanisms are the only truthful implementations for utilitarian problems [10].
The allocation rule of a VCG mechanism is required to maximize the objective function in the range of the allocation function.
This makes the mechanism computationally intractable in many cases. Furthermore, replacing an optimal algorithm for computing the output with an approximation algorithm usually leads to untruthful mechanisms if a VCG payment scheme is used. In this paper, we study how to design a truthful mechanism that does not optimize a utilitarian objective function.
A binary demand game is a game G = (S, M), where M = (O, P) and the range of O is {0, 1}n . In other words, the output is a n-tuple vector O(t) = (O1(t), O2(t), . . . , On(t)), where Oi(t) = 1 (respectively, 0) means that agent i is (respectively, is not) selected. Examples of binary demand games include: unicast [14, 22, 9] and multicast [23, 24, 8] (generally subgraph construction by selecting some links/nodes to satisfy some property), facility location [7], and a certain auction [12, 2, 13].
Hereafter, we make the following further assumptions.
a function of v(ti, oi) only is denoted as v(ti, oi).
normalized to 0. This assumption is needed to guarantee the IR property.
Thus, throughout his paper, we only consider these direct-revelation mechanisms in which every agent only needs to reveal its valuation vi = v(ti, 1). 214 Notice that in applications where agents providing service and receiving payment, e.g., unicast and job scheduling, the valuation vi of an agent i is usually negative. For the convenience of presentation, we define the cost of agent as ci = −v(ti, 1), i.e., it costs agent i ci to provide the service. Throughout this paper, we will use ci instead of vi in our analysis. All our results can apply to the case where the agents receive the service rather than provide by setting ci to negative, as in auction.
In a binary demand game, if we want to optimize an objective function g(o, t), then we call it a binary optimization demand game. The main differences between the binary demand games and those problems that can be solved by VCG mechanisms are:
problem) for a problem solvable by VCG while there is no restriction on the objective function for a binary demand game.
optimize an objective function, while a VCG mechanism only uses the output that optimizes the objective function. We even do not require the existence of an objective function.
a binary demand game, while the agents" valuations may be correlated in a VCG mechanism.
In this paper, we assume for technical convenience that the objective function g(o, c), if exists, is continuous with respect to the cost ci, but most of our results are directly applicable to the discrete case without any modification.
Lehmann et al. [12] studied how to design an efficient truthful mechanism for single-minded combinatorial auction. In a singleminded combinatorial auction, each agent i (1 ≤ i ≤ n) only wants to buy a subset Si ⊆ S with private price ci. A single-minded bidder i declares a bid bi = Si, ai with Si ⊆ S and ai ∈ R+ .
In [12], it is assumed that the set of goods allocated to an agent i is either Si or ∅, which is known as exactness. Lehmann et al. gave a greedy round-based allocation algorithm, based on the rank ai |Si|1/2 , that has an approximation ratio √ m, where m is the number of goods in S. Based on the approximation algorithm, they gave a truthful payment scheme. For an allocation rule satisfying (1) exactness: the set of goods allocated to an agent i is either Si or ∅; (2) monotonicity: proposing more money for fewer goods cannot cause a bidder to lose its bid, they proposed a truthful payment scheme as follows: (1) charge a winning bidder a certain amount that does not depend on its own bidding; (2) charge a losing bidder 0. Notice the assumption of exactness reveals that the single minded auction is indeed a binary demand game. Their payment scheme inspired our payment scheme for binary demand game.
In [1], Archer et al. studied the combinatorial auctions where multiple copies of many different items are on sale, and each bidder i desires only one subset Si. They devised a randomized rounding method that is incentive compatible and gave a truthful mechanism for combinatorial auctions with single parameter agents that approximately maximizes the social value of the auction. As they pointed out, their method is strongly truthful in sense that it is truthful with high probability 1 − , where is an error probability. On the contrary, in this paper, we study how to design a deterministic mechanism that is truthful based on some given allocation rules.
In [2], Archer and Tardos showed how to design truthful mechanisms for several combinatorial problems where each agent"s private information is naturally expressed by a single positive real number, which will always be the cost incurred per unit load. The mechanism"s output could be arbitrary real number but their valuation is a quasi-linear function t · w, where t is the private per unit cost and w is the work load. Archer and Tardos characterized that all truthful mechanism should have decreasing work curves w and that the truthful payment should be Pi(bi) = Pi(0) + biwi(bi) − R bi 0 wi(u)du Using this model, Archer and Tardos designed truthful mechanisms for several scheduling related problems, including minimizing the span, maximizing flow and minimizing the weighted sum of completion time problems. Notice when the load of the problems is w = {0, 1}, it is indeed a binary demand game. If we apply their characterization of the truthful mechanism, their decreasing work curves w implies exactly the monotonicity property of the output. But notice that their proof is heavily based on the assumption that the output is a continuous function of the cost, thus their conclusion can"t directly apply to binary demand games.
The paper of Ahuva Mu"alem and Noam Nisan [13] is closest in spirit to our work. They clearly stated that we only discussed a limited class of bidders, single minded bidders, that was introduced by [12]. They proved that all truthful mechanisms should have a monotonicity output and their payment scheme is based on the cut value. With a simple generalization, we get our conclusion for general binary demand game. They proposed several combination methods including MAX, IF-THEN-ELSE construction to perform partial search. All of their methods required the welfare function associated with the output satisfying bitonic property.
Distinction between our contributions and previous results: It has been shown in [2, 6, 12, 13] that for the single minded combinatorial auction, there exists a payment scheme which results in a truthful mechanism if the allocation rule satisfies a certain monotonicity property. Theorem 4 also depends on the monotonicity property, but it is applicable to a broader setting than the single minded combinatorial auction. In addition, the binary demand game studied here is different from the traditional packing IP"s: we only require that the allocation to each agent is binary and the allocation rule satisfies a certain monotonicity property; we do not put any restrictions on the objective function. Furthermore, the main focus of this paper is to design some general techniques to find the truthful payment scheme for a given allocation rule O satisfying a certain monotonicity property.
We discuss several properties that mechanisms need to satisfy in order to be truthful.
THEOREM 1. If a mechanism M = (O, P) satisfies IC, then ∀i, if Oi(t|i ti1 ) = Oi(t|i ti2 ), then pi(t|i ti1 ) = pi(t|i ti2 ).
COROLLARY 2. For any strategy-proof mechanism for a binary demand game G with setting S, if we fix the cost c−i of all agents other than i, the payment to agent i is a constant p1 i if Oi(c) = 1, and it is another constant p0 i if Oi(c) = 0.
THEOREM 3. Fixed the setting S for a binary demand game, if mechanism M = (O, P) satisfies IC, then mechanism M = (O, P ) with the same output method O and pi(c) = pi(c) − δi(c−i) for any function δi(c−i) also satisfies IC.
The proofs of above theorems are straightforward and thus omitted due to space limit. This theorem implies that for the binary demand games we can always normalize the payment to an agent i such that the payment to the agent is 0 when it is not selected.
Hereafter, we will only consider normalized payment schemes. 215
Notice, given the setting S, a mechanism design problem is composed of two parts: the allocation rule O and a payment scheme P.
In this paper, given an allocation rule O we focus our attention on how to design a truthful payment scheme based on O. Given an allocation rule O for a binary demand game, we first present a sufficient and necessary condition for the existence of a truthful payment scheme P.
DEFINITION 1 (MONOTONE NON-INCREASING PROPERTY (MP)).
An output method O is said to satisfy the monotone non-increasing property if for every agent i and two of its possible costs ci1 < ci2 ,
Oi(c|i ci2 ) ≤ Oi(c|i ci1 ).
This definition is not restricted only to binary demand games.
For binary demand games, this definition implies that if Oi(c|i ci2 ) =
ci1 ) = 1.
THEOREM 4. Fix the setting S, c−i in a binary demand game G with the allocation rule O, the following three conditions are equivalent:
such that Oi(c) = 1 if ci < κi(O, c−i) and Oi(c) = 0 if ci > κi(O, c−i). When ci = κi(O, c−i), Oi(c) can be either 0 or 1 depending on the tie-breaker of the allocation rule O. Hereafter, we will not consider the tie-breaker scenario in our proofs.
demand game.
PROOF. The proof that Condition 2 implies Condition is straightforward and is omitted here.
We then show Condition 3 implies Condition 2. The proof of this is similar to a proof in [13]. To prove this direction, we assume there exists an agent i and two valuation vectors c|i ci1 and c|i ci2 , where ci1 < ci2 , Oi(c|i ci2 ) = 1 and Oi(c|i ci1 ) = 0. From corollary 2, we know that pi(c|i ci1 ) = p0 i and pi(c|i ci2 ) = p1 i .
Now fix c−i, the utility for i when ci = ci1 is ui(ci1 ) = p0 i .
When agent i lies its valuation to ci2 , its utility is p1 i − ci1 . Since M = (O, P) is truthful, we have p0 i > p1 i − ci1 .
Now consider the scenario when the actual valuation of agent i is ci = ci2 . Its utility is p1 i − ci2 when it reports its true valuation.
Similarly, if it lies its valuation to ci1 , its utility is p0 i . Since M = (O, P) is truthful, we have p0 i < p1 i − ci2 .
Consequently, we have p1 i −ci2 > p0 i > p1 i −ci1 . This inequality implies that ci1 > ci2 , which is a contradiction.
We then show Condition 1 implies Condition 3. We prove this by constructing a payment scheme and proving that this payment scheme is truthful. The payment scheme is: If Oi(c) = 1, then agent i gets payment pi(c) = κi(O, c−i); else it gets payment pi(c) = 0.
From condition 1, if Oi(c) = 1 then ci > κi(O, c−i). Thus, its utility is κi(O, c−i) − ci > 0, which implies that the payment scheme satisfies the IR. In the following we prove that this payment scheme also satisfies IC property. There are two cases here.
Case 1: ci < κ(O, c−i). In this case, when i declares its true cost ci, its utility is κi(O, c−i) − ci > 0. Now consider the situation when i declares a cost di = ci. If di < κi(O, c−i), then i gets the same payment and utility since it is still selected. If di > κi(O, c−i), then its utility becomes 0 since it is not selected anymore. Thus, it has no incentive to lie in this case.
Case 2: ci ≥ κ(O, c−i). In this case, when i reveals its true valuation, its payment is 0 and the utility is 0. Now consider the situation when i declares a valuation di = ci. If di > κi(O, c−i), then i gets the same payment and utility since it is still not selected.
If di ≤ κi(O, c−i), then its utility becomes κi(O, c−i) − ci ≤ 0 since it is selected now. Thus, it has no incentive to lie.
The equivalence of the monotonicity property of the allocation rule O and the existence of a truthful mechanism using O can be extended to games beyond binary demand games. The details are omitted here due to space limit. We now summarize the process to design a truthful payment scheme for a binary demand game based on an output method O.
General Framework 1 Truthful mechanism design for a binary demand game Stage 1: Check whether the allocation rule O satisfies MP. If it does not, then there is no payment scheme P such that mechanism M = (O, P) is truthful. Otherwise, define the payment scheme P as follows.
Stage 2: Based on the allocation rule O, find the cut value κi(O, c−i) for agent i such that Oi(c|i di) = 1 when di < κi(O, c−i), and Oi(c|i di) = 0 when di > κi(O, c−i).
Stage 3: The payment for agent i is 0 if Oi(c) = 0; the payment is κi(O, c−i) if Oi(c) = 1.
THEOREM 5. The payment defined by our general framework is minimum among all truthful payment schemes using O as output.
To find the truthful payment scheme by using General Framework 1, the most difficult stage seems to be the stage 2. Notice that binary search does not work generally since the valuations of agents may be continuous. We give some general techniques that can help with finding the cut value function under certain circumstances. Our basic approach is as follows. First, we decompose the allocation rule into several allocation rules. Next find the cut value function for each of these new allocation rules. Then, we compute the original cut value function by combining these cut value functions of the new allocation rules.
In this subsection, we introduce techniques to compute the cut value function by combining multiple allocation rules with conjunctions or disconjunctions. For simplicity, given an allocation rule O, we will use κ(O, c) to denote a n-tuple vector (κ1(O, c−1), κ2(O, c−2), . . . , κn(O, c−n)).
Here, κi(O, c−i) is the cut value for agent i when the allocation rule is O and the costs c−i of all other agents are fixed.
THEOREM 6. With a fixed setting S of a binary demand game, assume that there are m allocation rules O1 , O2 , · · · , Om  satisfying the monotonicity property, and κ(Oi , c) is the cut value vector for Oi . Then the allocation rule O(c) = Wm i=1 Oi (c) satisfies the monotonicity property. Moreover, the cut value for O is κ(O, c) = maxm i=1{κ(Oi , c)} Here κ(O, c) = maxm i=1{κ(Oi , c)} means, ∀j ∈ [1, n], κj(O, c−j) = maxm i=1{κj(Oi , c−j)} and O(c) =Wm i=1 Oi (c) means, ∀j ∈ [1, n], Oj(c) = O1 j (c) ∨ O2 j (c) ∨ · · · ∨ Om j (c).
PROOF. Assume that ci > ci and Oi(c) = 1. Without loss of generality, we assume that Ok i (c) = 1 for some k, 1 ≤ k ≤ m. From the assumption that Ok i (c) satisfies MP, we obtain that 216 Ok i (c|i ci) = 1. Thus, Oi(c|i ci) = Wm j=1 Oj (c) = 1. This proves that O(c) satisfies MP. The correctness of the cut value function follows directly from Theorem 4.
Many algorithms indeed fall into this category. To demonstrate the usefulness of Theorem 6, we discuss a concrete example here.
In a network, sometimes we want to deliver a packet to a set of nodes instead of one. This problem is known as multicast. The most commonly used structure in multicast routing is so called shortest path tree (SPT). Consider a network G = (V, E, c), where V is the set of nodes, and vector c is the actual cost of the nodes forwarding the data. Assume that the source node is s and the receivers are Q ⊂ V . For each receiver qi ∈ Q, we compute the shortest path (least cost path), denoted by LCP(s, qi, d), from the source s to qi under the reported cost profile d. The union of all such shortest paths forms the shortest path tree. We then use General Framework 1 to design the truthful payment scheme P when the SPT structure is used as the output for multicast, i.e., we design a mechanism M = (SPT, P). Notice that VCG mechanisms cannot be applied here since SPT is not an affine maximization.
We define LCP(s,qi) as the allocation corresponds to the path LCP(s, qi, d), i.e., LCP (s,qi) k (d) = 1 if and only if node vk is in LCP(s, qi, d). Then the output SPT is defined as W qi∈Q LCP(s,qi) .
In other words, SPTk(d) = 1 if and only if qk is selected in some LCP(s, qi, d). The shortest path allocation rule is a utilitarian and satisfies MP. Thus, from Theorem 6, SPT also satisfies MP, and the cut value function vector for SPT can be calculated as κ(SPT, c) = maxqi∈Q κ(LCP(s,qi) , c), where κ(LCP(s,qi) , c) is the cut value function vector for the shortest path LCP(s, qi, c).
Consequently, the payment scheme above is truthful and the minimum among all truthful payment schemes when the allocation rule is SPT.
THEOREM 7. Fixed the setting S of a binary demand game, assume that there are m output methods O1 , O2 , · · · , Om  satisfying MP, and κ(Oi , c) are the cut value functions respectively for Oi where i = 1, 2, · · · , m. Then the allocation rule O(c) =Vm i=1 Oi (c) satisfies MP. Moreover, the cut value function for O is κ(O, c) = minm i=1{κ(Oi , c)}.
We show that our simple combination generalizes the IF-THENELSE function defined in [13]. For an agent i, assume that there are two allocation rules O1 and O2 satisfying MP. Let κi(O1 , c−i), κi(O2 , c−i) be the cut value functions for O1 , O2 respectively.
Then the IF-THEN-ELSE function Oi(c) is actually Oi(c) = [(ci ≤ κi(O1 , c−i) + δ1(c−i)) ∧ O2 (c−i, ci)] ∨ (ci < κi(O1 , c−i) − δ2(c−i)) where δ1(c−i) and δ2(c−i) are two positive functions. By applying Theorems 6 and 7, we know that the allocation rule O satisfies MP and consequently κi(O, c−i) = max{min(κi(O1 , c−i)+ δ1(c−i), κi(O2 , c−i)), κi(O1 , c−i) − δ2(c−i))}.
Some approximation algorithms are round-based, where each round of an algorithm selects some agents and updates the setting and the cost profile if necessary. For example, several approximation algorithms for minimum weight vertex cover [19], maximum weight independent set, minimum weight set cover [4], and minimum weight Steiner [18] tree fall into this category.
As an example, we discuss the minimum weighted vertex cover problem (MWVC) [16, 15] to show how to compute the cut value for a round-based output. Given a graph G = (V, E), where the nodes v1, v2, . . . , vn are the agents and each agent vi has a weight ci, we want to find a node set V ⊆ V such that for every edge (u, v) ∈ E at least one of u and v is in V . Such V is called a vertex cover of G. The valuation of a node i is −ci if it is selected; otherwise its valuation is 0. For a subset of nodes V ∈ V , we define its weight as c(V ) = P i∈V ci.
We want to find a vertex cover with the minimum weight. Hence, the objective function to be implemented is utilitarian. To use the VCG mechanism, we need to find the vertex cover with the minimum weight, which is NP-hard [16]. Since we are interested in mechanisms that can be computed in polynomial time, we must use polynomial-time computable allocation rules. Many algorithms have been proposed in the literature to approximate the optimal solution. In this paper, we use a 2-approximation algorithm given in [16]. For the sake of completeness, we briefly review this algorithm here. The algorithm is round-based. Each round selects some vertices and discards some vertices. For each node i, w(i) is initialized to its weight ci, and when w(i) drops to 0, i is included in the vertex cover. To make the presentation clear, we say an edge (i1, j1) is lexicographically smaller than edge (i2, j2) if (1) min(i1, j1) < min(i2, j2), or (2) min(i1, j1) = min(i2, j2) and max(i1, j1) < max(i2, j2).
Algorithm 2 Approximate Minimum Weighted Vertex Cover Input: A node weighted graph G = (V, E, c).
Output: A vertex cover V . 1: Set V = ∅. For each i ∈ V , set w(i) = ci. 2: while V is not a vertex cover do 3: Pick an uncovered edge (i, j) with the least lexicographic order among all uncovered edges. 4: Let m = min(w(i), w(j)). 5: Update w(i) to w(i) − m and w(j) to w(j) − m. 6: If w(i) = 0, add i to V . If w(j) = 0, add j to V .
Notice, selecting an edge using the lexicographic order is crutial to guarantee the monotonicity property. Algorithm 2 outputs a vertex cover V whose weight is within 2 times of the optimum.
For convenience, we use VC(c) to denote the vertex cover computed by Algorithm 2 when the cost vector of vertices is c. Below we generalize Algorithm 2 to a more general scenario. Typically, a round-based output can be characterized as follows (Algorithm 3).
DEFINITION 2. An updating rule Ur is said to be crossingindependent if, for any agent i not selected in round r, (1) Sr+1 and cr+1 −i do not depend on cr j (2) for fixed cr −i, cr i1 ≤ cr i2 implies that cr+1 i1 ≤ cr+1 i2 .
We have the following theorem about the existence of a truthful payment using a round based allocation rule A.
THEOREM 8. A round-based output A, with the framework defined in Algorithm 3, satisfies MP if the output methods Or satisfy MP and all updating rules Ur are crossing-independent.
PROOF. Consider an agent i and fixed c−i. We prove that when an agent i is selected with cost ci, then it is also selected with cost di < ci. Assume that i is selected in round r with cost ci. Then under cost di, if agent i is selected in a round before r, our claim holds. Otherwise, consider in round r. Clearly, the setting Sr and the costs of all other agents are the same as what if agent i had cost ci since i is not selected in the previous rounds due to the crossindependent property. Since i is selected in round r with cost ci, i is also selected in round r with di < ci due to the reason that Or satisfies MP. This finishes the proof. 217 Algorithm 3 A General Round-Based Allocation Rule A 1: Set r = 0, c0 = c, and G0 = G initially. 2: repeat 3: Compute an output or using a deterministic algorithm Or : Sr × cr → {0, 1}n .
Here Or , cr and Sr are allocation rule, cost vector and game setting in game Gr , respectively.
Remark: Or is often a simple greedy algorithm such as selecting the agents that minimize some utilitarian function.
For the example of vertex cover, Or will always select the light-weighted node on the lexicographically least uncovered edge (i, j). 4: Let r = r + 1. Update the game Gr−1 to obtain a new game Gr with setting Sr and cost vector cr according to some rule Ur : Or−1 × (Sr−1 , cr−1 ) → (Sr , cr ).
Here we updates the cost and setting of the game.
Remark: For the example of vertex cover, the updating rule will decrease the weight of vertices i and j by min(w(i), w(j)). 5: until a valid output is found 6: Return the union of the set of selected players of each round as the final output. For the example of vertex cover, it is the union of nodes selected in all rounds.
Algorithm 4 Compute Cut Value for Round-Based Algorithms Input: A round-based output A, a game G1 = G, and a updating function vector U.
Output: The cut value x for agent k. 1: Set r = 0 and ck = ζ. Recall that ζ is a value that can guarantee Ak = 0 when an agent reports the cost ζ. 2: repeat 3: Compute an output or using a deterministic algorithm based on setting Sr using allocation rule Or : Sr ×cr → {0, 1}n . 4: Find the cut value for agent k based on the allocation rule Or for costs cr −k. Let r = κk(Or , cr −k) be the cut value. 5: Set r = r + 1 and obtain a new game Gr from Gr−1 and or according to the updating rule Ur . 6: Let cr be the new cost vector for game Gr . 7: until a valid output is found. 8: Let gi(x) be the cost of ci k when the original cost vector is c|k x. 9: Find the minimum value x such that 8 >>>>>< >>>>>: g1(x) ≥ 1; g2(x) ≥ 2; ... gt−1(x) ≥ t−1; gt(x) ≥ t.
Here, t is the total number of rounds. 10: Output the value x as the cut value.
If the round-based output satisfies monotonicity property, the cut-value always exists. We then show how to find the cut value for a selected agent k in Algorithm 4.
The correctness of Algorithm 4 is straightforward. To compute the cut value, we assume that (1) the cut value r for each round r can be computed in polynomial time; (2) we can solve the equation gr(x) = r to find x in polynomial time when the cost vector c−i and b are given.
Now we consider the vertex cover problem. For each round r, we select a vertex with the least weight and that is incident on the lexicographically least uncovered edge. The output satisfies MP.
For agent i, we update its cost to cr i − cr j iff edge (i, j) is selected.
It is easy to verify this updating rule is crossing-independent, thus we can apply Algorithm 4 to compute the cut value for the set cover game as shown in Algorithm 5.
Algorithm 5 Compute Cut Value for MVC.
Input: A node weighted graph G = (V, E, c) and a node k selected by Algorithm 2.
Output: The cut value κk(V C, c−k). 1: For each i ∈ V , set w(i) = ci. 2: Set w(k) = ∞, pk = 0 and V = ∅. 3: while V is not a vertex cover do 4: Pick an uncovered edge (i, j) with the least lexicographic order among all uncovered edges. 5: Set m = min(w(i), w(j)). 6: Update w(i) = w(i) − m and w(j) = w(j) − m. 7: If w(i) = 0, add i to V ; else add j to V . 8: If i == k or j == k then set pk = pk + m. 9: Output pk as the cut value κk(V C, c−k).
In subsection 4.1, we discussed how to find the cut value function when the output of the binary demand game is a simple combination of some outputs, whose cut values can be computed through other means (typically VCG). However, some algorithms cannot be decomposed in the way described in subsection 4.1.
Next we present a more complex way to combine allocation rules, and as we may expected, the way to find the cut value is also more complicated. Assume that there are n agents 1 ≤ i ≤ n with cost vector c, and there are m binary demand games Gi with objective functions fi(o, c), setting Si and allocation rule ψi where i = 1, 2, · · · , m. There is another binary demand game with setting S and allocation rule O, whose input is a cost vector d = (d1, d2, · · · , dm). Let f be the function vector (f1, f2, · · · , fm), ψ be the allocation rule vector (ψ1 , ψ2 , · · · , ψm ) and ∫ be the setting vector (S1, S2, · · · , Sm). For notation simplicity, we define Fi(c) = fi(ψi (c), c), for each 1 ≤ i ≤ m, and F(c) = (F1(c), F2(c), · · · , Fm(c)).
Let us see a concrete example of these combinations. Consider a link weighted graph G = (V, E, c), and a subset of q nodes Q ⊆ V . The Steiner tree problem is to find a set of links with minimum total cost to connect Q. One way to find an approximation of the Steiner tree is as follows: (1) we build a virtual complete graph H using Q as its vertices, and the cost of each edge (i, j) is the cost of LCP(i, j, c) in graph G; (2) build the minimum spanning tree of H, denoted as MST(H); (3) an edge of G is selected iff it is selected in some LCP(i, j, c) and edge (i, j) of H is selected to MST(H).
In this game, we define q(q − 1)/2 games Gi,j, where i, j ∈ Q, with objective functions fi,j(o, c) being the minimum cost of 218 connecting i and j in graph G, setting Si being the original graph G and allocation rule is LCP(i, j, c). The game G corresponds to the MST game on graph H. The cost of the pair-wise q(q − 1)/2 shortest paths defines the input vector d = (d1, d2, · · · , dm) for game MST. More details will be given in Section 5.2.
DEFINITION 3. Given an allocation rule O and setting S, an objective function vector f, an allocation rule vector ψ and setting vector ∫, we define a compound binary demand game with setting S and output O ◦ F as (O ◦ F)i(c) = Wm j=1(Oj(F(c)) ∧ ψj i (c)).
The allocation rule of the above definition can be interpreted as follows. An agent i is selected if and only if there is a j such that (1) i is selected in ψj (c), and (2) the allocation rule O will select index j under cost profile F(c). For simplicity, we will use O ◦ F to denote the output of this compound binary demand game.
Notice that a truthful payment scheme using O ◦ F as output exists if and only if it satisfies the monotonicity property. To study when O ◦F satisfies MP, several necessary definitions are in order.
DEFINITION 4. Function Monotonicity Property (FMP) Given an objective function g and an allocation rule O, a function H(c) = g(O(c), c) is said to satisfy the function monotonicity property, if, given fixed c−i, it satisfies:
DEFINITION 5. Strong Monotonicity Property (SMP) An allocation rule O is said to satisfy the strong monotonicity property if O satisfies MP, and for any agent i with Oi(c) = 1 and agent j = i, Oi(c|j cj) = 1 if cj ≥ cj or Oj(c|j cj) = 0.
LEMMA 1. For a given allocation rule O satisfying SMP and cost vectors c, c with ci = ci, if Oi(c) = 1 and Oi(c ) = 0, then there must exist j = i such that cj < cj and Oj(c ) = 1.
From the definition of the strong monotonicity property, we have Lemma 1 directly. We now can give a sufficient condition when O ◦ F satisfies the monotonicity property.
THEOREM 9. If ∀i ∈ [1, m], Fi satisfies FMP, ψi satisfies MP, and the output O satisfies SMP, then O ◦ F satisfies MP.
PROOF. Assuming for cost vector c we have (O ◦ F)i(c) = 1, we should prove for any cost vector c = c|i ci with ci < ci, (O ◦ F)i(c ) = 1. Noticing that (O ◦ F)i(c) = 1, without loss of generality, we assume that Ok(F(c)) = 1 and ψk i (c) = 1 for some index 1 ≤ k ≤ m.
Now consider the output O with the cost vector F(c )|k Fk(c).
There are two scenarios, which will be studied one by one as follows.
One scenario is that index k is not chosen by the output function O. From Lemma 1, there must exist j = k such that Fj(c ) < Fj(c) (1) Oj(F(c )|k Fk(c)) = 1 (2) We then prove that agent i will be selected in the output ψj (c ), i.e., ψj i (c ) = 1. If it is not, since ψj (c) satisfies MP, we have ψj i (c) = ψj i (c ) = 0 from ci < ci. Since Fj satisfies FMP, we know Fj(c ) ≥ Fj(c), which is a contradiction to the inequality (1). Consequently, we have ψj i (c ) = 1. From Equation (2), the fact that index k is not selected by allocation rule O and the definition of SMP, we have Oj(F(c )) = 1, Thus, agent i is selected by O ◦ F because of Oj(F(c )) = 1 and ψj i (c ) = 1.
The other scenario is that index k is chosen by the output function O. First, agent i is chosen in ψk (c ) since the output ψk (c) satisfies the monotonicity property and ci < ci and ψk i (c) = 1.
Secondly, since the function Fk satisfies FMP, we know that Fk(c ) ≤ Fk(c). Remember that output O satisfies the SMP, thus we can obtain Ok(F(c )) = 1 from the fact that Ok(F(c )|k Fk(c)) = 1 and Fk(c ) ≤ Fk(c). Consequently, agent i will also be selected in the final output O ◦ F. This finishes our proof.
This theorem implies that there is a cut value for the compound output O ◦ F. We then discuss how to find the cut value for this output. Below we will give an algorithm to calculate κi(O ◦ F) when (1) O satisfies SMP, (2) ψj satisfies MP, and (3) for fixed c−i,
Fj(c) is a constant, say hj, when ψj i (c) = 0, and Fj(c) increases when ψj i (c) = 1. Notice that here hj can be easily computed by setting ci = ∞ since ψj satisfies the monotonicity property. When given i and fixed c−i, we define (Fi j )−1 (y) as the smallest x such that Fj(c|i x) = y. For simplicity, we denote (Fi j )−1 as F−1 j if no confusion is caused when i is a fixed agent. In this paper, we assume that given any y, we can find such x in polynomial time.
Algorithm 6 Find Cut Value for Compound Method O ◦ F Input: allocation rule O, objective function vector F and inverse function vector F−1 = {F−1
m }, allocation rule vector ψ and fixed c−i.
Output: Cut value for agent i based on O ◦ F. 1: for 1 ≤ j ≤ m do 2: Compute the outputs ψj (ci). 3: Compute hj = Fj(c|i ∞). 4: Use h = (h1, h2, · · · , hm) as the input for the output function O. Denote τj = κj(O, h−j) as the cut value function of output O based on input h. 5: for 1 ≤ j ≤ m do 6: Set κi,j = F−1 j (min{τj, hj}). 7: The cut value for i is κi(O ◦ F, c−i) = maxm j=1 κi,j.
THEOREM 10. Algorithm 6 computes the correct cut value for agent i based on the allocation rule O ◦ F.
PROOF. In order to prove the correctness of the cut value function calculated by Algorithm 6, we prove the following two cases.
For our convenience, we will use κi to represent κi(O ◦ F, c−i) if no confusion caused.
First, if di < κi then (O ◦ F)i(c|i di) = 1. Without loss of generality, we assume that κi = κi,j for some j. Since function Fj satisfies FMP and ψj i (c|i di) = 1, we have Fj(c|i di) < Fj(κi).
Notice di < κi,j, from the definition of κi,j = F−1 j (min{τj, hj}) we have (1) ψj i (c|i di) = 1, (2) Fj(c|i di) < τj due to the fact that Fj(x) is a non-decreasing function when j is selected. Thus, from the monotonicity property of O and τj is the cut value for output O, we have Oj(h|j Fj(c|i di)) = 1. (3) If Oj(F(c|i di)) = 1 then (O◦F)i(c|i di) = 1. Otherwise, since O satisfies SMP, Lemma 1 and equation 3 imply that there exists at least one index k such that Ok(F(c|i di)) = 1 and Fk(c|i di) < hk. Note Fk(c|i di) < hk implies that i is selected in ψk (c|i di) since hk = Fk(ci|i ∞). In other words, agent i is selected in O◦F. 219 Second, if di ≥ κi(O ◦ F, c−i) then (O ◦ F)i(c|i di) = 0.
Assume for the sake of contradiction that (O ◦ F)i(c|i di) = 1. Then there exists an index 1 ≤ j ≤ m such that Oj(F(c|i di)) = 1 and ψj i (c|i di) = 1. Remember that hk ≥ Fk(c|i di) for any k. Thus, from the fact that O satisfies SMP, when changing the cost vector from F(c|i di) to h|j Fj(c|i di), we still have Oj(h|j Fj(c|i di)) =
Fj(c|i di) < τj.
Combining the above inequality and the fact that Fj(c|i c|i di) < hj, we have Fj(c|i di) < min{hj, τj}. This implies di < F−1 j (min{hj, τj}) = κi,j < κi(O ◦ F, c−i). which is a contradiction. This finishes our proof.
In most applications, the allocation rule ψj implements the objective function fj and fj is utilitarian. Thus, we can compute the inverse of F−1 j efficiently. Another issue is that it seems the conditions when we can apply Algorithm 6 are restrictive.
However, lots of games in practice satisfy these properties and here we show how to deduct the MAX combination in [13]. Assume A1 and A2 are two allocation rules for single minded combinatorial auction, then the combination MAX(A1, A2) returns the allocation with the larger welfare. If algorithm A1 and A2 satisfy MP and FMP, the operation max(x, y) which returns the larger element of x and y satisfies SMP. From Theorem 9 we obtain that combination MAX(A1, A2) also satisfies MP. Further, the cut value of the MAX combination can be found by Algorithm 6. As we will show in Section 5, the complex combination can apply to some more complicated problems.
In the set cover problem, there is a set U of m elements needed to be covered, and each agent 1 ≤ i ≤ n can cover a subset of elements Si with a cost ci. Let S = {S1, S2, · · · , Sn} and c = (c1, c2, · · · , cn). We want to find a subset of agents D such that U ⊆ S i∈D Si. The selected subsets is called the set cover for U. The social efficiency of the output D is defined as P i∈D ci, which is the objective function to be minimized. Clearly, this is a utilitarian and thus VCG mechanism can be applied if we can find the subset of S that covers U with the minimum cost. It is well-known that finding the optimal solution is NP-hard. In [4], an algorithm of approximation ratio of Hm has been proposed and it has been proved that this is the best ratio possible for the set cover problem. For the completeness of presentation, we review their method here.
Algorithm 7 Greedy Set Cover (GSC) Input: Agent i"s subset Si covered and cost ci. (1 ≤ i ≤ n).
Output: A set of agents that can cover all elements. 1: Initialize r = 1, T0 = ∅, and R = ∅. 2: while R = U do 3: Find the set Sj with the minimum density cj |Sj −Tr| . 4: Set Tr+1 = Tr S Sj and R = R S j. 5: r = r + 1 6: Output R.
Let GSC(S) be the sets selected by the Algorithm 7.Notice that the output set is a function of S and c. Some works assume that the type of an agent could be ci, i.e., Si is assumed to be a public knowledge. Here, we consider a more general case in which the type of an agent is (Si, ci). In other words, we assume that every agent i can not only lie about its cost ci but also can lie about the set Si. This problem now looks similar to the combinatorial auction with single minded bidder studied in [12], but with the following differences: in the set cover problem we want to cover all the elements and the sets chosen can have some overlap while in combinatorial auction the chosen sets are disjoint.
We can show that the mechanism M = (GSC, PV CG ), using Algorithm 7 to find a set cover and apply VCG mechanism to compute the payment to the selected agents, is not truthful. Obviously, the set cover problem is a binary demand game. For the moment, we assume that agent i won"t be able to lie about Si. We will drop this assumption later. We show how to design a truthful mechanism by applying our general framework.
Algorithm 7 is a round-based output. Thus, for an agent i, we first focus on the output of one round r. In round r, if i is selected by Algorithm 7, then it has the minimum ratio ci |Si−Tr| among all remaining agents. Now consider the case when i lies its cost to ci < ci, obviously ci |Si−Tr| is still minimum among all remaining agents. Consequently, agent i is still selected in round r, which means the output of round r satisfies MP. Now we look into the updating rules. For every round, we only update the Tr+1 = Tr S Sj and R = R S j, which is obviously cross-independent. Thus, by applying Theorem 8, we know the output by Algorithm 7 satisfies MP.
with fixed cost vector c−i, we follow the steps in Algorithm
agent selected in round r and T−i r+1 be the corresponding set.
Then the cut value of round r is r = cir |Sir − T−i r | · |Si − T−i r |.
Remember the updating rule only updates the game setting but not the cost of the agent, thus we have gr(x) = x ≥ r for 1 ≤ r ≤ t. Therefore, the final cut value for agent i is κi(GSC, c−i) = max r { cir |Sir − T−i r | · |Si − T−i r |} The payment to an agent i is κi if i is selected; otherwise its payment is 0.
We now consider the scenario when agent i can lie about Si.
Assume that agent i cannot lie upward, i.e., it can only report a set Si ⊆ Si. We argue that agent i will not lie about its elements Si.
Notice that the cut value computed for round r is r = cir |Sir −T −i r | · |Si − T−i r |. Obviously |Si − T−i r | ≤ |Si − T−i r | for any Si ⊆ Si.
Thus, lying its set as Si will not increase the cut value for each round. Thus lying about Si will not improve agent i"s utility.
Consider any link weighted network G = (V, E, c), where E = {e1, e2, · · · , em} are the set of links and ci is the weight of the link ei. The link weighted Steiner tree problem is to find a tree rooted at source node s spanning a given set of nodes Q = {q1, q2, · · · , qk} ⊂ V . For simplicity, we assume that qi = vi, for 1 ≤ i ≤ k. Here the links are agents. The total cost of links in a graph H ⊆ G is called the weight of H, denoted as ω(H). It is NP-hard to find the minimum cost multicast tree when given an arbitrary link weighted 220 graph G [17, 20]. The currently best polynomial time method has approximation ratio 1 + ln 3 2 [17]. Here, we review and discuss the first approximation method by Takahashi and Matsuyama [20].
Algorithm 8 Find LinkWeighted SteinerTree (LST) Input: Network G = (V, E, c) where c is the cost vector for link set E. Source node s and receiver set Q.
Output: A tree LST rooted at s and spanned all receivers. 1: Set r = 1, G1 = G, Q1 = Q and s1 = s. 2: repeat 3: In graph Gr, find the receiver, say qi, that is closest to the source s, i.e., LCP(s, qi, c) has the least cost among the shortest paths from s to all receivers in Qr . 4: Select all links on LCP(s, qi, c) as relay links and set their cost to 0. The new graph is denoted as Gr+1. 5: Set tr as qi and Pr = LCP(s, qi, c). 6: Set Qr+1 = Qr \qi and r = r + 1. 7: until all receivers are spanned.
Hereafter, let LST(G) be the final tree constructed using the above method. It is shown in [24] that mechanism M = (LST, pV CG ) is not truthful, where pV CG is the payment calculated based on VCG mechanism.
We then show how to design a truthful payment scheme using our general framework. Observe that the output Pr, for any round r, satisfies MP, and the update rule for every round satisfies crossing-independence. Thus, from Theorem 8, the roundbased output LST satisfies MP. In round r, the cut value for a link ei can be obtained by using the VCG mechanism. Now we set ci = ∞ and execute Algorithm 8. Let w−i r (ci) be the cost of the path Pr(ci) selected in the rth round and Πi r(ci) be the shortest path selected in round r if the cost of ci is temporarily set to −∞. Then the cut value for round r is r = wi r(c−i) − |Πi r(c−i)| where |Πi r(c−i)| is the cost of the path Πi r(c−i) excluding node vi. Using Algorithm 4, we obtain the final cut value for agent i: κi(LST, c−i) = maxr{ r}. Thus, the payment to a link ei is κi(LST, c−i) if its reported cost is di < κi(LST, d−i); otherwise, its payment is 0.
To connect the given set of receivers to the source node, besides the Steiner tree constructed by the algorithms described before, a virtual minimum spanning tree is also often used. Assume that Q is the set of receivers, including the sender. Assume that the nodes in a node-weighted graph are all agents. The virtual minimum spanning tree is constructed as follows.
Algorithm 9 Construct VMST 1: for all pairs of receivers qi, qj ∈ Q do 2: Calculate the least cost path LCP(qi, qj, d). 3: Construct a virtual complete link weighted graph K(d) using Q as its node set, where the link qiqj corresponds to the least cost path LCP(qi, qj, d), and its weight is w(qiqj) = |LCP(qi, qj, d)|. 4: Build the minimum spanning tree on K(d), denoted as V MST(d). 5: for every virtual link qiqj in V MST(d) do 6: Find the corresponding least cost path LCP(qi, qj, d) in the original network. 7: Mark the agents on LCP(qi, qj, d) selected.
The mechanism M = (V MST, pV CG ) is not truthful [24], where the payment pV CG to a node is based on the VCG mechanism. We then show how to design a truthful mechanism based on the framework we described.
complete graph K(d), the weight of a link qiqj is |LCP(qi, qj, d)|.
In other words, we implicitly defined |Q|(|Q| − 1)/2 functions fi,j, for all i < j and qi ∈ Q and qj ∈ Q, with fi,j(d) = |LCP(qi, qj, d)|. We can show that the function fi,j(d) = |LCP(qi, qj, d)| satisfies FMP, LCP satisfies MP, and the output MST satisfies SMP. From Theorem 9, the allocation rule VMST satisfies the monotonicity property.
MST and function fi,j, so cut value for VMST can be computed based on Algorithm 6 as follows. (a) Given a link weighted complete graph K(d) on Q, we should find the cut value function for edge ek = (qi, qj) based on MST. Given a spanning tree T and a pair of terminals p and q, clearly there is a unique path connecting them on T. We denote this path as ΠT (p, q), and the edge with the maximum length on this path as LE(p, q, T). Thus, the cut value can be represented as κk(MST, d) = LE(qi, qj, MST(d|k ∞)) (b) We find the value-cost function for LCP. Assume vk ∈ LCP(qi, qj, d), then the value-cost function is xk = yk − |LCPvk (qi, qj, d|k 0)|. Here, LCPvk (qi, qj, d) is the least cost path between qi and qj with node vk on this path. (c) Remove vk and calculate the value K(d|k ∞). Set h(i,j) = |LCP(qi, qj, d|∞ ))| for every pair of node i = j and let h = {h(i,j)} be the vector. Then it is easy to show that τ(i,j) = |LE(qi, qj, MST(h|(i,j) ∞))| is the cut value for output VMST. It easy to verify that min{h(i,j), τ(i,j)} = |LE(qi, qj, MST(h)|. Thus, we know κ (i,j) k (V MST, d) is |LE(qi, qj, MST(h)|− |LCPvk (qi, qj, d|k 0)|. The cut value for agent k is κk(V MST, d−k) = max0≤i,j≤r κij k (V MST, d−k).
in V MST(d); else we pay it 0.
Lehmann et al. [12] studied how to design an efficient truthful mechanism for single-minded combinatorial auction. In a singleminded combinatorial auction, there is a set of items S to be sold and there is a set of agents 1 ≤ i ≤ n who wants to buy some of the items: agent i wants to buy a subset Si ⊆ S with maximum price mi. A single-minded bidder i declares a bid bi = Si, ai with Si ⊆ S and ai ∈ R+ . Two bids Si, ai and Sj, aj conflict if Si ∩ Sj = ∅. Given the bids b1, b2, · · · , bn, they gave a greedy round-based algorithm as follows. First the bids are sorted by some criterion ( ai |Si|1/2 is used in[12]) in an increasing order and let L be the list of sorted bids. The first bid is granted. Then the algorithm exams each bid of L in order and grants the bid if it does not conflict with any of the bids previously granted. If it does, it is denied. They proved that this greedy allocation scheme using criterion ai |Si|1/2 approximates the optimal allocation within a factor of √ m, where m is the number of goods in S.
In the auction settings, we have ci = −ai. It is easy to verify the output of the greedy algorithm is a round-based output.
Remember after bidder j is selected for round r, every bidder has conflict 221 with j will not be selected in the rounds after. This equals to update the cost of every bidder having conflict with j to 0, which satisfies crossing-independence. In addition, in any round, if bidder i is selected with ai then it will still be selected when it declares ai > ai. Thus, for every round, it satisfies MP and the cut value is |Si|1/2 · ajr |Sjr |1/2 where jr is the bidder selected in round r if we did not consider the agent i at all. Notice ajr |Sjr |1/2 does not increase when round r increases, so the final cut value is |Si|1/2 · aj |Sj |1/2 where bj is the first bid that has been denied but would have been selected were it not only for the presence of bidder i. Thus, the payment by agent i is |Si|1/2 · aj |Sj |1/2 if ai ≥ |Si|1/2 · aj |Sj |1/2 , and 0 otherwise. This payment scheme is exactly the same as the payment scheme in [12].
In this paper, we have studied how to design a truthful mechanism M = (O, P) for a given allocation rule O for a binary demand game. We first showed that the allocation rule O satisfying the MP is a necessary and sufficient condition for a truthful mechanism M to exist. We then formulate a general framework for designing payment P such that the mechanism M = (O, P) is truthful and computable in polynomial time. We further presented several general composition-based techniques to compute P efficiently for various allocation rules O. Several concrete examples were discussed to demonstrate our general framework for designing P and for composition-based techniques of computing P in polynomial time.
In this paper, we have concentrated on how to compute P in polynomial time. Our algorithms do not necessarily have the optimal running time for computing P given O. It would be of interest to design algorithms to compute P in optimal time. We have made some progress in this research direction in [22] by providing an algorithm to compute the payments for unicast in a node weighted graph in optimal O(n log n + m) time.
Another research direction is to design an approximation allocation rule O satisfying MP with a good approximation ratio for a given binary demand game. Many works [12, 13] in the mechanism design literature are in this direction. We point out here that the goal of this paper is not to design a better allocation rule for a problem, but to design an algorithm to compute the payments efficiently when O is given. It would be of significance to design allocation rules with good approximation ratios such that a given binary demand game has a computationally efficient payment scheme.
In this paper, we have studied mechanism design for binary demand games. However, some problems cannot be directly formulated as binary demand games. The job scheduling problem in [2] is such an example. For this problem, a truthful payment scheme P exists for an allocation rule O if and only if the workload assigned by O is monotonic in a certain manner. It wound be of interest to generalize our framework for designing a truthful payment scheme for a binary demand game to non-binary demand games. Towards this research direction, Theorem 4 can be extended to a general allocation rule O, whose range is R+ . The remaining difficulty is then how to compute the payment P under mild assumptions about the valuations if a truthful mechanism M = (O, P) does exist.
Acknowledgements We would like to thank Rakesh Vohra, Tuomas Sandholm, and anonymous reviewers for helpful comments and discussions.

A set of jobs need to be served by a server. The server can process only one job at a time. Each job has a finite processing time and a per unit time waiting cost. Efficient ordering of this queue directs us to serve the jobs in increasing order of the ratio of per unit time waiting cost and processing time. To compensate for waiting by jobs, monetary transfers to jobs are allowed. How should the jobs share the cost equitably amongst themselves (through transfers)?
The problem of fair division of costs among agents in a queue has many practical applications. For example, computer programs are regularly scheduled on servers, data are scheduled to be transmitted over networks, jobs are scheduled in shop-floor on machines, and queues appear in many public services (post offices, banks). Study of queueing problems has attracted economists for a long time [7, 17].
Cost sharing is a fundamental problem in many settings on the Internet. Internet can be seen as a common resource shared by many users and the cost incured by using the resource needs to be shared in an equitable manner. The current surge in cost sharing literature from computer scientists validate this claim [8, 11, 12, 6, 24]. Internet has many settings in which our model of job scheduling appears and the agents waiting in a queue incur costs (jobs scheduled on servers, queries answered from a database, data scheduled to be transmitted over a fixed bandwidth network etc.). We hope that our analysis will give new insights on cost sharing problems of this nature.
Recently, there has been increased interest in cost sharing methods with submodular cost functions [11, 12, 6, 24].
While many settings do have submodular cost functions (for example, multi-cast transmission games [8]), while the cost function of our game is supermodular. Also, such literature typically does not assume budget-balance (transfers adding up to zero), while it is an inherent feature of our model.
A recent paper by Maniquet [15] is the closest to our model and is the motivation behind our work 1 . Maniquet [15] studies a model where he assumes all processing times are unity. For such a model, he characterizes the Shapley value rule using classical fairness axioms. Chun [1] interprets the worth of a coalition of jobs in a different manner for the same model and derives a reverse rule. Chun characterizes this rule using similar fairness axioms. Chun [2] also studies the envy properties of these rules. Moulin [22, 21] studies the queueing problem from a strategic point view when per unit waiting costs are unity. Moulin introduces new concepts in the queueing settings such as splitting and merging of jobs, and ways to prevent them.
Another stream of literature is on sequencing games, first introduced by Curiel et al. [4]. For a detailed survey, refer to Curiel et al. [3]. Curiel et al. [4] defined sequencing games similar to our model, but in which an initial ordering of jobs is given. Besides, their notion of worth of a coalition is very different from the notions studied in Maniquet [15] and Chun [1] (these are the notions used in our work too).
The particular notion of the worth of a coalition makes the sequencing game of Curiel et al. [4] convex, whereas our game is not convex and does not assume the presence of any initial order. In summary, the focus of this stream of 1 The authors thank Fran¸cois Maniquet for several fruitful discussions. 232 research is how to share the savings in costs from the initial ordering to the optimal ordering amongst jobs (also see Hamers et al. [9], Curiel et al. [5]). Recently, Klijn and S´anchez [13, 14] considered sequencing games without any initial ordering of jobs. They take two approaches to define the worth of coalitions. One of their approaches, called the tail game, is related to the reverse rule of Chun [1]. In the tail game, jobs in a coalition are served after the jobs not in the coalition are served. Klijn and S´anchez [14] showed that the tail game is balanced. Further, they provide expressions for the Shapley value in tail game in terms of marginal vectors and reversed marginal vectors. We provide a simpler expression of the Shapley value in the tail game, generalizing the result in Chun [1]. Klijn and S´anchez [13] study the core of this game in detail.
Strategic aspects of queueing problems have also been researched. Mitra [19] studies the first best implementation in queueing models with generic cost functions. First best implementation means that there exists an efficient mechanism in which jobs in the queue have a dominant strategy to reveal their true types and their transfers add up to zero.
Suijs [27] shows that if waiting costs of jobs are linear then first best implementation is possible. Mitra [19] shows that among a more general class of queueing problems first best implementation is possible if and only if the cost is linear.
For another queueing model, Mitra [18] shows that first best implementation is possible if and only if the cost function satisfies a combinatorial property and an independence property. Moulin [22, 21] studies strategic concepts such as splitting and merging in queueing problems with unit per unit waiting costs.
The general cost sharing literature is vast and has a long history. For a good survey, we refer to [20]. From the seminal work of Shapley [25] to recent works on cost sharing in multi-cast transmission and optimization problems [8, 6, 23] this area has attracted economists, computer scientists, and operations researchers.
Ours is the first model which considers cost sharing when both processing time and per unit waiting cost of jobs are present. We take a cooperative game theory approach and apply the classical Shapley value rule to the problem. We show that the Shapley value rule satisfies many intuitive fairness axioms. Due to two dimensional nature of our model and one dimensional nature of Maniquet"s model [15], his axioms are insufficient to characterize the Shapley value in our setting. We introduce axioms such as independece of preceding jobs" unit waiting cost and independence of following jobs" processing time. A key axiom that we introduce gives us a bound on cost share of a job in a group of jobs which have the same ratio of unit time waiting cost and processing time (these jobs can be ordered in any manner between themseleves in an efficient ordering). If such a group consists of just one job, then the axiom says that such a job should at least pay his own processing cost (i.e., the cost it would have incurred if it was the only job in the queue). If there are multiple jobs in such a group, the probability of any two jobs from such a group inflicting costs on each other is same (1 2 ) in an efficient ordering. Depending on the ordering selected, one job inflicts cost on the other. Our fairness axiom says that each job should at least bear such expected costs.
We characterize the Shapley value rule using these fairness axioms. We also extend the envy results in [2] to our setting and discuss a class of reasonable cost sharing mechanisms.
There are n jobs that need to be served by one server which can process only one job at a time. The set of jobs are denoted as N = {1, . . . , n}. σ : N → N is an ordering of jobs in N and σi denotes the position of job i in the ordering σ. Given an ordering σ, define Fi(σ) = {j ∈ N : σi < σj} and Pi(σ) = {j ∈ N : σi > σj}.
Every job i is identified by two parameters: (pi, θi). pi is the processing time and θi is the cost per unit waiting time of job i. Thus, a queueing problem is defined by a list q = (N, p, θ) ∈ Q, where Q is the set of all possible lists. We will denote γi = θi pi . Given an ordering of jobs σ, the cost incurred by job i is given by ci(σ) = piθi + θi   j∈Pi(σ) pj.
The total cost incurred by all jobs due to an ordering σ can be written in two ways: (i) by summing the cost incurred by every job and (ii) by summing the costs inflicted by a job on other jobs with their own processing cost.
C(N, σ) =   i∈N ci(σ) =   i∈N piθi +   i∈N ¡θi   j∈Pi(σ) pj¢. =   i∈N piθi +   i∈N ¡pi   j∈Fi(σ) θj¢.
An efficient ordering σ∗ is the one which minimizes the total cost incurred by all jobs. So, C(N, σ∗ ) ≤ C(N, σ) ∀ σ ∈ Σ. To achieve notational simplicity, we will write the total cost in an efficient ordering of jobs from N as C(N) whenever it is not confusing. Sometimes, we will deal with only a subset of jobs S ⊆ N. The ordering σ will then be defined on jobs in S only and we will write the total cost from an efficient ordering of jobs in S as C(S). The following lemma shows that jobs are ordered in decreasing γ in an efficient ordering. This is also known as the weighted shortest processing time rule, first introduced by Smith [26].
Lemma 1. For any S ⊆ N, let σ∗ be an efficient ordering of jobs in S. For every i = j, i, j ∈ S, if σ∗ i > σ∗ j , then γi ≤ γj.
Proof. Assume for contradiction that the statment of the lemma is not true. This means, we can find two consecutive jobs i, j ∈ S (σ∗ i = σ∗ j + 1) such that γi > γj.
Define a new ordering σ by interchanging i and j in σ∗ .
The costs to jobs in S \ {i, j} is not changed from σ∗ to σ.
The difference between total costs in σ∗ and σ is given by,
C(S, σ) − C(S, σ∗ ) = θjpi − θipj. From efficiency we get θjpi − θipj ≥ 0. This gives us γj ≥ γi, which is a contradiction.
An allocation for q = (N, p, θ) ∈ Q has two components: an ordering σ and a transfer ti for every job i ∈ N. ti denotes the payment received by job i. Given a transfer ti and an ordering σ, the cost share of job i is defined as, πi = ci(σ) − ti = θi   j∈N:σj ≤σi pj − ti. 233 An allocation (σ, t) is efficient for q = (N, p, θ) whenever σ is an efficient ordering and £i∈N ti = 0. The set of efficient orderings of q is denoted as Σ∗ (q) and σ∗ (q) will be used to refer to a typical element of the set. The following straightforward lemma says that for two different efficient orderings, the cost share in one efficient allocation is possible to achieve in the other by appropriately modifying the transfers.
Lemma 2. Let (σ, t) be an efficient allocation and π be the vector of cost shares of jobs from this allocation. If σ∗ = σ be an efficient ordering and t∗ i = ci(σ∗ ) − πi ∀ i ∈ N, then (σ∗ , t∗ ) is also an efficient allocation.
Proof. Since (σ, t) is efficient, £i∈N ti = 0. This gives £i∈N πi = C(N). Since σ∗ is an efficient ordering, £i∈N ci(σ∗ ) = C(N). This means, £i∈N t∗ i = £i∈N [ci(σ∗ ) − πi] = 0. So, (σ∗ , t∗ ) is an efficient allocation.
Depending on the transfers, the cost shares in different efficient allocations may differ. An allocation rule ψ associates with every q ∈ Q a non-empty subset ψ(q) of allocations.
VALUE In this section, we define the coalitional cost of this game and analyze the solution proposed by the Shapley value.
Given a queue q ∈ Q, the cost of a coalition of S ⊆ N jobs in the queue is defined as the cost incurred by jobs in S if these are the only jobs served in the queue using an efficient ordering. Formally, the cost of a coalition S ⊆ N is,
C(S) =   i∈S   j∈S:σ∗ j ≤σ∗ i θjpj, where σ∗ = σ∗ (S) is an efficient ordering considering jobs from S only. The worth of a coalition of S jobs is just −C(S). Maniquet [15] observes another equivalent way to define the worth of a coalition is using the dual function of the cost function C(·). Other interesting ways to define the worth of a coalition in such games is discussed by Chun [1], who assume that a coalition of jobs are served after the jobs not in the coalition are served.
The Shapley value (or cost share) of a job i is defined as,
SVi =   S⊆N\{i} |S|!(|N| − |S| − 1)! |N|! ¡C(S∪{i})−C(S)¢. (1) The Shapley value allocation rule says that jobs are ordered using an efficient ordering and transfers are assigned to jobs such that the cost share of job i is given by Equation 1.
Lemma 3. Let σ∗ be an efficient ordering of jobs in set N. For all i ∈ N, the Shapley value is given by,
SVi = piθi + 1 2 ¡Li + Ri¢, where Li = θi £j∈Pi(σ∗) pj and Ri = pi £j∈Fi(σ∗) θj.
Proof. Another way to write the Shapley value formula is the following [10],
SVi =   S⊆N:i∈S ∆(S) |S| , where ∆(S) = C(S) if |S| = 1 and ∆(S) = C(S)−£T S ∆(T).
This gives ∆({i}) = C({i}) = piθi ∀i ∈ N. For any i, j ∈ N with i = j, we have ∆({i, j}) = C({i, j}) − C({i}) − C({j}) = min(piθi + pjθj + pjθi, piθi + pjθj + piθj) − piθi − pjθj = min(pjθi, piθj).
We will show by induction that ∆(S) = 0 if |S| > 2. For |S| = 3, let S = {i, j, k}. Without loss of generality, assume θi pi ≥ θj pj ≥ θk pk . So, ∆(S) = C(S) − ∆({i, j}) − ∆({j, k}) − ∆({i, k})−∆({i})−∆({j})−∆({k}) = C(S)−piθj −pjθk − piθk − piθi − pjθj − pkθk = C(S) − C(S) = 0.
Now, assume for T S, ∆(T) = 0 if |T| > 2. Without loss of generality assume that σ to be the identity mapping.
Now, ∆(S) = C(S) −   T S ∆(T) = C(S) −   i∈S   j∈S:j<i ∆({i, j}) −   i∈S ∆({i}) = C(S) −   i∈S   j∈S:j<i pjθi −   i∈S piθi = C(S) − C(S) = 0.
This proves that ∆(S) = 0 if |S| > 2. Using the Shapley value formula now,
SVi =   S⊆N:i∈S ∆(S) |S| = ∆({i}) + 1 2   j∈N:j=i ∆({i, j}) = piθi + 1 2 ¡  j<i ∆({i, j}) +   j>i ∆({i, j})¢ = piθi + 1 2 ¡  j<i pjθi +   j>i piθj¢= piθi + 1 2 ¡Li + Ri¢.
THE SHAPLEY VALUE In this section, we will define serveral axioms on fairness and characterize the Shapley value using them. For a given q ∈ Q, we will denote ψ(q) as the set of allocations from allocation rule ψ. Also, we will denote the cost share vector associated with an allocation rule (σ, t) as π and that with allocation rule (σ , t ) as π etc.
We will define three types of fairness axioms: (i) related to efficiency, (ii) related to equity, and (iii) related to independence.
Efficiency Axioms We define two types of efficiency axioms. One related to efficiency which states that an efficient ordering should be selected and the transfers of jobs should add up to zero (budget balance).
Definition 1. An allocation rule ψ satisfies efficiency if for every q ∈ Q and (σ, t) ∈ ψ(q), (σ, t) is an efficient allocation. 234 The second axiom related to efficiency says that the allocation rule should not discriminate between two allocations which are equivalent to each other in terms of cost shares of jobs.
Definition 2. An allocation rule ψ satisfies Pareto indifference if for every q ∈ Q, (σ, t) ∈ ψ(q), and (σ , t ) ∈ Σ(q), we have ¡πi = πi ∀ i ∈ N¢⇒ ¡(σ , t ) ∈ ψ(q)¢.
An implication of Pareto indifference axiom and Lemma
transfers of jobs such that it is part of an efficient rule and the cost share of a job in all these allocations are same.
Equity Axioms How should the cost be shared between two jobs if the jobs have some kind of similarity between them? Equity axioms provide us with fairness properties which help us answer this question. We provide five such axioms. Some of these axioms (for example anonymity, equal treatment of equals) are standard in the literature, while some are new.
We start with a well known equity axiom called anonymity.
Denote ρ : N → N as a permutation of elements in N. Let ρ(σ, t) denote the allocation obtained by permuting elements in σ and t according to ρ. Similarly, let ρ(p, θ) denote the new list of (p, θ) obtained by permuting elements of p and θ according to ρ. Our first equity axiom states that allocation rules should be immune to such permutation of data.
Definition 3. An allocation rule ψ satisfies anonymity if for all q ∈ Q, (σ, t) ∈ ψ(q) and every permutation ρ, we then ρ(σ, t) ∈ ψ(N, ρ(q)).
The next equity axiom is classical in literature and says that two similar jobs should be compensated such that their cost shares are equal. This implies that if all the jobs are of same type, then jobs should equally share the total system cost.
Definition 4. An allocation rule ψ satisfies equal treatment of equals (ETE) if for all q ∈ Q, (σ, t) ∈ ψ(q), i, j ∈ N, then ¡pi = pj; θi = θj¢⇒ ¡πi = πj¢.
ETE directs us to share costs equally between jobs if they are of the same per unit waiting cost and processing time.
But it is silent about the cost shares of two jobs i and j which satisfy θi pi = θj pj . We introduce a new axiom for this.
If an efficient rule chooses σ such that σi < σj for some i, j ∈ N, then job i is inflicting a cost of piθj on job j and job j is inflicting zero cost on job i. Define for some γ ≥ 0, S(γ) = {i ∈ N : γi = γ}. In an efficient rule, the elements in S(γ) can be ordered in any manner (in |S(γ)|! ways). If i, j ∈ S(γ) then we have pjθi = piθj. Probability of σi < σj is 1 2 and so is the probability of σi > σj. The expected cost i inflicts on j is 1 2 piθj and j inflicts on i is 1 2 pjθi. Our next fairness axiom says that i and j should each be responsible for their own processing cost and this expected cost they inflict on each other. Arguing for every pair of jobs i, j ∈ S(γ), we establish a bound on the cost share of jobs in S(γ). We impose this as an equity axiom below.
Definition 5. An allocation rule satisfies expected cost bound (ECB) if for all q ∈ Q, (σ, t) ∈ ψ(q) with π being the resulting cost share, for any γ ≥ 0, and for every i ∈ S(γ), we have πi ≥ piθi + 1 2 ¡   j∈S(γ):σj <σi pjθi +   j∈S(γ):σj >σi piθj¢.
The central idea behind this axiom is that of expected cost inflicted. If an allocation rule chooses multiple allocations, we can assign equal probabilities of selecting one of the allocations. In that case, the expected cost inflicted by a job i on another job j in the allocation rule can be calculated. Our axiom says that the cost share of a job should be at least its own processing cost and the total expected cost it inflicts on others. Note that the above bound poses no constraints on how the costs are shared among different groups. Also observe that if S(γ) contains just one job, ECB says that job should at least bear its own processing cost.
A direct consequence of ECB is the following lemma.
Lemma 4. Let ψ be an efficient rule which satisfies ECB.
For a q ∈ Q if S(γ) = N, then for any (σ, t) ∈ ψ(q) which gives a cost share of π, πi = piθi + 1 2 ¡Li + Ri¢∀ i ∈ N.
Proof. From ECB, we get πi ≥ piθi+1 2 ¡Li+Ri¢∀ i ∈ N.
Assume for contradiction that there exists j ∈ N such that πj > pjθj + 1 2 ¡Li + Ri¢. Using efficiency and the fact that £i∈N Li = £i∈N Ri, we get £i∈N πi = C(N) > £i∈N piθi + 1 2 £i∈N ¡Li + Ri¢ = C(N). This gives us a contradiction.
Next, we introduce an axiom about sharing the transfer of a job between a set of jobs. In particular, if the last job quits the system, then the ordering need not change.
But the transfer to the last job needs to be shared between the other jobs. This should be done in proportion to their processing times because every job influenced the last job based on its processing time.
Definition 6. An allocation rule ψ satisfies proportionate responsibility of p (PRp) if for all q ∈ Q, for all (σ, t) ∈ ψ(q), k ∈ N such that σk = |N|, q = (N \ {k}, p , θ ) ∈ Q, such that for all i ∈ N\{k}: θi = θi, pi = pi, there exists (σ , t ) ∈ ψ(q ) such that for all i ∈ N \ {k}: σi = σi and ti = ti + tk pi £j=k pj .
An analogous fairness axiom results if we remove the job from the beginning of the queue. Since the presence of the first job influenced each job depending on their θ values, its transfer needs to be shared in proportion to θ values.
Definition 7. An allocation rule ψ satisfies proportionate responsibility of θ (PRθ) if for all q ∈ Q, for all (σ, t) ∈ ψ(q), k ∈ N such that σk = 1, q = (N \{k}, p , θ ) ∈ Q, such that for all i ∈ N \{k}: θi = θi, pi = pi, there exists (σ , t ) ∈ ψ(q ) such that for all i ∈ N \ {k}: σi = σi and ti = ti + tk θi £j=k θj .
The proportionate responsibility axioms are generalizations of equal responsibility axioms introduced by Maniquet [15]. 235 Independence Axioms The waiting cost of a job does not depend on the per unit waiting cost of its preceding jobs. Similarly, the waiting cost inflicted by a job to its following jobs is independent of the processing times of the following jobs. These independence properties should be carried over to the cost sharing rules.
This gives us two independence axioms.
Definition 8. An allocation rule ψ satisfies independence of preceding jobs" θ (IPJθ) if for all q = (N, p, θ), q = (N, p , θ ) ∈ Q, (σ, t) ∈ ψ(q), (σ , t ) ∈ ψ(q ), if for all i ∈ N \ {k}: θi = θi, pi = pi and γk < γk, pk = pk, then for all j ∈ N such that σj > σk: πj = πj, where π is the cost share in (σ, t) and π is the cost share in (σ , t ).
Definition 9. An allocation rule ψ satisfies independence of following jobs" p (IFJp) if for all q = (N, p, θ), q = (N, p , θ ) ∈ Q, (σ, t) ∈ ψ(q), (σ , t ) ∈ ψ(q ), if for all i ∈ N \ {k}: θi = θi, pi = pi and γk > γk, θk = θk, then for all j ∈ N such that σj < σk: πj = πj, where π is the cost share in (σ, t) and π is the cost share in (σ , t ).
Having stated the fairness axioms, we propose three different ways to characterize the Shapley value rule using these axioms. All our characterizations involve efficiency and ECB. But if we have IPJθ, we either need IFJp or PRp.
Similarly if we have IFJp, we either need IPJθ or PRθ.
Proposition 1. Any efficient rule ψ that satisfies ECB,
IPJθ, and IFJp is a rule implied by the Shapley value rule.
Proof. Define for any i, j ∈ N, θi j = γipj and pi j = θj γi . Assume without loss of generality that σ is an efficient ordering with σi = i ∀ i ∈ N.
Consider the following q = (N, p , θ ) corresponding to job i with pj = pj if j ≤ i and pj = pi j if j > i, θj = θi j if j < i and θj = θj if j ≥ i. Observe that all jobs have the same γ: γi. By Lemma 2 and efficiency, (σ, t ) ∈ ψ(q ) for some set of transfers t . Using Lemma 4, we get cost share of i from (σ, t ) as πi = piθi + 1 2 ¡Li + Ri¢. Now, for any j < i, if we change θj to θj without changing processing time, the new γ of j is γj ≥ γi. Applying IPJθ, the cost share of job i should not change. Similarly, for any job j > i, if we change pj to pj without changing θj, the new γ of j is γj ≤ γi.
Applying IFJp, the cost share of job i should not change.
Applying this procedure for every j < i with IPJθ and for every j > i with IFJp, we reach q = (N, p, θ) and the payoff of i does not change from πi. Using this argument for every i ∈ N and using the expression for the Shapley value in Lemma 3, we get the Shapley value rule.
It is possible to replace one of the independence axioms with an equity axiom on sharing the transfer of a job. This is shown in Propositions 2 and 3.
Proposition 2. Any efficient rule ψ that satisfies ECB,
IPJθ, and PRp is a rule implied by the Shapley value rule.
Proof. As in the proof of Proposition 1, define θi j = γipj ∀ i, j ∈ N. Assume without loss of generality that σ is an efficient ordering with σi = i ∀ i ∈ N.
Consider a queue with jobs in set K = {1, . . . , i, i + 1}, where i < n. Define q = (K, p, θ ), where θj = θi+1 j ∀ j ∈ K. Define σj = σj ∀ j ∈ K. σ is an efficient ordering for q . By ECB and Lemma 4 the cost share of job i +
1 2 ¡£j<i+1 pjθi+1¢. Now, consider q = (K, p, θ ) such that θj = θi j ∀ j ≤ i and θi+1 = θi+1. σ remains an efficient ordering in q and by IPJθ the cost share of i + 1 remains πi+1. In q = (K \ {i + 1}, p, θ ), we can calculate the cost share of job i using ECB and Lemma 4 as πi = piθi + 1 2 £j<i pjθi. So, using PRp we get the new cost share of job i in q as πi = πi + ti+1 pi j<i+1 pj = piθi + 1 2 ¡£j<i pjθi + piθi+1¢.
Now, we can set K = K ∪ {i + 2}. As before, we can find cost share of i + 2 in this queue as πi+2 = pi+2θi+2 + 1 2 ¡£j<i+2 pjθi+2¢. Using PRp we get the new cost share of job i in the new queue as πi = piθi + 1 2 ¡£j<i pjθi + piθi+1 + piθi+2¢. This process can be repeated till we add job n at which point cost share of i is piθi + 1 2 ¡£j<i pjθi + £j>i piθj¢. Then, we can adjust the θ of preceding jobs of i to their original value and applying IPJθ, the payoffs of jobs i through n will not change. This gives us the Shapley values of jobs i through n. Setting i = 1, we get cost shares of all the jobs from ψ as the Shapley value.
Proposition 3. Any efficient rule ψ that satisfies ECB,
IFJp, and PRθ is a rule implied by the Shapley value rule.
Proof. The proof mirrors the proof of Proposition 2. We provide a short sketch. Analogous to the proof of Proposition 2, θs are kept equal to original data and processing times are initialized to pi+1 j . This allows us to use IFJp. Also, contrast to Proposition 2, we consider K = {i, i + 1, . . . , n} and repeatedly add jobs to the beginning of the queue maintaining the same efficient ordering. So, we add the cost components of preceding jobs to the cost share of jobs in each iteration and converge to the Shapley value rule.
The next proposition shows that the Shapley value rule satisfies all the fairness axioms discussed.
Proposition 4. The Shapley value rule satisfies efficiency, pareto indifference, anonymity, ETE, ECB, IPJθ, IFJp, PRp, and PRθ.
Proof. The Shapley value rule chooses an efficient ordering and by definition the payments add upto zero. So, it satisfies efficiency.
The Shapley value assigns same cost share to a job irrespective of the efficient ordering chosen. So, it is pareto indifferent.
The Shapley value is anonymous because the particular index of a job does not effect his ordering or cost share.
For ETE, consider two jobs i, j ∈ N such that pi = pj and θi = θj. Without loss of generality assume the efficient ordering to be 1, . . . , i, . . . , j, . . . , n. Now, the Shapley value of job i is 236 SVi = piθi + 1 2 ¡Li + Ri¢(From Lemma 3) = pjθj + 1 2 ¡Lj + Rj¢− 1 2 ¡Li − Lj + Ri − Rj¢ = SVj − 1 2 ¡   i<k≤j piθk −   i≤k<j pkθi¢ = SVj − 1 2   i<k≤j (piθk − pkθi) (Using pi = pj and θi = θj) = SVj (Using θk pk = θi pi for all i ≤ k ≤ j).
The Shapley value satisfies ECB by its expression in Lemma
Consider any job i, in an efficient ordering σ, if we increase the value of γj for some j = i such that σj > σi, then the set Pi ( preceding jobs) does not change in the new efficient ordering. If γj is changed such that pj remains the same, then the expression £j∈Pi θipj is unchanged. If (p, θ) values of no other jobs are changed, then the Shapley value is unchanged by increasing γj for some j ∈ Pi while keeping pj unchanged. Thus, the Shapley value rule satisfies IPJθ.
An analogous argument shows that the Shapley value rule satisfies IFJp.
For PRp, assume without loss of generality that jobs are ordered 1, . . . , n in an efficient ordering. Denote the transfer of job i = n due to the Shapley value with set of jobs N and set of jobs N \ {n} as ti and ti respectively. Transfer of last job is tn = 1 2 θn £j<n pj. Now, ti = 1 2 ¡θi   j<i pj − pi   j>i θj¢ = 1 2 ¡θi   j<i pj − pi   j>i:j=n θj¢− 1 2 piθn = ti − 1 2 θn   j<n pj pi £j<n pj = ti − tn pi £j<n pj .
A similar argument shows that the Shapley value rule satisfies PRθ.
These series of propositions lead us to our main result.
Theorem 1. Let ψ be an allocation rule. The following statements are equivalent: 1) For each q ∈ Q, ψ(q) selects all the allocation assigning jobs cost shares implied by the Shapley value. 2) ψ satisfies efficiency, ECB, IFJp, and IPJθ. 3) ψ satisfies efficiency, ECB, IFJp, and PRθ. 4) ψ satisfies efficiency, ECB, PRp, and IPJθ.
Proof. The proof follows from Propositions 1, 2, 3, and
Mechanisms In this section, we will define a reasonable class of cost sharing mechanisms. We will show how these reasonable mechanisms lead to the Shapley value mechanism.
Definition 10. An allocation rule ψ is reasonable if for all q ∈ Q and (σ, t) ∈ ψ(q) we have for all i ∈ N, ti = α ¡θi   j∈Pi(σ) pj − pi   j∈Fi(σ) θj¢∀ i ∈ N, where 0 ≤ α ≤ 1.
The reasonable cost sharing mechanism says that every job should be paid a constant fraction of the difference between the waiting cost he incurs and the waiting cost he inflicts on other jobs. If α = 0, then every job bears its own cost. If α = 1, then every job gets compensated for its waiting cost but compensates others for the cost he inflicts on others. The Shapley value rule comes as a result of ETE as shown in the following proposition.
Proposition 5. Any efficient and reasonable allocation rule ψ that satisfies ETE is a rule implied by the Shapley value rule.
Proof. Consider a q ∈ Q in which pi = pj and θi = θj.
Let (σ, t) ∈ ψ(q) and π be the resulting cost shares. From ETE, we get, πi = πj ⇒ ci(σ) − ti = cj(σ) − tj ⇒ piθi + (1 − α)Li + αRi = pjθj + (1 − α)Lj + αRj (Since ψ is efficient and reasonable) ⇒ (1 − α)(Li − Lj) = α(Rj − Ri) (Using pi = pj, θi = θj) ⇒ 1 − α = α (Using Li − Lj = Rj − Ri = 0) ⇒ α = 1 2 .
This gives us the Shapley value rule by Lemma 3.
Chun [2] discusses a fariness condition called no-envy for the case when processing times of all jobs are unity.
Definition 11. An allocation rule satisfies no-envy if for all q ∈ Q, (σ, t) ∈ ψ(q), and i, j ∈ N, we have πi ≤ ci(σij ) − tj, where π is the cost share from allocation rule (σ, t) and σij is the ordering obtaining by swapping i and j.
From the result in [2], the Shapley value rule does not satisfy no-envy in our model also. To overcome this, Chun [2] introduces the notion of adjusted no-envy, which he shows is satisfied in the Shapley value rule when processing times of all jobs are unity. Here, we show that adjusted envy continues to hold in the Shapley value rule in our model (when processing times need not be unity).
As before denote σij be an ordering where the position of i and j is swapped from an ordering σ. For adjusted noenvy, if (σ, t) is an allocation for some q ∈ Q, let tij be the 237 transfer of job i when the transfer of i is calculated with respect to ordering σij . Observe that an allocation may not allow for calculation of tij . For example, if ψ is efficient, then tij cannot be calculated if σij is also not efficient. For simplicity, we state the definition of adjusted no-envy to apply to all such rules.
Definition 12. An allocation rule satisfies adjusted noenvy if for all q ∈ Q, (σ, t) ∈ ψ(q), and i, j ∈ N, we have πi ≤ ci(σij ) − tij i .
Proposition 6. The Shapley value rule satisfies adjusted no-envy.
Proof. Without loss of generality, assume efficient ordering of jobs is: 1, . . . , n. Consider two jobs i and i + k.
From Lemma 3,
SVi = piθi + 1 2 ¡  j<i θipj +   j>i θjpi¢.
Let ˆπi be the cost share of i due to adjusted transfer tii+k i in the ordering σii+k . ˆπi = ci(σii+k ) − tii+k i = piθi + 1 2 ¡  j<i θipj + θipi+k +   i<j<i+k θipj +   j>i θjpi − θi+kpi −   i<j<i+k θjpi¢ = SVi + 1 2   i<j≤i+k ¡θipj − θjpi¢ ≥ SVi (Using the fact that θi pi ≥ θj pj for i < j).
We studied the problem of sharing costs for a job scheduling problem on a single server, when jobs have processing times and unit time waiting costs. We took a cooperative game theory approach and show that the famous the Shapley value rule satisfies many nice fairness properties. We characterized the Shapley value rule using different intuitive fairness axioms.
In future, we plan to further simplify some of the fairness axioms. Some initial simplifications already appear in [16], where we provide an alternative axiom to ECB and also discuss the implication of transfers between jobs (in stead of transfers from jobs to a central server). We also plan to look at cost sharing mechanisms other than the Shapley value.
Investigating the strategic power of jobs in such mechanisms is another line of future research.

Recently, global networks have attracted widespread study.
The emergence of popular scalable shared networks with self-interested entities - such as peer-to-peer systems over the Internet and mobile wireless communication ad-hoc networks - poses fundamental challenges.
Naturally, the study of such giant decentralized systems involves aspects of game theory [32, 34]. In particular, the subfield of Mechanism Design deals with the construction of mechanisms: for a given social goal the challenge is to design rules for interaction such that selfish behavior of the agents will result in the desired social goal [23, 33].
Algorithmic Mechanism Design (AMD) focuses on efficiently computable constructions [32]. Distributed Algorithmic Mechanism Design (DAMD) studies mechanism design in inherently decentralized settings [30, 12]. The standard model assumes rational agents with quasi-linear utilities and private information, playing dominant strategies.
The solution concept of dominant strategies - in which each player has a best response strategy regardless of the strategy played by any other player - is well suited to the assumption of private information, in which each player is not assumed to have knowledge or beliefs regarding the other players. The appropriateness of this set-up stems from the strength of the solution concept, which complements the weak information assumption. Many mechanisms have been constructed using this set-up, e.g., [1, 4, 6, 11, 14, 22]. Most of these apply to severely-restricted cases (e.g., single-item auctions with no externalities) in which a player"s preference is described by only one parameter (single-parameter domains).
To date, Vickrey-Clarke-Groves (VCG) mechanisms are the only known general method for designing dominant strategy mechanisms for general domains of preferences.
However, in distributed settings without available subsidies from outside sources, VCG mechanisms cannot be accepted as valid solutions due to a serious lack of budget balance.
Additionally, for some domains of preferences, VCG mechanisms and weighted VCG mechanisms are faced with computational hardness [22, 20]. Further limitations of the set-up are discussed in subsection 1.3.
In most distributed environments, players can take advantage of the network structure to collect and distribute information about other players. This paper thus studies the effects of relaxing the private information assumption. 240 One model that has been extensively studied recently is the Peer-to-Peer (P2P) network. A P2P network is a distributed network with no centralized authority, in which the participants share their individual resources (e.g., processing power, storage capacity, bandwidth and content). The aggregation of such resources provides inexpensive computational platforms. The most popular P2P networks are those for sharing media files, such as Napster, Gnutella, and Kazaa. Recent work on P2P Incentives include micropayment methods [15] and reputation-based methods [9, 13].
The following description of a P2P network scenario illustrates the relevance of our relaxed informational assumption.
Example 1. Consider a Peer-to-Peer network for file sharing. Whenever agent B uploads a file from agent A, all peers along the routing path know that B has loaded the file. They can record this information about agent B. In addition, they can distribute this information.
However, it is impossible to record all the information everywhere. First, such duplication induces huge costs.
Second, as agents dynamically enter and exit from the network, the information might not be always available. And so it is seems natural to consider environments in which the information is locally recorded, that is, the information is recorded in the closest neighborhood with some probability p.
In this paper we shall see that if the information is available with some probability, then this enables us to implement a wider range of social goals. As a result, cooperation is achieved independent of agents" belief. This demonstrates that in some computational contexts our approach is far less demanding than the Bayesian approach (that assumes that players" types are drawn according to some identified probability density function).
Set-ups In complete information environments, each agent is informed about everyone else. That is, each agent observes his own preference and the preferences of all other agents.
However, no outsider can observe this information. Specifically, neither the mechanism designer nor the court. Many positive results were shown for such arguably realistic settings.
For recent surveys see [25, 27, 18].
Moore and Repullo implement a large class of social goals using sequential mechanisms with a small number of rounds [28]. The concept they used is subgame-perfect implementations (SPE).
The SPE-implementability concept seems natural for the following reasons: the designed mechanisms usually have non-artificial constructs and a small strategy space. As a result, it is straightforward for a player to compute his strategy.1 Second, sequential mechanisms avoid simultaneous moves, and thus can be considered for distributed networks. Third, the constructed mechanisms are often decentralized (i.e., lacking a centralized authority or designer) 1 Interestingly, in real life players do not always use their subgame perfect strategies. One such widely studied case is the Ultimatum Bargaining 2-person game. In this simple game, the proposer first makes an offer of how to divide a certain known sum of money, and the responder either agrees or refuses, in the latter case both players earn zero. Somewhat surprisingly, experiments show that the responder often rejects the suggested offer, even if it is bounded away from zero and the game is played only once (see e.g. [38]). and budget-balanced (i.e., transfers always sum up to zero).
This happens essentially if there are at least three players, and a direct network link between any two agents. Finally,
Moore and Repullo observed that they actually use a relaxed complete information assumption: it is only required that for every player there exists only one other player who is informed about him.
Set-ups and Our Results The complete information assumption is realistic for small groups of players, but not in general. In this paper we consider players that are informed about each other with some probability. More formally, we say that agent B is p-informed about agent A, if B knows the type of A with probability p.
For such partially-informed environments, we show how to use the solution concept of iterative elimination of weakly dominated strategies. We demonstrate this concept through some motivating examples that (i) seem natural in distributed settings and (ii) cannot be implemented in dominant strategies even if there is an authorized center with a direct connection to every agent or even if players have single-parameter domains.
Moore and Repullo [28] can be applied to p-informed environments and further adjusted to the concept of iterative elimination of weakly dominated strategies (for large enough p).
that is more natural in computerized p-informed environments and different from the one introduced by Moore and Repullo [28] (for p ∈ (0, 1]).
structures.
As a case study we apply our methods to derive: (1) Simplified Peer-to-Peer network for file sharing with no payments in equilibrium. Our approach is (agent, file)-specific. (2) Web-cache budget-balanced and economically efficient mechanism.
Our mechanisms use reasonable punishments that inversely depend on p. And so, if the fines are large then small p is enough to induce cooperation. Essentially, large p implies a large amount of recorded information.
Decentralized mechanisms often utilize punishing outcomes.
As a result, malicious players might cause severe harm to others. We suggest a quantified notion of malicious player, who benefits from his own gained surplus and from harm caused to others. [12] suggests several categories to classify non-cooperating players. Our approach is similar to [7] (and the references therein), who considered independently such players in different context. We show a simple decentralized mechanism in which q-malicious players cooperate and in particular, do not use their punishing actions in equilibrium. 241
In this subsection we shall refer to some recent results demonstrating that the set-up of private information with the concept of dominant strategies is restrictive in general.
First, Roberts" classical impossibility result shows that if players" preferences are not restricted and there are at least
mechanism must be weighted VCG (with the social goal that maximizes the weighted welfare) [35]. For slightly-restricted preference domains, it is not known how to turn efficiently computable algorithms into dominant strategy mechanisms.
This was observed and analyzed in [32, 22, 31]. Recently [20] extends Roberts" result to some leading examples. They showed that under mild assumptions any dominant strategy mechanism for variety of Combinatorial Auctions over multi-dimensional domains must be almost weighted VCG.
Additionally, it turns out that the dominant strategy requirement implies that the social goal must be monotone [35, 36, 22, 20, 5, 37]. This condition is very restrictive, as many desired natural goals are non-monotone2 .
Several recent papers consider relaxations of the dominant strategy concept: [32, 1, 2, 19, 16, 17, 26, 21]. However, most of these positive results either apply to severely restricted cases (e.g., single-parameter, 2 players) or amount to VCG or almost VCG mechanisms (e.g., [19]). Recently, [8, 3] considered implementations for generalized single-parameter players.
Organization of this paper: In section 2 we illustrate the concepts of subgame perfect and iterative elimination of weakly dominated strategies in completely-informed and partially-informed environments. In section 3 we show a mechanism for Peer-to-Peer file sharing networks. In section
Future work is briefly discussed in section 5.
In this section we examine the concepts of subgame perfect and iterative elimination of weakly dominated strategies for completely informed and p-informed environments. We also present the notion of q-maliciousness and some other related considerations through two illustrative examples.
Our first example is an adjustment to computerized context of an ancient procedure to ensure that the wealthiest man in Athens would sponsor a theatrical production known as the Choregia [27]. In the fair assignment problem, Alice and Bob are two workers, and there is a new task to be performed. Their goal is to assign the task to the least loaded worker without any monetary transfers. The informational assumption is that Alice and Bob know both loads and the duration of the new task.3 2 E.g., minimizing the makespan within a factor of 2 [32] and Rawls" Rule over some multi-dimensional domains [20]. 3 In first glance one might ask why the completely informed agents could not simply sign a contract, specifying the desired goal. Such a contract is sometimes infeasible due to fact that the true state cannot be observed by outsiders, especially not the court.
Claim 1. The fair assignment goal cannot be implemented in dominant strategies.4
The following simple mechanism implements this goal in subgame perfect equilibrium. • Stage 1: Alice either agrees to perform the new task or refuses. • Stage 2: If she refuses, Bob has to choose between: - (a) Performing the task himself. - (b) Exchanging his load with Alice and performing the new task as well.
Let LT A, LT B be the true loads of Alice and Bob, and let t > 0 be the load of the new task. Assume that load exchanging takes zero time and cost. We shall see that the basic mechanism achieves the goal in a subgame perfect equilibrium. Intuitively this means that in equilibrium each player will choose his best action at each point he might reach, assuming similar behavior of others, and thus every SPE is a Nash equilibrium.
Claim 2. ([27]) The task is assigned to the least loaded worker in subgame perfect equilibrium.
Proof. By backward induction argument (look forward and reason backward), consider the following cases:
B ≤ LT A. If stage 2 is reached then Bob will not exchange.
A < LT B < LT A + t. If stage 2 is reached Bob will exchange, and this is what Alice prefers.
A + t ≤ LT B. If stage 2 is reached then Bob would exchange, as a result it is strictly preferable by Alice to perform the task.
Note that the basic mechanism does not use monetary transfers at all and is decentralized in the sense that no third party is needed to run the procedure. The goal is achieved in equilibrium (ties are broken in favor of Alice). However, in the second case exchange do occur in an equilibrium point.
Recall the unrealistic assumption that load exchange takes zero time and cost. Introducing fines, the next mechanism overcomes this drawback.
In this subsection we shall see a centralized mechanism for the fair assignment goal without load exchange in equilibrium. The additional assumptions are as follows. The cost performing a load of duration d is exactly d. We assume that the duration t of the new task is < T. The payoffs 4 proof: Assume that there exists a mechanism that implements this goal in dominant strategies. Then by the Revelation Principle [23] there exists a mechanism that implements this goal for which the dominant strategy of each player is to report his true load. Clearly, truthfully reporting cannot be a dominant strategy for this goal (if monetary transfers are not available), as players would prefer to report higher loads. 242 of the utility maximizers agents are quasilinear. The following mechanism is an adaptation of Moore and Repullo"s elicitation mechanism [28]5 . • Stage 1: (Elicitation of Alice"s load) Alice announces LA.
Bob announces LA ≤ LA.
If LA = LA (Bob agrees) goto the next Stage.
Otherwise (Bob challenges), Alice is assigned the task.
She then has to choose between: - (a) Transferring her original load to Bob and paying him LA − 0.5 · min{ , LA − LA}.
Alice pays to the mechanism.
Bob pays the fine of T + to the mechanism. - (b) No load transfer. Alice pays to Bob. STOP. • Stage 2: The elicitation of Bob"s load is similar to Stage 1 (switching the roles of Alice and Bob). • Stage 3: If LA < LB Alice is assigned the task, otherwise Bob. STOP.
Observe that Alice is assigned the task and fined with whenever Bob challenges. We shall see that the bonus of is paid to a challenging player only in out of equilibria cases.
Claim 3. If the mechanism stops at Stage 3, then the payoff of each agent is at least −t and at most 0.
Proposition 1. It is a subgame perfect equilibrium of the elicitation mechanism to report the true load, and to challenge with the true load only if the other agent overreports.
Proof. Assume w.l.o.g that the elicitation of Alice"s load is done after Bob"s, and that Stage 2 is reached. If Alice truly reports LA = LT A, Bob strictly prefers to agree.
Otherwise, if Bob challenges, Alice would always strictly prefer to transfer (as in this case Bob would perform her load for smaller cost), as a result Bob would pay T + to the mechanism. This punishing outcome is less preferable than the normal outcome of Stage 3 achieved had he agreed.
If Alice misreports LA > LT A, then Bob can ensure himself the bonus (which is always strictly preferable than reaching Stage 3) by challenging with LA = LT A, and so whenever Bob gets the bonus Alice gains the worst of all payoffs.
Reporting a lower load LA < LT A is not beneficial for Alice.
In this case, Bob would strictly prefer to agree (and not to announce LA < LA, as he limited to challenge with a smaller load than what she announces). Thus such misreporting can only increase the possibility that she is assigned the task.
And so there is no incentive for Alice to do so.
All together, Alice would prefer to report the truth in this stage. And so Stage 2 would not abnormally end by STOP, and similarly Stage 1.
Observe that the elicitation mechanism is almost balanced: in all outcomes no money comes in or out, except for the non-equilibrium outcome (a), in which both players pay to the mechanism. 5 In [28], if an agent misreport his type then it is always beneficial to the other agent to challenge. In particular, even if the agent reports a lower load.
Agents In this subsection we consider partially informed agents.
Formally: Definition 1. An agent A is p-informed about agent B, if A knows the type of B with probability p (independently of what B knows).
It turns out that a version of the elicitation mechanism works for this relaxed information assumption, if we use the concept of iterative elimination of weakly dominated strategies6 . We replace the fixed fine of in the elicitation mechanism with the fine: βp = max{L,
2p − 1 T} + , and assume the bounds LT A, LT B ≤ L.
Proposition 2. If all agents are p-informed, p > 0.5, the elicitation mechanism(βp) implements the fair assignment goal with the concept of iterative elimination of weakly dominated strategies. The strategy of each player is to report the true load and to challenge with the true load if the other agent overreport.
Proof. Assume w.l.o.g that the elicitation of Alice"s load is done after Bob"s, and that Stage 2 is reached. First observe that underreporting the true value is a dominated strategy, whether Bob is not informed and mistakenly challenges with a lower load (as βp ≥ L) or not, or even if t is very small. Now we shall see that overreporting her value is a dominated strategy, as well.
Alice"s expected payoff gained by misreporting ≤ p (payoff if she lies and Bob is informed) +(1 − p) (max payoff if Bob is not informed) ≤ p (−t − βp) < p (−t) + (1 − p) (−t − βp) ≤ p (min payoff of true report if Bob is informed) + (1 − p) (min payoff if Bob is not informed) ≤ Alice"s expected payoff if she truly reports.
The term (−t−βp) in the left hand side is due to the fact that if Bob is informed he will always prefer to challenge.
In the right hand side, if he is informed, then challenging is a dominated strategy, and if he is not informed the worst harm he can make is to challenge. Thus in stage 2 Alice will report her true load. This implies that challenging without being informed is a dominated strategy for Bob.
This argument can be reasoned also for the first stage, when Bob reports his value. Bob knows the maximum payoff he can gain is at most zero since he cannot expect to get the bonus in the next stage.
The elicitation mechanism for partially informed agents is rather general. As in [28], we need the capability to judge between two distinct declarations in the elicitation rounds, 6 A strategy si of player i is weakly dominated if there exists si such that (i) the payoff gained by si is at least as high as the payoff gained by si, for all strategies of the other players and all preferences, and (ii) there exist a preference and a combination of strategies for the other players such that the payoff gained by si is strictly higher than the payoff gained by si. 243 and upper and lower bounds based on the possible payoffs derived from the last stage. In addition, for p-informed environments, some structure is needed to ensure that underbidding is a dominated strategy.
The Choregia-type mechanisms can be applied to more than 2 players with the same number of stages: the player in the first stage can simply points out the name of the wealthiest agent. Similarly, the elicitation mechanisms can be extended in a straightforward manner. These mechanisms can be budget-balanced, as some player might replace the role of the designer, and collect the fines, as observed in [28].
Open Problem 1. Design a decentralized budget balanced mechanism with reasonable fines for independently p-informed n players, where p ≤ 1 − 1/2 1 n−1 .
A player might cause severe harm to others by choosing a non-equilibrium outcome. In the mechanism for the fair assignment goal, an agent might maliciously challenge even if the other agent truly reports his load. In this subsection we consider such malicious scenarios. For the ease of exposition we present a second example. We demonstrate that equilibria remain unchanged even if players are malicious.
In the seller-buyer example there is one item to be traded and two possible future states. The goal is to sell the item for the average low price pl = ls+lb 2 in state L, and the higher price ph = hs+hb 2 in the other state H, where ls is seller"s cost and lb is buyer"s value in state L, and similarly hs, hb in H. The players fix the prices without knowing what will be the future state. Assume that ls < hs < lb < hb, and that trade can occur in both prices (that is, pl, ph ∈ (hs, lb)).
Only the players can observe the realization of the true state. The payoffs are of the form ub = xv−tb, us = ts −xvs, where the binary variable x indicates if trade occurred, and tb, ts are the transfers. Consider the following decentralized trade mechanism. • Stage 1: If seller reports H goto Stage 2. Otherwise, trade at the low price pl. STOP. • Stage 2: The buyer has to choose between: - (a) Trade at the high price ph. - (b) No trade and seller pays ∆ to the buyer.
Claim 4. Let ∆ = lb−ph+ . The unique subgame perfect equilibrium of the trade mechanism is to report the true state in Stage 1 and trading if Stage 2 is reached.
Note that the outcome (b) is never chosen in equilibrium.
The buyer might maliciously punish the seller by choosing the outcome (b) when the true state is H. The following notion quantifies the consideration that a player is not indifferent to the private surpluses of others.
Definition 2. A player is q-malicious if his payoff equals: (1 − q) (his private surplus) − q (summation of others surpluses), q ∈ [0, 1].
This definition appeared independently in [7] in different context. We shall see that the traders would avoid such bad behavior if they are q-malicious, where q < 0.5, that is if their non-indifference impact is bounded by 0.5.
Equilibria outcomes remain unchanged, and so cooperation is achieved as in the original case of non-malicious players.
Consider the trade mechanism with pl = (1 − q) hs + q lb , ph = q hs + (1 − q) lb , ∆ = (1 − q) (hb − lb − ). Note that pl < ph for q < 0.5.
Claim 5. If q < 0.5, then the unique subgame perfect equilibrium for q-malicious players remains unchanged.
Proof. By backward induction we consider two cases.
In state H, the q-malicious buyer would prefer to trade if (1 − q)(hb − ph) + q(hs − ph) > (1 − q)∆ + q(∆). Indeed, (1 − q)hb + qhs > ∆ + ph. Trivially, the seller prefers to trade at the higher price, (1 − q)(pl − hs) + q(pl − hb) < (1 − q)(ph − hs) + q(ph − hb).
In state L the buyer prefers the no trade outcome, as (1−q)(lb −ph)+q(ls −ph) < ∆. The seller prefers to trade at a low price, as (1 − q)(pl − ls) + q(pl − lb) > 0 > −∆.
No mechanism can Nash-implement this trading goal if the only possible outcomes are trade at pl and trade at ph.
To see this, it is enough to consider normal forms (as any extensive form mechanism can be presented as a normal one).
Consider a matrix representation, where the seller is the row player and the buyer is the column player, in which every entry includes an outcome. Suppose there is equilibrium entry for the state L. The associate column must be all pl, otherwise the seller would have an incentive to deviate. Similarly, the associate row of the H equilibrium entry must be all ph (otherwise the buyer would deviate), a contradiction. 7 8 The buyer prefers pl and seller ph, and so the preferences are identical in both states. Hence reporting preferences over outcomes is not enough - players must supply additional information. This is captured by outcome (b) in the trade mechanism.
Intuitively, if a goal is not Nash-implementable we need to add more outcomes. The drawback is that some new additional equilibria must be ruled out. E.g., additional Nash equilibrium for the trade mechanism is (trade at pl, (b)). That is, the seller chooses to trade at low price at either states, and the buyer always chooses the no trade option that fines the seller, if the second stage is reached. Such buyer"s threat is not credible, because if the mechanism is played only once, and Stage 2 is reached in state H, the buyer would strictly decrease his payoff if he chooses (b). Clearly, this is not a subgame perfect equilibrium. Although each extensive game-form is strategically equivalent to a normal form one, the extensive form representation places more structure and so it seems plausible that the subgame perfect equilibrium will be played.9 7 Formally, this goal is not Maskin monotonic, a necessary condition for Nash-implementability [24]. 8 A similar argument applies for the Fair Assignment Problem. 9 Interestingly, it is a straight forward to construct a sequential mechanism with unique SPE, and additional NE with a strictly larger payoff for every player. 244
In this section we describe a simplified Peer-to-Peer network for file sharing, without payments in equilibrium, using a certificate-based challenging method. In this challenging method - as opposed to [28] - an agent that challenges cannot harm other agents, unless he provides a valid certificate.
In general, if agent B copied a file f from agent A, then agent A knows that agent B holds a copy of the file. We denote such information as a certificate(B, f) (we shall omit cryptographic details). Such a certificate can be recorded and distributed along the network, and so we can treat each agent holding the certificate as an informed agent.
Assumptions: We assume an homogeneous system with files of equal size. The benefit each agent gains by holding a copy of any file is V . The only cost each agent has is the uploading cost C (induced while transferring a file to an immediate neighbor). All other costs are negligible (e.g., storing the certificates, forwarding messages, providing acknowledgements, digital signatures, etc). Let upA, downA be the numbers of agent A uploads and downloads if he always cooperates. We assume that each agent A enters the system if upA · C < downA · V .
Each agent has a quasilinear utility and only cares about his current bandwidth usage. In particular, he ignores future scenarios (e.g., whether forwarding or dropping of a packet might affect future demand).
We start with a mechanism for a network with 3 p-informed agents: B, A1, A2. We assume that B is directly connected to A1 and A2.
If B has the certificate(A1, f), then he can apply directly to A1 and request the file (if he refuses, then B can go to court). The following basic sequential mechanism is applicable whenever agent B is not informed and still would like to download the file if it exists in the network. Note that this goal cannot be implemented in dominant strategies without payments (similar to Claim 1, when the type of each agent here is the set of files he holds). Define tA,B to be the monetary amount that agent A should transfer to B. • Stage 1: Agent B requests the file f from A1. - If A1 replies yes then B downloads the file from A1. STOP. - Otherwise, agent B sends A1s no reply to agent A2. ∗ If A2 declares agree then goto the next stage. ∗ Else, A2 sends a certificate(A1, f) to agent B. · If the certificate is correct then tA1,A2 = βp. STOP. · Else tA2,A1 = |C| + . STOP.
Stage 2: Agent B requests the file f from A2. Switch the roles of the agents A1, A2.
Claim 6. The basic mechanism is budget-balanced (transfers always sum to zero) and decentralized.
Theorem 1. Let βp = |C| p + , p ∈ (0, 1]. A strategy that survives iterative elimination of weakly dominated strategies is to reply yes if Ai holds the file, and to challenge only with a valid certificate. As a result, B downloads the file if some agent holds it, in equilibrium. There are no payments or transfers in equilibrium.
Proof. Clearly if the mechanism ends without challenging: −C ≤ u(Ai) ≤ 0. And so, challenging with an invalid certificate is always a dominated strategy. Now, when Stage
has the file it is a weakly undominated strategy to misreport, whether A1 is informed or not: A2"s expected payoff gained by misreporting no ≤ p · (−βp) + (1 − p) · 0 < −C ≤ A2"s payoff if she reports yes.
This argument can be reasoned also for Stage 1, when A1 reports whether he has the file. A1 knows that A2 will report yes if and only if she has the file in the next stage, and so the maximum payoff he can gain is at most zero since he cannot expect to get a bonus.
In a chain network, agent B is directly connected to A1, and Ai is directly connected to agent Ai+1. Assume that we have an acknowledgment protocol to confirm the receipt of a particular message. To avoid message dropping, we add the fine (βp +2 ) to be paid by an agent who hasn"t properly forwarded a message. The chain mechanism follows: • Stage i: Agent B forwards a request for the file f to Ai (through {Ak}k≤i). • If Ai reports yes, then B downloads f from Ai.
STOP. • Otherwise Ai reports no.
If Aj sends a certificate(Ak, f) to B, ( j, k ≤ i), then - If certificate(Ak, f) is correct, then t(Ak, Aj) = βp. STOP. - Else, t(Aj, Ak) = C + . STOP.
If Ai reports that he has no copy of the file, then any agent in between might challenge. Using digital signatures and acknowledgements, observe that every agent must forward each message, even if it contains a certificate showing that he himself has misreported.
We use the same fine, βp, as in the basic mechanism, because the protocol might end at stage 1 (clearly, the former analysis still applies, since the actual p increases with the number of players).
In this subsection we consider general network structures.
We need the assumption that there is a ping protocol that checks whether a neighbor agent is on-line or not (that is, an on-line agent cannot hide himself). To limit the amount of information to be recorded, we assume that an agent is committed to keep any downloaded file to at least one hour, and so certificates are valid for a limited amount of time. We assume that each agent has a digitally signed listing of his current immediate neighbors. As in real P2P file sharing applications, we restrict each request for a file to be forwarded at most r times (that is, downloads are possible only inside a neighborhood of radius r). 245 The network mechanism utilizes the chain mechanism in the following way: When agent B requests a file from agent A (at most r − 1 far), then A sends to B the list of his neighbors and the output of the ping protocol to all of these neighbors. As a result, B can explore the network.
Remark: In this mechanism we assumed that the environment is p-informed. An important design issue that it is not addressed here is the incentives for the information propagation phase.
Web caches are widely used tool to improve overall system efficiency by allowing fast local access. They were listed in [12] as a challenging application of Distributed Algorithmic Mechanism Design.
Nisan [30] considered a single cache shared by strategic agents. In this problem, agent i gains the value vT i if a particular item is loaded to the local shared cache. The efficient goal is to load the item if and only if ΣvT i ≥ C, where C is the loading cost. This goal reduces to the public project problem analyzed by Clarke [10]. However, it is well known that this mechanism is not budget-balanced (e.g., if the valuation of each player is C, then everyone pays zero).
In this section we suggest informational and environmental assumptions for which we describe a decentralized budgetbalanced efficient mechanism. We consider environments for which future demand of each agent depends on past demand. The underlying informational and environmental requirements are as follows.
is the target node (even if he has to forward the message as an intermediate node of some routing path).
An agent cannot initiate a message on behalf of other agents.
every agent can provide a certificate indicating that he handled a certain message properly.
p is such that the agent"s induced cost for keeping records of information is negligible. We also assume that the cost incurred by sending and forwarding messages is negligible.
i initiated for the item during the time slot t. We assume that vT i (t), the value for caching the item in the beginning of slot t depends only on most recent slot, formally vT i (t) = max{Vi(qi(t − 1)), C}, where Vi(·) is a non-decreasing real function. In addition,
Vi(·) is a common knowledge among the players.
agent j happens to handle k requests initiated by agent i during the time slot t, then qi(t) = kα, where α depends on the routing protocol and the environment (α might be smaller than 1, if each request is flooded several times). We assume that the only way agent i can affect the true qi(t) is by superficially increasing his demand for the cached item, but not the other way (that is, agent"s loss, incurred by giving up a necessary request for the item, is not negligible).
The first requirement is to avoid free riding, and also to avoid the case that an agent superficially increases the demand of others and as a result decreases his own demand.
The second requirement is to avoid the case that an agent who gets a routing request for the item, records it and then drops it. The third is to ensure that the environment stays well informed. In addition, if the forwarding cost is negligible each agent cooperates and forwards messages as he would not like to decrease the future demand (that monotonically depends on the current time slot, as assumed in the forth requirement) of some other agent. Given that the payments are increasing with the declared values, the forth and fifth requirements ensure that the agent would not increase his demand superficially and so qi(t) is the true demand.
The following Web-Cache Mechanism implements the efficient goal that shares the cost proportionally. For simplicity it is described for two players and w.l.o.g vT i (t) equals the number of requests initiated by i and observed by any informed j (that is, α = 1 and Vi(qi(t − 1)) = qi(t − 1)). • Stage 1: (Elicitation of vT A(t)) Alice announces vA.
Bob announces vA ≥ vA. If vA = vA goto the next Stage. Otherwise (Bob challenges): - If Bob provides vA valid records then Alice pays C to finance the loading of the item into the cache.
She also pays βp to Bob. STOP. - Otherwise, Bob finances the loading of the item into the cache. STOP. • Stage 2: The elicitation of vT B(t) is done analogously. • Stage 3: If vA + vB < C, then STOP.
Otherwise, load the item to the cache, Alice pays pA = vA vA+vB · C, and Bob pays pB = vB vA+vB · C.
Claim 7. It is a dominated strategy to overreport the true value.
Proof. Let vT A < VA. There are two cases to consider: • If vT A + vB < C and vA + vB ≥ C.
We need to show that if the mechanism stops normally Alice would pay more than vT A, that is: vA vA+vB ·C > vT A.
Indeed, vA C > vA (vT A + vB) > vT A (vA + vB). • If vT A + vB ≥ C, then clearly, vA vA+vB > vT A vT A +vB .
Theorem 2. Let βp = max{0, 1−2p p · C} + , p ∈ (0, 1]. A strategy that survives iterative elimination of weakly dominated strategies is to report the truth and to challenge only when the agent is informed. The mechanism is efficient, budget-balanced, exhibits consumer sovereignty, no positive transfer and individual rationality10 .
Proof. Challenging without being informed (that is, without providing enough valid records) is always dominated strategy in this mechanism. Now, assume w.l.o.g. Alice is 10 See [29] or [12] for exact definitions. 246 the last to report her value. Alice"s expected payoff gained by underreporting ≤ p · (−C − βp) + (1 − p) · C < p · 0 + (1 − p) · 0 ≤ Alice"s expected payoff if she honestly reports.
The right hand side equals zero as the participation costs are negligible. Reasoning back, Bob cannot expect to get the bonus and so misreporting is dominated strategy for him.
In this paper we have seen a new partial informational assumption, and we have demonstrated its suitability to networks in which computational agents can easily collect and distribute information. We then described some mechanisms using the concept of iterative elimination of weakly dominated strategies. Some issues for future work include: • As we have seen, the implementation issue in p-informed environments is straightforward - it is easy to construct incentive compatible mechanisms even for non-singleparameter cases. The challenge is to find more realistic scenarios in which the partial informational assumption is applicable. • Mechanisms for information propagation and maintenance. In our examples we choose p such that the maintenance cost over time is negligible. However, the dynamics of the general case is delicate: an agent can use the recorded information to eliminate data that is not likely to be needed, in order to decrease his maintenance costs. As a result, the probability that the environment is informed decreases, and selfish agents would not cooperate. Incentives for information propagation should be considered as well (e.g., for P2P networks for file sharing). • It seems that some social choice goals cannot be implemented if each player is at least 1/n-malicious (where n is the number of players). It would be interesting to identify these cases.
Acknowledgements We thank Meitav Ackerman, Moshe Babaioff, Liad Blumrozen, Michal Feldman, Daniel Lehmann, Noam Nisan, Motty Perry and Eyal Winter for helpful discussions.
[1] A. Archer and E. Tardos. Truthful mechanisms for one-parameter agents. In IEEE Symposium on Foundations of Computer Science, pages 482-491,
[2] Aaron Archer, Christos Papadimitriou, Kunal Talwar, and Eva Tardos. An approximate truthful mechanism for combinatorial auctions with single parameter agent. In SODA, 2003. [3] Moshe Babaioff, Ron Lavi, and Elan Pavlov.
Single-parameter domains and implementation in undominated strategies, 2004. Working paper. [4] Yair Bartal, Rica Gonen, and Noam Nisan. Incentive compatible multi-unit combinatorial auctions, 2003.
TARK-03. [5] Sushil Bikhchandani, Shurojit Chatterji, and Arunava Sen. Incentive compatibility in multi-unit auctions,
[6] Liad Blumrosen, Noam Nisan, and Ilya Segal.
Auctions with severely bounded communication, 2004.
Working paper. [7] F. Brandt, T. Sandholm, and Y. Shoham. Spiteful bidding in sealed-bid auctions, 2005. [8] Patrick Briest, Piotr Krysta, and Berthold Voecking.
Approximation techniques for utilitarian mechanism design. In STOC, 2005. [9] Chiranjeeb Buragohain, Divy Agrawal, and Subhash Suri. A game-theoretic framework for incentives in p2p systems. In IEEE P2P, 2003. [10] E. H. Clarke. Multipart pricing of public goods. Public Choice, 11:17-33, 1971. [11] Joan Feigenbaum, Christos Papadimitrios, and Scott Shenkar. Sharing the cost of multicast transmissions.
Computer and system Sciences, 63(1), 2001. [12] Joan Feigenbaum and Scott Shenker. Distributed algorithmic mechanism design: Recent results and future directions. In Proceedings of the 6th International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications, pages 1-13. ACM Press, New York, 2002. [13] M. Feldman, K. Lai, I. Stoica, and J. Chuang. Robust incentive techniques for peer-to-peer networks. In EC,
[14] A. Goldberg, J. Hartline, A. Karlin, and A. Wright.
Competitive auctions, 2004. Working paper. [15] Philippe Golle, Kevin Leyton-Brown, Ilya Mironov, and Mark Lillibridge. Incentives for sharing in peer-to-peer networks. In EC, 2001. [16] Ron Holzman, Noa Kfir-Dahav, Dov Monderer, and Moshe Tennenholtz. Bundling equilibrium in combinatorial auctions. Games and Economic Behavior, 47:104-123, 2004. [17] Ron Holzman and Dov Monderer. Characterization of ex post equilibrium in the vcg combinatorial auctions.
Games and Economic Behavior, 47:87-103, 2004. [18] Matthew O. Jackson. A crash course in implementation theory, 1997. mimeo: California Institute of Technology. 25. [19] A. Kothari, D. Parkes, and S. Suri.
Approximately-strategyproof and tractable multi-unit auctions. In EC, 2003. [20] Ron Lavi, Ahuva Mu"alem, and Noam Nisan. Towards a characterization of truthful combinatorial auctions.
In FOCS, 2003. [21] Ron Lavi and Noam Nisan. Online ascending auctions for gradually expiring goods. In SODA, 2005. [22] Daniel Lehmann, Liadan O"Callaghan, and Yoav Shoham. Truth revelation in approximately efficient combinatorial auctions. Journal of the ACM, 49(5):577-602, 2002. [23] A. Mas-Collel, W. Whinston, and J. Green.
Microeconomic Theory. Oxford university press, 1995. [24] Eric Maskin. Nash equilibrium and welfare optimality.
Review of Economic Studies, 66:23-38, 1999. [25] Eric Maskin and Tomas Sj¨ostr¨om. Implementation theory, 2002. 247 [26] Aranyak Mehta and Vijay Vazirani. Randomized truthful auctions of digital goods are randomizations over truthful auctions. In EC, 2004. [27] John Moore. Implementation, contract and renegotiation in environments with complete information, 1992. [28] John Moore and Rafael Repullo. Subgame perfect implementation. Econometrica, 56(5):1191-1220, 1988. [29] H. Moulin and S. Shenker. Strategyproof sharing of submodular costs: Budget balance versus efficiency.
Economic Theory, 18(3):511-533, 2001. [30] Noam Nisan. Algorithms for selfish agents. In STACS,
[31] Noam Nisan and Amir Ronen. Computationally feasable vcg mechanisms. In EC, 2000. [32] Noam Nisan and Amir Ronen. Algorithmic mechanism design. Games and Economic Behavior, 35:166-196,
[33] M. J. Osborne and A. Rubinstein. A Course in Game Theory. MIT press, 1994. [34] Christos H. Papadimitriou. Algorithms, games, and the internet. In STOC, 2001. [35] Kevin Roberts. The characterization of implementable choice rules. In Jean-Jacques Laffont, editor,
Aggregation and Revelation of Preferences. Papers presented at the 1st European Summer Workshop of the Econometric Society, pages 321-349.
North-Holland, 1979. [36] Irit Rozenshtrom. Dominant strategy implementation with quasi-linear preferences, 1999. Master"s thesis,
Dept. of Economics, The Hebrew University,

Combinatorial exchanges combine and generalize two different mechanisms: double auctions and combinatorial auctions. In a double auction (DA), multiple buyers and sellers trade units of an identical good [20]. In a combinatorial auction (CA), a single seller has multiple heterogeneous items up for sale [11]. Buyers may have complementarities or substitutabilities between goods, and are provided with an expressive bidding language. A common goal in both market designs is to determine the efficient allocation, which is the allocation that maximizes total value.
A combinatorial exchange (CE) [24] is a combinatorial double auction that brings together multiple buyers and sellers to trade multiple heterogeneous goods. For example, in an exchange for wireless spectrum, a bidder may declare that she is willing to pay $1 million for a trade where she obtains licenses for New York City, Boston, and Philadelphia, and loses her license for Washington DC. Thus, unlike a DA, a CE allows all participants to express complex valuations via expressive bids. Unlike a CA, a CE allows for fragmented ownership, with multiple buyers and sellers and agents that are both buying and selling.
CEs have received recent attention both in the context of wireless spectrum allocation [18] and for airport takeoff and landing slot allocation [3]. In both of these domains there are incumbents with property rights, and it is important to facilitate a complex multi-way reallocation of resources.
Another potential application domain for CEs is to resource allocation in shared distributed systems, such as PlanetLab [13]. The instantiation of our general purpose design to specific domains is a compelling next step in our research.
This paper presents the first design for an iterative combinatorial exchange (ICE). The genesis of this project was a class, CS 286r Topics at the Interface between Economics and Computer Science, taught at Harvard University in Spring 2004.1 The entire class was dedicated to the design and prototyping of an iterative CE.
The ICE design problem is multi-faceted and quite hard.
The main innovation in our design is an expressive yet concise tree-based bidding language (which generalizes known languages such as XOR/OR [23]), and the tight coupling of this language with efficient algorithms for price-feedback to guide bidding, winner-determination to determine trades, and revealed-preference activity rules to ensure progress across rounds. The exchange is iterative: bidders express upper and lower valuations on trades by annotating their bid-tree, and then tighten these bounds in response to price feedback in each round. The Threshold payment rule, introduced by Parkes et al. [24], is used to determine final payments.
The exchange has a number of interesting theoretical properties. For instance, when there exist linear prices we establish soundness and completeness: for straightforward bidders that adjust their bounds to meet activity rules while keeping their true value within the bounds, the exchange will terminate with the efficient allocation. In addition, the 1 http://www.eecs.harvard.edu/∼parkes/cs286r/ice.html 249 Truth Agent Act Rule WD ACC FAIR BALClosing RuleVickreyThreshold DONE ! DONE 2,2 +A +10 +B +10 BUYER 2,2 -A -5 -B -5 SELLER 2,2 +A +15 +8 +B +15 +8 BUYER 2,2 -A -2 -6 -B -2 -6 SELLER BUYER, buy AB SELLER, sell AB
PA+PB=14 PA=PB=7 PBUYER = 16 - (4-0) = 12 PSELLER = -12 - (4-0) = -16 PBUYER = 14 PSELLER = -14 Pessim istic O ptim istic = 1 Figure 1: ICE System Flow of Control efficient allocation can often be determined without bidders revealing, or even knowing, their exact value for all trades.
This is essential in complex domains where the valuation problem can itself be very challenging for a participant [28].
While we cannot claim that straightforward bidding is an equilibrium of the exchange (and indeed, should not expect to by the Myerson-Satterthwaite impossibility theorem [22]), the Threshold payment rule minimizes the ex post incentive to manipulate across all budget-balanced payment rules.
The exchange is implemented in Java and is currently in validation. In describing the exchange we will first provide an overview of the main components and introduce several working examples. Then, we introduce the basic components for a simple one-shot variation in which bidders state their exact values for trades in a single round. We then describe the full iterative exchange, with upper and lower values, price-feedback, activity rules, and termination conditions. We state some theoretical properties of the exchange, and end with a discussion to motivate our main design decisions, and suggest some next steps.
The design has four main components, which we will introduce in order through the rest of the paper: • Expressive and concise tree-based bidding language.
The language describes values for trades, such as my value for selling AB and buying C is $100, or my value for selling ABC is -$50, with negative values indicating that a bidder must receive a payment for the trade to be acceptable. The language allows bidders to express upper and lower bounds on value, which can be tightened across rounds. • Winner Determination. Winner-determination (WD) is formulated as a mixed-integer program (MIP), with the structure of the bid-trees captured explicitly in the formulation. Comparing the solution at upper and lower values allows for a determination to be made about termination, with progress in intermediate rounds driven by an intermediate valuation and the lower values adopted on termination. • Payments. Payments are computed using the Threshold payment rule [24], with the intermediate valuations adopted in early rounds and lower values adopted on termination. • Price feedback. An approximate price is computed for each item in the exchange in each round, in terms of the intermediate valuations and the provisional trade. The prices are optimized to approximate competitive equilibrium prices, and further optimized to best approximate the current Threshold payments with remaining ties broken to favor prices that are balanced across different items. In computing the prices, we adopt the methods of constraint-generation to exploit the structure of the bidding language and avoid enumerating all feasible trades. The subproblem to generate new constraints is a variation of the WD problem. • Activity rule. A revealed-preference activity rule [1] ensures progress across rounds. In order to remain active, a bidder must tighten bounds so that there is enough information to define a trade that maximizes surplus at the current prices. Another variation on the WD problem is formulated, both to verify that the activity rule is met and also to provide feedback to a bidder to explain how to meet the rule.
An outline of the ICE system flow of control is provided in Figure 1. We will return to this example later in the paper. For now, just observe in this two-agent example that the agents state lower and upper bounds that are checked in the activity rule, and then passed to winner-determination (WD), and then through three stages of pricing (accuracy, fairness, balance). On passing the closing rule (in which parameters αeff and αthresh are checked for convergence of the trade and payments), the exchange goes to a last-and-final round. At the end of this round, the trade and payments are finally determined, based on the lower valuations.
Many ascending-price one-sided CAs are known in the literature [10, 25, 29]. Direct elicitation approaches have also been proposed for one-sided CAs in which agents respond to explicit queries about their valuations [8, 14, 19]. A number of ascending CAs are designed to work with simple prices on items [12, 17]. The price generation methods that we use in ICE generalize the methods in these earlier papers.
Parkes et al. [24] studied sealed-bid combinatorial exchanges and introduced the Threshold payment rule.
Subsequently, Krych [16] demonstrated experimentally that the Threshold rule promotes efficient allocations. We are not aware of any previous studies of iterative CEs. Dominant strategy DAs are known for unit demand [20] and also for single-minded agents [2]. No dominant strategy mechanisms are known for the general CE problem.
ICE is a hybrid auction design, in that it couples simple item prices to drive bidding in early rounds with combinatorial WD and payments, a feature it shares with the clock-proxy design of Ausubel et al. [1] for one-sided CAs.
We adopt a variation on the clock-proxy auctions"s revealedpreference activity rule.
The bidding language shares some structural elements with the LGB language of Boutilier and Hoos [7], but has very different semantics. Rothkopf et al. [27] also describe a restricted tree-based bidding language. In LGB, the semantics are those of propositional logic, with the same items in an allocation able to satisfy a tree in multiple places.
Although this can make LGB especially concise in some settings, the semantics that we propose appear to provide useful locality, so that the value of one component in a tree can be understood independently from the rest of the tree.
The idea of capturing the structure of our bidding language explicitly within a mixed-integer programming formulation follows the developments in Boutilier [6].
In our model, we consider a set of goods, indexed {1, . . . , m} and a set of bidders, indexed {1, . . . , n}. The initial allocation of goods is denoted x0 = (x0 1, . . . , x0 n), with x0 i = (x0 i1, . . . , x0 im) and x0 ij ≥ 0 for good j indicating the number 250 of units of good j held by bidder i. A trade λ = (λ1, . . . , λn) denotes the change in allocation, with λi = (λi1, . . . , λim) where λij ∈   is the change in the number of units of item j to bidder i. So, the final allocation is x1 = x0 + λ.
Each bidder has a value vi(λi) ∈ ¡ for a trade λi. This value can be positive or negative, and represents the change in value between the final allocation x0 i +λi and the initial allocation x0 i . Utility is quasi-linear, with ui(λi, p) = vi(λi)−p for trade λi and payment p ∈ ¡ . Price p can be negative, indicating the bidder receives a payment for the trade. We use the term payoff interchangeably with utility.
Our goal in the ICE design is to implement the efficient trade. The efficient trade, λ∗ , maximizes the total increase in value across bidders.
Definition 1 (Efficient trade). The efficient trade λ∗ solves max (λ1,...,λn) ¢ i vi(λi) s.t. λij + x0 ij ≥ 0, ∀i, ∀j (1) ¢ i λij ≤ 0, ∀j (2) λij ∈   (3) Constraints (1) ensure that no agent sells more items than it has in its initial allocation. Constraints (2) provide free disposal, and allows feasible trades to sell more items than are purchased (but not vice versa).
Later, we adopt Feas(x0 ) to denote the set of feasible trades, given these constraints and given an initial allocation x0 = (x0 1, . . . , x0 n).
In this section, we provide three simple examples of instances that we will use to illustrate various components of the exchange. All three examples have only one seller, but this is purely illustrative.
Example 1. One seller and one buyer, two goods {A, B}, with the seller having an initial allocation of AB. Changes in values for trades: seller buyer AND(−A, −B) AND(+A, +B) -10 +20 The AND indicates that both the buyer and the seller are only interested in trading both goods as a bundle. Here, the efficient (value-maximizing) trade is for the seller to sell AB to the buyer, denoted λ∗ = ([−1, −1], [+1, +1]).
Example 2. One seller and four buyers, four goods {A, B,
C, D}, with the seller having an initial allocation of ABCD.
Changes in values for trades: seller buyer1 buyer 2 buyer 3 buyer 4 OR(−A, −B, AND(+A, XOR(+A, AND(+C, XOR(+C, −C, −D) +B) +B) +D) +D)
The OR indicates that the seller is willing to sell any number of goods. The XOR indicates that buyers 2 and
they are interested. The efficient trade is for bundle AB to go to buyer 1 and bundle CD to buyer 3, denoted λ∗ = ([−1, −1, −1, −1], [+1, +1, 0, 0], [0, 0, 0, 0], [0, 0, +1, +1], [0, 0, 0, 0]). 2,2 +A +10 +B +10 BUYER 2,2 -A -5 -B -5 SELLER Example 1: Example 3: 2,2 +C +D BUYER 2 2,2 +A +B BUYER 1 +11 +84,4 -B SELLER -A -C -D Example 2: 1,1 +A +B BUYER 2 2,2 +A +B BUYER 1 +6 +40,4 -B SELLER -C -D-A 1,1 +C +D BUYER 4 2,2 +C +D +3 +2 BUYER 3 -18 Figure 2: Example Bid Trees.
Example 3. One seller and two buyers, four goods {A, B,
C, D}, with the seller having an initial allocation of ABCD.
Changes in values for trades: seller buyer1 buyer 2 AND(−A, −B, −C, −D) AND(+A, +B) AND(+C, +D) -18 +11 +8 The efficient trade is for bundle AB to go to buyer 1 and bundle CD to go to buyer 2, denoted λ∗ = ([−1, −1, −1, −1], [+1, +1, 0, 0], [0, 0, +1, +1]).
The description of ICE is broken down into two sections: one-shot (sealed-bid) and iterative. In this section we abstract away the iterative aspect and introduce a specialization of the tree-based language that supports only exact values on nodes.
The bidding language is designed to be expressive and concise, entirely symmetric with respect to buyers and sellers, and to extend to capture bids from mixed buyers and sellers, ranging from simple swaps to highly complex trades.
Bids are expressed as annotated bid trees, and define a bidder"s value for all possible trades.
The language defines changes in values on trades, with leaves annotated with traded items and nodes annotated with changes in values (either positive or negative). The main feature is that it has a general interval-choose logical operator on internal nodes, and that it defines careful semantics for propagating values within the tree. We illustrate the language on each of Examples 1-3 in Figure 2.
The language has a tree structure, with trades on items defined on leaves and values annotated on nodes and leaves.
The nodes have zero values where no value is indicated.
Internal nodes are also labeled with interval-choose (IC) ranges. Given a trade, the semantics of the language define which nodes in the tree can be satisfied, or switched-on.
First, if a child is on then its parent must be on. Second, if a parent node is on, then the number of children that are on must be within the IC range on the parent node. Finally, leaves in which the bidder is buying items can only be on if the items are provided in the trade.
For instance, in Example 2 we can consider the efficient trade, and observe that in this trade all nodes in the trees of buyers 1 and 3 (and also the seller), but none of the nodes in the trees of buyers 2 and 4, can be on. On the other hand, in 251 the trade in which A goes to buyer 2 and D to buyer 4, then the root and appropriate leaf nodes can be on for buyers 2 and 4, but no nodes can be on for buyers 1 and 3. Given a trade there is often a number of ways to choose the set of satisfied nodes. The semantics of the language require that the nodes that maximize the summed value across satisfied nodes be activated.
Consider bid tree Ti from bidder i. This defines nodes β ∈ Ti, of which some are leaves, Leaf (i) ⊆ Ti. Let Child(β) ⊆ Ti denote the children of a node β (that is not itself a leaf).
All nodes except leaves are labeled with the interval-choose operator [IC x i (β), ICy i (β)]. Every node is also labeled with a value, viβ ∈ ¡ . Each leaf β is labeled with a trade, qiβ ∈   m (i.e., leaves can define a bundled trade on more than one type of item.) Given a trade λi to bidder i, the interval-choose operators and trades on leaves define which nodes can be satisfied.
There will often be a choice. Ties are broken to maximize value. Let satiβ ∈ {0, 1} denote whether node β is satisfied.
Solution sati is valid given tree Ti and trade λi, written sati ∈ valid(Ti, λi), if and only if: ¢ β∈Leaf (i) qiβj · satiβ ≤ λij , ∀i, ∀j (4) ICx i (β)satiβ ≤ ¢ β ∈Child(β) satiβ ≤ ICy i (β)satiβ, ∀β /∈ Leaf (i) (5) In words, a set of leaves can only be considered satisfied given trade λi if the total increase in quantity summed across all such leaves is covered by the trade, for all goods (Eq. 4).
This works for sellers as well as buyers: for sellers a trade is negative and this requires that the total number of items indicated sold in the tree is at least the total number sold as defined in the trade. We also need upwards-propagation: any time a node other than the root is satisfied then its parent must be satisfied (by   β ∈Child(β) satiβ ≤ ICy i (β)satiβ in Eq. 5). Finally, we need downwards-propagation: any time an internal node is satisfied then the appropriate number of children must also be satisfied (Eq. 5). The total value of trade λi, given bid-tree Ti, is defined as: vi(Ti, λi) = max sat∈valid(Ti,λi) ¢ β∈T vβ · satβ (6) The tree-based language generalizes existing languages.
For instance: IC(2, 2) on a node with 2 children is equivalent to an AND operator; IC(1, 3) on a node with 3 children is equivalent to an OR operator; and IC(1, 1) on a node with
XOR/OR bidding languages can be directly expressed as a bid tree in our language.2
This section defines the winner determination problem, which is formulated as a MIP and solved in our implementation with a commercial solver.3 The solver uses branchand-bound search with dynamic cut generation and branching heuristics to solve large MIPs in economically feasible run times. 2 The OR* language is the OR language with dummy items to provide additional structure. OR* is known to be expressive and concise. However, it is not known whether OR* dominates XOR/OR in terms of conciseness [23]. 3 CPLEX, www.ilog.com In defining the MIP representation we are careful to avoid an XOR-based enumeration of all bundles. A variation on the WD problem is reused many times within the exchange, e.g. for column generation in pricing and for checking revealed preference.
Given bid trees T = (T1, . . . , Tn) and initial allocation x0 , the mixed-integer formulation for WD is: WD(T, x0 ) : max λ,sat ¢ i ¢ β∈Ti viβ · satiβ s.t. (1), (2), satiβ ∈ {0, 1}, λij ∈   sati ∈ valid(Ti, λi), ∀i Some goods may go unassigned because free disposal is allowed within the clearing rules of winner determination.
These items can be allocated back to agents that sold the items, i.e. for which λij < 0.
The Threshold payment rule is based on the payments in the Vickrey-Clarke-Groves (VCG) mechanism [15], which itself is truthful and efficient but does not satisfy budget balance. Budget-balance requires that the total payments to the exchange are equal to the total payments made by the exchange. In VCG, the payment paid by agent i is pvcg,i = ˆv(λ∗ i ) − (V ∗ − V−i) (7) where λ∗ is the efficient trade, V ∗ is the reported value of this trade, and V−i is the reported value of the efficient trade that would be implemented without bidder i. We call ∆vcg,i = V ∗ − V−i the VCG discount. For instance, in Example 1 pvcg,seller = −10 − (+10 − 0) = −20 and pvcg,buyer = +20 − (+10 − 0) = 10, and the exchange would run at a budget deficit of −20 + 10 = −10.
The Threshold payment rule [24] determines budgetbalanced payments to minimize the maximal error across all agents to the VCG outcome.
Definition 2. The Threshold payment scheme implements the efficient trade λ∗ given bids, and sets payments pthresh,i = ˆvi(λ∗ i ) − ∆i, where ∆ = (∆1, . . . , ∆n) is set to minimize maxi(∆vcg,i − ∆i) subject to ∆i ≤ ∆vcg,i and   i ∆i ≤ V ∗ (this gives budget-balance).
Example 4. In Example 2, the VCG discounts are (9, 2, 0, 1, 0) to the seller and four buyers respectively, VCG payments are (−9, 4, 0, 2, 0) and the exchange runs at a deficit of -3. In Threshold, the discounts are (8, 1, 0, 0, 0) and the payments are (−8, 5, 0, 3, 0). This minimizes the worst-case error to VCG discounts across all budget-balanced payment schemes.
Threshold payments are designed to minimize the maximal ex post incentive to manipulate. Krych [16] confirmed that Threshold promotes allocative efficiency in restricted and approximate Bayes-Nash equilibrium.
We are now ready to introduce the iterative combinatorial exchange (ICE) design. Several new components are introduced, relative to the design for the one-shot exchange.
Rather than provide precise valuations, bidders can provide lower and upper valuations and revise this bid information across rounds. The exchange provides price-based feedback 252 to guide bidders in this process, and terminates with an efficient (or approximately-efficient) trade with respect to reported valuations.
In each round t ∈ {0, 1, . . .} the current lower and upper bounds, vt and vt , are used to define a provisional valuation profile vα (the α-valuation), together with a provisional trade λt and provisional prices pt = (pt 1, . . . , pt m) on items.
The α-valuation is a linear combination of the current upper and lower valuations, with αEFF ∈ [0, 1] chosen endogenously based on the closeness of the optimistic trade (at v) and the pessimistic trade (at v). Prices pt are used to inform an activity rule, and drive progress towards an efficient trade.
The bidding language is extended to allow a bidder i to report a lower and upper value (viβ, viβ) on each node. These take the place of the exact value viβ defined in Section 4.1.
Based on these labels, we can define the valuation functions vi(Ti, λi) and vi(Ti, λi), using the exact same semantics as in Eq. (6). We say that such a bid-tree is well-formed if viβ ≤ viβ for all nodes. The following lemma is useful: Lemma 1. Given a well-formed tree, T, then vi(Ti, λi) ≤ vi(Ti, λi) for all trades.
Proof. Suppose there is some λi for which vi(Ti, λi) > vi(Ti, λi). Then, maxsat∈valid(Ti,λi)   β∈Ti viβ · satβ > maxsat∈valid(Ti,λi)   β∈Ti viβ · satβ. But, this is a contradiction because the trade λ that defines vi(Ti, λi) is still feasible with upper bounds vi, and viβ ≥ viβ for all nodes β in a well-formed tree.
In each round, approximate competitive-equilibrium (CE) prices, pt = (pt 1, . . . , pt m), are determined. Given these provisional prices, the price on trade λi for bidder i is pt (λi) =   j≤m pt j · λij.
Definition 3 (CE prices). Prices p∗ are competitive equilibrium prices if the efficient trade λ∗ is supported at prices p∗ , so that for each bidder: λ∗ i ∈ arg max λ∈Feas(x0) {vi(λi) − p∗ (λi)} (8) CE prices will not always exist and we will often need to compute approximate prices [5]. We extend ideas due to Rassenti et al. [26], Kwasnica et al. [17] and Dunford et al. [12], and select approximate prices as follows: I: Accuracy. First, we compute prices that minimize the maximal error in the best-response constraints across all bidders.
II: Fairness. Second, we break ties to prefer prices that minimize the maximal deviation from Threshold payments across all bidders.
III: Balance. Third, we break ties to prefer prices that minimize the maximal price across all items.
Taken together, these steps are designed to promote the informativeness of the prices in driving progress across rounds.
In computing prices, we explain how to compute approximate (or otherwise) prices for structured bidding languages, and without enumerating all possible trades. For this, we adopt constraint generation to efficient handle an exponential number of constraints. Each step is described in detail below.
I: Accuracy. We adopt a definition of price accuracy that generalizes the notions adopted in previous papers for unstructured bidding languages. Let λt denote the current provisional trade and suppose the provisional valuation is vα . To compute accurate CE prices, we consider: min p,δ δ (9) s.t. vα i (λ) − p(λ) ≤ vα i (λt i) − p(λt i) + δ, ∀i, ∀λ (10) δ ≥ 0,pj ≥ 0, ∀j.
This linear program (LP) is designed to find prices that minimize the worst-case error across all agents.
From the definition of CE prices, it follows that CE prices would have δ = 0 as a solution to (9), at which point trade λt i would be in the best-response set of every agent (with λt i = ∅, i.e. no trade, for all agents with no surplus for trade at the prices.) Example 5. We can illustrate the formulation (9) on Example 2, assuming for simplicity that vα = v (i.e. truth).
The efficient trade allocates AB to buyer 1 and CD to buyer
minimize the δ ≥ 0 required to satisfy constraints: p(A) + p(B) + p(C) + p(D) ≥ 0 (seller) p(A) + p(B) ≤ 6 + δ (buyer 1) p(A) + δ ≥ 4, p(B) + δ ≥ 4 (buyer 2) p(C) + p(D) ≤ 3 (buyer 3) p(C) + δ ≥ 2, p(D) + δ ≥ 2 (buyer 4) An optimal solution requires p(A) = p(B) = 10/3, with δ = 2/3, with p(C) and p(D) taking values such as p(C) = p(D) = 3/2.
But, (9) has an exponential number of constraints (Eq. 10).
Rather than solve it explicitly we use constraint generation [4] and dynamically generate a sufficient subset of constraints. Let   i denote a manageable subset of all possible feasible trades to bidder i. Then, a relaxed version of (9) (written ACC) is formulated by substituting (10) with vα i (λ) − p(λ) ≤ vα i (λt i) − p(λt i) + δ, ∀i, ∀λ ∈   i , (11) where   i is a set of trades that are feasible for bidder i given the other bids. Fixing the prices p∗ , we then solve n subproblems (one for each bidder), max λ vα i (λi) − p∗ (λi) [R-WD(i)] s.t. λ ∈ Feas(x0 ), (12) to check whether solution (p∗ , δ∗ ) to ACC is feasible in problem (9). In R-WD(i) the objective is to determine a most preferred trade for each bidder at these prices. Let ˆλi denote the solution to R-WD(i). Check condition: vα i (ˆλi) − p∗ (ˆλ) ≤ vα i (λt i) − p∗ (λt i) + δ∗ , (13) and if this condition holds for all bidders i, then solution (p∗ , δ∗ ) is optimal for problem (9). Otherwise, trade ˆλi is added to   i for all bidders i for which this constraint is 253 violated and we re-solve the LP with the new set of constraints.4 II: Fairness. Second, we break remaining ties to prefer fair prices: choosing prices that minimize the worst-case error with respect to Threshold payoffs (i.e. utility to bidders with Threshold payments), but without choosing prices that are less accurate.5 Example 6. For example, accuracy in Example 1 (depicted in Figure 1) requires 12 ≤ pA +pB ≤ 16 (for vα = v).
At these valuations the Threshold payoffs would be 2 to both the seller and the buyer. This can be exactly achieved in pricing with pA + pB = 14.
The fairness tie-breaking method is formulated as the following LP: min p,π π [FAIR] s.t. vα i (λ) − p(λ) ≤ vα i (λt i) − p(λt i) + δ∗ i , ∀i, ∀λ ∈   i (14) π ≥ πvcg,i − (vα i (λt i) − p(λt i)), ∀i (15) π ≥ 0,pj ≥ 0, ∀j, where δ∗ represents the error in the optimal solution, from ACC. The objective here is the same as in the Threshold payment rule (see Section 4.3): minimize the maximal error between bidder payoff (at vα ) for the provisional trade and the VCG payoff (at vα ). Problem FAIR is also solved through constraint generation, using R-WD(i) to add additional violated constraints as necessary.
III: Balance. Third, we break remaining ties to prefer balanced prices: choosing prices that minimize the maximal price across all items. Returning again to Example 1, depicted in Figure 1, we see that accuracy and fairness require p(A) + p(B) = 14. Finally, balance sets p(A) = p(B) = 7.
Balance is justified when, all else being equal, items are more likely to have similar than dissimilar values.6 The LP for balance is formulated as follows: min p,Y Y [BAL] s.t. vα i (λ) − p(λ) ≤ vα i (λt i) − p(λt i) + δ∗ i , ∀i, ∀λ ∈   i (16) π∗ i ≥ πvcg,i − (vα i (λt i) − p(λt i)), ∀i, (17) Y ≥ pj, ∀j (18) Y ≥ 0, pj ≥ 0, ∀j, where δ∗ represents the error in the optimal solution from ACC and π∗ represents the error in the optimal solution from FAIR. Constraint generation is also used to solve BAL, generating new trades for   i as necessary. 4 Problem R-WD(i) is a specialization of the WD problem, in which the objective is to maximize the payoff of a single bidder, rather than the total value across all bidders. It is solved as a MIP, by rewriting the objective in WD(T, x0 ) as max{viβ · satiβ −   j p∗ j · λij } for agent i. Thus, the structure of the bid-tree language is exploited in generating new constraints, because this is solved as a concise MIP.
The other bidders are kept around in the MIP (but do not appear in the objective), and are used to define the space of feasible trades. 5 The methods of Dunford et al. [12], that use a nucleolus approach, are also closely related. 6 The use of balance was advocated by Kwasnica et al. [17].
Dunford et al. [12] prefer to smooth prices across rounds.
Comment 1: Lexicographical Refinement. For all three sub-problems we also perform lexicographical refinement (with respect to bidders in ACC and FAIR, and with respect to goods in BAL). For instance, in ACC we successively minimize the maximal error across all bidders. Given an initial solution we first pin down the error on all bidders for whom a constraint (11) is binding. For such a bidder i, the constraint is replaced with vα i (λ) − p(λ) ≤ vα i (λt i) − p(λt i) + δ∗ i , ∀λ ∈   i , (19) and the error to bidder i no longer appears explicitly in the objective. ACC is then re-solved, and makes progress by further minimizing the maximal error across all bidders yet to be pinned down. This continues, pinning down any new bidders for whom one of constraints (11) is binding, until the error is lexicographically optimized for all bidders.7 The exact same process is repeated for FAIR and BAL, with bidders pinned down and constraints (15) replaced with π∗ i ≥ πvcg,i − (vα i (λt i) − p(λt i)), ∀λ ∈   i , (where π∗ i is the current objective) in FAIR, and items pinned down and constraints (18) replaced with p∗ j ≥ pj (where p∗ j represents the target for the maximal price on that item) in BAL.
Comment 2: Computation. All constraints in   i are retained, and this set grows across all stages and across all rounds of the exchange. Thus, the computational effort in constraint generation is re-used. In implementation we are careful to address a number of  -issues that arise due to floating-point issues. We prefer to err on the side of being conservative in determining whether or not to add another constraint in performing check (13). This avoids later infeasibility issues. In addition, when pinning-down bidders for the purpose of lexicographical refinement we relax the associated bidder-constraints with a small > 0 on the righthand side.
The role of activity rules in the auction is to ensure both consistency and progress across rounds [21]. Consistency in our exchange requires that bidders tighten bounds as the exchange progresses. Activity rules ensure that bidders are active during early rounds, and promote useful elicitation throughout the exchange.
We adopt a simple revealed-preference (RP) activity rule.
The idea is loosely based around the RP-rule in Ausubel et al. [1], where it is used for one-sided CAs. The motivation is to require more than simply consistency: we need bidders to provide enough information for the system to be able to to prove that an allocation is (approximately) efficient.
It is helpful to think about the bidders interacting with proxy agents that will act on their behalf in responding to provisional prices pt−1 determined at the end of round t − 1. The only knowledge that such a proxy has of the valuation of a bidder is through the bid-tree. Suppose a proxy was queried by the exchange and asked which trade the bidder was most interested in at the provisional prices.
The RP rule says the following: the proxy must have enough 7 For example, applying this to accuracy on Example 2 we solve once and find bidders 1 and 2 are binding, for error δ∗ = 2/3. We pin these down and then minimize the error to bidders 3 and 4. Finally, this gives p(A) = p(B) = 10/3 and p(C) = p(D) = 5/3, with accuracy 2/3 to bidders 1 and
254 information to be able to determine this surplus-maximizing trade at current prices. Consider the following examples: Example 7. A bidder has XOR(+A, +B) and a value of +5 on the leaf +A and a value range of [5,10] on leaf +B.
Suppose prices are currently 3 for each of A and B. The RP rule is satisfied because the proxy knows that however the remaining value uncertainty on +B is resolved the bidder will always (weakly) prefer +B to +A.
Example 8. A bidder has XOR(+A, +B) and value bounds [5, 10] on the root node and a value of 1 on leaf +A.
Suppose prices are currently 3 for each of A and B. The RP rule is satisfied because the bidder will always prefer +A to +B at equal prices, whichever way the uncertain value on the root node is ultimately resolved.
Overloading notation, let vi ∈ Ti denote a valuation that is consistent with lower and upper valuations in bid tree Ti.
Definition 4. Bid tree Ti satisfies RP at prices pt−1 if and only if there exists some feasible trade L∗ for which, vi(L∗ i ) − pt−1 (L∗ i ) ≥ max λ∈Feas(x0) vi(λi) − pt−1 (λi), ∀vi ∈ Ti. (20) To make this determination for bidder i we solve a sequence of problems, each of which is a variation on the WD problem. First, we construct a candidate lower-bound trade, which is a feasible trade that solves: max λ vi(λi) − pt−1 (λi) [RP1(i)] s.t. λ ∈ Feas(x0 ), (21) The solution π∗ l to RP1(i) represents the maximal payoff that bidder i can achieve across all feasible trades, given its pessimistic valuation.
Second, we break ties to find a trade with maximal value uncertainty across all possible solutions to RP1(i): max λ vi(λi) − vi(λi) [RP2(i)] s.t. λ ∈ Feas(x0 ) (22) vi(λi) − pt−1 (λi) ≥ π∗ l (23) We adopt solution L∗ i as our candidate for the trade that may satisfy RP. To understand the importance of this tiebreaking rule consider Example 7. The proxy can prove +B but not +A is a best-response for all vi ∈ Ti, and should choose +B as its candidate. Notice that +B is a counterexample to +A, but not the other way round.
Now, we construct a modified valuation ˜vi, by setting ˜viβ =   viβ , if β ∈ sat(L∗ i ) viβ , otherwise. (24) where sat(L∗ i ) is the set of nodes that are satisfied in the lower-bound tree for trade L∗ i . Given this modified valuation, we find U∗ to solve: max λ ˜vi(λi) − pt−1 (λi) [RP3(i)] s.t. λ ∈ Feas(x0 ) (25) Let π∗ u denote the payoff from this optimal trade at modified values ˜v. We call trade U∗ i the witness trade. We show in Proposition 1 that the RP rule is satisfied if and only if π∗ l ≥ π∗ u.
Constructing the modified valuation as ˜vi recognizes that there is shared uncertainty across trades that satisfy the same nodes in a bid tree. Example 8 helps to illustrate this.
Just using vi in RP3(i), we would find L∗ i is buy A with payoff π∗ l = 3 but then find U∗ i is buy B with π∗ u = 7 and fail RP. We must recognize that however the uncertainty on the root node is resolved it will affect +A and +B in exactly the same way. For this reason, we set ˜viβ = viβ = 5 on the root node, which is exactly the same value that was adopted in determining π∗ l . Then, RP3(i) applied to U∗ i gives buy A and the RP test is judged to be passed.
Proposition 1. Bid tree Ti satisfies RP given prices pt−1 if and only if any lower-bound trade L∗ i that solves RP1(i) and RP2(i) satisfies: vi(Ti, L∗ i ) − pt−1 (L∗ i ) ≥ ˜vi(Ti, U∗ i ) − pt−1 (U∗ i ), (26) where ˜vi is the modified valuation in Eq. (24).
Proof. For sufficiency, notice that the difference in payoff between trade L∗ i and another trade λi is unaffected by the way uncertainty is resolved on any node that is satisfied in both L∗ i and λi. Fixing the values in ˜vi on nodes satisfied in L∗ i has the effect of removing this consideration when a trade U∗ i is selected that satisfies one of these nodes. On the other hand, fixing the values on these nodes has no effect on trades considered in RP3(i) that do not share a node with L∗ i . For the necessary direction, we first show that any trade that satisfies RP must solve RP1(i). Suppose otherwise, that some λi with payoff greater than π∗ l satisfies RP.
But, valuation vi ∈ Ti together with L∗ i presents a counterexample to RP (Eq. 20). Now, suppose (for contradiction) that some λi with maximal payoff π∗ l but uncertainty less than L∗ i satisfies RP. Proceed by case analysis. Case a): only one solution to RP1(i) has uncertain value and so λi has certain value. But, this cannot satisfy RP because L∗ i with uncertain value would be a counterexample to RP (Eq. 20). Case b): two or more solutions to RP1(i) have uncertain value. Here, we first argue that one of these trades must satisfy a (weak) superset of all the nodes with uncertain value that are satisfied by all other trades in this set.
This is by RP. Without this, then for any choice of trade that solves RP1(i), there is another trade with a disjoint set of uncertain but satisfied nodes that provides a counterexample to RP (Eq. 20). Now, consider the case that some trade contains a superset of all the uncertain satisfied nodes of the other trades. Clearly RP2(i) will choose this trade,
L∗ i , and λi must satisfy a subset of these nodes (by assumption). But, we now see that λi cannot satisfy RP because L∗ i would be a counterexample to RP.
Failure to meet the activity rule must have some consequence. In the current rules, the default action we choose is to set the upper bounds in valuations down to the maximal value of the provisional price on a node8 and the lowerbound value on that node.9 Such a bidder can remain active 8 The provisional price on a node is defined as the minimal total price across all feasible trades for which the subtree rooted at the tree is satisfied. 9 This is entirely analogous to when a bidder in an ascending clock auction stops bidding at a price: she is not permitted to bid at a higher price again in future rounds. 255 within the exchange, but only with valuations that are consistent with these new bounds.
In each round, our default design provides every bidder with the provisional trade and also with the current provisional prices. See 7 for an additional discussion. We also provide guidance to help a bidder meet the RP rule. Let sat(L∗ i ) and sat(U∗ i ) denote the nodes that are satisfied in trades L∗ i and U∗ i , as computed in RP1-RP3.
Lemma 2. When RP fails, a bidder must increase a lower bound on at least one node in sat(L∗ i ) \ sat(U∗ i ) or decrease an upper bound on at least one node in sat(U∗ i ) \ sat(L∗ i ) in order to meet the activity rule.
Proof. Changing the upper- or lower- values on nodes that are not satisfied by either trade does not change L∗ i or U∗ i , and does not change the payoff from these trades. Thus, the RP condition will continue to fail. Similarly, changing the bounds on nodes that are satisfied in both trades has no effect on revealed preference. A change to a lower bound on a shared node affects both L∗ i and U∗ i identically because of the use of the modified valuation to determine U∗ i . A change to an upper bound on a shared node has no effect in determining either L∗ i or U∗ i .
Note that when sat(U∗ i ) = sat(L∗ i ) then condition (26) is always trivially satisfied, and so the guidance in the lemma is always well-defined when RP fails. This is an elegant feedback mechanism because it is adaptive. Once a bidder makes some changes on some subset of these nodes, the bidder can query the exchange. The exchange can then respond yes, or can revise the set of nodes sat(λ∗ l ) and sat(λ∗ u) as necessary.
Once each bidder has committed its new bids (and either met the RP rule or suffered the penalty) then round t closes.
At this point, the task is to determine the new α-valuation, and in turn the provisional allocation λt and provisional prices pt . A termination condition is also checked, to determine whether to move the exchange to a last-and-final round. To define the α-valuation we compute the following two quantities: Pessimistic at Pessimistic (PP) Determine an efficient trade, λ∗ l , at pessimistic values, i.e. to solve maxλ   i vi(λi), and set PP=  i vi(λ∗ li).
Pessimistic at Optimistic (PO) Determine an efficient trade, λ∗ u, at optimistic values, i.e. to solve maxλ   i vi(λi), and set PO=  i vi(λ∗ ui).
First, note that PP ≥ PO and PP ≥ 0 by definition, for all bid-trees, although PO can be negative (because the right trade at v is not currently a useful trade at v).
Recognizing this, define γeff (PP, PO) = 1 + PP − PO PP , (27) when PP > 0, and observe that γeff (PP, PO) ≥ 1 when this is defined, and that γeff (PP, PO) will start large and then trend towards 1 as the optimistic allocation converges towards the pessimistic allocation. In each round, we define αeff ∈ [0, 1] as: αeff =  
1/γeff otherwise (28) which is 0 while PP is 0 and then trends towards 1 once PP> 0 in some round. This is used to define α-valuation vα i = αeff vi + (1 − αeff )vi, ∀i, (29) which is used to define the provisional allocation and provisional prices. The effect is to endogenously define a schedule for moving from optimistic to pessimistic values across rounds, based on how close the trades are to one another.
Termination Condition. In moving to the last-and-final round, and finally closing, we also care about the convergence of payments, in addition to the convergence towards an efficient trade. For this we introduce another parameter, αthresh ∈ [0, 1], that trends from 0 to 1 as the Threshold payments at lower and upper valuations converge. Consider the following parameter: γthresh = 1 + ||pthresh(v) − pthresh(v)||2 (PP/Nactive) , (30) which is defined for PP > 0, where pthresh(v) denotes the Threshold payments at valuation profile v, Nactive is the number of bidders that are actively engaged in trade in the PP trade, and || · ||2 is the L2-norm. Note that γthresh is defined for payments and not payoffs. This is appropriate because it is the accuracy of the outcome of the exchange that matters: i.e. the trade and the payments. Given this, we define αthresh =  
1/γthresh otherwise (31) which is 0 while PP is 0 and then trends towards 1 as progress is made.
Definition 5 (termination). ICE transitions to a lastand-final round when one of the following holds:
≥ CUTOFFeff and αthresh ≥ CUTOFFthresh,
where CUTOFFeff , CUTOFFthresh ∈ (0, 1] determine the accuracy required for termination.
At the end of the last-and-final round vα = v is used to define the final trade and the final Threshold payments.
Example 9. Consider again Example 1, and consider the upper and lower bounds as depicted in Figure 1. First, if the seller"s bounds were [−20, −4] then there is an optimistic trade but no pessimistic trade, and PO = −4 and PP = 0, and αeff = 0. At the bounds depicted, both the optimistic and the pessimistic trades occur and PO = PP = 4 and αeff = 1. However, we can see the Threshold payments are (17, −17) at v but (14, −14) at v. Evaluating γthresh , we have γthresh = 1 + √ 1/2(32+32) (4/2) = 5/2, and αthresh = 2/5.
For CUTOFFthresh < 2/5 the exchange would remain open.
On the other hand, if the buyer"s value for +AB was between [18, 24] and the seller"s value for −AB was between [−12, −6], the Threshold payments are (15, −15) at both upper and lower bounds, and αthresh = 1. 256 Component Purpose Lines Agent. Captures strategic behavior and information revelation decisions 762 Model Support Provides XML support to load goods and valuations into world 200 World Keeps track of all agent, good, and valuation details 998 Exchange Driver & Communication Controls exchange, and coordinates remote agent behavior 585 Bidding Language Implements the tree-based bidding language 1119 Activity Rule Engine Implements the revealed preference rule with range support 203 Closing Rule Engine Checks if auction termination condition reached 137 WD Engine Provides WD-related logic 377 Pricing Engine Provides Pricing-related logic 460 MIP Builders Translates logic used by engines into our general optimizer formulation 346 Pricing Builders Used by three pricing stages 256 Winner Determination Builders Used by WD, activity rule, closing rule, and pricing constraint generation 365 Framework Support code; eases modular replacement of above components 510 Table 1: Exchange Component and Code Breakdown.
ICE is approximately 6502 lines of Java code, broken up into the functional packages described in Table 1.10 The prototype is modular so that researchers may easily replace components for experimentation. In addition to the core exchange discussed in this paper, we have developed an agent component that allows a user to simulate the behavior and knowledge of other players in the system, better allowing a user to formulate their strategy in advance of actual play. A user specifies a valuation model in an XMLinterpretation of our bidding language, which is revealed to the exchange via the agent"s strategy.
Major exchange tasks are handled by engines that dictate the non-optimizer specific logic. These engines drive the appropriate MIP/LP builders. We realized that all of our optimization formulations boil down to two classes of optimization problem. The first, used by winner determination, activity rule, closing rule, and constraint generation in pricing, is a MIP that finds trades that maximize value, holding prices and slacks constant. The second, used by the three pricing stages, is an LP that holds trades constant, seeking to minimize slack, profit, or prices. We take advantage of the commonality of these problems by using common LP/MIP builders that differ only by a few functional hooks to provide the correct variables for optimization.
We have generalized our back-end optimization solver interface11 (we currently support CPLEX and the LGPL- licensed LPSolve), and can take advantage of the load-balancing and parallel MIP/LP solving capability that this library provides.
The bidding language was defined to allow for perfect symmetry between buyers and sellers and provide expressiveness in an exchange domain, for instance for mixed bidders interested in executing trades such as swaps. This proved especially challenging. The breakthrough came when we focused on changes in value for trades rather than providing absolute values for allocations. For simplicity, we require the same tree structure for both the upper and lower valuations. 10 Code size is measured in physical source line of code (SLOC), as generated using David A. Wheeler"s SLOC Count. The total of 6502 includes 184 for instrumentation (not shown in the table). The JOpt solver interface is another 1964 lines, and Castor automatically generates around
11 http://econcs.eecs.harvard.edu/jopt This allows the language itself to ensure consistency (with the upper value at least the lower value on all trades) and enforce monotonic tightening of these bounds for all trades across rounds. It also provides for an efficient method to check the RP activity rule, because it makes it simple to reason about shared uncertainty between trades.
The decision to adopt a direct and proxied approach in which bidders express their upper and lower values to a trusted proxy agent that interacts with the exchange was made early in the design process. In many ways this is the clearest and most immediate way to generalize the design in Parkes et al. [24] and make it iterative. In addition, this removes much opportunity for strategic manipulation: bidders are restricted to making (incremental) statements about their valuations. Another advantage is that it makes the activity rule easy to explain: bidders can always meet the activity rule by tightening bounds such that their true value remains in the support.12 Perhaps most importantly, having explicit information on upper and lower values permits progress in early rounds, even while there is no efficient trade at pessimistic values.
Upper and lower bound information also provides guidance about when to terminate. Note that taken by itself,
PP = PO does not imply that the current provisional trade is efficient with respect to all values consistent with current value information. The difference in values between different trades, aggregated across all bidders, could be similar at lower and upper bounds but quite different at intermediate values (including truth). Nevertheless, we conjecture that PP = PO will prove an excellent indicator of efficiency in practical settings where the shape of the upper and lower valuations does convey useful information. This is worthy of experimental investigation. Moreover, the use of price and RP activity provides additional guarantees.
We adopted linear prices (prices on individual items) rather than non-linear prices (with prices on a trade not equal to the sum of the prices on the component items) early in the design process. The conciseness of this price representation is very important for computational tractability within the exchange and also to promote simplicity and transparency for bidders. The RP activity rule was adopted later, and is a good choice because of its excellent theoretical properties when coupled with CE prices. The following can be easily established: given exact CE prices pt−1 for provisional trade 12 This is in contrast to indirect price-based approaches, such as clock-proxy [1], in which bidders must be able to reason about the RP-constraints implied by bids in each round. 257 λt−1 at valuations vα , then if the upper and lower values at the start of round t already satisfy the RP rule (and without the need for any tie-breaking), the provisional trade is efficient for all valuations consistent with the current bid trees.
When linear CE prices exist, this provides for a soundness and completeness statement: if PP = PO, linear CE prices exist, and the RP rule is satisfied, the provisional trade is efficient (soundness); if prices are exact CE prices for the provisional trade at vα , but the trade is inefficient with respect to some valuation profile consistent with the current bid trees, then at least one bidder must fail RP with her current bid tree and progress will be made (completeness).
Future work must study convergence experimentally, and extend this theory to allow for approximate prices.
Some strategic aspects of our ICE design deserve comment, and further study. First, we do not claim that truthfully responding to the RP rule is an ex post equilibrium.13 However, the exchange is designed to mimic the Threshold rule in its payment scheme, which is known to have useful incentive properties [16]. We must be careful, though.
For instance we do not suggest to provide αeff to bidders, because as αeff approaches 1 it would inform bidders that bid values are becoming irrelevant to determining the trade but merely used to determine payments (and bidders would become increasingly reluctant to increase their lower valuations). Also, no consideration has been given in this work to collusion by bidders. This is an issue that deserves some attention in future work.
In this work we designed and prototyped a scalable and highly-expressive iterative combinatorial exchange. The design includes many interesting features, including: a new bid-tree language for exchanges, a new method to construct approximate linear prices from expressive languages, and a proxied elicitation method with optimistic and pessimistic valuations with a new method to evaluate a revealed- preference activity rule. The exchange is fully implemented in Java and is in a validation phase.
The next steps for our work are to allow bidders to refine the structure of the bid tree in addition to values on the tree. We intend to study the elicitation properties of the exchange and we have put together a test suite of exchange problem instances. In addition, we are beginning to engage in collaborations to apply the design to airline takeoff and landing slot scheduling and to resource allocation in widearea network distributed computational systems.
Acknowledgments We would like to dedicate this paper to all of the participants in CS 286r at Harvard University in Spring 2004. This work is supported in part by NSF grant IIS-0238147.
[1] L. Ausubel, P. Cramton, and P. Milgrom. The clock-proxy auction: A practical combinatorial auction design. In Cramton et al. [9], chapter 5. [2] M. Babaioff, N. Nisan, and E. Pavlov. Mechanisms for a spatially distributed market. In Proc. 5th ACM Conf. on Electronic Commerce, pages 9-20. ACM Press, 2001. 13 Given the Myerson-Satterthwaite impossibility theorem [22] and the method by which we determine the trade we should not expect this. [3] M. Ball, G. Donohue, and K. Hoffman. Auctions for the safe, efficient, and equitable allocation of airspace system resources.
In S. Cramton, Shoham, editor, Combinatorial Auctions.
[4] D. Bertsimas and J. Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, 1997. [5] S. Bikhchandani and J. M. Ostroy. The package assignment model. Journal of Economic Theory, 107(2):377-406, 2002. [6] C. Boutilier. A pomdp formulation of preference elicitation problems. In Proc. 18th National Conference on Artificial Intelligence (AAAI-02), 2002. [7] C. Boutilier and H. Hoos. Bidding languages for combinatorial auctions. In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI-01), 2001. [8] W. Conen and T. Sandholm. Preference elicitation in combinatorial auctions. In Proc. 3rd ACM Conf. on Electronic Commerce (EC-01), pages 256-259. ACM Press,
New York, 2001. [9] P. Cramton, Y. Shoham, and R. Steinberg, editors.
Combinatorial Auctions. MIT Press, 2004. [10] S. de Vries, J. Schummer, and R. V. Vohra. On ascending Vickrey auctions for heterogeneous objects. Technical report,
MEDS, Kellogg School, Northwestern University, 2003. [11] S. de Vries and R. V. Vohra. Combinatorial auctions: A survey. Informs Journal on Computing, 15(3):284-309, 2003. [12] M. Dunford, K. Hoffman, D. Menon, R. Sultana, and T. Wilson. Testing linear pricing algorithms for use in ascending combinatorial auctions. Technical report, SEOR,
George Mason University, 2003. [13] Y. Fu, J. Chase, B. Chun, S. Schwab, and A. Vahdat. Sharp: an architecture for secure resource peering. In Proceedings of the nineteenth ACM symposium on Operating systems principles, pages 133-148. ACM Press, 2003. [14] B. Hudson and T. Sandholm. Effectiveness of query types and policies for preference elicitation in combinatorial auctions. In Proc. 3rd Int. Joint. Conf. on Autonomous Agents and Multi Agent Systems, pages 386-393, 2004. [15] V. Krishna. Auction Theory. Academic Press, 2002. [16] D. Krych. Calculation and analysis of Nash equilibria of Vickrey-based payment rules for combinatorial exchanges,
Harvard College, April 2003. [17] A. M. Kwasnica, J. O. Ledyard, D. Porter, and C. DeMartini.
A new and improved design for multi-object iterative auctions.
Management Science, 2004. To appear. [18] E. Kwerel and J. Williams. A proposal for a rapid transition to market allocation of spectrum. Technical report, FCC Office of Plans and Policy, Nov 2002. [19] S. M. Lahaie and D. C. Parkes. Applying learning algorithms to preference elicitation. In Proc. ACM Conf. on Electronic Commerce, pages 180-188, 2004. [20] R. P. McAfee. A dominant strategy double auction. J. of Economic Theory, 56:434-450, 1992. [21] P. Milgrom. Putting auction theory to work: The simultaneous ascending auction. J.Pol. Econ., 108:245-272, 2000. [22] R. B. Myerson and M. A. Satterthwaite. Efficient mechanisms for bilateral trading. Journal of Economic Theory, 28:265-281, 1983. [23] N. Nisan. Bidding and allocation in combinatorial auctions. In Proc. 2nd ACM Conf. on Electronic Commerce (EC-00), pages 1-12, 2000. [24] D. C. Parkes, J. R. Kalagnanam, and M. Eso. Achieving budget-balance with Vickrey-based payment schemes in exchanges. In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI-01), pages 1161-1168, 2001. [25] D. C. Parkes and L. H. Ungar. Iterative combinatorial auctions: Theory and practice. In Proc. 17th National Conference on Artificial Intelligence (AAAI-00), pages 74-81, July 2000. [26] S. J. Rassenti, V. L. Smith, and R. L. Bulfin. A combinatorial mechanism for airport time slot allocation. Bell Journal of Economics, 13:402-417, 1982. [27] M. H. Rothkopf, A. Pekeˇc, and R. M. Harstad.
Computationally manageable combinatorial auctions.

Social choice theory centers around the general problem of selecting a single outcome out of a set A of alternative outcomes based on the individual preferences of a set P of players. A method for aggregating player preferences to select one outcome is called a social choice function. In this paper we assume that the range A is finite and that each player"s preference is expressed by a valuation function which assigns to each possible outcome a real number representing the benefit the player derives from that outcome. The ensemble of player valuation functions is viewed as a valuation matrix with rows indexed by players and columns by outcomes.
A major difficulty connected with social choice functions is that players can not be required to tell the truth about their preferences. Since each player seeks to maximize his own benefit, he may find it in his interest to misrepresent his valuation function. An important approach for dealing with this problem is to augment a given social choice function with a payment function, which assigns to each player a (positive or negative) payment as a function of all of the individual preferences. By carefully choosing the payment function, one can hope to entice each player to tell the truth.
A social choice function augmented with a payment function is called a mechanism 1 and the mechanism is said to implement the social choice function. A mechanism is truthful (or to be strategyproof or to have a dominant strategy) if each player"s best strategy, knowing the preferences of the others, is always to declare his own true preferences. A social choice function is truthfully implementable, or truthful if it has a truthful implementation. (The property of truthful implementability is sometimes called dominant strategy incentive compatibility). This framework leads naturally to the question: which social choice functions are truthful?
This question is of the following general type: given a class of functions (here, social choice functions) and a property that holds for some of them (here, truthfulness), characterize the property. The definition of the property itself provides a characterization, so what more is needed? Here are some useful notions of characterization: • Recognition algorithm. Give an algorithm which, given an appropriate representation of a function in the class, determines whether the function has the property. • Parametric representation. Give an explicit parametrized family of functions and show that each function in the 1 The usual definition of mechanism is more general than this (see [8] Chapter 23.C or [9]); the mechanisms we consider here are usually called direct revelation mechanisms. 286 family has the property, and that every function with the property is in the family.
A third notion applies in the case of hereditary properties of functions. A function g is a subfunction of function f, or f contains g, if g is obtained by restricting the domain of f. A property P of functions is hereditary if it is preserved under taking subfunctions. Truthfulness is easily seen to be hereditary. • Sets of obstructions. For a hereditary property P, a function g that does not have the property is an obstruction to the property in the sense that any function containing g doesn"t have the property. An obstruction is minimal if every proper subfunction has the property. A set of obstructions is complete if every function that does not have the property contains one of them as a subfunction. The set of all functions that don"t satisfy P is a complete (but trivial and uninteresting) set of obstructions; one seeks a set of small (ideally, minimal) obstructions.
We are not aware of any work on recognition algorithms for the property of truthfulness, but there are significant results concerning parametric representations and obstruction characterizations of truthfulness. It turns out that the domain of the function, i.e., the set of allowed valuation matrices, is crucial. For functions with unrestricted domain, i.e., whose domain is the set of all real matrices, there are very good characterizations of truthfulness. For general domains, however, the picture is far from complete. Typically, the domains of social choice functions are specified by a system of constraints. For example, an order constraint requires that one specified entry in some row be larger than another in the same row, a range constraint places an upper or lower bound on an entry, and a zero constraint forces an entry to be 0. These are all examples of linear inequality constraints on the matrix entries.
Building on work of Roberts [10], Lavi, Mu"alem and Nisan [6] defined a condition called weak monotonicity (WMON). (Independently, in the context of multi-unit auctions, Bikhchandani, Chatterji and Sen [3] identified the same condition and called it nondecreasing in marginal utilities (NDMU).) The definition of W-MON can be formulated in terms of obstructions: for some specified simple set F of functions each having domains of size 2, a function satisfies W-MON if it contains no function from F. The functions in F are not truthful, and therefore W-MON is a necessary condition for truthfulness. Lavi, Mu"alem and Nisan [6] showed that W-MON is also sufficient for truthfulness for social choice functions whose domain is order-based, i.e., defined by order constraints and zero constraints, and Gui,
Muller and Vohra [5] extended this to other domains. The domain constraints considered in both papers are special cases of linear inequality constraints, and it is natural to ask whether W-MON is sufficient for any domain defined by such constraints. Lavi, Mu"alem and Nisan [6] conjectured that W-MON suffices for convex domains. The main result of this paper is an affirmative answer to this conjecture: Theorem 1. For any social choice function having convex domain and finite range, weak monotonicity is necessary and sufficient for truthfulness.
Using the interpretation of weak monotonicity in terms of obstructions each having domain size 2, this provides a complete set of minimal obstructions for truthfulness within the class of social choice functions with convex domains.
The two hypotheses on the social choice function, that the domain is convex and that the range is finite, can not be omitted as is shown by the examples given in section 7.
There is a simple and natural parametrized set of truthful social choice functions called affine maximizers. Roberts [10] showed that for functions with unrestricted domain, every truthful function is an affine maximizer, thus providing a parametrized representation for truthful functions with unrestricted domain. There are many known examples of truthful functions over restricted domains that are not affine maximizers (see [1], [2], [4], [6] and [7]). Each of these examples has a special structure and it seems plausible that there might be some mild restrictions on the class of all social choice functions such that all truthful functions obeying these restrictions are affine maximizers. Lavi, Mu"alem and Nisan [6] obtained a result in this direction by showing that for order-based domains, under certain technical assumptions, every truthful social choice function is almost an affine maximizer.
There are a number of results about truthfulness that can be viewed as providing obstruction characterizations, although the notion of obstruction is not explicitly discussed.
For a player i, a set of valuation matrices is said to be i-local if all of the matrices in the set are identical except for row i. Call a social choice function i-local if its domain is ilocal and call it local if it is i-local for some i. The following easily proved fact is used extensively in the literature: Proposition 2. The social choice function f is truthful if and only if every local subfunction of f is truthful.
This implies that the set of all local non-truthful functions comprises a complete set of obstructions for truthfulness.
This set is much smaller than the set of all non-truthful functions, but is still far from a minimal set of obstructions.
Rochet [11], Rozenshtrom [12] and Gui, Muller and Vohra [5] identified a necessary and sufficient condition for truthfulness (see lemma 3 below) called the nonnegative cycle property. This condition can be viewed as providing a minimal complete set of non-truthful functions. As is required by proposition 2, each function in the set is local.
Furthermore it is one-to-one. In particular its domain has size at most the number of possible outcomes |A|.
As this complete set of obstructions consists of minimal non-truthful functions, this provides the optimal obstruction characterization of non-truthful functions within the class of all social choice functions. But by restricting attention to interesting subclasses of social choice functions, one may hope to get simpler sets of obstructions for truthfulness within that class.
The condition of weak monotonicity mentioned earlier can be defined by a set of obstructions, each of which is a local function of domain size exactly 2. Thus the results of Lavi,
Mu"alem and Nisan [6], and of Gui, Muller and Vohra [5] give a very simple set of obstructions for truthfulness within certain subclasses of social choice functions. Theorem 1 extends these results to a much larger subclass of functions. 287
Cycle Property By proposition 2, a function is truthful if and only if each of its local subfunctions is truthful. Therefore, to get a set of obstructions for truthfulness, it suffices to obtain such a set for local functions.
The domain of an i-local function consists of matrices that are fixed on all rows but row i. Fix such a function f and let D ⊆ RA be the set of allowed choices for row i. Since f depends only on row i and row i is chosen from D, we can view f as a function from D to A. Therefore, f is a social choice function having one player; we refer to such a function as a single player function.
Associated to any single player function f with domain D we define an edge-weighted directed graph Hf whose vertex set is the image of f. For convenience, we assume that f is surjective and so this image is A. For each a, b ∈ A, x ∈ f−1 (a) there is an edge ex(a, b) from a to b with weight x(a) − x(b). The weight of a set of edges is just the sum of the weights of the edges. We say that f satisfies: • the nonnegative cycle property if every directed cycle has nonnegative weight. • the nonnegative two-cycle property if every directed cycle between two vertices has nonnegative weight.
We say a local function g satisfies nonnegative cycle property/nonnegative two-cycle property if its associated single player function f does.
The graph Hf has a possibly infinite number of edges between any two vertices. We define Gf to be the edgeweighted directed graph with exactly one edge from a to b, whose weight δab is the infimum (possibly −∞) of all of the edge weights ex(a, b) for x ∈ f−1 (a). It is easy to see that Hf has the nonnegative cycle property/nonnegative two-cycle property if and only if Gf does. Gf is called the outcome graph of f.
The weak monotonicity property mentioned earlier can be defined for arbitrary social choice functions by the condition that every local subfunction satisfies the nonnegative two-cycle property. The following result was obtained by Rochet [11] in a slightly different form and rediscovered by Rozenshtrom [12] and Gui, Muller and Vohra [5]: Lemma 3. A local social choice function is truthful if and only if it has the nonnegative cycle property. Thus a social choice function is truthful if and only if every local subfunction satisfies the nonnegative cycle property.
In light of this, theorem 1 follows from: Theorem 4. For any surjective single player function f : D −→ A where D is a convex subset of RA and A is finite, the nonnegative two-cycle property implies the nonnegative cycle property.
This is the result we will prove.
Let D ⊆ RA be convex and let f : D −→ A be a single player function such that Gf has no negative two-cycles. We want to conclude that Gf has no negative cycles. For two vertices a, b, let δ∗ ab denote the minimum weight of any path from a to b. Clearly δ∗ ab ≤ δab. Our proof shows that the δ∗ -weight of every cycle is exactly 0, from which theorem 4 follows.
There seems to be no direct way to compute δ∗ and so we proceed indirectly. Based on geometric considerations, we identify a subset of paths in Gf called admissible paths and a subset of admissible paths called straight paths. We prove that for any two outcomes a, b, there is a straight path from a to b (lemma 8 and corollary 10), and all straight paths from a to b have the same weight, which we denote ρab (theorem 12). We show that ρab ≤ δab (lemma 14) and that the ρ-weight of every cycle is 0. The key step to this proof is showing that the ρ-weight of every directed triangle is 0 (lemma 17).
It turns out that ρ is equal to δ∗ (corollary 20), although this equality is not needed in the proof of theorem 4.
To expand on the above summary, we give the definitions of an admissible path and a straight path. These are somewhat technical and rely on the geometry of f. We first observe that, without loss of generality, we can assume that D is (topologically) closed (section 2). In section 3, for each a ∈ A, we enlarge the set f−1 (a) to a closed convex set Da ⊆ D in such a way that for a, b ∈ A with a = b, Da and Db have disjoint interiors. We define an admissible path to be a sequence of outcomes (a1, . . . , ak) such that each of the sets Ij = Daj ∩ Daj+1 is nonempty (section 4). An admissible path is straight if there is a straight line that meets one point from each of the sets I1, . . . , Ik−1 in order (section 5).
Finally, we mention how the hypotheses of convex domain and finite range are used in the proof. Both hypotheses are needed to show: (1) the existence of a straight path from a to b for all a, b (lemma 8). (2) that the ρ-weight of a directed triangle is 0 (lemma 17). The convex domain hypothesis is also needed for the convexity of the sets Da (section 3). The finite range hypothesis is also needed to reduce theorem 4 to the case that D is closed (section 2) and to prove that every straight path from a to b has the same δ-weight (theorem 12).
We first reduce the theorem to the case that D is closed.
Write DC for the closure of D. Since A is finite, DC = ∪a∈A(f−1 (a))C . Thus for each v ∈ DC − D, there is an a = a(v) ∈ A such that v ∈ (f−1 (a))C . Extend f to the function g on DC by defining g(v) = a(v) for v ∈ DC − D and g(v) = f(v) for v ∈ D. It is easy to check that δab(g) = δab(f) for all a, b ∈ A and therefore it suffices to show that the nonnegative two-cycle property for g implies the nonnegative cycle property for g.
Henceforth we assume D is convex and closed.
In this section, we construct a family of closed convex sets {Da : a ∈ A} with disjoint interiors whose union is D and satisfying f−1 (a) ⊆ Da for each a ∈ A.
Let Ra = {v : ∀b ∈ A, v(a) − v(b) ≥ δab}. Ra is a closed polyhedron containing f−1 (a). The next proposition implies that any two of these polyhedra intersect only on their boundary.
Proposition 5. Let a, b ∈ A. If v ∈ Ra ∩Rb then v(a)− v(b) = δab = −δba. 288 Da Db Dc Dd De v w x y z u p Figure 1: A 2-dimensional domain with 5 outcomes.
Proof. v ∈ Ra implies v(a) − v(b) ≥ δab and v ∈ Rb implies v(b)−v(a) ≥ δba which, by the nonnegative two-cycle property, implies v(a) − v(b) ≤ δab. Thus v(a) − v(b) = δab and by symmetry v(b) − v(a) = δba.
Finally, we restrict the collection of sets {Ra : a ∈ A} to the domain D by defining Da = Ra ∩ D for each a ∈ A. Clearly, Da is closed and convex, and contains f−1 (a).
Therefore S a∈A Da = D. Also, by proposition 5, any point v in Da ∩ Db satisfies v(a) − v(b) = δab = −δba.
A path of size k is a sequence −→a = (a1, . . . , ak) with each ai ∈ A (possibly with repetition). We call −→a an (a1, ak)path. For a path −→a , we write |−→a | for the size of −→a . −→a is simple if the ai"s are distinct.
For b, c ∈ A we write Pbc for the set of (b, c)-paths and SPbc for the set of simple (b, c)-paths. The δ-weight of path −→a is defined by δ(−→a ) = k−1X i=1 δaiai+1 .
A D-sequence of order k is a sequence −→u = (u0, . . . , uk) with each ui ∈ D (possibly with repetition). We call −→u a (u0, uk)-sequence. For a D-sequence −→u , we write ord(u) for the order of −→u . For v, w ∈ D we write Svw for the set of (v, w)-sequences.
A compatible pair is a pair (−→a , −→u ) where −→a is a path and −→u is a D-sequence satisfying ord(−→u ) = |−→a | and for each i ∈ [k], both ui−1 and ui belong to Dai .
We write C(−→a ) for the set of D-sequences −→u that are compatible with −→a . We say that −→a is admissible if C(−→a ) is nonempty. For −→u ∈ C(−→a ) we define ∆−→a (−→u ) = |−→a |−1 X i=1 (ui(ai) − ui(ai+1)).
For v, w ∈ D and b, c ∈ A, we define Cvw bc to be the set of compatible pairs (−→a , −→u ) such that −→a ∈ Pbc and −→u ∈ Svw .
To illustrate these definitions, figure 1 gives the dissection of a domain, a 2-dimensional plane, into five regions Da, Db, Dc, Dd, De. D-sequence (v, w, x, y, z) is compatible with both path (a, b, c, e) and path (a, b, d, e); D-sequence (v, w, u, y, z) is compatible with a unique path (a, b, d, e).
D-sequence (x, w, p, y, z) is compatible with a unique path (b, a, d, e). Hence (a, b, c, e), (a, b, d, e) and (b, a, d, e) are admissible paths. However, path (a, c, d) or path (b, e) is not admissible.
Proposition 6. For any compatible pair (−→a , −→u ), ∆−→a (−→u ) = δ(−→a ).
Proof. Let k = ord(−→u ) = |−→a |. By the definition of a compatible pair, ui ∈ Dai ∩ Dai+1 for i ∈ [k − 1]. ui(ai) − ui(ai+1) = δaiai+1 from proposition 5. Therefore, ∆−→a (−→u ) = k−1X i=1 (ui(ai) − ui(ai+1)) = k−1X i=1 δaiai+1 = δ(−→a ).
Lemma 7. Let b, c ∈ A and let −→a , −→a ∈ Pbc. If C(−→a ) ∩ C(−→a ) = ∅ then δ(−→a ) = δ(−→a ).
Proof. Let −→u be a D-sequence in C(−→a ) ∩ C(−→a ). By proposition 6, δ(−→a ) = ∆−→a (−→u ) and δ(−→a ) = ∆−→a (−→u ), it suffices to show ∆−→a (−→u ) = ∆−→a (−→u ).
Let k = ord(−→u ) = |−→a | = |−→a |. Since ∆−→a (−→u ) = k−1X i=1 (ui(ai) − ui(ai+1)) = u1(a1) + k−1X i=2 (ui(ai) − ui−1(ai)) − uk−1(ak) = u1(b) + k−1X i=2 (ui(ai) − ui−1(ai)) − uk−1(c), ∆−→a (−→u ) − ∆−→a (−→u ) = k−1X i=2 ((ui(ai) − ui−1(ai)) − (ui(ai) − ui−1(ai))) = k−1X i=2 ((ui(ai) − ui(ai)) − (ui−1(ai) − ui−1(ai))).
Noticing both ui−1 and ui belong to Dai ∩ Dai , we have by proposition 5 ui−1(ai) − ui−1(ai) = δaiai = ui(ai) − ui(ai).
Hence ∆−→a (−→u ) − ∆−→a (−→u ) = 0.
PATHS For v, w ∈ D we write vw for the (closed) line segment joining v and w.
A D-sequence −→u of order k is linear provided that there is a sequence of real numbers 0 = λ0 ≤ λ1 ≤ . . . ≤ λk = 1 such that ui = (1 − λi)u0 + λiuk. In particular, each ui belongs to u0uk. For v, w ∈ D we write Lvw for the set of linear (v, w)-sequences.
For b, c ∈ A and v, w ∈ D we write LCvw bc for the set of compatible pairs (−→a , −→u ) such that −→a ∈ Pbc and −→u ∈ Lvw .
For a path −→a , we write L(−→a ) for the set of linear sequences compatible with −→a . We say that −→a is straight if L(−→a ) = ∅.
For example, in figure 1, D-sequence (v, w, x, y, z) is linear while (v, w, u, y, z), (x, w, p, y, z), and (x, v, w, y, z) are not. Hence path (a, b, c, e) and (a, b, d, e) are both straight.
However, path (b, a, d, e) is not straight. 289 Lemma 8. Let b, c ∈ A and v ∈ Db, w ∈ Dc. There is a simple path −→a and D-sequence −→u such that (−→a , −→u ) ∈ LCvw bc . Furthermore, for any such path −→a , δ(−→a ) ≤ v(b) − v(c).
Proof. By the convexity of D, any sequence of points on vw is a D-sequence.
If b = c, singleton path −→a = (b) and D-sequence −→u = (v, w) are obviously compatible. δ(−→a ) = 0 = v(b) − v(c).
So assume b = c. If Db ∩Dc ∩vw = ∅, we pick an arbitrary x from this set and let −→a = (b, c) ∈ SPbc, −→u = (v, x, w) ∈ Lvw . Again it is easy to check the compatibility of (−→a , −→u ).
Since v ∈ Db, v(b) − v(c) ≥ δbc = δ(−→a ).
For the remaining case b = c and Db ∩Dc ∩vw = ∅, notice v = w otherwise v = w ∈ Db ∩ Dc ∩ vw. So we can define λx for every point x on vw to be the unique number in [0, 1] such that x = (1 − λx)v + λxw. For convenience, we write x ≤ y for λx ≤ λy.
Let Ia = Da ∩ vw for each a ∈ A. Since D = ∪a∈ADa, we have vw = ∪a∈AIa. Moreover, by the convexity of Da and vw, Ia is a (possibly trivial) closed interval.
We begin by considering the case that Ib and Ic are each a single point, that is, Ib = {v} and Ic = {w}.
Let S be a minimal subset of A satisfying ∪s∈SIs = vw.
For each s ∈ S, Is is maximal, i.e., not contained in any other It, for t ∈ S. In particular, the intervals {Is : s ∈ S} have all left endpoints distinct and all right endpoints distinct and the order of the left endpoints is the same as that of the right endpoints. Let k = |S| + 2 and index S as a2, . . . , ak−1 in the order defined by the right endpoints.
Denote the interval Iai by [li, ri]. Thus l2 < l3 < . . . < lk−1, r2 < r3 < . . . < rk−1 and the fact that these intervals cover vw implies l2 = v, rk−1 = w and for all 2 ≤ i ≤ k − 2, li+1 ≤ ri which further implies li < ri. Now we define the path −→a = (a1, a2, . . . , ak−1, ak) with a1 = b, ak = c and a2, a3, . . . , ak−1 as above. Define the linear D-sequence −→u = (u0, u1, . . . , uk) by u0 = u1 = v, uk = w and for
LCvw bc . Neither b nor c is in S since lb = rb and lc = rc. Thus −→a is simple.
Finally to show δ(−→a ) ≤ v(b) − v(c), we note v(b) − v(c) = v(a1) − v(ak) = k−1X i=1 (v(ai) − v(ai+1)) and δ(−→a ) = ∆−→a (−→u ) = k−1X i=1 (ui(ai) − ui(ai+1)) = v(a1) − v(a2) + k−1X i=2 (ri(ai) − ri(ai+1)).
For two outcomes d, e ∈ A, let us define fde(z) = z(d)−z(e) for all z ∈ D. It suffices to show faiai+1 (ri) ≤ faiai+1 (v) for
Fact 9. For d, e ∈ A, fde(z) is a linear function of z.
Furthermore, if x ∈ Dd and y ∈ De with x = y, then fde(x) = x(d) − x(e) ≥ δde ≥ −δed ≥ −(y(e) − y(d)) = fde(y). Therefore fde(z) is monotonically nonincreasing along the line ←→ xy as z moves in the direction from x to y.
Applying this fact with d = ai, e = ai+1, x = li and y = ri gives the desired conclusion. This completes the proof for the case that Ib = {v} and Ic = {w}.
For general Ib, Ic, rb < lc otherwise Db ∩ Dc ∩ vw = Ib ∩ Ic = ∅. Let v = rb and w = lc. Clearly we can apply the above conclusion to v ∈ Db, w ∈ Dc and get a compatible pair (−→a , −→u ) ∈ LCv w bc with −→a simple and δ(−→a ) ≤ v (b) − v (c). Define the linear D-sequence −→u by u0 = v, uk = w and ui = ui for i ∈ [k − 1]. (−→a , −→u ) ∈ LCvw bc is evident.
Moreover, applying the above fact with d = b, e = c, x = v and y = w, we get v(b) − v(c) ≥ v (b) − v (c) ≥ δ(−→a ).
Corollary 10. For any b, c ∈ A there is a straight (b, c)path.
The main result of this section (theorem 12) says that for any b, c ∈ A, every straight (b, c)-path has the same δ-weight.
To prove this, we first fix v ∈ Db and w ∈ Dc and show (lemma 11) that every straight (b, c)-path compatible with some linear (v, w)-sequence has the same δ-weight ρbc(v, w).
We then show in theorem 12 that ρbc(v, w) is the same for all choices of v ∈ Db and w ∈ Dc.
Lemma 11. For b, c ∈ A, there is a function ρbc : Db × Dc −→ R satisfying that for any (−→a , −→u ) ∈ LCvw bc , δ(−→a ) = ρbc(v, w).
Proof. Let (−→a , −→u ), (−→a , −→u ) ∈ LCvw bc . It suffices to show δ(−→a ) = δ(−→a ). To do this we construct a linear (v, w)-sequence −→u and paths −→a ∗ , −→a ∗∗ ∈ Pbc, both compatible with −→u , satisfying δ(−→a ∗ ) = δ(−→a ) and δ(−→a ∗∗ ) = δ(−→a ).
Lemma 7 implies δ(−→a ∗ ) = δ(−→a ∗∗ ), which will complete the proof.
Let |−→a | = ord(−→u ) = k and |−→a | = ord(−→u ) = l. We select −→u to be any linear (v, w)-sequence (u0, u1, . . . , ut) such that −→u and −→u are both subsequences of −→u , i.e., there are indices 0 = i0 < i1 < · · · < ik = t and 0 = j0 < j1 < · · · < jl = t such that −→u = (ui0 , ui1 , . . . , uik ) and −→u = (uj0 , uj1 , . . . , ujl ). We now construct a (b, c)-path −→a ∗ compatible with −→u such that δ(−→a ∗ ) = δ(−→a ). (An analogous construction gives −→a ∗∗ compatible with −→u such that δ(−→a ∗∗ ) = δ(−→a ).) This will complete the proof. −→a ∗ is defined as follows: for 1 ≤ j ≤ t, a∗ j = ar where r is the unique index satisfying ir−1 < j ≤ ir. Since both uir−1 = ur−1 and uir = ur belong to Dar , uj ∈ Dar for ir−1 ≤ j ≤ ir by the convexity of Dar . The compatibility of (−→a ∗ , −→u ) follows immediately. Clearly, a∗
t = ak = c, so −→a ∗ ∈ Pbc. Furthermore, as δa∗ j a∗ j+1 = δarar = 0 for each r ∈ [k], ir−1 < j < ir, δ(−→a ∗ ) = k−1X r=1 δa∗ ir a∗ ir+1 = k−1X r=1 δarar+1 = δ(−→a ).
We are now ready for the main theorem of the section: Theorem 12. ρbc is a constant map on Db × Dc. Thus for any b, c ∈ A, every straight (b, c)-path has the same δweight.
Proof. For a path −→a , (v, w) is compatible with −→a if there is a linear (v, w)-sequence compatible with −→a . We write CP(−→a ) for the set of pairs (v, w) compatible with −→a . ρbc is constant on CP(−→a ) because for each (v, w) ∈ CP(−→a ), ρbc(v, w) = δ(−→a ). By lemma 8, we also haveS −→a ∈SPbc CP(−→a ) = Db ×Dc. Since A is finite, SPbc, the set of simple paths from b to c, is finite as well. 290 Next we prove that for any path −→a , CP(−→a ) is closed.
Let ((vn , wn ) : n ∈ N) be a convergent sequence in CP(−→a ) and let (v, w) be the limit. We want to show that (v, w) ∈ CP(−→a ). For each n ∈ N, since (vn , wn ) ∈ CP(−→a ), there is a linear (vn , wn )-sequence un compatible with −→a , i.e., there are 0 = λn
k = 1 (k = |−→a |) such that un j = (1 − λn j )vn + λn j wn (j = 0, 1, . . . , k). Since for each n, λn = (λn
k ) belongs to the closed bounded set [0, 1]k+1 we can choose an infinite subset I ⊆ N such that the sequence (λn : n ∈ I) converges. Let λ = (λ0, λ1, . . . , λk) be the limit. Clearly 0 = λ0 ≤ λ1 ≤ · · · ≤ λk = 1.
Define the linear (v, w)-sequence −→u by uj = (1 − λj )v + λj w (j = 0, 1, . . . , k). Then for each j ∈ {0, . . . , k}, uj is the limit of the sequence (un j : n ∈ I). For j > 0, each un j belongs to the closed set Daj , so uj ∈ Daj . Similarly, for j < k each un j belongs to the closed set Daj+1 , so uj ∈ Daj+1 .
Hence (−→a , −→u ) is compatible, implying (v, w) ∈ CP(−→a ).
Now we have Db × Dc covered by finitely many closed subsets on each of them ρbc is a constant.
Suppose for contradiction that there are (v, w), (v , w ) ∈ Db × Dc such that ρbc(v, w) = ρbc(v , w ).
L = {((1 − λ)v + λv , (1 − λ)w + λw ) : λ ∈ [0, 1]} is a line segment in Db ×Dc by the convexity of Db, Dc. Let L1 = {(x, y) ∈ L : ρbc(x, y) = ρbc(v, w)} and L2 = L − L1. Clearly (v, w) ∈ L1, (v , w ) ∈ L2. Let P = {−→a ∈ SPbc : δ(−→a ) = ρbc(v, w)}.
L1 = `S −→a ∈P CP(−→a ) ´ ∩ L, L2 = S −→a ∈SPbc−P CP(−→a )  ∩ L are closed by the finiteness of P. This is a contradiction, since it is well known (and easy to prove) that a line segment can not be expressed as the disjoint union of two nonempty closed sets.
Summarizing corollary 10, lemma 11 and theorem 12, we have Corollary 13. For any b, c ∈ A, there is a real number ρbc with the property that (1) There is at least one straight (b, c)-path of δ-weight ρbc and (2) Every straight (b, c)-path has δ-weight ρbc.
Lemma 14. ρbc ≤ δbc for all b, c ∈ A.
Proof. For contradiction, suppose ρbc − δbc = > 0.
By the definition of δbc, there exists v ∈ f−1 (b) ⊆ Db with v(b) − v(c) < δbc + = ρbc. Pick an arbitrary w ∈ Dc.
By lemma 8, there is a compatible pair (−→a , −→u ) ∈ LCvw bc with δ(−→a ) ≤ v(b) − v(c). Since −→a is a straight (b, c)-path, ρbc = δ(−→a ) ≤ v(b) − v(c), leading to a contradiction.
Define another edge-weighted complete directed graph Gf on vertex set A where the weight of arc (a, b) is ρab.
Immediately from lemma 14, the weight of every directed cycle in Gf is bounded below by its weight in Gf . To prove theorem 4, it suffices to show the zero cycle property of Gf , i.e., every directed cycle has weight zero. We begin by considering two-cycles.
Lemma 15. ρbc + ρcb = 0 for all b, c ∈ A.
Proof. Let −→a be a straight (b, c)-path compatible with linear sequence −→u . let −→a be the reverse of −→a and −→u the reverse of −→u . Obviously, (−→a , −→u ) is compatible as well and thus −→a is a straight (c, b)-path. Therefore, ρbc + ρcb = δ(−→a ) + δ(−→a ) = k−1X i=1 δaiai+1 + k−1X i=1 δai+1ai = k−1X i=1 (δaiai+1 + δai+1ai ) = 0, where the final equality uses proposition 5.
Next, for three cycles, we first consider those compatible with linear triples.
Lemma 16. If there are collinear points u ∈ Da, v ∈ Db, w ∈ Dc (a, b, c ∈ A), ρab + ρbc + ρca = 0.
Proof. First, we prove for the case where v is between u and w. From lemma 8, there are compatible pairs (−→a , −→u ) ∈ LCuv ab , (−→a , −→u ) ∈ LCvw bc . Let |−→a | = ord(−→u ) = k and |−→a | = ord(−→u ) = l. We paste −→a and −→a together as −→a = (a = a1, a2, . . . , ak−1, ak, a1 , . . . , al = c), −→u and −→u as −→u = (u = u0, u1, . . . , uk = v = u0 , u1 , . . . , ul = w).
Clearly (−→a , −→u ) ∈ LCuw ac and δ(−→a ) = k−1X i=1 δaiai+1 + δak a1 + l−1X i=1 δai ai+1 = δ(−→a ) + δbb + δ(−→a ) = δ(−→a ) + δ(−→a ).
Therefore, ρac = δ(−→a ) = δ(−→a ) + δ(−→a ) = ρab + ρbc.
Moreover, ρac = −ρca by lemma 15, so we get ρab + ρbc + ρca = 0.
Now suppose w is between u and v. By the above argument, we have ρac + ρcb + ρba = 0 and by lemma 15, ρab + ρbc + ρca = −ρba − ρcb − ρac = 0.
The case that u is between v and w is similar.
Now we are ready for the zero three-cycle property: Lemma 17. ρab + ρbc + ρca = 0 for all a, b, c ∈ A.
Proof. Let S = {(a, b, c) : ρab + ρbc + ρca = 0} and for contradiction, suppose S = ∅. S is finite. For each a ∈ A, choose va ∈ Da arbitrarily and let T be the convex hull of {va : a ∈ A}. For each (a, b, c) ∈ S, let Rabc = Da × Db × Dc ∩ T3 . Clearly, each Rabc is nonempty and compact. Moreover, by lemma 16, no (u, v, w) ∈ Rabc is collinear.
Define f : D3 → R by f(u, v, w) = |v−u|+|w−v|+|u−w|.
For (a, b, c) ∈ S, the restriction of f to the compact set Rabc attains a minimum m(a, b, c) at some point (u, v, w) ∈ Rabc by the continuity of f, i.e., there exists a triangle ∆uvw of minimum perimeter within T with u ∈ Da, v ∈ Db, w ∈ Dc.
Choose (a∗ , b∗ , c∗ ) ∈ S so that m(a∗ , b∗ , c∗ ) is minimum and let (u∗ , v∗ , w∗ ) ∈ Ra∗b∗c∗ be a triple achieving it. Pick an arbitrary point p in the interior of ∆u∗ v∗ w∗ . By the convexity of domain D, there is d ∈ A such that p ∈ Dd. 291 Consider triangles ∆u∗ pw∗ , ∆w∗ pv∗ and ∆v∗ pu∗ . Since each of them has perimeter less than that of ∆u∗ v∗ w∗ and all three triangles are contained in T, by the minimality of ∆u∗ v∗ w∗ , (a∗ , d, c∗ ), (c∗ , d, b∗ ), (b∗ , d, a∗ ) ∈ S. Thus ρa∗d + ρdc∗ + ρc∗a∗ = 0, ρc∗d + ρdb∗ + ρb∗c∗ = 0, ρb∗d + ρda∗ + ρa∗b∗ = 0.
Summing up the three equalities, (ρa∗d + ρdc∗ + ρc∗d + ρdb∗ + ρb∗d + ρda∗ ) +(ρc∗a∗ + ρb∗c∗ + ρa∗b∗ ) = 0, which yields a contradiction ρa∗b∗ + ρb∗c∗ + ρc∗a∗ = 0.
With the zero two-cycle and three-cycle properties, the zero cycle property of Gf is immediate. As noted earlier, this completes the proof of theorem 4.
Theorem 18. Every directed cycle of Gf has weight zero.
Proof. Clearly, zero two-cycle and three-cycle properties imply triangle equality ρab +ρbc = ρac for all a, b, c ∈ A. For a directed cycle C = a1a2 . . . aka1, by inductively applying triangle equality, we have Pk−1 i=1 ρaiai+1 = ρa1ak . Therefore, the weight of C is k−1X i=1 ρaiai+1 + ρaka1 = ρa1ak + ρaka1 = 0.
As final remarks, we note that our result implies the following strengthenings of theorem 12: Corollary 19. For any b, c ∈ A, every admissible (b, c)path has the same δ-weight ρbc.
Proof. First notice that for any b, c ∈ A, if Db ∩Dc = ∅, δbc = ρbc. To see this, pick v ∈ Db ∩ Dc arbitrarily.
Obviously, path −→a = (b, c) is compatible with linear sequence −→u = (v, v, v) and is thus a straight (b, c)-path. Hence ρbc = δ(−→a ) = δbc.
Now for any b, c ∈ A and any (b, c)-path −→a with C(−→a ) = ∅, let −→u ∈ C(−→a ). Since ui ∈ Dai ∩ Dai+1 for i ∈ [|−→a | − 1], δ(−→a ) = |−→a |−1 X i=1 δaiai+1 = |−→a |−1 X i=1 ρaiai+1 , which by theorem 18, = −ρa|−→a |a1 = ρa1a|−→a | = ρbc.
Corollary 20. For any b, c ∈ A, ρbc is equal to δ∗ bc, the minimum δ-weight over all (b, c)-paths.
Proof. Clearly ρbc ≥ δ∗ bc by corollary 13. On the other hand, for every (b, c)-path −→a = (b = a1, a2, . . . , ak = c), by lemma 14, δ(−→a ) = k−1X i=1 δaiai+1 ≥ k−1X i=1 ρaiai+1 , which by theorem 18, = −ρaka1 = ρa1ak = ρbc. Hence ρbc ≤ δ∗ bc, which completes the proof.
FORMS OF THEOREM 4 Theorem 4 applies to social choice functions with convex domain and finite range. We now show that neither of these hypotheses can be omitted. Our examples are single player functions.
The first example illustrates that convexity can not be omitted. We present an untruthful single player social choice function with three outcomes a, b, c satisfying W-MON on a path-connected but non-convex domain. The domain is the boundary of a triangle whose vertices are x = (0, 1, −1), y = (−1, 0, 1) and z = (1, −1, 0). x and the open line segment zx is assigned outcome a, y and the open line segment xy is assigned outcome b, and z and the open line segment yz is assigned outcome c. Clearly, δab = −δba = δbc = −δcb = δca = −δac = −1, W-MON (the nonnegative twocycle property) holds. Since there is a negative cycle δab + δbc + δca = −3, by lemma 3, this is not a truthful choice function.
We now show that the hypothesis of finite range can not be omitted. We construct a family of single player social choice functions each having a convex domain and an infinite number of outcomes, and satisfying weak monotonicity but not truthfulness.
Our examples will be specified by a positive integer n and an n × n matrix M satisfying the following properties: (1) M is non-singular. (2) M is positive semidefinite. (3) There are distinct i1, i2, . . . , ik ∈ [n] satisfying k−1X j=1 (M(ij, ij) − M(ij , ij+1)) + (M(ik, ik) − M(ik, i1)) < 0.
Here is an example matrix with n = 3 and (i1, i2, i3) = (1, 2, 3): 0 @
−1 0 1
1 A Let e1, e2, . . . , en denote the standard basis of Rn . Let Sn denote the convex hull of {e1, e2 . . . , en}, which is the set of vectors in Rn with nonnegative coordinates that sum to 1. The range of our social choice function will be the set Sn and the domain D will be indexed by Sn, that is D = {yλ : λ ∈ Sn}, where yλ is defined below. The function f maps yλ to λ.
Next we specify yλ. By definition, D must be a set of functions from Sn to R. For λ ∈ Sn, the domain element yλ : Sn −→ R is defined by yλ(α) = λT Mα. The nonsingularity of M guarantees that yλ = yµ for λ = µ ∈ Sn.
It is easy to see that D is a convex subset of the set of all functions from Sn to R.
The outcome graph Gf is an infinite graph whose vertex set is the outcome set A = Sn. For outcomes λ, µ ∈ A, the edge weight δλµ is equal to δλµ = inf{v(λ) − v(µ) : f(v) = λ} = yλ(λ) − yλ(µ) = λT Mλ − λT Mµ = λT M(λ − µ).
We claim that Gf satisfies the nonnegative two-cycle property (W-MON) but has a negative cycle (and hence is not truthful).
For outcomes λ, µ ∈ A, δλµ +δµλ = λT M(λ−µ)+µT M(µ−λ) = (λ−µ)T M(λ−µ), 292 which is nonnegative since M is positive semidefinite. Hence the nonnegative two-cycle property holds. Next we show that Gf has a negative cycle. Let i1, i2, . . . , ik be a sequence of indices satisfying property 3 of M. We claim ei1 ei2 . . . eik ei1 is a negative cycle. Since δeiej = eT i M(ei − ej) = M(i, i) − M(i, j) for any i, j ∈ [k], the weight of the cycle k−1X j=1 δeij eij+1 + δeik ei1 = k−1X j=1 (M(ij , ij ) − M(ij, ij+1)) + (M(ik, ik) − M(ik, i1)) < 0, which completes the proof.
Finally, we point out that the third property imposed on the matrix M has the following interpretation. Let R(M) = {r1, r2, . . . , rn} be the set of row vectors of M and let hM be the single player social choice function with domain R(M) and range {1, 2, . . . , n} mapping ri to i. Property 3 is equivalent to the condition that the outcome graph GhM has a negative cycle. By lemma 3, this is equivalent to the condition that hM is untruthful.
As stated in the introduction, the goal underlying the work in this paper is to obtain useful and general characterizations of truthfulness.
Let us say that a set D of P × A real valuation matrices is a WM-domain if any social choice function on D satisfying weak monotonicity is truthful. In this paper, we showed that for finite A, any convex D is a WM-domain. Typically, the domains of social choice functions considered in mechanism design are convex, but there are interesting examples with non-convex domains, e.g., combinatorial auctions with unknown single-minded bidders. It is intriguing to find the most general conditions under which a set D of real matrices is a WM-domain. We believe that convexity is the main part of the story, i.e., a WM-domain is, after excluding some exceptional cases, essentially a convex set.
Turning to parametric representations, let us say a set D of P × A matrices is an AM-domain if any truthful social choice function with domain D is an affine maximizer.
Roberts" theorem says that the unrestricted domain is an AM-domain. What are the most general conditions under which a set D of real matrices is an AM-domain?
Acknowledgments We thank Ron Lavi for helpful discussions and the two anonymous referees for helpful comments.
[1] A. Archer and E. Tardos. Truthful mechanisms for one-parameter agents. In IEEE Symposium on Foundations of Computer Science, pages 482-491, 2001. [2] Y. Bartal, R. Gonen, and N. Nisan. Incentive compatible multi unit combinatorial auctions. In TARK "03: Proceedings of the 9th conference on Theoretical aspects of rationality and knowledge, pages 72-87. ACM Press, 2003. [3] S. Bikhchandani, S. Chatterjee, and A. Sen. Incentive compatibility in multi-unit auctions. Technical report,
UCLA Department of Economics, Dec. 2004. [4] A. Goldberg, J. Hartline, A. Karlin, M. Saks and A. Wright. Competitive Auctions, 2004. [5] H. Gui, R. Muller, and R. Vohra. Dominant strategy mechanisms with multidimensional types. Technical Report 047, Maastricht: METEOR, Maastricht Research School of Economics of Technology and Organization, 2004. available at http://ideas.repec.org/p/dgr/umamet/2004047.html. [6] R. Lavi, A. Mu"alem, and N. Nisan. Towards a characterization of truthful combinatorial auctions. In FOCS "03: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science, page
[7] D. Lehmann, L. O"Callaghan, and Y. Shoham. Truth revelation in approximately efficient combinatorial auctions. J. ACM, 49(5):577-602, 2002. [8] A. Mas-Colell, M. Whinston, and J. Green.
Microeconomic Theory. Oxford University Press, 1995. [9] N. Nisan. Algorithms for selfish agents. Lecture Notes in Computer Science, 1563:1-15, 1999. [10] K. Roberts. The characterization of implementable choice rules. Aggregation and Revelation of Preferences,
J-J. Laffont (ed.), North Holland Publishing Company. [11] J.-C. Rochet. A necessary and sufficient condition for rationalizability in a quasi-linear context. Journal of Mathematical Economics, 16:191-200, 1987. [12] I. Rozenshtrom. Dominant strategy implementation with quasi-linear preferences. Master"s thesis, Dept. of Economics, The Hebrew University, Jerusalem, Israel,

In this paper we introduce the concept of negotiation based mechanisms in the context of the theory of efficient truthful markets. A market consists of multiple buyers and sellers who wish to exchange goods. The market"s main objective is to produce an allocation of sellers" goods to buyers as to maximize the total gain from trade.
A commonly studied model of participant behavior is taken from the field of economic mechanism design [3, 4, 11]. In this model each player has a private valuation function that assigns real values to each possible allocation. The algorithm motivates players to participate truthfully by handing payments to them.
The mechanism in an exchange collects buyer bids and seller bids and clears the exchange by computing:(i) a set of trades, and (ii) the payments made and received by players.
In designing a mechanism to compute trades and payments we must consider the bidding strategies of self-interested players, i.e. rational players that follow expected-utility maximizing strategies. We set allocative efficiency as our primary goal. That is the mechanism must compute a set of trades that maximizes gain from trade. In addition we require individual rationality (IR) so that all players have positive expected utility to participate, budget balance (BB) so that the exchange does not run at a loss, and incentive compatibility (IC) so that reporting the truth is a dominant strategy for each player.
Unfortunately, Myerson and Satterthwaite"s (1983) well known result demonstrates that in bilateral trade it is impossible to simultaneously achieve perfect efficiency, BB, and IR using an IC mechanism [10]. A unique approach to overcome Myerson and Satterthwaite"s impossibility result was attempted by Parkes, Kalagnanam and Eso [12]. This result designs both a regular and a combinatorial bilateral trade mechanism (which imposes BB and IR) that approximates truth revelation and allocation efficiency.
In this paper we circumvent Myerson and Satterthwaite"s impossibility result by introducing a new model of negotiationrange markets. A negotiation-range mechanism does not produce payment prices for the market participants. Rather, is assigns each buyer-seller pair a price range, called Zone Of Possible Agreements (ZOPA). The buyer is provided with the high end of the range and the seller with the low end of the range. This allows the trading parties to engage in negotiation over the final price with guarantee that the deal is beneficial for both of them. The negotiation process is not considered part of the mechanism but left up to the interested parties, or to some external mechanism to perform. In effect a negotiation-range mechanism operates as a mediator between the market participants, offering them the grounds to be able to finalize the terms of the trade by themselves.
This concept is natural to many real-world market environments such as the real estate market. 1 We focus on the single-unit heterogeneous setting: n sellers offer one unique good each by placing sealed bids specifying their willingness to sell, and m buyers, interested in buying a single good each, placing sealed bids specifying their willingness to pay for each good they may be interested in.
Our main result is a single-unit heterogeneous bilateral trade negotiation-range mechanism (ZOPAS) that is efficient, individually rational, incentive compatible and budget balanced.
Our result does not contradict Myerson and Satterthwaite"s important theorem. Myerson-Satterthwaite"s proof relies on a theorem assuring that in two different efficient IC markets; if the sellers have the same upper bound utility then they will receive the same prices in each market and if the buyers have the same lower bound utility then they will receive the same prices in each market. Our single-unit heterogeneous mechanism bypasses Myerson and Satterthwaite"s theorem by producing a price range, defined by a seller"s floor and a buyer"s ceiling, for each pair of matched players. In our market mechanism the seller"s upper bound utility may be the same while the seller"s floor is different and the buyer"s lower bound utility may be the same while the buyer"s ceiling is different. Moreover, the final price is not fixed by the mechanism at all. Instead, it is determined by an independent negotiation between the buyer and seller.
More specifically, in a negotiation-range mechanism, the range of prices each matched pair is given is resolved by a negotiation stage where a final price is determined. This negotiation stage is crucial for our mechanism to be IC.
Intuitively, a negotiation range mechanism is incentive compatible if truth telling promises the best ZOPA from the point of view of the player in question. That is, he would tell the truth if this strategy maximizes the upper and lower bounds on his utility as expressed by the ZOPA boundaries.
Yet, when carefully examined it turns out that it is impossible (by [10]) that this goal will always hold. This is simply because such a mechanism could be easily modified to determine final prices for the players (e.g. by taking the average of the range"s boundaries). Here, the negotiation stage come into play. We show that if the above utility maximizing condition does not hold then it is the case that the player cannot influence the negotiation bound that is assigned to his deal partner no matter what value he declares. This means that the only thing that he may achieve by reporting a false valuation is modifying his own negotiation bound, something that he could alternatively achieve by reporting his true valuation and incorporating the effect of the modified negotiation bound into his negotiation strategy. This eliminates the benefit of reporting false valuations and allows our mechanism to compute the optimal gain from trade according to the players" true values.
The problem of computing the optimal allocation which maximizes gain from trade can be conceptualized as the problem of finding the maximum weighted matching in a weighted bipartite graph connecting buyers and sellers, where each edge in the graph is assigned a weight equal to the difference between the respective buyer bid and seller bid. It is well known that this problem can be solved efficiently in polynomial time.
VCG IC payment schemes [2, 7, 13] support efficient and IR bilateral trade but not simultaneously BB. Our particular approach adapts the VCG payment scheme to achieve budget balance. The philosophy of the VCG payment schemes in bilateral trade is that the buyer pays the seller"s opportunity cost of not selling the good to another buyer and not keeping the good to herself. The seller is paid in addition to the buyer"s price a compensation for the damage the mechanism did to the seller by not extracting the buyer"s full bid. Our philosophy is a bit different: The seller is paid at least her opportunity cost of not selling the good to another buyer and not keeping the good for herself. The buyer pays at most his alternate seller"s opportunity cost of not selling the good to another buyer and not keeping the alternate good for herself.
The rest of this paper is organized as follows. In Section 2 we describe our model and definitions. In section 3 we present the single-unit heterogeneous negotiation-range mechanism and show that it is efficient, IR, IC and BB.
Finally, we conclude with a discussion in Section 4.
PRELIMINARIES Let Π denote the set of players, N the set of n selling players, and M the set of m buying players, where Π = N ∪ M. Let Ψ = {1, ..., t} denote the set of goods.
Let Ti ∈ {−1, 0, 1}t denote an exchange vector for a trade, such that player i buys goods {A ∈ Ψ|Ti (A) = 1} and sells goods {A ∈ Ψ|Ti (A) = −1}. Let T = (T1, ..., T|Π|) denote the complete trade between all players. We view T as describing the allocation of goods by the mechanism to the buyers and sellers.
In the single-unit heterogeneous setting every good belongs to specific seller, and every buyer is interested in buying one good. The buyer may bid for several or all goods.
At the end of the auction every good is either assigned to one of the buyers who bid for it or kept unsold by the seller.
It is convenient to assume the sets of buyers and sellers are disjoint (though it is not required), i.e. N ∩ M = ∅. Each seller i is associated with exactly one good Ai, for which she has true valuation ci which expresses the price at which it is beneficial for her to sell the good. If the seller reports a false valuation at an attempt to improve the auction results for her, this valuation is denoted ˆci. A buyer has a valuation vector describing his valuation for each of the goods according to their owner. Specifically, vj(k) denotes buyer j"s valuation for good Ak. Similarly, if he reports a false valuation it is denoted ˆvj(k).
If buyer j is matched by the mechanism with seller i then Ti(Ai) = −1 and Tj(Ai) = 1. Notice, that in our setting for every k = i, Ti(Ak) = 0 and Tj(Ai) = 0 and also for every z = j, Tz(Ai) = 0.
For a matched buyer j - seller i pair, the gain from trade on the deal is defined as vj(i) − ci. Given and allocation T, the gain from trade associated with T is V = j∈M,i∈N (vj(i) − ci) · Tj(Ai).
Let T∗ denote the optimal allocation which maximizes the gain from trade, computed according to the players" true valuations. Let V ∗ denote the optimal gain from trade associated with this allocation.
When players" report false valuations we use ˆT∗ and ˆV ∗ to denote the optimal allocation and gain from trade, respectively, when computed according to the reported valuations. 2 We are interested in the design of negotiation-range mechanisms. In contrast to a standard auction mechanism where the buyer and seller are provided with the prices they should pay, the goal of a negotiation-range mechanism is to provide the player"s with a range of prices within which they can negotiate the final terms of the deal by themselves. The mechanism would provide the buyer with the upper bound on the range and the seller with the lower bound on the range. This gives each of them a promise that it will be beneficial for them to close the deal, but does not provide information about the other player"s terms of negotiation.
Definition 1. Negotiation Range: Zone Of Possible Agreements, ZOPA, between a matched buyer and seller. The ZOPA is a range, (L, H), 0 ≤ L ≤ H, where H is an upper bound (ceiling) price for the buyer and L is a lower bound (floor) price for the seller.
Definition 2. Negotiation-Range Mechanism: A mechanism that computes a ZOPA, (L, H), for each matched buyer and seller in T∗ , and provides the buyer with the upper bound H and the seller with the lower bound L.
The basic assumption is that participants in the auction are self-interested players. That is their main goal is to maximize their expected utility. The utility for a buyer who does not participate in the trade is 0. If he does win some good, his utility is the surplus between his valuation for that good and the price he pays. For a seller, if she keeps the good unsold, her utility is just her valuation of the good, and the surplus is 0. If she gets to sell it, her utility is the price she is paid for it, and the surplus is the difference between this price and her valuation.
Since negotiation-range mechanisms assign bounds on the range of prices rather than the final price, it is useful to define the upper and lower bounds on the player"s utilities defined by the range"s limits.
Definition 3. Consider a buyer j - seller i pair matched by a negotiation-range mechanism and let (L, H) be their associated negotiation range. • The buyer"s top utility is: vj(i) − L, and the buyer"s bottom utility is vj(i) − H. • The seller"s top utility is H, with surplus H − ci, and the seller"s bottom utility is L, with surplus L − ci.
MECHANISM (ZOPAS)
ZOPAS is a negotiation-range mechanism, it finds the optimal allocation T∗ and uses it to define a ZOPA for each buyer-seller pair.
The first stage in applying the mechanism is for the buyers and sellers to submit their sealed bids. The mechanism then allocates buyers to sellers by computing the allocation T ∗ , which results in the optimal gain from trade V ∗ , and defines a ZOPA for each buyer-seller pair. Finally, buyers and sellers use the ZOPA to negotiate a final price.
Computing T∗ involves solving the maximum weighted bipartite matching problem for the complete bipartite graph Kn,m constructed by placing the buyers on one side of the Find the optimal allocation T ∗ Compute the maximum weighted bipartite matching for the bipartite graph of buyers and sellers, and edge weights equal to the gain from trade.
Calculate Sellers" Floors For every buyer j, allocated good Ai Find the optimal allocation (T−j)∗ Li = vj(i) + (V−j)∗ − V ∗ Calculate Buyers" Ceilings For every buyer j, allocated good Ai Find the optimal allocation (T −i )∗ Find the optimal allocation (T −i −j )∗ Hj = vj(i) + (V −i −j )∗ − (V −i )∗ Negotiation Phase For every buyer j and every seller i of good Ai Report to seller i her floor Li and identify her matched buyer j Report to buyer j his ceiling Hj and identify his matched seller i i, j negotiate the good"s final price Figure 1: The ZOPAS mechanism graph, the seller on another and giving the edge between buyer j and seller i weight equal to vj(i) − ci. The maximum weighted matching problem in solvable in polynomial time (e.g., using the Hungarian Method). This results in a matching between buyers and sellers that maximizes gain from trade.
The next step is to compute for each buyer-seller pair a seller"s floor, which provides the lower bound of the ZOPA for this pair, and assigns it to the seller.
A seller"s floor is computed by calculating the difference between the total gain from trade when the buyer is excluded and the total gain from trade of the other participants when the buyer is included (the VCG principle).
Let (T−j)∗ denote the gain from trade of the optimal allocation when buyer j"s bids are discarded. Denote by (V−j)∗ the total gain from trade in the allocation (T−j)∗ .
Definition 4. Seller Floor: The lowest price the seller should expect to receive, communicated to the seller by the mechanism. The seller floor for player i who was matched with buyer j on good Ai, i.e., Tj(Ai) = 1, is computed as: Li = vj(i) + (V−j)∗ − V ∗ .
The seller is instructed not to accept less than this price from her matched buyer.
Next, the mechanism computes for each buyer-seller pair a buyer ceiling, which provides the upper bound of the ZOPA for this pair, and assigns it to the buyer.
Each buyer"s ceiling is computed by removing the buyer"s matched seller and calculating the difference between the total gain from trade when the buyer is excluded and the total gain from trade of the other participants when the 3 buyer is included. Let (T−i )∗ denote the gain from trade of the optimal allocation when seller i is removed from the trade. Denote by (V −i )∗ the total gain from trade in the allocation (T−i )∗ .
Let (T−i −j )∗ denote the gain from trade of the optimal allocation when seller i is removed from the trade and buyer j"s bids are discarded. Denote by (V −i −j )∗ the total gain from trade in the allocation (T −i −j )∗ .
Definition 5. Buyer Ceiling: The highest price the seller should expect to pay, communicated to the buyer by the mechanism. The buyer ceiling for player j who was matched with seller i on good Ai, i.e., Tj(Ai) = 1, is computed as: Hj = vj(i) + (V −i −j )∗ − (V −i )∗ .
The buyer is instructed not to pay more than this price to his matched seller.
Once the negotiation range lower bound and upper bound are computed for every matched pair, the mechanism reports the lower bound price to the seller and the upper bound price to the buyer. At this point each buyer-seller pair negotiates on the final price and concludes the deal.
A schematic description the ZOPAS mechanism is given in Figure 3.1.
In this section we analyze the properties of the ZOPAS mechanism.
Theorem 1. The ZOPAS market negotiation-range mechanism is an incentive-compatible bilateral trade mechanism that is efficient, individually rational and budget balanced.
Clearly ZOPAS is an efficient polynomial time mechanism.
Let us show it satisfies the rest of the properties in the theorem.
Claim 1. ZOPAS is individually rational, i.e., the mechanism maintains nonnegative utility surplus for all participants.
Proof. If a participant does not trade in the optimal allocation then his utility surplus is zero by definition.
Consider a pair of buyer j and seller i which are matched in the optimal allocation T ∗ . Then the buyer"s utility is at least vj(i) − Hj. Recall that Hj = vj(i) + (V −i −j )∗ − (V −i )∗ , so that vj(i) − Hj = (V −i )∗ − (V −i −j )∗ . Since the optimal gain from trade which includes j is higher than that which does not, we have that the utility is nonnegative: vj(i) − Hj ≥ 0.
Now, consider the seller i. Her utility surplus is at least Li − ci. Recall that Li = vj(i) + (V−j)∗ − V ∗ . If we removed from the optimal allocation T ∗ the contribution of the buyer j - seller i pair, we are left with an allocation which excludes j, and has value V ∗ − (vj(i) − ci). This implies that (V−j)∗ ≥ V ∗ − vj(i) + ci, which implies that Li − ci ≥ 0.
The fact that ZOPAS is a budget-balanced mechanism follows from the following lemma which ensures the validity of the negotiation range, i.e., that every seller"s floor is below her matched buyer"s ceiling. This ensures that they can close the deal at a final price which lies in this range.
Lemma 1. For every buyer j- seller i pair matched by the mechanism: Li ≤ Hj.
Proof. Recall that Li = vj(i) + (V−j)∗ − V ∗ and Hj = vj(i)+(V −i −j )∗ −(V −i )∗ . To prove that Li ≤ Hj it is enough to show that (V −i )∗ + (V−j)∗ ≤ V ∗ + (V −i −j )∗ . (1) The proof of (1) is based on a method which we apply several times in our analysis. We start with the allocations (T−i )∗ and (T−j)∗ which together have value equal to (V −i )∗ + (V−j)∗ . We now use them to create a pair of new valid allocations, by using the same pairs that were matched in the original allocations. This means that the sum of values of the new allocations is the same as the original pair of allocations. We also require that one of the new allocations does not include buyer j or seller i. This means that the sum of values of these new allocations is at most V ∗ + (V −i −j )∗ , which proves (1).
Let G be the bipartite graph where the nodes on one side of G represent the buyers and the nodes on the other side represent the sellers, and edge weights represent the gain from trade for the particular pair. The different allocations represent bipartite matchings in G. It will be convenient for the sake of our argument to think of the edges that belong to each of the matchings as being colored with a specific color representing this matching.
Assign color 1 to the edges in the matching (T −i )∗ and assign color 2 to the edges in the matching (T−j)∗ . We claim that these edges can be recolored using colors 3 and 4 so that the new coloring represents allocations T (represented by color 3) and (T−i −j ) (represented by color 4). This implies the that inequality (1) holds. Figure 2 illustrates the graph G and the colorings of the different matchings.
Define an alternating path P starting at j. Let S1 be the seller matched to j in (T −i )∗ (if none exists then P is empty). Let B1 be the buyer matched to S1 in (T−j)∗ , S2 be the seller matched to B1 in (T−i )∗ , B2 be the buyer matched to S2 in (T−j)∗ , and so on. This defines an alternating path P, starting at j, whose edges" colors alternate between colors 1 and 2 (starting with 1). This path ends either in a seller who is not matched in (T−j)∗ or in a buyer who is not matched in (T−i )∗ . Since all sellers in this path are matched in (T−i )∗ , we have that seller i does not belong to P. This ensures that edges in P may be colored by alternating colors
others do not involve i or j and thus may be colored 4 and be part of an allocation (T −i −j ) .
We are left to recolor the edges that do not belong to P.
Since none of these edges includes j we have that the edges that were colored 1, which are part of (T −i )∗ , may now be colored 4, and be included in the allocation (T −i −j ) . It is also clear that the edges that were colored 2, which are part of (T−j)∗ , may now be colored 3, and be included in the allocation T . This completes the proof of the lemma.
The basic requirement in mechanism design is for an exchange mechanism to be incentive compatible. This means that its payment structure enforces that truth-telling is the players" weakly dominant strategy, that is that the strategy by which the player is stating his true valuation results 4 ... jS1 S2 B1 S3 B2 B3S4 S5 B4 S6 B5 S8 B7 S7 B6 ...
Figure 2: Alternating path argument for Lemma 1 (Validity of the Negotiation Range) and Claim 2 (part of Buyer"s IC proof) Colors Bidders
UnmatchedMatched Figure 3: Key to Figure 2 in bigger or equal utility as any other strategy. The utility surplus is defined as the absolute difference between the player"s bid and his price.
Negotiation-range mechanisms assign bounds on the range of prices rather than the final price and therefore the player"s valuation only influences the minimum and maximum bounds on his utility. For a buyer the minimum (bottom) utility would be based on the top of the negotiation range (ceiling), and the maximum (top) utility would be based on the bottom of the negotiation range (floor). For a seller it"s the other way around. Therefore the basic natural requirement from negotiation-range mechanisms would be that stating the player"s true valuation results in both the higher bottom utility and higher top utility for the player, compared with other strategies. Unfortunately, this requirement is still too strong and it is impossible (by [10]) that this will always hold. Therefore we slightly relax it as follows: we require this holds when the false valuation based strategy changes the player"s allocation. When the allocation stays unchanged we require instead that the player would not be able to change his matched player"s bound (e.g. a buyer cannot change the seller"s floor). This means that the only thing he can influence is his own bound, something that he can alternatively achieve through means of negotiation.
The following formally summarizes our incentive compatibility requirements from the negotiation-range mechanism.
Buyer"s incentive compatibility: • Let j be a buyer matched with seller i by the mechanism according to valuation vj and the negotiationrange assigned is (Li, Hj). Assume that when the mechanism is applied according to valuation ˆvj, seller k = i is matched with j and the negotiation-range assigned is (ˆLk, ˆHj). Then vj(i) − Hj ≥ vj(k) − ˆHj. (2) vj(i) − Li ≥ vj(k) − ˆLk. (3) • Let j be a buyer not matched by the mechanism according to valuation vj. Assume that when the mechanism is applied according to valuation ˆvj, seller k = i is matched with j and the negotiation-range assigned is (ˆLk, ˆHj). Then vj(k) − ˆHj ≤ vj(k) − ˆLk ≤ 0. (4) • Let j be a buyer matched with seller i by the mechanism according to valuation vj and let the assigned bottom of the negotiation range (seller"s floor) be Li.
Assume that when the mechanism is applied according to valuation ˆvj, the matching between i and j remains unchanged and let the assigned bottom of the negotiation range (seller"s floor) be ˆLi. Then, ˆLi = Li. (5) Notice that the first inequality of (4) always holds for a valid negotiation range mechanism (Lemma 1).
Seller"s incentive compatibility: • Let i be a seller not matched by the mechanism according to valuation ci. Assume that when the mechanism 5 is applied according to valuation ˆci, buyer z = j is matched with i and the negotiation-range assigned is (ˆLi, ˆHz). Then ˆLi − ci ≤ ˆHz − ci ≤ 0. (6) • Let i be a buyer matched with buyer j by the mechanism according to valuation ci and let the assigned top of the negotiation range (buyer"s ceiling) be Hj.
Assume that when the mechanism is applied according to valuation ˆci, the matching between i and j remains unchanged and let the assigned top of the negotiation range (buyer"s ceiling) be ˆHj. Then, ˆHj = Hj. (7) Notice that the first inequality of (6) always holds for a valid negotiation range mechanism (Lemma 1).
Observe that in the case of sellers in our setting, the case expressed by requirement (6) is the only case in which the seller may change the allocation to her benefit. In particular, it is not possible for seller i who is matched in T ∗ to change her buyer by reporting a false valuation. This fact simply follows from the observation that reducing the seller"s valuation increases the gain from trade for the current allocation by at least as much than any other allocation, whereas increasing the seller"s valuation decreases the gain from trade for the current allocation by exactly the same amount as any other allocation in which it is matched. Therefore, the only case the optimal allocation may change is when in the new allocation i is not matched in which case her utility surplus is 0.
Theorem 2. ZOPAS is an incentive compatible negotiationrange mechanism.
Proof. We begin with the incentive compatibility for buyers.
Consider a buyer j who is matched with seller i according to his true valuation v. Consider that j is reporting instead a false valuation ˆv which results in a different allocation in which j is matched with seller k = i. The following claim shows that a buyer j which changed his allocation due to a false declaration of his valuation cannot improve his top utility.
Claim 2. Let j be a buyer matched to seller i in T ∗ , and let k = i be the seller matched to j in ˆT∗ . Then, vj(i) − Hj ≥ vj(k) − ˆHj. (8) Proof. Recall that Hj = vj(i) + (V −i −j )∗ − (V −i )∗ and ˆHj = ˆvj(k) + ( ˆV −k −j )∗ − ( ˆV −k )∗ . Therefore, vj(i) − Hj = (V −i )∗ − (V −i −j )∗ and vj(k) − ˆHj = vj(k) − ˆvj(k) + ( ˆV −k )∗ − ( ˆV −k −j )∗ .
It follows that in order to prove (8) we need to show ( ˆV −k )∗ + (V −i −j )∗ ≤ (V −i )∗ + ( ˆV −k −j )∗ + ˆvj(k) − vj(k). (9) Consider first the case were j is matched to i in ( ˆT−k )∗ . If we remove this pair and instead match j with k we obtain a matching which excludes i, if the gain from trade on the new pair is taken according to the true valuation then we get ( ˆV −k )∗ − (ˆvj(i) − ci) + (vj(k) − ck) ≤ (V −i )∗ .
Now, since the optimal allocation ˆT∗ matches j with k rather than with i we have that (V −i −j )∗ + (ˆvj(i) − ci) ≤ ˆV ∗ = ( ˆV −k −j )∗ + (ˆvj(k) − ck), where we have used that ( ˆV −i −j )∗ = (V −i −j )∗ since these allocations exclude j. Adding up these two inequalities implies (9) in this case.
It is left to prove (9) when j is not matched to i in ( ˆT−k )∗ .
In fact, in this case we prove the stronger inequality ( ˆV −k )∗ + (V −i −j )∗ ≤ (V −i )∗ + ( ˆV −k −j )∗ . (10) It is easy to see that (10) indeed implies (9) since it follows from the fact that k is assigned to j in ˆT∗ that ˆvj(k) ≥ vj(k). The proof of (10) works as follows. We start with the allocations ( ˆT−k )∗ and (T−i −j )∗ which together have value equal to ( ˆV −k )∗ + (V −i −j )∗ . We now use them to create a pair of new valid allocations, by using the same pairs that were matched in the original allocations. This means that the sum of values of the new allocations is the same as the original pair of allocations. We also require that one of the new allocations does not include seller i and is based on the true valuation v, while the other allocation does not include buyer j or seller k and is based on the false valuation ˆv. This means that the sum of values of these new allocations is at most (V −i )∗ + ( ˆV −k −j )∗ , which proves (10).
Let G be the bipartite graph where the nodes on one side of G represent the buyers and the nodes on the other side represent the sellers, and edge weights represent the gain from trade for the particular pair. The different allocations represent bipartite matchings in G. It will be convenient for the sake of our argument to think of the edges that belong to each of the matchings as being colored with a specific color representing this matching.
Assign color 1 to the edges in the matching ( ˆT−k )∗ and assign color 2 to the edges in the matching (T −i −j )∗ . We claim that these edges can be recolored using colors 3 and 4 so that the new coloring represents allocations (T −i ) (represented by color 3) and ( ˆT−k −j ) (represented by color 4). This implies the that inequality (10) holds. Figure 2 illustrates the graph G and the colorings of the different matchings.
Define an alternating path P starting at j. Let S1 = i be the seller matched to j in ( ˆT−k )∗ (if none exists then P is empty). Let B1 be the buyer matched to S1 in (T−i −j )∗ , S2 be the seller matched to B1 in ( ˆT−k )∗ , B2 be the buyer matched to S2 in (T−i −j )∗ , and so on. This defines an alternating path P, starting at j, whose edges" colors alternate between colors 1 and 2 (starting with 1). This path ends either in a seller who is not matched in (T −i −j )∗ or in a buyer who is not matched in ( ˆT−k )∗ . Since all sellers in this path are matched in ( ˆT−k )∗ , we have that seller k does not belong to P. Since in this case S1 = i and the rest of the sellers in P are matched in (T−i −j )∗ we have that seller i as well does not belong to P. This ensures that edges in P may be colored by alternating colors 3 and 4 (starting with 3). Since S1 = i, we may use color 3 for the first edge and thus assign it to the allocation (T−i ) . All other edges, do not involve i, j or k and thus may be either colored 4 and be part of an allocation ( ˆT−k −j ) or colored 3 and be part of an allocation (T−i ) , in an alternating fashion.
We are left to recolor the edges that do not belong to P.
Since none of these edges includes j we have that the edges 6 that were colored 1, which are part of ( ˆT−k )∗ , may now be colored 4, and be included in the allocation ( ˆT−k −j ) . It is also clear that the edges that were colored 2, which are part of (T−i −j )∗ , may now be colored 3, and be included in the allocation (T−i ) . This completes the proof of (10) and the claim.
The following claim shows that a buyer j which changed his allocation due to a false declaration of his valuation cannot improve his bottom utility. The proof is basically the standard VCG argument.
Claim 3. Let j be a buyer matched to seller i in T ∗ , and k = i be the seller matched to j in ˆT∗ . Then, vj(i) − Li ≥ vj(k) − ˆLk. (11) Proof. Recall that Li = vj(i) + (V−j)∗ − V ∗ , and ˆLk = ˆvj(k) + ( ˆV−j)∗ − ˆV ∗ = ˆvj(k) + (V−j)∗ − ˆV ∗ . Therefore, vj(i) − Li = V ∗ − (V−j)∗ and vj(k) − ˆLk = vj(k) − ˆvj(k) + ˆV ∗ − (V−j)∗ .
It follows that in order to prove (11) we need to show V ∗ ≥ vj(k) − ˆvj(k) + ˆV ∗ . (12) The scenario of this claim occurs when j understates his value for Ai or overstated his value for Ak. Consider these two cases: • ˆvj(k) > vj(k): Since Ak was allocated to j in the allocation ˆT∗ we have that using the allocation of ˆT∗ according to the true valuation gives an allocation of value U satisfying ˆV ∗ − ˆvj(k) + vj(k) ≤ U ≤ V ∗ . • ˆvj(k) = vj(k) and ˆvj(i) < vj(i): In this case (12) reduces to V ∗ ≥ ˆV ∗ . Since j is not allocated i in ˆT∗ we have that ˆT∗ is an allocation that uses only true valuations. From the optimality of T ∗ we conclude that V ∗ ≥ ˆV ∗ .
Another case in which a buyer may try to improve his utility is when he does not win any good by stating his true valuation. He may give a false valuation under which he wins some good. The following claim shows that doing this is not beneficial to him.
Claim 4. Let j be a buyer not matched in T ∗ , and assume seller k is matched to j in ˆT∗ . Then, vj(k) − ˆLk ≤ 0.
Proof. The scenario of this claim occurs if j did not buy in the truth-telling allocation and overstates his value for Ak, ˆvj(k) > vj(k) in his false valuation. Recall that ˆLk = ˆvj(k) + ( ˆV−j)∗ − ˆV ∗ . Thus we need to show that
− (V−j)∗ . Since j is not allocated in T∗ then (V−j)∗ = V ∗ . Since j is allocated Ak in ˆT∗ we have that using the allocation of ˆT∗ according to the true valuation gives an allocation of value U satisfying ˆV ∗ − ˆvj(k) + vj(k) ≤ U ≤ V ∗ . Thus we can conclude that 0 ≥ vj(k) − ˆvj(k) + ˆV ∗ − (V−j)∗ .
Finally, the following claim ensures that a buyer cannot influence the floor bound of the ZOPA for the good he wins.
Claim 5. Let j be a buyer matched to seller i in T ∗ , and assume that ˆT∗ = T∗ , then ˆLi = Li.
Proof. Recall that Li = vj(i) + (V−j)∗ − V ∗ , and ˆLi = ˆvj(i) + ( ˆV−j)∗ − ˆV ∗ = ˆvj(i) + (V−j)∗ − ˆV ∗ . Therefore we need to show that ˆV ∗ = V ∗ + ˆvj(i) − vj(i).
Since j is allocated Ai in T∗ , we have that using the allocation of T∗ according to the false valuation gives an allocation of value U satisfying V ∗ − vj(i) + ˆvj(i) ≤ U ≤ ˆV ∗ .
Similarly since j is allocated Ai in ˆT∗ , we have that using the allocation of ˆT∗ according to the true valuation gives an allocation of value U satisfying ˆV ∗ − ˆvj(i)+vj(i) ≤ U ≤ V ∗ , which together with the previous inequality completes the proof.
This completes the analysis of the buyer"s incentive compatibility. We now turn to prove the seller"s incentive compatibility properties of our mechanism.
The following claim handles the case where a seller that was not matched in T ∗ falsely understates her valuation such that she gets matched n ˆT∗ .
Claim 6. Let i be a seller not matched in T ∗ , and assume buyer z is matched to i in ˆT∗ . Then, ˆHz − ci ≤ 0.
Proof. Recall that ˆHz = vz(i) + ( ˆV −i −z )∗ − ( ˆV −i )∗ .
Since i is not matched in T ∗ and ( ˆT−i )∗ involves only true valuations we have that ( ˆV −i )∗ = V ∗ . Since i is matched with z in ˆT∗ it can be obtained by adding the buyer z - seller i pair to ( ˆT−i −z)∗ . It follows that ˆV ∗ = ( ˆV −i −z )∗ + vz(i) − ˆci.
Thus, we have that ˆHz = ˆV ∗ + ˆci − V ∗ . Now, since i is matched in ˆT∗ , using this allocation according to the true valuation gives an allocation of value U satisfying ˆV ∗ + ˆci − ci ≤ U ≤ V ∗ . Therefore ˆHz −ci = ˆV ∗ +ˆci −V ∗ −ci ≤ 0.
Finally, the following simple claim ensures that a seller cannot influence the ceiling bound of the ZOPA for the good she sells.
Claim 7. Let i be a seller matched to buyer j in T ∗ , and assume that ˆT∗ = T∗ , then ˆHj = Hj.
Proof. Since ( ˆV −i −j )∗ = (V −i −j )∗ and ( ˆV −i )∗ = (V −i )∗ it follows that ˆHj = vj(i)+( ˆV −i −j )∗ −( ˆV −i )∗ = vj(i)+(V −i −j )∗ −(V −i )∗ = Hj.
In this paper we suggest a way to deal with the impossibility of producing mechanisms which are efficient, individually rational, incentive compatible and budget balanced.
To this aim we introduce the concept of negotiation-range mechanisms which avoid the problem by leaving the final determination of prices to a negotiation between the buyer and seller. The goal of the mechanism is to provide the initial range (ZOPA) for negotiation in a way that it will be beneficial for the participants to close the proposed deals.
We present a negotiation range mechanism that is efficient, individually rational, incentive compatible and budget balanced. The ZOPA produced by our mechanism is based 7 on a natural adaptation of the VCG payment scheme in a way that promises valid negotiation ranges which permit a budget balanced allocation.
The basic question that we aimed to tackle seems very exciting: which properties can we expect a market mechanism to achieve ? Are there different market models and requirements from the mechanisms that are more feasible than classic mechanism design goals ?
In the context of our negotiation-range model, is natural to further study negotiation based mechanisms in more general settings. A natural extension is that of a combinatorial market. Unfortunately, finding the optimal allocation in a combinatorial setting is NP-hard, and thus the problem of maintaining BB is compounded by the problem of maintaining IC when efficiency is approximated [1, 5, 6, 9, 11].
Applying the approach in this paper to develop negotiationrange mechanisms for combinatorial markets, even in restricted settings, seems a promising direction for research.
[1] Y. Bartal, R. Gonen, and N. Nisan. Incentive Compatible Multi-Unit Combinatorial Auctions.
Proceeding of 9th TARK 2003, pp. 72-87, June 2003. [2] E. H. Clarke. Multipart Pricing of Public Goods. In journal Public Choice 1971, volume 2, pages 17-33. [3] J.Feigenbaum, C. Papadimitriou, and S. Shenker.
Sharing the Cost of Multicast Transmissions. Journal of Computer and System Sciences, 63(1),2001. [4] A. Fiat, A. Goldberg, J. Hartline, and A. Karlin.
Competitive Generalized Auctions. Proceeding of 34th ACM Symposium on Theory of Computing,2002. [5] R. Gonen, and D. Lehmann. Optimal Solutions for Multi-Unit Combinatorial Auctions: Branch and Bound Heuristics. Proceeding of ACM Conference on Electronic Commerce EC"00, pages 13-20, October
[6] R. Gonen, and D. Lehmann. Linear Programming helps solving Large Multi-unit Combinatorial Auctions. In Proceeding of INFORMS 2001,
November, 2001. [7] T. Groves. Incentives in teams. In journal Econometrica 1973, volume 41, pages 617-631. [8] R. Lavi, A. Mu"alem and N. Nisan. Towards a Characterization of Truthful Combinatorial Auctions.
Proceeding of 44th Annual IEEE Symposium on Foundations of Computer Science,2003. [9] D. Lehmann, L. I. O"Callaghan, and Y. Shoham.
Truth revelation in rapid, approximately efficient combinatorial auctions. In Proceedings of the First ACM Conference on Electronic Commerce, pages 96-102, November 1999. [10] R. Myerson, M. Satterthwaite. Efficient Mechanisms for Bilateral Trading. Journal of Economic Theory, 28, pages 265-81, 1983. [11] N. Nisan and A. Ronen. Algorithmic Mechanism Design. In Proceeding of 31th ACM Symposium on Theory of Computing, 1999. [12] D.C. Parkes, J. Kalagnanam, and M. Eso. Achieving Budget-Balance with Vickrey-Based Payment Schemes in Exchanges. Proceeding of 17th International Joint Conference on Artificial Intelligence, pages 1161-1168,

COMMERCE Privacy remains an important issue for electronic commerce. A PriceWaterhouseCoopers study in 2000 showed that nearly two thirds of the consumers surveyed would shop more online if they knew retail sites would not do anything with their personal information [15]. A Federal Trade Commission study reported in 2000 that sixty-seven percent of consumers were very concerned about the privacy of the personal information provided on-line [11]. More recently, a February 2002 Harris Interactive survey found that the three biggest consumer concerns in the area of on-line personal information security were: companies trading personal data without permission, the consequences of insecure transactions, and theft of personal data [19]. According to a Jupiter Research study in 2002, $24.5 billion in on-line sales will be lost by 2006 - up from $5.5 billion in 2001.
Online retail sales would be approximately twenty-four percent higher in 2006 if consumers" fears about privacy and security were addressed effectively [21]. Although the media hype has somewhat diminished, risks and costs have notas evidenced by the increasing volumes of electronic spam and identity theft [16].
Surveys in this field, however, as well as experiments and anecdotal evidence, have also painted a different picture. [36, 10, 18, 21] have found evidence that even privacy concerned individuals are willing to trade-off privacy for convenience, or bargain the release of very personal information in exchange for relatively small rewards. The failure of several on-line services aimed at providing anonymity for Internet users [6] offers additional indirect evidence of the reluctance by most individuals to spend any effort in protecting their personal information.
The dichotomy between privacy attitudes and behavior has been highlighted in the literature. Preliminary interpretations of this phenomenon have been provided [2, 38, 33, 40]. Still missing are: an explanation grounded in economic or psychological theories; an empirical validation of the proposed explanation; and, of course, the answer to the most recurring question: should people bother at all about privacy?
In this paper we focus on the first question: we formally analyze the individual decision making process with respect to privacy and its possible shortcomings. We focus on individual (mis)conceptions about their handling of risks they face when revealing private information. We do not address the issue of whether people should actually protect themselves. We will comment on that in Section 5, where we will also discuss strategies to empirically validate our theory.
We apply lessons from behavioral economics. Traditional economics postulates that people are forward-looking and bayesian updaters: they take into account how current behavior will influence their future well-being and preferences.
For example, [5] study rational models of addiction. This approach can be compared to those who see in the decision 21 not to protect one"s privacy a rational choice given the (supposedly) low risks at stake. However, developments in the area of behavioral economics have highlighted various forms of psychological inconsistencies (self-control problems, hyperbolic discounting, present-biases, etc.) that clash with the fully rational view of the economic agent. In this paper we draw from these developments to reach the following conclusions: • We show that it is unlikely that individuals can act rationally in the economic sense when facing privacy sensitive decisions. • We show that alternative models of personal behavior and time-inconsistent preferences are compatible with the dichotomy between attitudes and behavior and can better match current data. For example, they can explain the results presented by [36] at the ACM EC "01 conference. In their experiment, self-proclaimed privacy advocates were found to be willing to reveal varying amounts of personal information in exchange for small rewards. • In particular, we show that individuals may have a tendency to under-protect themselves against the privacy risks they perceive, and over-provide personal information even when wary of (perceived) risks involved. • We show that the magnitude of the perceived costs of privacy under certain conditions will not act as deterrent against behavior the individual admits is risky. • We show, following similar studies in the economics of immediate gratification [31], that even ‘sophisticated" individuals may under certain conditions become ‘privacy myopic." Our conclusion is that simply providing more information and awareness in a self-regulative environment is not sufficient to protect individual privacy. Improved technologies, by lowering costs of adoption and protection, certainly can help. However, more fundamental human behavioral responses must also be addressed if privacy ought to be protected.
In the next section we propose a model of rational agents facing privacy sensitive decisions. In Section 3 we show the difficulties that hinder any model of privacy decision making based on full rationality. In Section 4 we show how behavioral models based on immediate gratification bias can better explain the attitudes-behavior dichotomy and match available data. In Section 5 we summarize and discuss our conclusions.
PRIVACY DECISION MAKING Some have used the dichotomy between privacy attitudes and behavior to claim that individuals are acting rationally when it comes to privacy. Under this view, individuals may accept small rewards for giving away information because they expect future damages to be even smaller (when discounted over time and with their probability of occurrence).
Here we want to investigate what underlying assumptions about personal behavior would support the hypothesis of full rationality in privacy decision making.
Since [28, 37, 29] economists have been interested in privacy, but only recently formal models have started appearing [3, 7, 39, 40]. While these studies focus on market interactions between one agent and other parties, here we are interested in formalizing the decision process of the single individual. We want to see if individuals can be economically rational (forward-lookers, bayesian updaters, utility maximizers, and so on) when it comes to protect their own personal information.
The concept of privacy, once intended as the right to be left alone [41], has transformed as our society has become more information oriented. In an information society the self is expressed, defined, and affected through and by information and information technology. The boundaries between private and public become blurred. Privacy has therefore become more a class of multifaceted interests than a single, unambiguous concept. Hence its value may be discussed (if not ascertained) only once its context has also been specified. This most often requires the study of a network of relations between a subject, certain information (related to the subject), other parties (that may have various linkages of interest or association with that information or that subject), and the context in which such linkages take place.
To understand how a rational agent could navigate through those complex relations, in Equation 1 we abstract the decision process of an idealized rational economic agent who is facing privacy trade-offs when completing a certain transaction. max d Ut = δ vE (a) , pd (a) + γ vE (t) , pd (t) − cd t (1) In Equation 1, δ and γ are unspecified functional forms that describe weighted relations between expected payoffs from a set of events v and the associated probabilities of occurrence of those events p. More precisely, the utility U of completing a transaction t (the transaction being any action - not necessarily a monetary operation - possibly involving exposure of personal information) is equal to some function of the expected payoff vE (a) from maintaining (or not) certain information private during that transaction, and the probability of maintaining [or not maintaining] that information private when using technology d, pd (a) [1 − pd (a)]; plus some function of the expected payoff vE (t) from completing (or non completing) the transaction (possibly revealing personal information), and the probability of completing [or not completing] that transaction with a certain technology d, pd (t) [1 − pd (t)]; minus the cost of using the technology t: cd t .1 The technology d may or may not be privacy enhancing.
Since the payoffs in Equation 1 can be either positive or negative, Equation 1 embodies the duality implicit in privacy issues: there are both costs and benefits gained from revealing or from protecting personal information, and the costs and benefits from completing a transaction, vE (t), might be distinct from the costs and benefits from keeping the associated information private, vE (a). For instance, revealing one"s identity to an on-line bookstore may earn a discount.
Viceversa, it may also cost a larger bill, because of price discrimination. Protecting one"s financial privacy by not divulging credit card information on-line may protect against future losses and hassles related to identity theft. But it may 1 See also [1]. 22 make one"s on-line shopping experience more cumbersome, and therefore more expensive.
The functional parameters δ and γ embody the variable weights and attitudes an individual may have towards keeping her information private (for example, her privacy sensitivity, or her belief that privacy is a right whose respect should be enforced by the government) and completing certain transactions. Note that vE and p could refer to sets of payoffs and the associated probabilities of occurrence. The payoffs are themselves only expected because, regardless of the probability that the transaction is completed or the information remains private, they may depend on other sets of events and their associated probabilities. vE() and pd (), in other words, can be read as multi-variate parameters inside which are hidden several other variables, expectations, and functions because of the complexity of the privacy network described above.
Over time, the probability of keeping certain information private, for instance, will not only depend on the chosen technology d but also on the efforts by other parties to appropriate that information. These efforts may be function, among other things, of the expected value of that information to those parties. The probability of keeping information private will also depend on the environment in which the transaction is taking place. Similarly, the expected benefit from keeping information private will also be a collection over time of probability distributions dependent on several parameters. Imagine that the probability of keeping your financial transactions private is very high when you use a bank in Bermuda: still, the expected value from keeping your financial information confidential will depend on a number of other factors.
A rational agent would, in theory, choose the technology d that maximizes her expected payoff in Equation 1. Maybe she would choose to complete the transaction under the protection of a privacy enhancing technology. Maybe she would complete the transaction without protection. Maybe she would not complete the transaction at all (d = 0). For example, the agent may consider the costs and benefits of sending an email through an anonymous MIX-net system [8] and compare those to the costs and benefits of sending that email through a conventional, non-anonymous channel. The magnitudes of the parameters in Equation 1 will change with the chosen technology. MIX-net systems may decrease the expected losses from privacy intrusions.
Nonanonymous email systems may promise comparably higher reliability and (possibly) reduced costs of operations.
DISTORTIONS IN PRIVACY Equation 1 is a comprehensive (while intentionally generic) road-map for navigation across privacy trade-offs that no human agent would be actually able to use.
We hinted to some difficulties as we noted that several layers of complexities are hidden inside concepts such as the expected value of maintaining certain information private, and the probability of succeeding doing so. More precisely, an agent will face three problems when comparing the tradeoffs implicit in Equation 1: incomplete information about all parameters; bounded power to process all available information; no deviation from the rational path towards utilitymaximization. Those three problems are precisely the same issues real people have to deal with on an everyday basis as they face privacy-sensitive decisions. We discuss each problem in detail.
individual access to as she prepares to take privacy sensitive decisions? For instance, is she aware of privacy invasions and the associated risks? What is her knowledge of the existence and characteristics of protective technologies?
Economic transactions are often characterized by incomplete or asymmetric information. Different parties involved may not have the same amount of information about the transaction and may be uncertain about some important aspects of it [4]. Incomplete information will affect almost all parameters in Equation 1, and in particular the estimation of costs and benefits. Costs and benefits associated with privacy protection and privacy intrusions are both monetary and immaterial. Monetary costs may for instance include adoption costs (which are probably fixed) and usage costs (which are variable) of protective technologies - if the individual decides to protect herself. Or they may include the financial costs associated to identity theft, if the individual"s information turns out not to have been adequately protected. Immaterial costs may include learning costs of a protective technology, switching costs between different applications, or social stigma when using anonymizing technologies, and many others. Likewise, the benefits from protecting (or not protecting) personal information may also be easy to quantify in monetary terms (the discount you receive for revealing personal data) or be intangible (the feeling of protection when you send encrypted emails).
It is difficult for an individual to estimate all these values. Through information technology, privacy invasions can be ubiquitous and invisible. Many of the payoffs associated with privacy protection or intrusion may be discovered or ascertained only ex post through actual experience. Consider, for instance, the difficulties in using privacy and encrypting technologies described in [43].
In addition, the calculations implicit in Equation 1 depend on incomplete information about the probability distribution of future events. Some of those distributions may be predicted after comparable data - for example, the probability that a certain credit card transaction will result in fraud today could be calculated using existing statistics. The probability distributions of other events may be very difficult to estimate because the environment is too dynamicfor example, the probability of being subject to identity theft
releasing now. And the distributions of some other events may be almost completely subjective - for example, the probability that a new and practical form of attack on a currently secure cryptosystem will expose all of your encrypted personal communications a few years from now.
This leads to a related problem: bounded rationality.
calculate all the parameters relevant to her choice? Or is she limited by bounded rationality?
In our context, bounded rationality refers to the inability to calculate and compare the magnitudes of payoffs associated with various strategies the individual may choose in privacy-sensitive situations. It also refers to the inability to process all the stochastic information related to risks and probabilities of events leading to privacy costs and benefits. 23 In traditional economic theory, the agent is assumed to have both rationality and unbounded ‘computational" power to process information. But human agents are unable to process all information in their hands and draw accurate conclusions from it [34]. In the scenario we consider, once an individual provides personal information to other parties, she literally loses control of that information. That loss of control propagates through other parties and persists for unpredictable spans of time. Being in a position of information asymmetry with respect to the party with whom she is transacting, decisions must be based on stochastic assessments, and the magnitudes of the factors that may affect the individual become very difficult to aggregate, calculate, and compare.2 Bounded rationality will affect the calculation of the parameters in Equation 1, and in particular δ, γ, vE(), and pt(). The cognitive costs involved in trying to calculate the best strategy could therefore be so high that the individual may just resort to simple heuristics.
individual had access to complete information and could appropriately compute it, she still may find it difficult to follow the rational strategy presented in Equation 1. A vast body of economic and psychological literature has by now confirmed the impact of several forms of psychological distortions on individual decision making. Privacy seems to be a case study encompassing many of those distortions: hyperbolic discounting, under insurance, self-control problems, immediate gratification, and others. The traditional dichotomy between attitude and behavior, observed in several aspects of human psychology and studied in the social psychology literature since [24] and [13], may also appear in the privacy space because of these distortions.
For example, individuals have a tendency to discount ‘hyperbolically" future costs or benefits [31, 27]. In economics, hyperbolic discounting implies inconsistency of personal preferences over time - future events may be discounted at different discount rates than near-term events. Hyperbolic discounting may affect privacy decisions, for instance when we heavily discount the (low) probability of (high) future risks such as identity theft.3 Related to hyperbolic discounting is the tendency to underinsure oneself against certain risks [22].
In general, individuals may put constraints on future behavior that limit their own achievement of maximum utility: people may genuinely want to protect themselves, but because of self-control bias, they will not actually take those steps, and opt for immediate gratification instead.
People tend to underappreciate the effects of changes in their states, and hence falsely project their current preferences over consumption onto their future preferences. Far more than suggesting merely that people mispredict future tastes, this projection bias posits a systematic pattern in these mispredictions which can lead to systematic errors in dynamicchoice environments [25, p. 2]. 2 The negative utility coming from future potential misuses of somebody"s personal information could be a random shock whose probability and scope are extremely variable.
For example, a small and apparently innocuous piece of information might become a crucial asset or a dangerous liability in the right context. 3 A more rigorous description and application of hyperbolic discounting is provided in Section 4.
In addition, individuals suffer from optimism bias [42], the misperception that one"s risks are lower than those of other individuals under similar conditions. Optimism bias may lead us to believe that we will not be subject to privacy intrusions.
Individuals encounter difficulties when dealing with cumulative risks. [35], for instance, shows that while young smokers appreciate the long term risks of smoking, they do not fully realize the cumulative relation between the low risks of each additional cigarette and the slow building up of a serious danger. Difficulties with dealing with cumulative risks apply to privacy, because our personal information, once released, can remain available over long periods of time. And since it can be correlated to other data, the ‘anonymity sets" [32, 14] in which we wish to remain hidden get smaller. As a result, the whole risk associated with revealing different pieces of personal information is more than the sum of the individual risks associated with each piece of data.
Also, it is easier to deal with actions and effects that are closer to us in time. Actions and effects that are in the distant future are difficult to focus on given our limited foresight perspective. As the foresight changes, so does behavior, even when preferences remain the same [20]. This phenomenon may also affects privacy decisions, since the costs of privacy protection may be immediate, but the rewards may be invisible (absence of intrusions) and spread over future periods of time.
To summarize: whenever we face privacy sensitive decisions, we hardly have all data necessary for an informed choice. But even if we had, we would be likely unable to process it. And even if we could process it, we may still end behaving against our own better judgment. In what follows, we present a model of privacy attitudes and behavior based on some of these findings, and in particular on the plight of immediate gratification.
IMMEDIATE GRATIFICATION The problem of immediate gratification (which is related to the concepts of time inconsistency, hyperbolic discounting, and self-control bias) is so described by O"Donoghue and Rabin [27, p. 4]: A person"s relative preference for wellbeing at an earlier date over a later date gets stronger as the earlier date gets closer. [...] [P]eople have self-control problems caused by a tendency to pursue immediate gratification in a way that their ‘long-run selves" do not appreciate. For example, if you were given only two alternatives, on Monday you may claim you will prefer working 5 hours on Saturday to 5 hours and half on Sunday. But as Saturday comes, you will be more likely to prefer postponing work until Sunday.
This simple observation has rather important consequences in economic theory, where time-consistency of preferences is the dominant model. Consider first the traditional model of utility that agents derive from consumption: the model states that utility discounts exponentially over time: Ut = T τ=t δτ uτ (2) In Equation 2, the cumulative utility U at time t is the discounted sum of all utilities from time t (the present) until time T (the future). δ is the discount factor, with a value 24 Period 1 Period 2 Period 3 Period 4 Benefits from selling period 1 2 0 0 0 Costs from selling period 1 0 1 1 1 Benefits from selling period 2 0 2 0 0 Costs from selling period 2 0 0 1 1 Benefits from selling period 3 0 0 2 0 Costs from selling period 3 0 0 0 1 Table 1: (Fictional) expected payoffs from joining loyalty program. between 0 and 1. A value of 0 would imply that the individual discounts so heavily that the utility from future periods is worth zero today. A value of 1 would imply that the individual is so patient she does not discount future utilities.
The discount factor is used in economics to capture the fact that having (say) one dollar one year from now is valuable, but not as much as having that dollar now. In Equation 2, if all uτ were constant - for instance, 10 - and δ was 0.9, then at time t = 0 (that is, now) u0 would be worth 10, but u1 would be worth 9.
Modifying the traditional model of utility discounting, [23] and then [31] have proposed a model which takes into account possible time-inconsistency of preferences. Consider Equation 3: Ut(ut, ut+1, ..., uT ) = δt ut + β T τ=t+1 δτ uτ (3) Assume that δ, β ∈ [0, 1]. δ is the discount factor for intertemporal utility as in Equation 2. β is the parameter that captures an individual"s tendency to gratify herself immediately (a form of time-inconsistent preferences). When β is 1, the model maps the traditional time-consistent utility model, and Equation 3 is identical to Equation 2. But when β is zero, the individual does not care for anything but today. In fact, any β smaller than 1 represents self-control bias.
The experimental literature has convincingly proved that human beings tend to have self-control problems even when they claim otherwise: we tend to avoid and postpone undesirable activities even when this will imply more effort tomorrow; and we tend to over-engage in pleasant activities even though this may cause suffering or reduced utility in the future.
This analytical framework can be applied to the study of privacy attitudes and behavior. Protecting your privacy sometimes means protecting yourself from a clear and present hassle (telemarketers, or people peeping through your window and seeing how you live - see [33]); but sometimes it represents something akin to getting an insurance against future and only uncertain risks. In surveys completed at time t = 0, subjects asked about their attitude towards privacy risks may mentally consider some costs of protecting themselves at a later time t = s and compare those to the avoided costs of privacy intrusions in an even more distant future t = s + n. Their alternatives at survey time 0 are represented in Equation 4. min wrt x DU0 = β[(E(cs,p)δs x) + (E(cs+n,i)δs+n (1 − x))] (4) x is a dummy variable that can take values 0 or 1. It represents the individual"s choice - which costs the individual opts to face: the expected cost of protecting herself at time s, E(cs,p) (in which case x = 1), or the expected costs of being subject to privacy intrusions at a later time s + n,
E(cs+n,i).
The individual is trying to minimize the disutility DU of these costs with respect to x. Because she discounts the two future events with the same discount factor (although at different times), for certain values of the parameters the individual may conclude that paying to protect herself is worthy. In particular, this will happen when: E(cs,p)δs < E(cs+n,i)δs+n (5) Now, consider what happens as the moment t = s comes.
Now a real price should be paid in order to enjoy some form of protection (say, starting to encrypt all of your emails to protect yourself from future intrusions). Now the individual will perceive a different picture: min wrt x DUs = δE(cs,p)x + βE(cn,i)δn (1 − x)] (6) Note that nothing has changed in the equation (certainly not the individual"s perceived risks) except time. If β (the parameter indicating the degree of self-control problems) is less than one, chances are that the individual now will actually choose not to protect herself. This will in fact happen when: δE(cs,p) > βE(cn,i)δn (7) Note that Disequalities 5 and 7 may be simultaneously met for certain β < 1. At survey time the individual honestly claimed she wanted to protect herself in principlethat is, some time in the future. But as she is asked to make an effort to protect herself right now, she chooses to run the risk of privacy intrusion.
Similar mathematical arguments can be made for the comparison between immediate costs with immediate benefits (subscribing to a ‘no-call" list to stop telemarketers from harassing you at dinner), and immediate costs with only future expected rewards (insuring yourself against identity theft, or protecting yourself from frauds by never using your credit card on-line), particularly when expected future rewards (or avoided risks) are also intangible: the immaterial consequences of living (or not) in a dossier society, or the chilling effects (or lack thereof) of being under surveillance.
The reader will have noticed that we have focused on perceived (expected) costs E(c), rather than real costs. We do not know the real costs and we do not claim that the 25 individual does. But we are able to show that under certain conditions even costs perceived as very high (as during periods of intense privacy debate) will be ignored.
We can provide some fictional numerical examples to make the analysis more concrete. We present some scenarios inspired by the calculations in [31].
Imagine an economy with just 4 periods (Table 1). Each individual can enroll in a supermarket"s loyalty program by revealing personal information. If she does so, the individual gets a discount of 2 during the period of enrollment, only to pay one unit each time thereafter because of price discrimination based on the information she revealed (we make no attempt at calibrating the realism of this obviously abstract example; the point we are focusing on is how time inconsistencies may affect individual behavior given the expected costs and benefits of certain actions).4 Depending on which period the individual chooses for ‘selling" her data, we have the undiscounted payoffs represented in Table 1.
Imagine that the individual is contemplating these options and discounting them according to Equation 3.
Suppose that δ = 1 for all types of individuals (this means that for simplicity we do not consider intertemporal discounting) but β = 1/2 for time-inconsistent individuals and β = 1 for everybody else. The time-consistent individual will choose to join the program at the very last period and rip off a benefit of 2-1=1. The individual with immediate gratification problems, for whom β = 1/2, will instead perceive the benefits from joining now or in period 3 as equivalent (0.5), and will join the program now, thus actually making herself worse off. [31] also suggest that, in addition to the distinction between time-consistent individuals and individuals with timeinconsistent preferences, we should also distinguish timeinconsistent individuals who are na¨ıve from those who are sophisticated. Na¨ıve time-inconsistent individuals are not aware of their self-control problems - for example, they are those who always plan to start a diet next week.
Sophisticated time-inconsistent individuals suffer of immediate gratification bias, but are at least aware of their inconsistencies.
People in this category choose their behavior today correctly estimating their future time-inconsistent behavior.
Now consider how this difference affects decisions in another scenario, represented in Table 2. An individual is considering the adoption of a certain privacy enhancing technology. It will cost her some money both to protect herself and not to protect herself. If she decides to protect herself, the cost will be the amount she pays - for example - for some technology that shields her personal information. If she decides not to protect herself, the cost will be the expected consequences of privacy intrusions.
We assume that both these aggregate costs increase over time, although because of separate dynamics. As time goes by, more and more information about the individual has been revealed, and it becomes more costly to be protected against privacy intrusions. At the same time, however, intrusions become more frequent and dangerous. 4 One may claim that loyalty cards keep on providing benefits over time. Here we make the simplifying assumption that such benefits are not larger than the future costs incurred after having revealed one"s tastes. We also assume that the economy ends in period 4 for all individuals, regardless of when they chose to join the loyalty program.
In period 1, the individual may protect herself by spending 5, or she may choose to face a risk of privacy intrusion the following period, expected to cost 7. In the second period, assuming that no intrusion has yet taken place, she may once again protect herself by spending a little more, 6; or she may choose to face a risk of privacy intrusion the next (third) period, expected to cost 9. In the third period she could protect herself for 8 or face an expected cost of 15 in the following last period.
Here too we make no attempt at calibrating the values in Table 2. Again, we focus on the different behavior driven by heterogeneity in time-consistency and sophistication versus na¨ıvete. We assume that β = 1 for individuals with no self control problems and β = 1/2 for everybody else. We assume for simplicity that δ = 1 for all.
The time-consistent individuals will obviously choose to protect themselves as soon as possible.
In the first period, na¨ıve time-inconsistent individuals will compare the costs of protecting themselves then or face a privacy intrusion in the second period. Because 5 > 7 ∗ (1/2), they will prefer to wait until the following period to protect themselves. But in the second period they will be comparing 6 > 9 ∗ (1/2) - and so they will postpone their protection again. They will keep on doing so, facing higher and higher risks. Eventually, they will risk to incur the highest perceived costs of privacy intrusions (note again that we are simply assuming that individuals believe there are privacy risks and that they increase over time; we will come back to this concept later on).
Time-inconsistent but sophisticated individuals, on the other side, will adopt a protective technology in period 2 and pay 6. By period 2, in fact, they will (correctly) realize that if they wait till period 3 (which they are tempted to do, because 6 > 9 ∗ (1/2)), their self-control bias will lead them to postpone adopting the technology once more (because 8 > 15 ∗ (1/2)). Therefore they predict they would incur the expected cost 15 ∗ (1/2), which is larger than 6the cost of protecting oneself in period 2. In period 1, however, they correctly predict that they will not wait to protect themselves further than period 2. So they wait till period 2, because 5 > 6 ∗ (1/2), at which time they will adopt a protective technology (see also [31]).
To summarize, time-inconsistent people tend not to fully appreciate future risks and, if na¨ıve, also their inability to deal with them. This happens even if they are aware of those risks and they are aware that those risks are increasing. As we learnt from the second scenario, time inconsistency can lead individuals to accept higher and higher risks.
Individuals may tend to downplay the fact that single actions present low risks, but their repetition forms a huge liability: it is a deceiving aspect of privacy that its value is truly appreciated only after privacy itself is lost. This dynamics captures the essence of privacy and the so-called anonymity sets [32, 14], where each bit of information we reveal can be linked to others, so that the whole is more than the sum of the parts.
In addition, [31] show that when costs are immediate, time-inconsistent individuals tend to procrastinate; when benefits are immediate, they tend to preoperate. In our context things are even more interesting because all privacy decisions involve at the same time costs and benefits. So we opt against using eCash [9] in order to save us the costs of switching from credit cards. But we accept the risk that our credit card number on the Internet could be used ma26 Period 1 Period 2 Period 3 Period 4 Protection costs 5 6 8 .
Expected intrusion costs . 7 9 15 Table 2: (Fictional) costs of protecting privacy and expected costs of privacy intrusions over time. liciously. And we give away our personal information to supermarkets in order to gain immediate discounts - which will likely turn into price discrimination in due time [3, 26].
We have shown in the second scenario above how sophisticated but time-inconsistent individuals may choose to protect their information only in period 2. Sophisticated people with self-control problems may be at a loss, sometimes even when compared to na¨ıve people with time inconsistency problems (how many privacy advocates do use privacy enhancing technologies all the time?). The reasoning is that sophisticated people are aware of their self-control problems, and rather than ignoring them, they incorporate them into their decision process. This may decrease their own incentive to behave in the optimal way now.
Sophisticated privacy advocates might realize that protecting themselves from any possible privacy intrusion is unrealistic, and so they may start misbehaving now (and may get used to that, a form of coherent arbitrariness). This is consistent with the results by [36] presented at the ACM EC "01 conference. [36] found that privacy advocates were also willing to reveal personal information in exchange for monetary rewards.
It is also interesting to note that these inconsistencies are not caused by ignorance of existing risks or confusion about available technologies. Individuals in the abstract scenarios we described are aware of their perceived risks and costs.
However, under certain conditions, the magnitude of those liabilities is almost irrelevant. The individual will take very slowly increasing risks, which become steps towards huge liabilities.
Applying models of self-control bias and immediate gratification to the study of privacy decision making may offer a new perspective on the ongoing privacy debate. We have shown that a model of rational privacy behavior is unrealistic, while models based on psychological distortions offer a more accurate depiction of the decision process. We have shown why individuals who genuinely would like to protect their privacy may not do so because of psychological distortions well documented in the behavioral economics literature. We have highlighted that these distortions may affect not only na¨ıve individuals but also sophisticated ones.
Surprisingly, we have also found that these inconsistencies may occur when individuals perceive the risks from not protecting their privacy as significant.
Additional uncertainties, risk aversion, and varying attitudes towards losses and gains may be confounding elements in our analysis. Empirical validation is necessary to calibrate the effects of different factors.
An empirical analysis may start with the comparison of available data on the adoption rate of privacy technologies that offer immediate refuge from minor but pressing privacy concerns (for example, ‘do not call" marketing lists), with data on the adoption of privacy technologies that offer less obviously perceivable protection from more dangerous but also less visible privacy risks (for example, identity theft insurances). However, only an experimental approach over different periods of time in a controlled environment may allow us to disentangle the influence of several factors. Surveys alone cannot suffice, since we have shown why survey-time attitudes will rarely match decision-time actions. An experimental verification is part of our ongoing research agenda.
The psychological distortions we have discussed may be considered in the ongoing debate on how to deal with the privacy problem: industry self-regulation, users" self protection (through technology or other strategies), or government"s intervention. The conclusions we have reached suggest that individuals may not be trusted to make decisions in their best interests when it comes to privacy. This does not mean that privacy technologies are ineffective. On the contrary, our results, by aiming at offering a more realistic model of user-behavior, could be of help to technologists in their design of privacy enhancing tools. However, our results also imply that technology alone or awareness alone may not address the heart of the privacy problem. Improved technologies (with lower costs of adoption and protection) and more information about risks and opportunities certainly can help. However, more fundamental human behavioral mechanisms must also be addressed. Self-regulation, even in presence of complete information and awareness, may not be trusted to work for the same reasons. A combination of technology, awareness, and regulative policies - calibrated to generate and enforce liabilities and incentives for the appropriate parties - may be needed for privacy-related welfare increase (as in other areas of an economy: see on a related analysis [25]).
Observing that people do not want to pay for privacy or do not care about privacy, therefore, is only a half truth. People may not be able to act as economically rational agents when it comes to personal privacy. And the question whether do consumers care? is a different question from does privacy matter? Whether from an economic standpoint privacy ought to be protected or not, is still an open question. It is a question that involves defining specific contexts in which the concept of privacy is being invoked. But the value of privacy eventually goes beyond the realms of economic reasoning and cost benefit analysis, and ends up relating to one"s views on society and freedom. Still, even from a purely economic perspective, anecdotal evidence suggest that the costs of privacy (from spam to identity theft, lost sales, intrusions, and the like [30, 12, 17, 33, 26]) are high and increasing.
The author gratefully acknowledges Carnegie Mellon University"s Berkman Development Fund, that partially supported this research. The author also wishes to thank Jens Grossklags, Charis Kaskiris, and three anonymous referees for their helpful comments. 27

When money is donated to a charitable (or other) cause (hereafter referred to as charity), often the donating party gives unconditionally: a fixed amount is transferred from the donator to the charity, and none of this transfer is contingent on other events-in particular, it is not contingent on the amount given by other parties. Indeed, this is currently often the only way to make a donation, especially for small donating parties such as private individuals. However, when multiple parties support the same charity, each of them would prefer to see the others give more rather than less to this charity. In such scenarios, it is sensible for a party to use its contemplated donation as negotiating material to induce the others to give more. This is done by making the donation conditional on the others" donations. The following example will illustrate this, and show that the donating parties as well as the charitable cause may simultaneously benefit from the potential for such negotiation.
Suppose we have two parties, 1 and 2, who are both supporters of charity A. To either of them, it would be worth $0.75 if A received $1. It follows neither of them will be willing to give unconditionally, because $0.75 < $1. However, if the two parties draw up a contract that says that they will each give $0.5, both the parties have an incentive to accept this contract (rather than have no contract at all): with the contract, the charity will receive $1 (rather than $0 without a contract), which is worth $0.75 to each party, which is greater than the $0.5 that that party will have to give.
Effectively, each party has made its donation conditional on the other party"s donation, leading to larger donations and greater happiness to all parties involved. 51 One method that is often used to effect this is to make a matching offer. Examples of matching offers are: I will give x dollars for every dollar donated., or I will give x dollars if the total collected from other parties exceeds y.
In our example above, one of the parties can make the offer I will donate $0.5 if the other party also donates at least that much, and the other party will have an incentive to indeed donate $0.5, so that the total amount given to the charity increases by $1. Thus this matching offer implements the contract suggested above. As a real-world example, the United States government has authorized a donation of up to $1 billion to the Global Fund to fight AIDS, TB and Malaria, under the condition that the American contribution does not exceed one third of the total-to encourage other countries to give more [23].
However, there are several severe limitations to the simple approach of matching offers as just described.
offers where each party"s offer is stated in terms of the amount that the other pays. (For example, it is not clear what the outcome should be when both parties offer to match the other"s donation.) Thus, matching offers can only be based on payments made by parties that are giving unconditionally (not in terms of a matching offer)-or at least there can be no circular dependencies.1
offers, it is impractical to make a matching offer depend on the amounts given to multiple charities. For instance, a party may wish to specify that it will pay $100 given that charity A receives a total of $1000, but that it will also count donations made to charity B, at half the rate. (Thus, a total payment of $500 to charity A combined with a total payment of $1000 to charity B would be just enough for the party"s offer to take effect.) In contrast, in this paper we propose a new approach where each party can express its relative preferences for different charities, and make its offer conditional on its own appreciation for the vector of donations made to the different charities. Moreover, the amount the party offers to donate at different levels of appreciation is allowed to vary arbitrarily (it does need to be a dollar-for-dollar (or n-dollarfor-dollar) matching arrangement, or an arrangement where the party offers a fixed amount provided a given (strike) total has been exceeded). Finally, there is a clear interpretation of what it means when multiple parties are making conditional offers that are stated in terms of each other.
Given each combination of (conditional) offers, there is a (usually) unique solution which determines how much each party pays, and how much each charity is paid.
However, as we will show, finding this solution (the clearing problem) requires solving a potentially difficult optimization problem. A large part of this paper is devoted to studying how difficult this problem is under different assumptions on the structure of the offers, and providing algorithms for solving it. 1 Typically, larger organizations match offers of private individuals. For example, the American Red Cross Liberty Disaster Fund maintains a list of businesses that match their customers" donations [8].
Towards the end of the paper, we also study the mechanism design problem of motivating the bidders to bid truthfully.
In short, expressive negotiation over donations to charities is a new way in which electronic commerce can help the world. A web-based implementation of the ideas described in this paper can facilitate voluntary reallocation of wealth on a global scale. Aditionally, optimally solving the clearing problem (and thereby generating the maximum economic welfare) requires the application of sophisticated algorithms.
AUCTIONS AND EXCHANGES This section discusses the relationship between expressive charity donation and combinatorial auctions and exchanges.
It can be skipped, but may be of interest to the reader with a background in combinatorial auctions and exchanges.
In a combinatorial auction, there are m items for sale, and bidders can place bids on bundles of one or more items.
The auctioneer subsequently labels each bid as winning or losing, under the constraint that no item can be in more than one winning bid, to maximize the sum of the values of the winning bids. (This is known as the clearing problem.) Variants include combinatorial reverse auctions, where the auctioneer is seeking to procure a set of items; and combinatorial exchanges, where bidders can both buy and and sell items (even within the same bid). Other extensions include allowing for side constraints, as well as the specification of attributes of the items in bids. Combinatorial auctions and exchanges have recently become a popular research topic [20, 21, 17, 22, 9, 18, 13, 3, 12, 26, 19, 25, 2].
The problems of clearing expressive charity donation markets and clearing combinatorial auctions or exchanges are very different in formulation. Nevertheless, there are interesting parallels. One of the main reasons for the interest in combinatorial auctions and exchanges is that it allows for expressive bidding. A bidder can express exactly how much each different allocation is worth to her, and thus the globally optimal allocation may be chosen by the auctioneer. Compare this to a bidder having to bid on two different items in two different (one-item) auctions, without any way of expressing that (for instance) one item is worthless if the other item is not won. In this scenario, the bidder may win the first item but not the second (because there was another high bid on the second item that she did not anticipate), leading to economic inefficiency.
Expressive bidding is also one of the main benefits of the expressive charity donation market. Here, bidders can express exactly how much they are willing to donate for every vector of amounts donated to charities. This may allow bidders to negotiate a complex arrangement of who gives how much to which charity, which is beneficial to all parties involved; whereas no such arrangement may have been possible if the bidders had been restricted to using simple matching offers on individual charities. Again, expressive bidding is necessary to achieve economic efficiency.
Another parallel is the computational complexity of the clearing problem. In order to achieve the full economic efficiency allowed by the market"s expressiveness (or even come close to it), hard computational problems must be solved in combinatorial auctions and exchanges, as well as in the charity donation market (as we will see). 52
Throughout this paper, we will refer to the offers that the donating parties make as bids, and to the donating parties as bidders. In our bidding framework, a bid will specify, for each vector of total payments made to the charities, how much that bidder is willing to contribute. (The contribution of this bidder is also counted in the vector of paymentsso, the vector of total payments to the charities represents the amount given by all donating parties, not just the ones other than this bidder.) The bidding language is expressive enough that no bidder should have to make more than one bid. The following definition makes the general form of a bid in our framework precise.
Definition 1. In a setting with m charities c1, c2, . . . , cm, a bid by bidder bj is a function vj : Rm → R. The interpretation is that if charity ci receives a total amount of πci , then bidder j is willing to donate (up to) vj(πc1 , πc2 , . . . , πcm ).
We now define possible outcomes in our model, and which outcomes are valid given the bids that were made.
Definition 2. An outcome is a vector of payments made by the bidders (πb1 , πb2 , . . . , πbn ), and a vector of payments received by the charities (πc1 , πc2 , . . . , πcm ). A valid outcome is an outcome where
n j=1 πbj ≥ m i=1 πci (at least as much money is collected as is given away);
bidder gives more than she is willing to).
Of course, in the end, only one of the valid outcomes can be chosen. We choose the valid outcome that maximizes the objective that we have for the donation process.
Definition 3. An objective is a function from the set of all outcomes to R.2 After all bids have been collected, a valid outcome will be chosen that maximizes this objective.
One example of an objective is surplus, given by n j=1 πbj − m i=1 πci . The surplus could be the profits of a company managing the expressive donation marketplace; but, alternatively, the surplus could be returned to the bidders, or given to the charities. Another objective is total amount donated, given by m i=1 πci . (Here, different weights could also be placed on the different charities.) Finding the valid outcome that maximizes the objective is a (nontrivial) computational problem. We will refer to it as the clearing problem. The formal definition follows.
Definition 4 (DONATION-CLEARING). We are given a set of n bids over charities c1, c2, . . . , cm.
Additionally, we are given an objective function. We are asked to find an objective-maximizing valid outcome.
How difficult the DONATION-CLEARING problem is depends on the types of bids used and the language in which they are expressed. This is the topic of the next section. 2 In general, the objective function may also depend on the bids, but the objective functions under consideration in this paper do not depend on the bids. The techniques presented in this paper will typically generalize to objectives that take the bids into account directly.
Specifying a general bid in our framework (as defined above) requires being able to specify an arbitrary real-valued function over Rm . Even if we restricted the possible total payment made to each charity to the set {0, 1, 2, . . . , s}, this would still require a bidder to specify (s+1)m values. Thus, we need a bidding language that will allow the bidders to at least specify some bids more concisely. We will specify a bidding language that only represents a subset of all possible bids, which can be described concisely.3 To introduce our bidding language, we will first describe the bidding function as a composition of two functions; then we will outline our assumptions on each of these functions.
First, there is a utility function uj : Rm → R, specifying how much bidder j appreciates a given vector of total donations to the charities. (Note that the way we define a bidder"s utility function, it does not take the payments the bidder makes into account.) Then, there is a donation willingness function wj : R → R, which specifies how much bidder j is willing to pay given her utility for the vector of donations to the charities. We emphasize that this function does not need to be linear, so that utilities should not be thought of as expressible in dollar amounts. (Indeed, when an individual is donating to a large charity, the reason that the individual donates only a bounded amount is typically not decreasing marginal value of the money given to the charity, but rather that the marginal value of a dollar to the bidder herself becomes larger as her budget becomes smaller.) So, we have wj(uj(πc1 , πc2 , . . . , πcm )) = vj(πc1 , πc2 , . . . , πcm ), and we let the bidder describe her functions uj and wj separately. (She will submit these functions as her bid.) Our first restriction is that the utility that a bidder derives from money donated to one charity is independent of the amount donated to another charity. Thus, uj(πc1 , πc2 , . . . , πcm ) = m i=1 ui j(πci ). (We observe that this does not imply that the bid function vj decomposes similarly, because of the nonlinearity of wj.) Furthermore, each ui j must be piecewise linear. An interesting special case which we will study is when each ui j is a line: ui j(πci ) = ai jπci . This special case is justified in settings where the scale of the donations by the bidders is small relative to the amounts the charities receive from other sources, so that the marginal use of a dollar to the charity is not affected by the amount given by the bidders.
The only restriction that we place on the payment willingness functions wj is that they are piecewise linear. One interesting special case is a threshold bid, where wj is a step function: the bidder will provide t dollars if her utility exceeds s, and otherwise 0. Another interesting case is when such a bid is partially acceptable: the bidder will provide t dollars if her utility exceeds s; but if her utility is u < s, she is still willing to provide ut s dollars.
One might wonder why, if we are given the bidders" utility functions, we do not simply maximize the sum of the utilities rather than surplus or total donated. There are several reasons. First, because affine transformations do not affect utility functions in a fundamental way, it would be possi3 Of course, our bidding language can be trivially extended to allow for fully expressive bids, by also allowing bids from a fully expressive bidding language, in addition to the bids in our bidding language. 53 ble for a bidder to inflate her utility by changing its units, thereby making her bid more important for utility maximization purposes. Second, a bidder could simply give a payment willingness function that is 0 everywhere, and have her utility be taken into account in deciding on the outcome, in spite of her not contributing anything.
In an initial implementation, the approach of having donations made out to a center, and having a center forward these payments to charities, may not be desirable. Rather, it may be preferable to have a partially decentralized solution, where the donating parties write out checks to the charities directly according to a solution prescribed by the center. In this scenario, the center merely has to verify that parties are giving the prescribed amounts. Advantages of this include that the center can keep its legal status minimal, as well as that we do not require the donating parties to trust the center to transfer their donations to the charities (or require some complicated verification protocol). It is also a step towards a fully decentralized solution, if this is desirable.
To bring this about, we can still use the approach described earlier. After we clear the market in the manner described before, we know the amount that each donator is supposed to give, and the amount that each charity is supposed to receive. Then, it is straightforward to give some specification of who should give how much to which charity, that is consistent with that clearing. Any greedy algorithm that increases the cash flow from any bidder who has not yet paid enough, to any charity that has not yet received enough, until either the bidder has paid enough or the charity has received enough, will provide such a specification. (All of this is assuming that bj πbj = ci πci . In the case where there is nonzero surplus, that is, bj πbj > ci πci , we can distribute this surplus across the bidders by not requiring them to pay the full amount, or across the charities by giving them more than the solution specifies.) Nevertheless, with this approach, a bidder may have to write out a check to a charity that she does not care for at all. (For example, an environmental activist who was using the system to increase donations to a wildlife preservation fund may be required to write a check to a group supporting a right-wing political party.) This is likely to lead to complaints and noncompliance with the clearing. We can address this issue by letting each bidder specify explicitly (before the clearing) which charities she would be willing to make a check out to. These additional constraints, of course, may change the optimal solution. In general, checking whether a given centralized solution (with zero surplus) can be accomplished through decentralized payments when there are such constraints can be modeled as a MAX-FLOW problem. In the MAX-FLOW instance, there is an edge from the source node s to each bidder bj, with a capacity of πbj (as specified in the centralized solution); an edge from each bidder bj to each charity ci that the bidder is willing to donate money to, with a capacity of ∞; and an edge from each charity ci to the target node t with capacity πci (as specified in the centralized solution).
In the remainder of this paper, all our hardness results apply even to the setting where there is no constraint on which bidders can pay to which charity (that is, even the problem as it was specified before this section is hard). We also generalize our clearing algorithms to the partially decentralized case with constraints.
MARKET In this section, we will show that the clearing problem is completely inapproximable, even when every bidder"s utility function is linear (with slope 0 or 1 in each charity"s payments), each bidder cares either about at most two charities or about all charities equally, and each bidder"s payment willingness function is a step function. We will reduce from MAX2SAT (given a formula in conjunctive normal form (where each clause has two literals) and a target number of satisfied clauses T, does there exist an assignment of truth values to the variables that makes at least T clauses true?), which is NP-complete [7].
Theorem 1. There exists a reduction from MAX2SAT instances to DONATION-CLEARING instances such that
only valid outcome is the zero outcome (no bidder pays anything and no charity receives anything); 2. Otherwise, there exists a solution with positive surplus. Additionally, the DONATION-CLEARING instances that we reduce to have the following properties: 1. Every ui j is a line; that is, the utility that each bidder derives from any charity is linear; 2.
All the ui j have slope either 0 or 1; 3. Every bidder either has at most 2 charities that affect her utility (with slope 1), or all charities affect her utility (with slope 1); 4. Every bid is a threshold bid; that is, every bidder"s payment willingness function wj is a step function.
Proof. The problem is in NP because we can nondeterministically choose the payments to be made and received, and check the validity and objective value of this outcome.
In the following, we will represent bids as follows: ({(ck, ak)}, s, t) indicates that uk j (πck ) = akπck (this function is 0 for ck not mentioned in the bid), and wj(uj) = t for uj ≥ s, wj(uj) = 0 otherwise.
To show NP-hardness, we reduce an arbitrary MAX2SAT instance, given by a set of clauses K = {k} = {(l1 k, l2 k)} over a variable set V together with a target number of satisfied clauses T, to the following DONATION-CLEARING instance. Let the set of charities be as follows. For every literal l ∈ L, there is a charity cl. Then, let the set of bids be as follows. For every variable v, there is a bid bv = ({(c+v, 1), (c−v, 1)}, 2, 1 − 1 4|V | ). For every literal l, there is a bid bl = ({(cl, 1)}, 2, 1). For every clause k = {l1 k, l2 k} ∈ K, there is a bid bk = ({(cl1 k , 1), (cl2 k , 1)}, 2, 1 8|V ||K| ). Finally, there is a single bid that values all charities equally: b0 = ({(c1, 1), (c2, 1), . . . , (cm, 1)}, 2|V |+ T 8|V ||K| , 1 4 + 1 16|V ||K| ). We show the two instances are equivalent.
First, suppose there exists a solution to the MAX2SAT instance. If in this solution, l is true, then let πcl = 2 + T 8|V |2|K| ; otherwise πcl = 0. Also, the only bids that are not accepted (meaning the threshold is not met) are the bl where l is false, and the bk such that both of l1 k, l2 k are false.
First we show that no bidder whose bid is accepted pays more than she is willing to. For each bv, either c+v or c−v receives at least 2, so this bidder"s threshold has been met. 54 For each bl, either l is false and the bid is not accepted, or l is true, cl receives at least 2, and the threshold has been met.
For each bk, either both of l1 k, l2 k are false and the bid is not accepted, or at least one of them (say li k) is true (that is, k is satisfied) and cli k receives at least 2, and the threshold has been met. Finally, because the total amount received by the charities is 2|V | + T 8|V ||K| , b0"s threshold has also been met.
The total amount that can be extracted from the accepted bids is at least |V |(1− 1 4|V | )+|V |+T 1 8|V ||K| + 1 4 + 1 16|V ||K| ) = 2|V |+ T 8|V ||K| + 1 16|V ||K| > 2|V |+ T 8|V ||K| , so there is positive surplus. So there exists a solution with positive surplus to the DONATION-CLEARING instance.
Now suppose there exists a nonzero outcome in the DONATION-CLEARING instance. First we show that it is not possible (for any v ∈ V ) that both b+v and b−v are accepted. For, this would require that πc+v + πc−v ≥ 4.
The bids bv, b+v, b−v cannot contribute more than 3, so we need another 1 at least. It is easily seen that for any other v , accepting any subset of {bv , b+v , b−v } would require that at least as much is given to c+v and c−v as can be extracted from these bids, so this cannot help.
Finally, all the other bids combined can contribute at most |K| 1 8|V ||K| + 1 4 + 1 16|V ||K| < 1. It follows that we can interpret the outcome in the DONATION-CLEARING instance as a partial assignment of truth values to variables: v is set to true if b+v is accepted, and to false if b−v is accepted. All that is left to show is that this partial assignment satisfies at least T clauses.
First we show that if a clause bid bk is accepted, then either bl1 k or bl2 k is accepted (and thus either l1 k or l2 k is set to true, hence k is satisfied). If bk is accepted, at least one of cl1 k and cl2 k must be receiving at least 1; without loss of generality, say it is cl1 k , and say l1 k corresponds to variable v1 k (that is, it is +v1 k or −v1 k). If cl1 k does not receive at least 2, bl1 k is not accepted, and it is easy to check that the bids bv1 k , b+v1 k , b−v1 k contribute (at least) 1 less than is paid to c+v1 k and c+v1 k . But this is the same situation that we analyzed before, and we know it is impossible. All that remains to show is that at least T clause bids are accepted.
We now show that b0 is accepted. Suppose it is not; then one of the bv must be accepted. (The solution is nonzero by assumption; if only some bk are accepted, the total payment from these bids is at most |K| 1 8|V ||K| < 1, which is not enough for any bid to be accepted; and if one of the bl is accepted, then the threshold for the corresponding bv is also reached.) For this v, bv1 k , b+v1 k , b−v1 k contribute (at least) 1 4|V | less than the total payments to c+v and c−v. Again, the other bv and bl cannot (by themselves) help to close this gap; and the bk can contribute at most |K| 1 8|V ||K| < 1 4|V | .
It follows that b0 is accepted.
Now, in order for b0 to be accepted, a total of 2|V |+ T 8|V ||K| must be donated. Because is not possible (for any v ∈ V ) that both b+v and b−v are accepted, it follows that the total payment by the bv and the bl can be at most 2|V | − 1 4 .
Adding b0"s payment of 1 4 + 1 16|V ||K| to this, we still need T − 1 2 8|V ||K| from the bk. But each one of them contributes at most 1 8|V ||K| , so at least T of them must be accepted.
Corollary 1. Unless P=NP, there is no polynomial-time algorithm for approximating DONATION-CLEARING (with either the surplus or the total amount donated as the objective) within any ratio f(n), where f is a nonzero function of the size of the instance. This holds even if the DONATIONCLEARING structures satisfy all the properties given in Theorem 1.
Proof. Suppose we had such a polynomial time algorithm, and applied it to the DONATION-CLEARING instances that were reduced from MAX2SAT instances in Theorem 1. It would return a nonzero solution when the MAX2SAT instance has a solution, and a zero solution otherwise. So we can decide whether arbitrary MAX2SAT instances are satisfiable this way, and it would follow that P=NP. (Solving the problem to optimality is NP-complete in many other (noncomparable or even more restricted) settings as well-we omit such results because of space constraint.) This should not be interpreted to mean that our approach is infeasible. First, as we will show, there are very expressive families of bids for which the problem is solvable in polynomial time. Second, NP-completeness is often overcome in practice (especially when the stakes are high). For instance, even though the problem of clearing combinatorial auctions is NP-complete [20] (even to approximate [21]), they are typically solved to optimality in practice.
FORMULATION In this section, we give a mixed integer programming (MIP) formulation for the general problem. We also discuss in which special cases this formulation reduces to a linear programming (LP) formulation. In such cases, the problem is solvable in polynomial time, because linear programs can be solved in polynomial time [11].
The variables of the MIP defining the final outcome are the payments made to the charities, denoted by πci , and the payments extracted from the bidders, πbj . In the case where we try to avoid direct payments and let the bidders pay the charities directly, we add variables πci,bj indicating how much bj pays to ci, with the constraints that for each ci, πci ≤ bj πci,bj ; and for each bj, πbj ≥ ci πci,bj .
Additionally, there is a constraint πci,bj = 0 whenever bidder bj is unwilling to pay charity ci. The rest of the MIP can be phrased in terms of the πci and πbj .
The objectives we have discussed earlier are both linear: surplus is given by n j=1 πbj − m i=1 πci , and total amount donated is given by m i=1 πci (coefficients can be added to represent different weights on the different charities in the objective).
The constraint that the outcome should be valid (no deficit) is given simply by: n j=1 πbj ≥ m i=1 πci .
For every bidder, for every charity, we define an additional utility variable ui j indicating the utility that this bidder derives from the payment to this charity. The bidder"s total 55 utility is given by another variable uj, with the constraint that uj = m i=1 ui j.
Each ui j is given as a function of πci by the (piecewise linear) function provided by the bidder. In order to represent this function in the MIP formulation, we will merely place upper bounding constraints on ui j, so that it cannot exceed the given functions. The MIP solver can then push the ui j variables all the way up to the constraint, in order to extract as much payment from this bidder as possible.
In the case where the ui j are concave, this is easy: if (sl, tl) and (sl+1, tl+1) are endpoints of a finite linear segment in the function, we add the constraint that ui j ≤ tl + πci −sl sl+1−sl (tl+1 − tl). If the final (infinite) segment starts at (sk, tk) and has slope d, we add the constraint that ui j ≤ tk + d(πci − sk).
Using the fact that the function is concave, for each value of πci , the tightest upper bound on ui j is the one corresponding to the segment above that value of πci , and therefore these constraints are sufficient to force the correct value of ui j.
When the function is not concave, we require (for the first time) some binary variables. First, we define another point on the function: (sk+1, tk+1) = (sk + M, tk + dM), where d is the slope of the infinite segment and M is any upper bound on the πcj . This has the effect that we will never be on the infinite segment again. Now, let xi,j l be an indicator variable that should be 1 if πci is below the lth segment of the function, and 0 otherwise. To effect this, first add a constraint k l=0 xi,j l = 1. Now, we aim to represent πci as a weighted average of its two neighboring si,j l . For 0 ≤ l ≤ k + 1, let λi,j l be the weight on si,j l . We add the constraint k+1 l=0 λi,j l = 1. Also, for 0 ≤ l ≤ k + 1, we add the constraint λi,j l ≤ xl−1 +xl (where x−1 and xk+1 are defined to be zero), so that indeed only the two neighboring si,j l have nonzero weight. Now we add the constraint πci = k+1 l=0 si,j l λi,j l , and now the λi,j l must be set correctly. Then, we can set ui j = k+1 l=0 ti,j l λi,j l . (This is a standard MIP technique [16].) Finally, each πbj is bounded by a function of uj by the (piecewise linear) function provided by the bidder (wj).
Representing this function is entirely analogous to how we represented ui j as a function of πci . (Again we will need binary variables only if the function is not concave.) Because we only use binary variables when either a utility function ui j or a payment willingness function wj is not concave, it follows that if all of these are concave, our MIP formulation is simply a linear program-which can be solved in polynomial time. Thus: Theorem 2. If all functions ui j and wj are concave (and piecewise linear), the DONATION-CLEARING problem can be solved in polynomial time using linear programming.
Even if some of these functions are not concave, we can simply replace each such function by the smallest upper bounding concave function, and use the linear programming formulation to obtain an upper bound on the objectivewhich may be useful in a search formulation of the general problem.
BETTER THAN LINEAR PROGRAMMING One may wonder if, for the special cases of the DONATIONCLEARING problem that can be solved in polynomial time with linear programming, there exist special purpose algorithms that are much faster than linear programming algorithms. In this section, we show that this is not the case.
We give a reduction from (the decision variant of) the general linear programming problem to (the decision variant of) a special case of the DONATION-CLEARING problem (which can be solved in polynomial time using linear programming). (The decision variant of an optimization problem asks the binary question: Can the objective value exceed o?) Thus, any special-purpose algorithm for solving the decision variant of this special case of the DONATIONCLEARING problem could be used to solve a decision question about an arbitrary linear program just as fast. (And thus, if we are willing to call the algorithm a logarithmic number of times, we can solve the optimization version of the linear program.) We first observe that for linear programming, a decision question about the objective can simply be phrased as another constraint in the LP (forcing the objective to exceed the given value); then, the original decision question coincides with asking whether the resulting linear program has a feasible solution.
Theorem 3. The question of whether an LP (given by a set of linear constraints4 ) has a feasible solution can be modeled as a DONATION-CLEARING instance with payment maximization as the objective, with 2v charities and v + c bids (where v is the number of variables in the LP, and c is the number of constraints). In this model, each bid bj has only linear ui j functions, and is a partially acceptable threshold bid (wj(u) = tj for u ≥ sj, otherwise wj(u) = utj sj ). The v bids corresponding to the variables mention only two charities each; the c bids corresponding to the constraints mention only two times the number of variables in the corresponding constraint.
Proof. For every variable xi in the LP, let there be two charities, c+xi and c−xi . Let H be some number such that if there is a feasible solution to the LP, there is one in which every variable has absolute value at most H.
In the following, we will represent bids as follows: ({(ck, ak)}, s, t) indicates that uk j (πck ) = akπck (this function is 0 for ck not mentioned in the bid), and wj(uj) = t for uj ≥ s, wj(uj) = uj t s otherwise.
For every variable xi in the LP, let there be a bid bxi = ({(c+xi , 1), (c−xi , 1)}, 2H, 2H − c v ). For every constraint i rj i xi ≤ sj in the linear program, let there be a bid bj = ({(c−xi , rj i )}i:r j i >0 ∪ {(c+xi , −rj i )}i:r j i <0 , ( i |rj i |)H − sj, 1).
Let the target total amount donated be 2vH.
Suppose there is a feasible solution (x∗ 1, x∗ 2, . . . , x∗ v) to the LP. Without loss of generality, we can suppose that |x∗ i | ≤ H for all i. Then, in the DONATION-CLEARING instance, 4 These constraints must include bounds on the variables (including nonnegativity bounds), if any. 56 for every i, let πc+xi = H + x∗ i , and let πc−xi = H − x∗ i (for a total payment of 2H to these two charities). This allows us to extract the maximum payment from the bids bxi -a total payment of 2vH − c. Additionally, the utility of bidder bj is now i:r j i >0 rj i (H − x∗ i ) + i:r j i <0 −rj i (H + x∗ i ) = ( i |rj i |)H − i rj i x∗ i ≥ ( i |rj i |)H − sj (where the last inequality stems from the fact that constraint j must be satisfied in the LP solution), so it follows we can extract the maximum payment from all the bidders bj, for a total payment of c. It follows that we can extract the required 2vH payment from the bidders, and there exists a solution to the DONATION-CLEARING instance with a total amount donated of at least 2vH.
Now suppose there is a solution to the DONATIONCLEARING instance with a total amount donated of at least vH. Then the maximum payment must be extracted from each bidder. From the fact that the maximum payment must be extracted from each bidder bxi , it follows that for each i, πc+xi + πc−xi ≥ 2H. Because the maximum extractable total payment is 2vH, it follows that for each i, πc+xi + πc−xi = 2H. Let x∗ i = πc+xi − H = H − πc−xi .
Then, from the fact that the maximum payment must be extracted from each bidder bj, it follows that ( i |rj i |)H − sj ≤ i:r j i >0 rj i πc−xi + i:r j i <0 −rj i πc+xi = i:r j i >0 rj i (H − x∗ i ) + i:r j i <0 −rj i (H + x∗ i ) = ( i |rj i |)H − i rj i x∗ i . Equivalently, i rj i x∗ i ≤ sj. It follows that the x∗ i constitute a feasible solution to the LP.
Another class of bids of interest is the class of quasilinear bids. In a quasilinear bid, the bidder"s payment willingness function is linear in utility: that is, wj = uj. (Because the units of utility are arbitrary, we may as well let them correspond exactly to units of money-so we do not need a constant multiplier.) In most cases, quasilinearity is an unreasonable assumption: for example, usually bidders have a limited budget for donations, so that the payment willingness will stop increasing in utility after some point (or at least increase slower in the case of a softer budget constraint). Nevertheless, quasilinearity may be a reasonable assumption in the case where the bidders are large organizations with large budgets, and the charities are a few small projects requiring relatively little money. In this setting, once a certain small amount has been donated to a charity, a bidder will derive no more utility from more money being donated from that charity. Thus, the bidders will never reach a high enough utility for their budget constraint (even when it is soft) to take effect, and thus a linear approximation of their payment willingness function is reasonable.
Another reason for studying the quasilinear setting is that it is the easiest setting for mechanism design, which we will discuss shortly. In this section, we will see that the clearing problem is much easier in the case of quasilinear bids.
First, we address the case where we are trying to maximize surplus (which is the most natural setting for mechanism design). The key observation here is that when bids are quasilinear, the clearing problem decomposes across charities.
Lemma 1. Suppose all bids are quasilinear, and surplus is the objective. Then we can clear the market optimally by clearing the market for each charity individually. That is, for each bidder bj, let πbj = ci πbi j . Then, for each charity ci, maximize ( bj πbi j ) − πci , under the constraint that for every bidder bj, πbi j ≤ ui j(πci ).
Proof. The resulting solution is certainly valid: first of all, at least as much money is collected as is given away, because bj πbj − ci πci = bj ci πbi j − ci πci = ci (( bj πbi j ) − πci )-and the terms of this summation are the objectives of the individual optimization problems, each of which can be set at least to 0 (by setting all the variables are set to 0), so it follows that the expression is nonnegative. Second, no bidder bj pays more than she is willing to, because uj −πbj = ci ui j(πci )− ci πbi j = ci (ui j(πci )−πbi j )-and the terms of this summation are nonnegative by the constraints we imposed on the individual optimization problems.
All that remains to show is that the solution is optimal. Because in an optimal solution, we will extract as much payment from the bidders as possible given the πci , all we need to show is that the πci are set optimally by this approach. Let π∗ ci be the amount paid to charity πci in some optimal solution. If we change this amount to πci and leave everything else unchanged, this will only affect the payment that we can extract from the bidders because of this particular charity, and the difference in surplus will be bj ui j(πci ) − ui j(π∗ ci ) − πci + π∗ ci . This expression is, of course, 0 if πci = π∗ ci . But now notice that this expression is maximized as a function of πci by the decomposed solution for this charity (the terms without πci in them do not matter, and of course in the decomposed solution we always set πbi j = ui j(πci )). It follows that if we change πci to the decomposed solution, the change in surplus will be at least
the πci one by one to the decomposed solution without ever losing any surplus.
Theorem 4. When all bids are quasilinear and surplus is the objective, DONATION-CLEARING can be done in linear time.
Proof. By Lemma 1, we can solve the problem separately for each charity. For charity ci, this amounts to maximizing ( bj ui j(πci )) − πci as a function of πci . Because all its terms are piecewise linear functions, this whole function is piecewise linear, and must be maximized at one of the points where it is nondifferentiable. It follows that we need only check all the points at which one of the terms is nondifferentiable.
Unfortunately, the decomposing lemma does not hold for payment maximization.
Proposition 1. When the objective is payment maximization, even when bids are quasilinear, the solution obtained by decomposing the problem across charities is in general not optimal (even with concave bids). 57 Proof. Consider a single bidder b1 placing the following quasilinear bid over two charities c1 and c2: u1 1(πc1 ) = 2πci for 0 ≤ πci ≤ 1, u1 1(πc1 ) = 2 + πci −1 4 otherwise; u2 1(πc2 ) = πci 2 . The decomposed solution is πc1 = 7 3 , πc2 = 0, for a total donation of 7 3 . But the solution πc1 = 1, πc2 = 2 is also valid, for a total donation of 3 > 7 3 .
In fact, when payment maximization is the objective,
DONATION-CLEARING remains (weakly) NP-complete in general. (In the remainder of the paper, proofs are omitted because of space constraint.) Theorem 5. DONATION-CLEARING is (weakly) NPcomplete when payment maximization is the objective, even when every bid is concerns only one charity (and has a stepfunction utility function for this charity), and is quasilinear.
However, when the bids are also concave, a simple greedy clearing algorithm is optimal.
Theorem 6. Given a DONATION-CLEARING instance with payment maximization as the objective where all bids are quasilinear and concave, consider the following algorithm. Start with πci = 0 for all charities. Then, letting γci = d bj ui j (πci ) dπci (at nondifferentiable points, these derivatives should be taken from the right), increase πc∗ i (where c∗ i ∈ arg maxci γci ), until either γc∗ i is no longer the highest (in which case, recompute c∗ i and start increasing the corresponding payment), or bj uj = ci πci and γc∗ i < 1. Finally, let πbj = uj. (A similar greedy algorithm works when the objective is surplus and the bids are quasilinear and concave, with as only difference that we stop increasing the payments as soon as γc∗ i < 1.)
Up to this point, we have not discussed the bidders" incentives for bidding any particular way. Specifically, the bids may not truthfully reflect the bidders" preferences over charities because a bidder may bid strategically, misrepresenting her preferences in order to obtain a result that is better to herself. This means the mechanism is not strategy-proof. (We will show some concrete examples of this shortly.) This is not too surprising, because the mechanism described so far is, in a sense, a first-price mechanism, where the mechanism will extract as much payment from a bidder as her bid allows. Such mechanisms (for example, first-price auctions, where winners pay the value of their bids) are typically not strategy-proof: if a bidder reports her true valuation for an outcome, then if this outcome occurs, the payment the bidder will have to make will offset her gains from the outcome completely. Of course, we could try to change the rules of the game-which outcome (payment vector to charities) do we select for which bid vector, and which bidder pays how much-in order to make bidding truthfully beneficial, and to make the outcome better with regard to the bidders" true preferences. This is the field of mechanism design. In this section, we will briefly discuss the options that mechanism design provides for the expressive charity donation problem.
mechanism We first point out some reasons for bidders to misreport their preferences under the first-price mechanism described in the paper up to this point. First of all, even when there is only one charity, it may make sense to underbid one"s true valuation for the charity. For example, suppose a bidder would like a charity to receive a certain amount x, but does not care if the charity receives more than that. Additionally, suppose that the other bids guarantee that the charity will receive at least x no matter what bid the bidder submits (and the bidder knows this). Then the bidder is best off not bidding at all (or submitting a utility for the charity of 0), to avoid having to make any payment. (This is known in economics as the free rider problem [14].
With multiple charities, another kind of manipulation may occur, where the bidder attempts to steer others" payments towards her preferred charity. Suppose that there are two charities, and three bidders. The first bidder bids u1 1(πc1 ) =
1(πc1 ) = 0 otherwise; u2 1(πc2 ) = 1 if πc2 ≥ 1, u2 1(πc2 ) = 0 otherwise; and w1(u1) = u1 if u1 ≤ 1, w1(u1) = 1+ 1 100 (u1 −1) otherwise. The second bidder bids u1 2(πc1 ) =
1(πc1 ) = 0 otherwise; u2 2(πc2 ) = 0 (always); w2(u2) = 1 4 u2 if u2 ≤ 1, w2(u2) = 1 4 + 1 100 (u2 −1) otherwise.
Now, the third bidder"s true preferences are accurately represented5 by the bid u1 3(πc1 ) = 1 if πc1 ≥ 1, u1 3(πc1 ) = 0 otherwise; u2 3(πc2 ) = 3 if πc2 ≥ 1, u2 3(πc1 ) = 0 otherwise; and w3(u3) = 1 3 u3 if u3 ≤ 1, w3(u3) = 1 3 + 1 100 (u3 − 1) otherwise. Now, it is straightforward to check that, if the third bidder bids truthfully, regardless of whether the objective is surplus maximization or total donated, charity 1 will receive at least 1, and charity 2 will receive less than 1. The same is true if bidder 3 does not place a bid at all (as in the previous type of manipulation); hence bidder 2"s utility will be 1 in this case. But now, if bidder 3 reports u1 3(πc1 ) = 0 everywhere; u2 3(πc2 ) = 3 if πc2 ≥ 1, u2 3(πc2 ) = 0 otherwise (this part of the bid is truthful); and w3(u3) = 1 3 u3 if u3 ≤ 1, w3(u3) = 1 3 otherwise; then charity 2 will receive at least 1, and bidder 3 will have to pay at most 1 3 . Because up to this amount of payment, one unit of money corresponds to three units of utility to bidder 3, it follows his utility is now at least 3 − 1 = 2 > 1. We observe that in this case, the strategic bidder is not only affecting how much the bidders pay, but also how much the charities receive.
setting There are four reasons why the mechanism design approach is likely to be most successful in the setting of quasilinear preferences. First, historically, mechanism design has been been most successful when the quasilinear assumption could be made. Second, because of this success, some very general mechanisms have been discovered for the quasilinear setting (for instance, the VCG mechanisms [24, 4, 10], or the dAGVA mechanism [6, 1]) which we could apply directly to the expressive charity donation problem. Third, as we saw in Section 9, the clearing problem is much easier in 5 Formally, this means that if the bidder is forced to pay the full amount that his bid allows for a particular vector of payments to charities, the bidder is indifferent between this and not participating in the mechanism at all. (Compare this to bidding truthfully in a first-price auction.) 58 this setting, and thus we are less likely to run into computational trouble for the mechanism design problem. Fourth, as we will show shortly, the quasilinearity assumption in some cases allows for decomposing the mechanism design problem over the charities (as it did for the simple clearing problem).
Moreover, in the quasilinear setting (unlike in the general setting), it makes sense to pursue social welfare (the sum of the utilities) as the objective, because now 1) units of utility correspond directly to units of money, so that we do not have the problem of the bidders arbitrarily scaling their utilities; and 2) it is no longer possible to give a payment willingness function of 0 while still affecting the donations through a utility function.
Before presenting the decomposition result, we introduce some terms from game theory. A type is a preference profile that a bidder can have and can report (thus, a type report is a bid). Incentive compatibility (IC) means that bidders are best off reporting their preferences truthfully; either regardless of the others" types (in dominant strategies), or in expectation over them (in Bayes-Nash equilibrium).
Individual rationality (IR) means agents are at least as well off participating in the mechanism as not participating; either regardless of the others" types (ex-post), or in expectation over them (ex-interim). A mechanism is budget balanced if there is no flow of money into or out of the system-in general (ex-post), or in expectation over the type reports (ex-ante). A mechanism is efficient if it (always) produces the efficient allocation of wealth to charities.
Theorem 7. Suppose all agents" preferences are quasilinear. Furthermore, suppose that there exists a single-charity mechanism M that, for a certain subclass P of (quasilinear) preferences, under a given solution concept S (implementation in dominant strategies or Bayes-Nash equilibrium) and a given notion of individual rationality R (ex post, ex interim, or none), satisfies a certain notion of budget balance (ex post, ex ante, or none), and is ex-post efficient. Then there exists such a mechanism for any number of charities.
Two mechanisms that satisfy efficiency (and can in fact be applied directly to the multiple-charity problem without use of the previous theorem) are the VCG (which is incentive compatible in dominant strategies) and dAGVA (which is incentive compatible only in Bayes-Nash equilibrium) mechanisms. Each of them, however, has a drawback that would probably make it impractical in the setting of donations to charities. The VCG mechanism is not budget balanced. The dAGVA mechanism does not satisfy ex-post individual rationality. In the next subsection, we will investigate if we can do better in the setting of donations to charities.
In this subsection, we show that even in a very restricted setting, and with minimal requirements on IC and IR constraints, it is impossible to create a mechanism that is efficient.
Theorem 8. There is no mechanism which is ex-post budget balanced, ex-post efficient, and ex-interim individually rational with Bayes-Nash equilibrium as the solution concept (even with only one charity, only two quasilinear bidders, with identical type distributions (uniform over two types, with either both utility functions being step functions or both utility functions being concave piecewise linear functions)).
The case of step-functions in this theorem corresponds exactly to the case of a single, fixed-size, nonexcludable public good (the public good being that the charity receives the desired amount)-for which such an impossibility result is already known [14]. Many similar results are known, probably the most famous of which is the Myerson-Satterthwaite impossibility result, which proves the impossibility of efficient bilateral trade under the same requirements [15].
Theorem 7 indicates that there is no reason to decide on donations to multiple charities under a single mechanism (rather than a separate one for each charity), when an efficient mechanism with the desired properties exists for the single-charity case. However, because under the requirements of Theorem 8, no such mechanism exists, there may be a benefit to bringing the charities under the same umbrella. The next proposition shows that this is indeed the case.
Proposition 2. There exist settings with two charities where there exists no ex-post budget balanced, ex-post efficient, and ex-interim individually rational mechanism with Bayes-Nash equilibrium as the solution concept for either charity alone; but there exists an ex-post budget balanced, ex-post efficient, and ex-post individually rational mechanism with dominant strategies as the solution concept for both charities together. (Even when the conditions are the same as in Theorem 8, apart from the fact that there are now two charities.)
We introduced a bidding language for expressing very general types of matching offers over multiple charities. We formulated the corresponding clearing problem (deciding how much each bidder pays, and how much each charity receives), and showed that it is NP-complete to approximate to any ratio even in very restricted settings. We gave a mixed-integer program formulation of the clearing problem, and showed that for concave bids (where utility functions and payment willingness function are concave), the program reduces to a linear program and can hence be solved in polynomial time.
We then showed that the clearing problem for a subclass of concave bids is at least as hard as the decision variant of linear programming, suggesting that we cannot do much better than a linear programming implementation for such bids.
Subsequently, we showed that the clearing problem is much easier when bids are quasilinear (where payment willingness functions are linear)-for surplus, the problem decomposes across charities, and for payment maximization, a greedy approach is optimal if the bids are concave (although this latter problem is weakly NP-complete when the bids are not concave). For the quasilinear setting, we studied the mechanism design question of making the bidders report their preferences truthfully rather than strategically. We showed that an ex-post efficient mechanism is impossible even with only one charity and a very restricted class of bids. We also showed that even though the clearing problem decomposes over charities in the quasilinear setting, there may be benefits to linking the charities from a mechanism design standpoint.
There are many directions for future research. One is to build a web-based implementation of the (first-price) mechanism proposed in this paper. Another is to study the computational scalability of our MIP/LP approach. It is also 59 important to identify other classes of bids (besides concave ones) for which the clearing problem is tractable. Much crucial work remains to be done on the mechanism design problem. Finally, are there good iterative mechanisms for charity donation?6
[1] K. Arrow. The property rights doctrine and demand revelation under incomplete information. In M. Boskin, editor, Economics and human welfare.
New York Academic Press, 1979. [2] L. M. Ausubel and P. Milgrom. Ascending auctions with package bidding. Frontiers of Theoretical Economics, 1, 2002. No. 1, Article 1. [3] Y. Bartal, R. Gonen, and N. Nisan. Incentive compatible multi-unit combinatorial auctions. In Theoretical Aspects of Rationality and Knowledge (TARK IX), Bloomington, Indiana, USA, 2003. [4] E. H. Clarke. Multipart pricing of public goods. Public Choice, 11:17-33, 1971. [5] V. Conitzer and T. Sandholm. Complexity of mechanism design. In Proceedings of the 18th Annual Conference on Uncertainty in Artificial Intelligence (UAI-02), pages 103-110, Edmonton, Canada, 2002. [6] C. d"Aspremont and L. A. G´erard-Varet. Incentives and incomplete information. Journal of Public Economics, 11:25-45, 1979. [7] M. R. Garey, D. S. Johnson, and L. Stockmeyer. Some simplified NP-complete graph problems. Theoretical Computer Science, 1:237-267, 1976. [8] D. Goldburg and S. McElligott. Red cross statement on official donation locations. 2001. Press release, http://www.redcross.org/press/disaster/ds pr/ 011017legitdonors.html. [9] R. Gonen and D. Lehmann. Optimal solutions for multi-unit combinatorial auctions: Branch and bound heuristics. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 13-20,
Minneapolis, MN, Oct. 2000. [10] T. Groves. Incentives in teams. Econometrica, 41:617-631, 1973. [11] L. Khachiyan. A polynomial algorithm in linear programming. Soviet Math. Doklady, 20:191-194,
[12] R. Lavi, A. Mu"Alem, and N. Nisan. Towards a characterization of truthful combinatorial auctions. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), 2003. [13] D. Lehmann, L. I. O"Callaghan, and Y. Shoham.
Truth revelation in rapid, approximately efficient combinatorial auctions. Journal of the ACM, 49(5):577-602, 2002. Early version appeared in ACMEC-99. 6 Compare, for example, iterative mechanisms in the combinatorial auction setting [19, 25, 2]. [14] A. Mas-Colell, M. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, 1995. [15] R. Myerson and M. Satterthwaite. Efficient mechanisms for bilateral trading. Journal of Economic Theory, 28:265-281, 1983. [16] G. L. Nemhauser and L. A. Wolsey. Integer and Combinatorial Optimization. John Wiley & Sons,
[17] N. Nisan. Bidding and allocation in combinatorial auctions. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 1-12,
Minneapolis, MN, 2000. [18] N. Nisan and A. Ronen. Computationally feasible VCG mechanisms. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 242-252, Minneapolis, MN, 2000. [19] D. C. Parkes. iBundle: An efficient ascending price bundle auction. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 148-157,
Denver, CO, Nov. 1999. [20] M. H. Rothkopf, A. Pekeˇc, and R. M. Harstad.
Computationally manageable combinatorial auctions.
Management Science, 44(8):1131-1147, 1998. [21] T. Sandholm. Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 135:1-54, Jan. 2002. Conference version appeared at the International Joint Conference on Artificial Intelligence (IJCAI), pp. 542-547,
Stockholm, Sweden, 1999. [22] T. Sandholm, S. Suri, A. Gilpin, and D. Levine.
CABOB: A fast optimal algorithm for combinatorial auctions. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence (IJCAI), pages 1102-1108, Seattle, WA,
[23] J. Tagliabue. Global AIDS Funds Is Given Attention, but Not Money. The New York Times, June 1, 2003.
Reprinted on http://www.healthgap.org/press releases/a03/
[24] W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. Journal of Finance, 16:8-37, 1961. [25] P. R. Wurman and M. P. Wellman. AkBA: A progressive, anonymous-price combinatorial auction.
In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 21-29, Minneapolis,

We consider the problem of online scheduling of jobs on a single processor. Each job is characterized by a release time, a deadline, a processing time, and a value for successful completion by its deadline. The objective is to maximize the sum of the values of the jobs completed by their respective deadlines. The key challenge in this online setting is that the schedule must be constructed in real-time, even though nothing is known about a job until its release time.
Competitive analysis [6, 10], with its roots in [12], is a well-studied approach for analyzing online algorithms by comparing them against the optimal oﬄine algorithm, which has full knowledge of the input at the beginning of its execution. One interpretation of this approach is as a game between the designer of the online algorithm and an adversary.
First, the designer selects the online algorithm. Then, the adversary observes the algorithm and selects the sequence of jobs that maximizes the competitive ratio: the ratio of the value of the jobs completed by an optimal oﬄine algorithm to the value of those completed by the online algorithm.
Two papers paint a complete picture in terms of competitive analysis for this setting, in which the algorithm is assumed to know k, the maximum ratio between the value densities (value divided by processing time) of any two jobs.
For k = 1, [4] presents a 4-competitive algorithm, and proves that this is a lower bound on the competitive ratio for deterministic algorithms. The same paper also generalizes the lower bound to (1 + √ k)2 for any k ≥ 1, and [15] then presents a matching (1 + √ k)2 -competitive algorithm.
The setting addressed by these papers is completely nonstrategic, and the algorithm is assumed to always know the true characteristics of each job upon its release. However, in domains such as grid computing (see, for example, [7, 8]) this assumption is invalid, because buyers of processor time choose when and how to submit their jobs.
Furthermore, sellers not only schedule jobs but also determine the amount that they charge buyers, an issue not addressed in the non-strategic setting.
Thus, we consider an extension of the setting in which each job is owned by a separate, self-interested agent.
Instead of being released to the algorithm, each job is now released only to its owning agent. Each agent now has four different ways in which it can manipulate the algorithm: it decides when to submit the job to the algorithm after the true release time, it can artificially inflate the length of the job, and it can declare an arbitrary value and deadline for the job. Because the agents are self-interested, they will choose to manipulate the algorithm if doing so will cause 61 their job to be completed; and, indeed, one can find examples in which agents have incentive to manipulate the algorithms presented in [4] and [15].
The addition of self-interested agents moves the problem from the area of algorithm design to that of mechanism design [17], the science of crafting protocols for self-interested agents. Recent years have seen much activity at the interface of computer science and mechanism design (see, e.g., [9, 18, 19]). In general, a mechanism defines a protocol for interaction between the agents and the center that culminates with the selection of an outcome. In our setting, a mechanism will take as input a job from each agent, and return a schedule for the jobs, and a payment to be made by each agent to the center. A basic solution concept of mechanism design is incentive compatibility, which, in our setting, requires that it is always in each agent"s best interests to immediately submit its job upon release, and to truthfully declare its value, length, and deadline.
In order to evaluate a mechanism using competitive analysis, the adversary model must be updated. In the new model, the adversary still determines the sequence of jobs, but it is the self-interested agents who determine the observed input of the mechanism. Thus, in order to achieve a competitive ratio of c, an online mechanism must both be incentive compatible, and always achieve at least 1 c of the value that the optimal oﬄine mechanism achieves on the same sequence of jobs.
The rest of the paper is structured as follows. In Section 2, we formally define and review results from the original, non-strategic setting. After introducing the incentive issues through an example, we formalize the mechanism design setting in Section 3. In Section 4 we present our first main result, a ((1 + √ k)2 + 1)-competitive mechanism, and formally prove incentive compatibility and the competitive ratio. We also show how we can simplify this mechanism for the special case in which k = 1 and each agent cannot alter the length of its job. Returning the general setting, we show in Section 5 that this competitive ratio is a lower bound for deterministic mechanisms that do not pay agents. Finally, in Section 6, we discuss related work other than the directly relevant [4] and [15], before concluding with Section 7.
In this section, we formally define the original, non-strategic setting, and recap previous results.
There exists a single processor on which jobs can execute, and N jobs, although this number is not known beforehand.
Each job i is characterized by a tuple θi = (ri, di, li, vi), which denotes the release time, deadline, length of processing time required, and value, respectively. The space Θi of possible tuples is the same for each job and consists of all θi such that ri, di, li, vi ∈ + (thus, the model of time is continuous). Each job is released at time ri, at which point its three other characteristics are known. Nothing is known about the job before its arrival. Each deadline is firm (or, hard), which means that no value is obtained for a job that is completed after its deadline. Preemption of jobs is allowed, and it takes no time to switch between jobs. Thus, job i is completed if and only if the total time it executes on the processor before di is at least li.
Let θ = (θ1, . . . , θN ) denote the vector of tuples for all jobs, and let θ−i = (θ1, . . . , θi−1, θi+1, . . . , θN ) denote the same vector without the tuple for job i. Thus, (θi, θ−i) denotes a complete vector of tuples.
Define the value density ρi = vi li of job i to be the ratio of its value to its length. For an input θ, denote the maximum and minimum value densities as ρmin = mini ρi and ρmax = maxi ρi. The importance ratio is then defined to be ρmax ρmin , the maximal ratio of value densities between two jobs. The algorithm is assumed to always know an upper bound k on the importance ratio. For simplicity, we normalize the range of possible value densities so that ρmin = 1.
An online algorithm is a function f : Θ1 × . . . × ΘN → O that maps the vector of tuples (for any number N) to an outcome o. An outcome o ∈ O is simply a schedule of jobs on the processor, recorded by the function S : + → {0, 1, . . . , N}, which maps each point in time to the active job, or to 0 if the processor is idle.
To denote the total elapsed time that a job has spent on the processor at time t, we will use the function ei(t) = t 0 µ(S(x) = i)dx, where µ(·) is an indicator function that returns 1 if the argument is true, and zero otherwise. A job"s laxity at time t is defined to be di − t − li + ei(t) , the amount of time that it can remain inactive and still be completed by its deadline. A job is abandoned if it cannot be completed by its deadline (formally, if di −t+ei(t) < li).
Also, overload S(·) and ei(·) so that they can also take a vector θ as an argument. For example, S(θ, t) is shorthand for the S(t) of the outcome f(θ), and it denotes the active job at time t when the input is θ.
Since a job cannot be executed before its release time, the space of possible outcomes is restricted in that S(θ, t) = i implies ri ≤ t. Also, because the online algorithm must produce the schedule over time, without knowledge of future inputs, it must make the same decision at time t for inputs that are indistinguishable at this time. Formally, let θ(t) denote the subset of the tuples in θ that satisfy ri ≤ t. The constraint is then that θ(t) = θ (t) implies S(θ, t) = S(θ , t).
The objective function is the sum of the values of the jobs that are completed by their respective deadlines: W(o, θ) = i vi · µ(ei(θ, di) ≥ li) . Let W∗ (θ) = maxo∈O W(o, θ) denote the maximum possible total value for the profile θ.
In competitive analysis, an online algorithm is evaluated by comparing it against an optimal oﬄine algorithm.
Because the oﬄine algorithm knows the entire input θ at time
always achieves W∗ (θ). An online algorithm f(·) is (strictly) c-competitive if there does not exist an input θ such that c · W(f(θ), θ) < W∗ (θ). An algorithm that is c-competitive is also said to achieve a competitive ratio of c.
We assume that there does not exist an overload period of infinite duration. A period of time [ts , tf ] is overloaded if the sum of the lengths of the jobs whose release time and deadline both fall within the time period exceeds the duration of the interval (formally, if tf −ts ≤ i|(ts≤ri,di≤tf ) li).
Without such an assumption, it is not possible to achieve a finite competitive ratio [15].
In the non-strategic setting, [4] presents a 4-competitive algorithm called TD1 (version 2) for the case of k = 1, while [15] presents a (1+ √ k)2 -competitive algorithm called Dover for the general case of k ≥ 1. Matching lower bounds for deterministic algorithms for both of these cases were shown 62 in [4]. In this section we provide a high-level description of TD1 (version 2) using an example.
TD1 (version 2) divides the schedule into intervals, each of which begins when the processor transitions from idle to busy (call this time tb ), and ends with the completion of a job. The first active job of an interval may have laxity; however, for the remainder of the interval, preemption of the active job is only considered when some other job has zero laxity. For example, when the input is the set of jobs listed in Table 1, the first interval is the complete execution of job 1 over the range [0.0, 0.9]. No preemption is considered during this interval, because job 2 has laxity until time 1.5.
Then, a new interval starts at tb = 0.9 when job 2 becomes active. Before job 2 can finish, preemption is considered at time 4.8, when job 3 is released with zero laxity.
In order to decide whether to preempt the active job, TD1 (version 2) uses two more variables: te and p loss. The former records the latest deadline of a job that would be abandoned if the active job executes to completion (or, if no such job exists, the time that the active job will finish if it is not preempted). In this case, te = 17.0. The value te −tb represents the an upper bound on the amount of possible execution time lost to the optimal oﬄine algorithm due to the completion of the active job. The other variable, p loss, is equal to the length of the first active job of the current interval. Because in general this job could have laxity, the oﬄine algorithm may be able to complete it outside of the range [tb , te ].1 If the algorithm completes the active job and this job"s length is at least te −tb +p loss 4 , then the algorithm is guaranteed to be 4-competitive for this interval (note that k = 1 implies that all jobs have the same value density and thus that lengths can used to compute the competitive ratio). Because this is not case at time 4.8 (since te −tb +p loss 4 = 17.0−0.9+4.0 4 > 4.0 = l2), the algorithm preempts job 2 for job 3, which then executes to completion.
Job ri di li vi
6 ? 6 ? 6 ?
Table 1: Input used to recap TD1 (version 2) [4].
The up and down arrows represent ri and di, respectively, while the length of the box equals li.
However, false information about job 2 would cause TD1 (version 2) to complete this job. For example, if job 2"s deadline were declared as ˆd2 = 4.7, then it would have zero laxity at time 0.7. At this time, the algorithm would preempt job
−tb +p loss 4 = 4.7−0.0+1.0 4 > 0.9 = l1.
Job 2 would then complete before the arrival of job 3.2 1 While it would be easy to alter the algorithm to recognize that this is not possible for the jobs in Table 1, our example does not depend on the use of p loss. 2 While we will not describe the significantly more complex In order to address incentive issues such as this one, we need to formalize the setting as a mechanism design problem. In this section we first present the mechanism design formulation, and then define our goals for the mechanism.
There exists a center, who controls the processor, and N agents, where the value of N is unknown by the center beforehand. Each job i is owned by a separate agent i. The characteristics of the job define the agent"s type θi ∈ Θi.
At time ri, agent i privately observes its type θi, and has no information about job i before ri. Thus, jobs are still released over time, but now each job is revealed only to the owning agent.
Agents interact with the center through a direct mechanism Γ = (Θ1, . . . , ΘN , g(·)), in which each agent declares a job, denoted by ˆθi = (ˆri, ˆdi, ˆli, ˆvi), and g : Θ1×. . .×ΘN → O maps the declared types to an outcome o ∈ O. An outcome o = (S(·), p1, . . . , pN ) consists of a schedule and a payment from each agent to the mechanism.
In a standard mechanism design setting, the outcome is enforced at the end of the mechanism. However, since the end is not well-defined in this online setting, we choose to model returning the job if it is completed and collecting a payment from each agent i as occurring at ˆdi, which, according to the agent"s declaration, is the latest relevant point of time for that agent. That is, even if job i is completed before ˆdi, the center does not return the job to agent i until that time. This modelling decision could instead be viewed as a decision by the mechanism designer from a larger space of possible mechanisms. Indeed, as we will discuss later, this decision of when to return a completed job is crucial to our mechanism.
Each agent"s utility, ui(g(ˆθ), θi) = vi · µ(ei(ˆθ, di) ≥ li) · µ( ˆdi ≤ di) − pi(ˆθ), is a quasi-linear function of its value for its job (if completed and returned by its true deadline) and the payment it makes to the center. We assume that each agent is a rational, expected utility maximizer.
Agent declarations are restricted in that an agent cannot declare a length shorter than the true length, since the center would be able to detect such a lie if the job were completed.
On the other hand, in the general formulation we will allow agents to declare longer lengths, since in some settings it may be possible add unnecessary work to a job. However, we will also consider a restricted formulation in which this type of lie is not possible. The declared release time ˆri is the time that the agent chooses to submit job i to the center, and it cannot precede the time ri at which the job is revealed to the agent. The agent can declare an arbitrary deadline or value. To summarize, agent i can declare any type ˆθi = (ˆri, ˆdi, ˆli, ˆvi) such that ˆli ≥ li and ˆri ≥ ri.
While in the non-strategic setting it was sufficient for the algorithm to know the upper bound k on the ratio ρmax ρmin , in the mechanism design setting we will strengthen this assumption so that the mechanism also knows ρmin (or, equivalently, the range [ρmin, ρmax] of possible value densities).3 Dover , we note that it is similar in its use of intervals and its preference for the active job. Also, we note that the lower bound we will show in Section 5 implies that false information can also benefit a job in Dover . 3 Note that we could then force agent declarations to satisfy ρmin ≤ ˆvi ˆli ≤ ρmax. However, this restriction would not 63 While we feel that it is unlikely that a center would know k without knowing this range, we later present a mechanism that does not depend on this extra knowledge in a restricted setting.
The restriction on the schedule is now that S(ˆθ, t) = i implies ˆri ≤ t, to capture the fact that a job cannot be scheduled on the processor before it is declared to the mechanism.
As before, preemption of jobs is allowed, and job switching takes no time.
The constraints due to the online mechanism"s lack of knowledge of the future are that ˆθ(t) = ˆθ (t) implies S(ˆθ, t) = S(ˆθ , t), and ˆθ( ˆdi) = ˆθ ( ˆdi) implies pi(ˆθ) = pi(ˆθ ) for each agent i. The setting can then be summarized as follows. 1Overview of the Setting: for all t do The center instantiates S(ˆθ, t) ← i, for some i s.t. ˆri ≤ t if ∃i, (ri = t) then θi is revealed to agent i if ∃i, (t ≥ ri) and agent i has not declared a job then Agent i can declare any job ˆθi, s.t. ˆri = t and ˆli ≥ li if ∃i, ( ˆdi = t) ∧ (ei(ˆθ, t) ≥ li) then Completed job i is returned to agent i if ∃i, ( ˆdi = t) then Center sets and collects payment pi(ˆθ) from agent i
Our aim as mechanism designer is to maximize the value of completed jobs, subject to the constraints of incentive compatibility and individual rationality.
The condition for (dominant strategy) incentive compatibility is that for each agent i, regardless of its true type and of the declared types of all other agents, agent i cannot increase its utility by unilaterally changing its declaration.
Definition 1. A direct mechanism Γ satisfies incentive compatibility (IC) if ∀i, θi, θi, ˆθ−i : ui(g(θi, ˆθ−i), θi) ≥ ui(g(θi, ˆθ−i), θi) From an agent perspective, dominant strategies are desirable because the agent does not have to reason about either the strategies of the other agents or the distribution from the which other agent"s types are drawn. From a mechanism designer perspective, dominant strategies are important because we can reasonably assume that an agent who has a dominant strategy will play according to it. For these reasons, in this paper we require dominant strategies, as opposed to a weaker equilibrium concept such as Bayes-Nash, under which we could improve upon our positive results.4 decrease the lower bound on the competitive ratio. 4 A possible argument against the need for incentive compatibility is that an agent"s lie may actually improve the schedule. In fact, this was the case in the example we showed for the false declaration ˆd2 = 4.7. However, if an agent lies due to incorrect beliefs over the future input, then the lie could instead make the schedule the worse (for example, if job 3 were never released, then job 1 would have been unnecessarily abandoned). Furthermore, if we do not know the beliefs of the agents, and thus cannot predict how they will lie, then we can no longer provide a competitive guarantee for our mechanism.
While restricting ourselves to incentive compatible direct mechanisms may seem limiting at first, the Revelation Principle for Dominant Strategies (see, e.g., [17]) tells us that if our goal is dominant strategy implementation, then we can make this restriction without loss of generality.
The second goal for our mechanism, individual rationality, requires that agents who truthfully reveal their type never have negative utility. The rationale behind this goal is that participation in the mechanism is assumed to be voluntary.
Definition 2. A direct mechanism Γ satisfies individual rationality (IR) if ∀i, θi, ˆθ−i, ui(g(θi, ˆθ−i), θi) ≥ 0.
Finally, the social welfare function that we aim to maximize is the same as the objective function of the non-strategic setting: W(o, θ) = i vi · µ(ei(θ, di) ≥ li) . As in the nonstrategic setting, we will evaluate an online mechanism using competitive analysis to compare it against an optimal oﬄine mechanism (which we will denote by Γoffline). An oﬄine mechanism knows all of the types at time 0, and thus can always achieve W∗ (θ).5 Definition 3. An online mechanism Γ is (strictly) ccompetitive if it satisfies IC and IR, and if there does not exist a profile of agent types θ such that c·W(g(θ), θ) < W∗ (θ).
In this section, we first present our main positive result: a (1+ √ k)2 +1 -competitive mechanism (Γ1). After providing some intuition as to why Γ1 satisfies individual rationality and incentive compatibility, we formally prove first these two properties and then the competitive ratio. We then consider a special case in which k = 1 and agents cannot lie about the length of their job, which allows us to alter this mechanism so that it no longer requires either knowledge of ρmin or the collection of payments from agents.
Unlike TD1 (version 2) and Dover , Γ1 gives no preference to the active job. Instead, it always executes the available job with the highest priority: (ˆvi + √ k · ei(ˆθ, t) · ρmin).
Each agent whose job is completed is then charged the lowest value that it could have declared such that its job still would have been completed, holding constant the rest of its declaration.
By the use of a payment rule similar to that of a secondprice auction, Γ1 satisfies both IC with respect to values and IR. We now argue why it satisfies IC with respect to the other three characteristics. Declaring an improved job (i.e., declaring an earlier release time, a shorter length, or a later deadline) could possibly decrease the payment of an agent. However, the first two lies are not possible in our setting, while the third would cause the job, if it is completed, to be returned to the agent after the true deadline. This is the reason why it is important to always return a completed job at its declared deadline, instead of at the point at which it is completed. 5 Another possibility is to allow only the agents to know their types at time 0, and to force Γoffline to be incentive compatible so that agents will truthfully declare their types at time 0. However, this would not affect our results, since executing a VCG mechanism (see, e.g., [17]) at time 0 both satisfies incentive compatibility and always maximizes social welfare. 64 Mechanism 1 Γ1 Execute S(ˆθ, ·) according to Algorithm 1 for all i do if ei(ˆθ, ˆdi) ≥ ˆli {Agent i"s job is completed} then pi(ˆθ) ← arg minvi≥0(ei(((ˆri, ˆdi, ˆli, vi), ˆθ−i), ˆdi) ≥ ˆli) else pi(ˆθ) ← 0 Algorithm 1 for all t do Avail ← {i|(t ≥ ˆri)∧(ei(ˆθ, t) < ˆli)∧(ei(ˆθ, t)+ ˆdi−t ≥ ˆli)} {Set of all released, non-completed, non-abandoned jobs} if Avail = ∅ then S(ˆθ, t) ← arg maxi∈Avail(ˆvi + √ k · ei(ˆθ, t) · ρmin) {Break ties in favor of lower ˆri} else S(ˆθ, t) ← 0 It remains to argue why an agent does not have incentive to worsen its job. The only possible effects of an inflated length are delaying the completion of the job and causing it to be abandoned, and the only possible effects of an earlier declared deadline are causing to be abandoned and causing it to be returned earlier (which has no effect on the agent"s utility in our setting). On the other hand, it is less obvious why agents do not have incentive to declare a later release time. Consider a mechanism Γ1 that differs from Γ1 in that it does not preempt the active job i unless there exists another job j such that (ˆvi + √ k·li(ˆθ, t)·ρmin) < ˆvj. Note that as an active job approaches completion in Γ1, its condition for preemption approaches that of Γ1.
However, the types in Table 2 for the case of k = 1 show why an agent may have incentive to delay the arrival of its job under Γ1. Job 1 becomes active at time 0, and job 2 is abandoned upon its release at time 6, because 10 + 10 = v1 +l1 > v2 = 13. Then, at time 8, job 1 is preempted by job 3, because 10 + 10 = v1 + l1 < v3 = 22. Job 3 then executes to completion, forcing job 1 to be abandoned. However, job
job 3 from being executed if it had been the active job at time 8, since 13 + 13 = v2 + l2 > v3 = 22. Thus, if agent
abandoned at time 8, and job 1 would have completed over the range [20, 30].
Job ri di li vi
6 ? 6 ? 6 ?
Table 2: Jobs used to show why a slightly altered version of Γ1 would not be incentive compatible with respect to release times.
Intuitively, Γ1 avoids this problem because of two properties. First, when a job becomes active, it must have a greater priority than all other available jobs. Second, because a job"s priority can only increase through the increase of its elapsed time, ei(ˆθ, t), the rate of increase of a job"s priority is independent of its characteristics. These two properties together imply that, while a job is active, there cannot exist a time at which its priority is less than the priority that one of these other jobs would have achieved by executing on the processor instead.
Incentive Compatibility After presenting the (trivial) proof of IR, we break the proof of IC into lemmas.
Theorem 1. Mechanism Γ1 satisfies individual rationality.
Proof. For arbitrary i, θi, ˆθ−i, if job i is not completed, then agent i pays nothing and thus has a utility of zero; that is, pi(θi, ˆθ−i) = 0 and ui(g(θi, ˆθ−i), θi) = 0. On the other hand, if job i is completed, then its value must exceed agent i"s payment. Formally, ui(g(θi, ˆθ−i), θi) = vi − arg minvi≥0(ei(((ri, di, li, vi), ˆθ−i), di) ≥ li) ≥ 0 must hold, since vi = vi satisfies the condition.
To prove IC, we need to show that for an arbitrary agent i, and an arbitrary profile ˆθ−i of declarations of the other agents, agent i can never gain by making a false declaration ˆθi = θi, subject to the constraints that ˆri ≥ ri and ˆli ≥ li.
We start by showing that, regardless of ˆvi, if truthful declarations of ri, di, and li do not cause job i to be completed, then worse declarations of these variables (that is, declarations that satisfy ˆri ≥ ri, ˆli ≥ li and ˆdi ≤ di) can never cause the job to be completed. We break this part of the proof into two lemmas, first showing that it holds for the release time, regardless of the declarations of the other variables, and then for length and deadline.
Lemma 2. In mechanism Γ1, the following condition holds for all i, θi, ˆθ−i: ∀ ˆvi, ˆli ≥ li, ˆdi ≤ di, ˆri ≥ ri, ei ((ˆri, ˆdi, ˆli, ˆvi), ˆθ−i), ˆdi ≥ ˆli =⇒ ei ((ri, ˆdi, ˆli, ˆvi), ˆθ−i), ˆdi ≥ ˆli Proof. Assume by contradiction that this condition does not hold- that is, job i is not completed when ri is truthfully declared, but is completed for some false declaration ˆri ≥ ri. We first analyze the case in which the release time is truthfully declared, and then we show that job i cannot be completed when agent i delays submitting it to the center.
Case I: Agent i declares ˆθi = (ri, ˆdi, ˆli, ˆvi).
First, define the following three points in the execution of job i. • Let ts = arg mint S((ˆθi, ˆθ−i), t) = i be the time that job i first starts execution. • Let tp = arg mint>ts S((ˆθi, ˆθ−i), t) = i be the time that job i is first preempted. • Let ta = arg mint ei((ˆθi, ˆθ−i), t) + ˆdi − t < ˆli be the time that job i is abandoned. 65 If ts and tp are undefined because job i never becomes active, then let ts = tp = ta .
Also, partition the jobs declared by other agents before ta into the following three sets. • X = {j|(ˆrj < tp ) ∧ (j = i)} consists of the jobs (other than i) that arrive before job i is first preempted. • Y = {j|(tp ≤ ˆrj ≤ ta )∧(ˆvj > ˆvi + √ k·ei((ˆθi, ˆθ−i), ˆrj)} consists of the jobs that arrive in the range [tp , ta ] and that when they arrive have higher priority than job i (note that we are make use of the normalization). • Z = {j|(tp ≤ ˆrj ≤ ta )∧(ˆvj ≤ ˆvi + √ k ·ei((ˆθi, ˆθ−i), ˆrj)} consists of the jobs that arrive in the range [tp , ta ] and that when they arrive have lower priority than job i.
We now show that all active jobs during the range (tp , ta ] must be either i or in the set Y . Unless tp = ta (in which case this property trivially holds), it must be the case that job i has a higher priority than an arbitrary job x ∈ X at time tp , since at the time just preceding tp job x was available and job i was active. Formally, ˆvx + √ k · ex((ˆθi, ˆθ−i), tp ) < ˆvi + √ k · ei((ˆθi, ˆθ−i), tp ) must hold.6 We can then show that, over the range [tp , ta ], no job x ∈ X runs on the processor. Assume by contradiction that this is not true. Let tf ∈ [tp , ta ] be the earliest time in this range that some job x ∈ X is active, which implies that ex((ˆθi, ˆθ−i), tf ) = ex((ˆθi, ˆθ−i), tp ). We can then show that job i has a higher priority at time tf as follows: ˆvx+ √ k·ex((ˆθi, ˆθ−i), tf ) = ˆvx+ √ k·ex((ˆθi, ˆθ−i), tp ) < ˆvi + √ k · ei((ˆθi, ˆθ−i), tp ) ≤ ˆvi + √ k · ei((ˆθi, ˆθ−i), tf ), contradicting the fact that job x is active at time tf .
A similar argument applies to an arbitrary job z ∈ Z, starting at it release time ˆrz > tp , since by definition job i has a higher priority at that time. The only remaining jobs that can be active over the range (tp , ta ] are i and those in the set Y .
Case II: Agent i declares ˆθi = (ˆri, ˆdi, ˆli, ˆvi), where ˆri > ri.
We now show that job i cannot be completed in this case, given that it was not completed in case I. First, we can restrict the range of ˆri that we need to consider as follows.
Declaring ˆri ∈ (ri, ts ] would not affect the schedule, since ts would still be the first time that job i executes. Also, declaring ˆri > ta could not cause the job to be completed, since di − ta < ˆli holds, which implies that job i would be abandoned at its release. Thus, we can restrict consideration to ˆri ∈ (ts , ta ].
In order for declaring ˆθi to cause job i to be completed, a necessary condition is that the execution of some job yc ∈ Y must change during the range (tp , ta ], since the only jobs other than i that are active during that range are in Y .
Let tc = arg mint∈(tp,ta][∃yc ∈ Y, (S((ˆθi, ˆθ−i), t) = yc ) ∧ (S((ˆθi, ˆθ−i), t) = yc )] be the first time that such a change occurs. We will now show that for any ˆri ∈ (ts , ta ], there cannot exist a job with higher priority than yc at time tc , contradicting (S((ˆθi, ˆθ−i), t) = yc ).
First note that job i cannot have a higher priority, since there would have to exist a t ∈ (tp , tc ) such that ∃y ∈ 6 For simplicity, when we give the formal condition for a job x to have a higher priority than another job y, we will assume that job x"s priority is strictly greater than job y"s, because, in the case of a tie that favors x, future ties would also be broken in favor of job x.
Y, (S((ˆθi, ˆθ−i), t) = y) ∧ (S((ˆθi, ˆθ−i), t) = i), contradicting the definition of tc .
Now consider an arbitrary y ∈ Y such that y = yc . In case I, we know that job y has lower priority than yc at time tc ; that is, ˆvy + √ k·ey((ˆθi, ˆθ−i), tc ) < ˆvyc + √ k·eyc ((ˆθi, ˆθ−i), tc ).
Thus, moving to case II, job y must replace some other job before tc . Since ˆry ≥ tp , the condition is that there must exist some t ∈ (tp , tc ) such that ∃w ∈ Y ∪{i}, (S((ˆθi, ˆθ−i), t) = w) ∧ (S((ˆθi, ˆθ−i), t) = y). Since w ∈ Y would contradict the definition of tc , we know that w = i. That is, the job that y replaces must be i. By definition of the set Y , we know that ˆvy > ˆvi + √ k · ei((ˆθi, ˆθ−i), ˆry). Thus, if ˆry ≤ t, then job i could not have executed instead of y in case I. On the other hand, if ˆry > t, then job y obviously could not execute at time t, contradicting the existence of such a time t.
Now consider an arbitrary job x ∈ X. We know that in case I job i has a higher priority than job x at time ts , or, formally, that ˆvx + √ k · ex((ˆθi, ˆθ−i), ts ) < ˆvi + √ k · ei((ˆθi, ˆθ−i), ts ). We also know that ˆvi + √ k·ei((ˆθi, ˆθ−i), tc ) < ˆvyc + √ k · eyc ((ˆθi, ˆθ−i), tc ). Since delaying i"s arrival will not affect the execution up to time ts , and since job x cannot execute instead of a job y ∈ Y at any time t ∈ (tp , tc ] by definition of tc , the only way for job x"s priority to increase before tc as we move from case I to II is to replace job i over the range (ts , tc ]. Thus, an upper bound on job x"s priority when agent i declares ˆθi is: ˆvx+ √ k· ex((ˆθi, ˆθ−i), ts )+ei((ˆθi, ˆθ−i), tc )−ei((ˆθi, ˆθ−i), ts ) < ˆvi + √ k· ei((ˆθi, ˆθ−i), ts )+ei((ˆθi, ˆθ−i), tc )−ei((ˆθi, ˆθ−i), ts ) = ˆvi + √ k · ei((ˆθi, ˆθ−i), tc ) < ˆvyc + √ k · eyc ((ˆθi, ˆθ−i), tc ).
Thus, even at this upper bound, job yc would execute instead of job x at time tc . A similar argument applies to an arbitrary job z ∈ Z, starting at it release time ˆrz. Since the sets {i}, X, Y, Z partition the set of jobs released before ta , we have shown that no job could execute instead of job yc , contradicting the existence of tc , and completing the proof.
Lemma 3. In mechanism Γ1, the following condition holds for all i, θi, ˆθ−i: ∀ ˆvi, ˆli ≥ li, ˆdi ≤ di, ei ((ri, ˆdi, ˆli, ˆvi), ˆθ−i), ˆdi ≥ ˆli =⇒ ei ((ri, di, li, ˆvi), ˆθ−i), ˆdi ≥ li Proof. Assume by contradiction there exists some instantiation of the above variables such that job i is not completed when li and di are truthfully declared, but is completed for some pair of false declarations ˆli ≥ li and ˆdi ≤ di.
Note that the only effect that ˆdi and ˆli have on the execution of the algorithm is on whether or not i ∈ Avail.
Specifically, they affect the two conditions: (ei(ˆθ, t) < ˆli) and (ei(ˆθ, t) + ˆdi − t ≥ ˆli). Because job i is completed when ˆli and ˆdi are declared, the former condition (for completion) must become false before the latter. Since truthfully declaring li ≤ ˆli and di ≥ ˆdi will only make the former condition become false earlier and the latter condition become false later, the execution of the algorithm will not be affected when moving to truthful declarations, and job i will be completed, a contradiction.
We now use these two lemmas to show that the payment for a completed job can only increase by falsely declaring worse ˆli, ˆdi, and ˆri. 66 Lemma 4. In mechanism Γ1, the following condition holds for all i, θi, ˆθ−i: ∀ ˆli ≥ li, ˆdi ≤ di, ˆri ≥ ri, arg min vi≥0 ei ((ˆri, ˆdi, ˆli, vi), ˆθ−i), ˆdi ≥ ˆli ≥ arg min vi≥0 ei ((ri, di, li, vi), ˆθ−i), di ≥ li Proof. Assume by contradiction that this condition does not hold. This implies that there exists some value vi such that the condition (ei(((ˆri, ˆdi, ˆli, vi), ˆθ−i), ˆdi) ≥ ˆli) holds, but (ei(((ri, di, li, vi), ˆθ−i), di) ≥ li) does not. Applying Lemmas 2 and 3: (ei(((ˆri, ˆdi, ˆli, vi), ˆθ−i), ˆdi) ≥ ˆli) =⇒ (ei(((ri, ˆdi, ˆli, vi), ˆθ−i), ˆdi) ≥ ˆli) =⇒ (ei(((ri, di, li, vi), ˆθ−i), di) ≥ li), a contradiction.
Finally, the following lemma tells us that the completion of a job is monotonic in its declared value.
Lemma 5. In mechanism Γ1, the following condition holds for all i, ˆθi, ˆθ−i: ∀ ˆvi ≥ ˆvi, ei ((ˆri, ˆdi, ˆli, ˆvi), ˆθ−i), ˆdi ≥ ˆli =⇒ ei ((ˆri, ˆdi, ˆli, ˆvi), ˆθ−i), ˆdi ≥ ˆli The proof, by contradiction, of this lemma is omitted because it is essentially identical to that of Lemma 2 for ˆri. In case I, agent i declares (ˆri, ˆdi, ˆli, ˆvi) and the job is not completed, while in case II he declares (ˆri, ˆdi, ˆli, ˆvi) and the job is completed. The analysis of the two cases then proceeds as before- the execution will not change up to time ts because the initial priority of job i decreases as we move from case I to II; and, as a result, there cannot be a change in the execution of a job other than i over the range (tp , ta ].
We can now combine the lemmas to show that no profitable deviation is possible.
Theorem 6. Mechanism Γ1 satisfies incentive compatibility.
Proof. For an arbitrary agent i, we know that ˆri ≥ ri and ˆli ≥ li hold by assumption. We also know that agent i has no incentive to declare ˆdi > di, because job i would never be returned before its true deadline. Then, because the payment function is non-negative, agent i"s utility could not exceed zero. By IR, this is the minimum utility it would achieve if it truthfully declared θi. Thus, we can restrict consideration to ˆθi that satisfy ˆri ≥ ri, ˆli ≥ li, and ˆdi ≤ di.
Again using IR, we can further restrict consideration to ˆθi that cause job i to be completed, since any other ˆθi yields a utility of zero.
If truthful declaration of θi causes job i to be completed, then by Lemma 4 any such false declaration ˆθi could not decrease the payment of agent i. On the other hand, if truthful declaration does not cause job i to be completed, then declaring such a ˆθi will cause agent i to have negative utility, since vi < arg minvi≥0 ei(((ri, di, li, vi), ˆθ−i), ˆdi) ≥ li ≤ arg minvi≥0 ei(((ˆri, ˆdi, ˆli, vi), ˆθ−i), ˆdi) ≥ ˆli holds by Lemmas 5 and 4, respectively.
The proof of the competitive ratio, which makes use of techniques adapted from those used in [15], is also broken into lemmas. Having shown IC, we can assume truthful declaration (ˆθ = θ). Since we have also shown IR, in order to prove the competitive ratio it remains to bound the loss of social welfare against Γoffline.
Denote by (1, 2, . . . , F) the sequence of jobs completed by Γ1. Divide time into intervals If = (topen f , tclose f ], one for each job f in this sequence. Set tclose f to be the time at which job f is completed, and set topen f = tclose f−1 for f ≥ 2, and topen
f be the first time that the processor is not idle in interval If .
Lemma 7. For any interval If , the following inequality holds: tclose f − tbegin f ≤ (1 + 1√ k ) · vf Proof. Interval If begins with a (possibly zero length) period of time in which the processor is idle because there is no available job. Then, it continuously executes a sequence of jobs (1, 2, . . . , c), where each job i in this sequence is preempted by job i + 1, except for job c, which is completed (thus, job c in this sequence is the same as job f is the global sequence of completed jobs). Let ts i be the time that job i begins execution. Note that ts
f .
Over the range [tbegin f , tclose f ], the priority (vi+ √ k·ei(θ, t)) of the active job is monotonically increasing with time, because this function linearly increases while a job is active, and can only increase at a point in time when preemption occurs. Thus, each job i > 1 in this sequence begins execution at its release time (that is, ts i = ri), because its priority does not increase while it is not active.
We now show that the value of the completed job c exceeds the product of √ k and the time spent in the interval on jobs 1 through c−1, or, more formally, that the following condition holds: vc ≥ √ k c−1 h=1(eh(θ, ts h+1) − eh(θ, ts h)). To show this, we will prove by induction that the stronger condition vi ≥ √ k i−1 h=1 eh(θ, ts h+1) holds for all jobs i in the sequence.
Base Case: For i = 1, v1 ≥ √ k 0 h=1 eh(θ, ts h+1) = 0, since the sum is over zero elements.
Inductive Step: For an arbitrary 1 ≤ i < c, we assume that vi ≥ √ k i−1 h=1 eh(θ, ts h+1) holds. At time ts i+1, we know that vi+1 ≥ vi + √ k · ei(θ, ts i+1) holds, because ts i+1 = ri+1. These two inequalities together imply that vi+1 ≥√ k i h=1 eh(θ, ts h+1), completing the inductive step.
We also know that tclose f − ts c ≤ lc ≤ vc must hold, by the simplifying normalization of ρmin = 1 and the fact that job c"s execution time cannot exceed its length. We can thus bound the total execution time of If by: tclose f − tbegin f = (tclose f −ts c)+ c−1 h=1(eh(θ, ts h+1)−eh(θ, ts h)) ≤ (1+ 1√ k )vf .
We now consider the possible execution of uncompleted jobs by Γoffline. Associate each job i that is not completed by Γ1 with the interval during which it was abandoned. All jobs are now associated with an interval, since there are no gaps between the intervals, and since no job i can be abandoned after the close of the last interval at tclose F . Because the processor is idle after tclose F , any such job i would become active at some time t ≥ tclose F , which would lead to the completion of some job, creating a new interval and contradicting the fact that IF is the last one. 67 The following lemma is equivalent to Lemma 5.6 of [15], but the proof is different for our mechanism.
Lemma 8. For any interval If and any job i abandoned in If , the following inequality holds: vi ≤ (1 + √ k)vf .
Proof. Assume by contradiction that there exists a job i abandoned in If such that vi > (1 + √ k)vf . At tclose f , the priority of job f is vf + √ k · lf < (1 + √ k)vf . Because the priority of the active job monotonically increases over the range [tbegin f , tclose f ], job i would have a higher priority than the active job (and thus begin execution) at some time t ∈ [tbegin f , tclose f ]. Again applying monotonicity, this would imply that the priority of the active job at tclose f exceeds (1 + √ k)vf , contradicting the fact that it is (1 + √ k)vf .
As in [15], for each interval If , we give Γoffline the following gift: k times the amount of time in the range [tbegin f , tclose f ] that it does not schedule a job. Additionally, we give the adversary vf , since the adversary may be able to complete this job at some future time, due to the fact that Γ1 ignores deadlines. The following lemma is Lemma
Lemma 9. [15] With the above gifts the total net gain obtained by the clairvoyant algorithm from scheduling the jobs abandoned during If is not greater than (1 + √ k) · vf .
The intuition behind this lemma is that the best that the adversary can do is to take almost all of the gift of k ·(tclose f −tbegin f ) (intuitively, this is equivalent to executing jobs with the maximum possible value density over the time that Γ1 is active), and then begin execution of a job abandoned by Γ1 right before tclose f . By Lemma 8, the value of this job is bounded by (1 + √ k) · vf . We can now combine the results of these lemmas to prove the competitive ratio.
Theorem 10. Mechanism Γ1 is (1+ √ k)2+1 -competitive.
Proof. Using the fact that the way in which jobs are associated with the intervals partitions the entire set of jobs, we can show the competitive ratio by showing that Γ1 is (1+ √ k)2 +1 -competitive for each interval in the sequence (1, . . . , F). Over an arbitrary interval If , the oﬄine algorithm can achieve at most (tclose f −tbegin f )·k+vf +(1+ √ k)vf , from the two gifts and the net gain bounded by Lemma
above by (1+ 1√ k )·vf ·k+vf +(1+ √ k)vf = ((1+ √ k)2 +1)·vf .
Since Γ1 achieves vf , the competitive ratio holds.
While so far we have allowed each agent to lie about all four characteristics of its job, lying about the length of the job is not possible in some settings. For example, a user may not know how to alter a computational problem in a way that both lengthens the job and allows the solution of the original problem to be extracted from the solution to the altered problem. Another restriction that is natural in some settings is uniform value densities (k = 1), which was the case considered by [4]. If the setting satisfies these two conditions, then, by using Mechanism Γ2, we can achieve a competitive ratio of 5 (which is the same competitive ratio as Γ1 for the case of k = 1) without knowledge of ρmin and without the use of payments. The latter property may be necessary in settings that are more local than grid computing (e.g., within a department) but in which the users are still self-interested.7 Mechanism 2 Γ2 Execute S(ˆθ, ·) according to Algorithm 2 for all i do pi(ˆθ) ← 0 Algorithm 2 for all t do Avail ← {i|(t ≥ ˆri)∧(ei(ˆθ, t) < li)∧(ei(ˆθ, t)+ ˆdi−t ≥ li)} if Avail = ∅ then S(ˆθ, t) ← arg maxi∈Avail(li + ei(ˆθ, t)) {Break ties in favor of lower ˆri} else S(ˆθ, t) ← 0 Theorem 11. When k = 1, and each agent i cannot falsely declare li, Mechanism Γ2 satisfies individual rationality and incentive compatibility.
Theorem 12. When k = 1, and each agent i cannot falsely declare li, Mechanism Γ2 is 5-competitive.
Since this mechanism is essentially a simplification of Γ1, we omit proofs of these theorems. Basically, the fact that k = 1 and ˆli = li both hold allows Γ2 to substitute the priority (li +ei(ˆθ, t)) for the priority used in Γ1; and, since ˆvi is ignored, payments are no longer needed to ensure incentive compatibility.
We now show that the competitive ratio of (1 + √ k)2 +
online mechanisms. To do so, we will appeal to third requirement on a mechanism, non-negative payments (NNP), which requires that the center never pays an agent (formally, ∀i, ˆθ, pi(ˆθi) ≥ 0). Unlike IC and IR, this requirement is not standard in mechanism design. We note, however, that both Γ1 and Γ2 satisfy it trivially, and that, in the following proof, zero only serves as a baseline utility for an agent, and could be replaced by any non-positive function of ˆθ−i.
The proof of the lower bound uses an adversary argument similar to that used in [4] to show a lower bound of (1 +√ k)2 in the non-strategic setting, with the main novelty lying in the perturbation of the job sequence and the related incentive compatibility arguments. We first present a lemma relating to the recurrence used for this argument, with the proof omitted due to space constraints.
Lemma 13. For any k ≥ 1, for the recurrence defined by li+1 = λ · li − k · i h=1 lh and l1 = 1, where (1 + √ k)2 − 1 < λ < (1 + √ k)2 , there exists an integer m ≥ 1 such that lm+k· m−1 h=1 lh lm > λ. 7 While payments are not required in this setting, Γ2 can be changed to collect a payments without affecting incentive compatibility by charging some fixed fraction of li for each job i that is completed. 68 Theorem 14. There does not exist a deterministic online mechanism that satisfies NNP and that achieves a competitive ratio less than (1 + √ k)2 + 1.
Proof. Assume by contradiction that there exists a deterministic online mechanism Γ that satisfies NNP and that achieves a competitive ratio of c = (1 + √ k)2 + 1 − for some > 0 (and, by implication, satisfies IC and IR as well). Since a competitive ratio of c implies a competitive ratio of c + x, for any x > 0, we assume without loss of generality that < 1. First, we will construct a profile of agent types θ using an adversary argument. After possibly slightly perturbing θ to assure that a strictness property is satisfied, we will then use a more significant perturbation of θ to reach a contradiction.
We now construct the original profile θ. Pick an α such that 0 < α < , and define δ = α ck+3k . The adversary uses two sequences of jobs: minor and major. Minor jobs i are characterized by li = δ, vi = k · δ, and zero laxity. The first minor job is released at time 0, and ri = di−1 for all i > 1.
The sequence stops whenever Γ completes any job.
Major jobs also have zero laxity, but they have the smallest possible value ratio (that is, vi = li). The lengths of the major jobs that may be released, starting with i = 1, are determined by the following recurrence relation. li+1 = (c − 1 + α) · li − k · i h=1 lh l1 = 1 The bounds on α imply that (1 + √ k)2 − 1 < c − 1 + α < (1+ √ k)2 , which allows us to apply Lemma 13. Let m be the smallest positive number such that lm+k· m−1 h=1 lh lm > c−1+α.
The first major job has a release time of 0, and each major job i > 1 has a release time of ri = di−1 − δ, just before the deadline of the previous job. The adversary releases major job i ≤ m if and only if each major job j < i was executed continuously over the range [ri, ri+1]. No major job is released after job m.
In order to achieve the desired competitive ratio, Γ must complete some major job f, because Γoffline can always at least complete major job 1 (for a value of 1), and Γ can complete at most one minor job (for a value of α c+3 < 1 c ).
Also, in order for this job f to be released, the processor time preceding rf can only be spent executing major jobs that are later abandoned. If f < m, then major job f + 1 will be released and it will be the final major job. Γ cannot complete job f +1, because rf +lf = df > rf+1. Therefore, θ consists of major jobs 1 through f + 1 (or, f, if f = m), plus minor jobs from time 0 through time df .
We now possibly perturb θ slightly. By IR, we know that vf ≥ pf (θ). Since we will later need this inequality to be strict, if vf = pf (θ), then change θf to θf , where rf = rf , but vf , lf , and df are all incremented by δ over their respective values in θf . By IC, job f must still be completed by Γ for the profile (θf , θ−f ). If not, then by IR and NNP we know that pf (θf , θ−f ) = 0, and thus that uf (g(θf , θ−f ), θf ) = 0. However, agent f could then increase its utility by falsely declaring the original type of θf , receiving a utility of: uf (g(θf , θ−f ), θf ) = vf − pf (θ) = δ > 0, violating IC. Furthermore, agent f must be charged the same amount (that is, pf (θf , θ−f ) = pf (θ)), due to a similar incentive compatibility argument. Thus, for the remainder of the proof, assume that vf > pf (θ).
We now use a more substantial perturbation of θ to complete the proof. If f < m, then define θf to be identical to θf , except that df = df+1 + lf , allowing job f to be completely executed after job f + 1 is completed. If f = m, then instead set df = df +lf . IC requires that for the profile (θf , θ−f ), Γ still executes job f continuously over the range [rf , rf +lf ], thus preventing job f +1 from being completed.
Assume by contradiction that this were not true. Then, at the original deadline of df , job f is not completed. Consider the possible profile (θf , θ−f , θx), which differs from the new profile only in the addition of a job x which has zero laxity, rx = df , and vx = lx = max(df − df , (c + 1) · (lf + lf+1)).
Because this new profile is indistinguishable from (θf , θ−f ) to Γ before time df , it must schedule jobs in the same way until df . Then, in order to achieve the desired competitive ratio, it must execute job x continuously until its deadline, which is by construction at least as late as the new deadline df of job f. Thus, job f will not be completed, and, by IR and NNP, it must be the case that pf (θf , θ−f , θx) =
indistinguishable from (θf , θ−f , θx) up to time df , if agent f falsely declared his type to be the original θf , then its job would be completed by df and it would be charged pf (θ).
Its utility would then increase to uf (g(θf , θ−f , θx), θf ) = vf − pf (θ) > 0, contradicting IC.
While Γ"s execution must be identical for both (θf , θ−f ) and (θf , θ−f ), Γoffline can take advantage of the change. If f < m, then Γ achieves a value of at most lf +δ (the value of job f if it were perturbed), while Γoffline achieves a value of at least k·( f h=1 lh −2δ)+lf+1 +lf by executing minor jobs until rf+1, followed by job f +1 and then job f (we subtract two δ"s instead of one because the last minor job before rf+1 may have to be abandoned). Substituting in for lf+1, the competitive ratio is then at least: k·( f h=1 lh−2δ)+lf+1+lf lf +δ = k·( f h=1 lh)−2k·δ+(c−1+α)·lf −k·( f h=1 lh)+lf lf +δ = c·lf +(α·lf −2k·δ) lf +δ ≥ c·lf +((ck+3k)δ−2k·δ) lf +δ > c.
If instead f = m, then Γ achieves a value of at most lm +δ, while Γoffline achieves a value of at least k · ( m h=1 lh − 2δ) + lm by completing minor jobs until dm = rm + lm, and then completing job m. The competitive ratio is then at least: k·( m h=1 lh−2δ)+lm lm+δ = k·( m−1 h=1 lh)−2k·δ+klm+lm lm+δ > (c−1+α)·lm−2k·δ+klm lm+δ = (c+k−1)·lm+(αlm−2k·δ) lm+δ > c.
In this section we describe related work other than the two papers ([4] and [15]) on which this paper is based.
Recent work related to this scheduling domain has focused on competitive analysis in which the online algorithm uses a faster processor than the oﬄine algorithm (see, e.g., [13, 14]). Mechanism design was also applied to a scheduling problem in [18]. In their model, the center owns the jobs in an oﬄine setting, and it is the agents who can execute them. The private information of an agent is the time it will require to execute each job. Several incentive compatible mechanisms are presented that are based on approximation algorithms for the computationally infeasible optimization problem. This paper also launched the area of algorithmic mechanism design, in which the mechanism must sat69 isfy computational requirements in addition to the standard incentive requirements. A growing sub-field in this area is multicast cost-sharing mechanism design (see, e.g., [1]), in which the mechanism must efficiently determine, for each agent in a multicast tree, whether the agent receives the transmission and the price it must pay. For a survey of this and other topics in distributed algorithmic mechanism design, see [9].
Online execution presents a different type of algorithmic challenge, and several other papers study online algorithms or mechanisms in economic settings. For example, [5] considers an online market clearing setting, in which the auctioneer matches buy and sells bids (which are assumed to be exogenous) that arrive and expire over time. In [2], a general method is presented for converting an online algorithm into an online mechanism that is incentive compatible with respect to values. Truthful declaration of values is also considered in [3] and [16], which both consider multi-unit online auctions. The main difference between the two is that the former considers the case of a digital good, which thus has unlimited supply. It is pointed out in [16] that their results continue to hold when the setting is extended so that bidders can delay their arrival.
The only other paper we are aware of that addresses the issue of incentive compatibility in a real-time system is [11], which considers several variants of a model in which the center allocates bandwidth to agents who declare both their value and their arrival time. A dominant strategy IC mechanism is presented for the variant in which every point in time is essentially independent, while a Bayes-Nash IC mechanism is presented for the variant in which the center"s current decision affects the cost of future actions.
In this paper, we considered an online scheduling domain for which algorithms with the best possible competitive ratio had been found, but for which new solutions were required when the setting is extended to include self-interested agents. We presented a mechanism that is incentive compatible with respect to release time, deadline, length and value, and that only increases the competitive ratio by one. We also showed how this mechanism could be simplified when k = 1 and each agent cannot lie about the length of its job.
We then showed a matching lower bound on the competitive ratio that can be achieved by a deterministic mechanism that never pays the agents.
Several open problems remain in this setting. One is to determine whether the lower bound can be strengthened by removing the restriction of non-negative payments. Also, while we feel that it is reasonable to strengthen the assumption of knowing the maximum possible ratio of value densities (k) to knowing the actual range of possible value densities, it would be interesting to determine whether there exists a ((1 + √ k)2 + 1)-competitive mechanism under the original assumption. Finally, randomized mechanisms provide an unexplored area for future work.
[1] A. Archer, J. Feigenbaum, A. Krishnamurthy,
R. Sami, and S. Shenker, Approximation and collusion in multicast cost sharing, Games and Economic Behavior (to appear). [2] B. Awerbuch, Y. Azar, and A. Meyerson, Reducing truth-telling online mechanisms to online optimization, Proceedings on the 35th Symposium on the Theory of Computing, 2003. [3] Z. Bar-Yossef, K. Hildrum, and F. Wu,
Incentive-compatible online auctions for digital goods,
Proceedings of the 13th Annual ACM-SIAM Symposium on Discrete Algorithms, 2002. [4] S. Baruah, G. Koren, D. Mao, B. Mishra,
A. Raghunathan, L. Rosier, D. Shasha, and F. Wang,
On the competitiveness of on-line real-time task scheduling, Journal of Real-Time Systems 4 (1992), no. 2, 125-144. [5] A. Blum, T. Sandholm, and M. Zinkevich, Online algorithms for market clearing, Proceedings of the 13th Annual ACM-SIAM Symposium on Discrete Algorithms, 2002. [6] A. Borodin and R. El-Yaniv, Online computation and competitive analysis, Cambridge University Press,
[7] R. Buyya, D. Abramson, J. Giddy, and H. Stockinger,
Economic models for resource management and scheduling in grid computing, The Journal of Concurrency and Computation: Practice and Experience 14 (2002), 1507-1542. [8] N. Camiel, S. London, N. Nisan, and O. Regev, The popcorn project: Distributed computation over the internet in java, 6th International World Wide Web Conference, 1997. [9] J. Feigenbaum and S. Shenker, Distributed algorithmic mechanism design: Recent results and future directions, Proceedings of the 6th International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications, 2002, pp. 1-13. [10] A. Fiat and G. Woeginger (editors), Online algorithms: The state of the art, Springer Verlag, 1998. [11] E. Friedman and D. Parkes, Pricing wifi at starbucksissues in online mechanism design, EC"03, 2003. [12] R. L. Graham, Bounds for certain multiprocessor anomalies, Bell System Technical Journal 45 (1966), 1563-1581. [13] B. Kalyanasundaram and K. Pruhs, Speed is as powerful as clairvoyance, Journal of the ACM 47 (2000), 617-643. [14] C. Koo, T. Lam, T. Ngan, and K. To, On-line scheduling with tight deadlines, Theoretical Computer Science 295 (2003), 251-261. [15] G. Koren and D. Shasha, D-over: An optimal on-line scheduling algorithm for overloaded real-time systems,
SIAM Journal of Computing 24 (1995), no. 2, 318-339. [16] R. Lavi and N. Nisan, Competitive analysis of online auctions, EC"00, 2000. [17] A. Mas-Colell, M. Whinston, and J. Green,

Many peer-to-peer (P2P) systems rely on cooperation among selfinterested users. For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3]. In a wireless ad-hoc network, overall packet latency and loss rate increase when nodes refuse to forward packets on behalf of others [26]. Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6]. In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance. As a result, each user"s attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest. A wants service from B,
B wants service form C, and C wants service from A. utility of the system. Avoiding this tragedy of the commons [18] requires incentives for cooperation.
We adopt a game-theoretic approach in addressing this problem. In particular, we use a prisoners" dilemma model to capture the essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.
While social dilemmas have been studied extensively, P2P applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing system such as Gnutella and KaZaa can exceed 100, 000 simultaneous users, and nodes can have an average life-time of the order of minutes [33]. • Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest. In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A. • Zero-cost identity: Many P2P systems allow peers to continuously switch identities (i.e., whitewash).
Strategies that work well in traditional prisoners" dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.
Therefore, we propose a family of scalable and robust incentive techniques, based upon a novel Reciprocative decision function, to address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires familiarity between entities either directly or indirectly.
However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity. We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative decision function can scale to large populations and moderate levels of turnover. • Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history. Consider the example in Figure 1. If everyone provides service, then the system operates optimally. However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc. We show that with shared history, B knows that A served C and consequently will serve A. This results in a higher level of cooperation than with private history. The cost of shared history is a distributed infrastructure (e.g., distributed hash table-based storage) to store the history. • Maxflow-based Subjective Reputation: Shared history creates the possibility for collusion. In the example in Figure 1,
C can falsely claim that A served him, thus deceiving B into providing service. We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population. The basic idea is that B would only believe C if C had already provided service to B. The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.
To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for complexity of computation. We show that the maxflow-based algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20]. • Adaptive Stranger Policy: Zero-cost identities allows noncooperating peers to escape the consequences of not cooperating and eventually destroy cooperation in the system if not stopped. We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system. The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25]. • Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.
The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host). Long-term history exacerbates this problem by allowing peers with many previous transactions to exploit that history for many new transactions. We show that short-term history prevents traitors from disrupting cooperation.
The rest of the paper is organized as follows. We describe the model in Section 2 and the reciprocative decision function in Section 3. We then proceed to the incentive techniques in Section 4. In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history. In Section 4.2, we describe collusion and demonstrate how subjective reputation mitigates it. In Section 4.3, we present the problem of zero-cost identities and show how an adaptive stranger policy promotes persistent identities. In Section 4.4, we show how traitors disrupt cooperation and how short-term history deals with them. We discuss related work in Section 5 and conclude in Section 6.
In this section, we present our assumptions about P2P systems and their users, and introduce a model that aims to capture the behavior of users in a P2P system.
We assume a P2P system in which users are strategic, i.e., they act rationally to maximize their benefit. However, to capture some of the real-life unpredictability in the behavior of users, we allow users to randomly change their behavior with a low probability (see Section 2.4).
For simplicity, we assume a homogeneous system in which all peers issue and satisfy requests at the same rate. A peer can satisfy any request, and, unless otherwise specified, peers request service uniformly at random from the population.1 . Finally, we assume that all transactions incur the same cost to all servers and provide the same benefit to all clients.
We assume that users can pollute shared history with false recommendations (Section 4.2), switch identities at zero-cost (Section 4.3), and spoof other users (Section 4.4). We do not assume any centralized trust or centralized infrastructure.
To aid the development and study of the incentive schemes, in this section we present a model of the users" behaviors. In particular, we model the benefits and costs of P2P interactions (the game) and population dynamics caused by mutation, learning, and turnover.
Our model is designed to have the following properties that characterize a large set of P2P systems: • Social Dilemma: Universal cooperation should result in optimal overall utility, but individuals who exploit the cooperation of others while not cooperating themselves (i.e., defecting) should benefit more than users who do cooperate. • Asymmetric Transactions: A peer may want service from another peer while not currently being able to provide the service that the second peer wants. Transactions should be able to have asymmetric payoffs. • Untraceable Defections: A peer should not be able to determine the identity of peers who have defected on her. This models the difficulty or expense of determining that a peer could have provided a service, but didn"t. For example, in the Gnutella file sharing system [21], a peer may simply ignore queries despite possessing the desired file, thus preventing the querying peer from identifying the defecting peer. • Dynamic Population: Peers should be able to change their behavior and enter or leave the system independently and continuously. 1The exception is discussed in Section 4.1.1 103 Cooperate Defect Cooperate DefectClient Server sc RR / sc ST / sc PP / sc TS / Figure 2: Payoff matrix for the Generalized Prisoner"s Dilemma. T, R,
P, and S stand for temptation, reward, punishment and sucker, respectively.
The Prisoner"s Dilemma, developed by Flood, Dresher, and Tucker in 1950 [22] is a non-cooperative repeated game satisfying the social dilemma requirement. Each game consists of two players who can defect or cooperate. Depending how each acts, the players receive a payoff. The players use a strategy to decide how to act.
Unfortunately, existing work either uses a specific asymmetric payoff matrix or only gives the general form for a symmetric one [4].
Instead, we use the Generalized Prisoner"s Dilemma (GPD), which specifies the general form for an asymmetric payoff matrix that preserves the social dilemma. In the GPD, one player is the client and one player is the server in each game, and it is only the decision of the server that is meaningful for determining the outome of the transaction. A player can be a client in one game and a server in another. The client and server receive the payoff from a generalized payoff matrix (Figure 2). Rc, Sc, Tc, and Pc are the client"s payoff and Rs, Ss, Ts, and Ps are the server"s payoff. A GPD payoff matrix must have the following properties to create a social dilemma:
defection (Rs + Rc > Ps + Pc).
suckering the other (Rs + Rc > Sc + Ts and Rs + Rc > Ss + Tc).
individual level for the entity who decides whether to cooperate or defect: (Ts ≥ Rs and Ps ≥ Ss and (Ts > Rs or Ps > Ss)) The last set of inequalities assume that clients do not incur a cost regardless of whether they cooperate or defect, and therefore clients always cooperate. These properties correspond to similar properties of the classic Prisoner"s Dilemma and allow any form of asymmetric transaction while still creating a social dilemma.
Furthermore, one or more of the four possible actions (client cooperate and defect, and server cooperate and defect) can be untraceable. If one player makes an untraceable action, the other player does not know the identity of the first player.
For example, to model a P2P application like file sharing or overlay routing, we use the specific payoff matrix values shown in Figure 3. This satisfies the inequalities specified above, where only the server can choose between cooperating and defecting. In addition, for this particular payoff matrix, clients are unable to trace server defections. This is the payoff matrix that we use in our simulation results.
Request Service Don't Request
Provide Service Ignore Request Client Server Figure 3: The payoff matrix for an application like P2P file sharing or overlay routing.
A characteristic of P2P systems is that peers change their behavior and enter or leave the system independently and continuously.
Several studies [4] [28] of repeated Prisoner"s Dilemma games use an evolutionary model [19] [34] of population dynamics. An evolutionary model is not suitable for P2P systems because it only specifies the global behavior and all changes occur at discrete times.
For example, it may specify that a population of 5 100% Cooperate players and 5 100% Defect players evolves into a population with 3 and 7 players, respectively. It does not specify which specific players switched. Furthermore, all the switching occurs at the end of a generation instead of continuously, like in a real P2P system. As a result, evolutionary population dynamics do not accurately model turnover, traitors, and strangers.
In our model, entities take independent and continuous actions that change the composition of the population. Time consists of rounds.
In each round, every player plays one game as a client and one game as a server. At the end of a round, a player may: 1) mutate 2) learn, 3) turnover, or 4) stay the same. If a player mutates, she switches to a randomly picked strategy. If she learns, she switches to a strategy that she believes will produce a higher score (described in more detail below). If she maintains her identity after switching strategies, then she is referred to as a traitor. If a player suffers turnover, she leaves the system and is replaced with a newcomer who uses the same strategy as the exiting player.
To learn, a player collects local information about the performance of different strategies. This information consists of both her personal observations of strategy performance and the observations of those players she interacts with. This models users communicating out-of-band about how strategies perform. Let s be the running average of the performance of a player"s current strategy per round and age be the number of rounds she has been using the strategy. A strategy"s rating is RunningAverage(s ∗ age) RunningAverage(age) .
We use the age and compute the running average before the ratio to prevent young samples (which are more likely to be outliers) from skewing the rating. At the end of a round, a player switches to highest rated strategy with a probability proportional to the difference in score between her current strategy and the highest rated strategy. 104
FUNCTION In this section, we present the new decision function, Reciprocative, that is the basis for our incentive techniques. A decision function maps from a history of a player"s actions to a decision whether to cooperate with or defect on that player. A strategy consists of a decision function, private or shared history, a server selection mechanism, and a stranger policy. Our approach to incentives is to design strategies which maximize both individual and social benefit.
Strategic users will choose to use such strategies and thereby drive the system to high levels of cooperation. Two examples of simple decision functions are 100% Cooperate and 100% Defect. 100% Cooperate models a naive user who does not yet realize that she is being exploited. 100% Defect models a greedy user who is intent on exploiting the system. In the absence of incentive techniques, 100% Defect users will quickly dominate the 100% Cooperate users and destroy cooperation in the system.
Our requirements for a decision function are that (1) it can use shared and subjective history, (2) it can deal with untraceable defections, and (3) it is robust against different patterns of defection.
Previous decision functions such as Tit-for-Tat[4] and Image[28] (see Section 5) do not satisfy these criteria. For example, Tit-for-Tat and Image base their decisions on both cooperations and defections, therefore cannot deal with untraceable defections . In this section and the remaining sections we demonstrate how the Reciprocativebased strategies satisfy all of the requirements stated above.
The probability that a Reciprocative player cooperates with a peer is a function of its normalized generosity. Generosity measures the benefit an entity has provided relative to the benefit it has consumed. This is important because entities which consume more services than they provide, even if they provide many services, will cause cooperation to collapse. For some entity i, let pi and ci be the services i has provided and consumed, respectively. Entity i"s generosity is simply the ratio of the service it provides to the service it consumes: g(i) = pi/ci. (1) One possibility is to cooperate with a probability equal to the generosity. Although this is effective in some cases, in other cases, a Reciprocative player may consume more than she provides (e.g., when initially using the Stranger Defect policy in 4.3). This will cause Reciprocative players to defect on each other. To prevent this situation, a Reciprocative player uses its own generosity as a measuring stick to judge its peer"s generosity. Normalized generosity measures entity i"s generosity relative to entity j"s generosity. More concretely, entity i"s normalized generosity as perceived by entity j is gj(i) = g(i)/g(j). (2) In the remainder of this section, we describe our simulation framework, and use it to demonstrate the benefits of the baseline Reciprocative decision function.
Parameter Nominal value Section Population Size 100 2.4 Run Time 1000 rounds 2.4 Payoff Matrix File Sharing 2.3 Ratio using 100% Cooperate 1/3 3 Ratio using 100% Defect 1/3 3 Ratio using Reciprocative 1/3 3 Mutation Probability 0.0 2.4 Learning Probability 0.05 2.4 Turnover Probability 0.0001 2.4 Hit Rate 1.0 4.1.1 Table 1: Default simulation parameters.
Our simulator implements the model described in Section 2. We use the asymmetric file sharing payoff matrix (Figure 3) with untraceable defections because it models transactions in many P2P systems like file-sharing and packet forwarding in ad-hoc and overlay networks. Our simulation study is composed of different scenarios reflecting the challenges of various non-cooperative behaviors.
Table 1 presents the nominal parameter values used in our simulation.
The Ratio using rows refer to the initial ratio of the total population using a particular strategy. In each scenario we vary the value range of a specific parameter to reflect a particular situation or attack. We then vary the exact properties of the Reciprocative strategy to defend against that situation or attack.
0 20 40 60 80 100 120 0 200 400 600 800 1000 Population Time (a) Total Population: 60 0 20 40 60 80 100 120 0 200 400 600 800 1000 Time (b) Total Population: 120 Defector Cooperator Recip. Private Figure 4: The evolution of strategy populations over time. Time the number of elapsed rounds. Population is the number of players using a strategy.
In this section, we present the dynamics of the game for the basic scenario presented in Table 1 to familiarize the reader and set a baseline for more complicated scenarios. Figures 4(a) (60 players) and (b) (120 players) show players switching to higher scoring strategies over time in two separate runs of the simulator. Each point in the graph represents the number of players using a particular strategy at one point in time. Figures 5(a) and (b) show the corresponding mean overall score per round. This measures the degree of cooperation in the system: 6 is the maximum possible (achieved when everybody cooperates) and 0 is the minimum (achieved when everybody defects). From the file sharing payoff matrix, a net of 6 means everyone is able to download a file and a 0 means that no one 105 0 1 2 3 4 5 6 0 200 400 600 800 1000 MeanOverallScore/Round Time (a) Total Population: 60 0 1 2 3 4 5 6 0 200 400 600 800 1000 Time (b) Total Population: 120 Figure 5: The mean overall per round score over time. is able to do so. We use this metric in all later results to evaluate our incentive techniques.
Figure 5(a) shows that the Reciprocative strategy using private history causes a system of 60 players to converge to a cooperation level of 3.7, but drops to 0.5 for 120 players. One would expect the
because all the defectors are eliminated from the system. It does not because of asymmetry of interest. For example, suppose player B is using Reciprocative with private history. Player A may happen to ask for service from player B twice in succession without providing service to player B in the interim. Player B does not know of the service player A has provided to others, so player B will reject service to player A, even though player A is cooperative. We discuss solutions to asymmetry of interest and the failure of Reciprocative in the 120 player system in Section 4.1.
TECHNIQUES In this section we present our incentives techniques and evaluate their behavior by simulation. To make the exposition clear we group our techniques by the challenges they address: large populations and high turnover (Section 4.1), collusions (Section 4.2), zero-cost identities (Section 4.3), and traitors (Section 4.4).
The large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.
Under these conditions, basing decisions only on private history (records about interactions the peer has been directly involved in) is not effective. In addition, private history does not deal well with asymmetry of interest. For example, if player B has cooperated with others but not with player A himself in the past, player A has no indication of player B"s generosity, thus may unduly defect on him.
We propose two mechanisms to alleviate the problem of few repeat transactions: server-selection and shared history.
A natural way to increase the probability of interacting with familiar peers is by discriminating server selection. However, the asymmetry of transactions challenges selection mechanisms. Unlike in the prisoner"s dilemma payoff matrix, where players can benefit one another within a single transaction, transactions in GPD are asymmetric. As a result, a player who selects her donor for the second time without contributing to her in the interim may face a defection.
In addition, due to untraceability of defections, it is impossible to maintain blacklists to avoid interactions with known defectors.
In order to deal with asymmetric transactions, every player holds (fixed size) lists of both past donors and past recipients, and selects a server from one of these lists at random with equal probabilities. This way, users approach their past recipients and give them a chance to reciprocate.
In scenarios with selective users we omit the complete availability assumption to prevent players from being clustered into a lot of very small groups; thus, we assume that every player can perform the requested service with probability p (for the results presented in this section, p = .3). In addition, in order to avoid bias in favor of the selective players, all players (including the non-discriminative ones) select servers for games.
Figure 6 demonstrates the effectiveness of the proposed selection mechanism in scenarios with large population sizes. We fix the initial ratio of Reciprocative in the population (33%) while varying the population size (between 24 to 1000) (Notice that while in Figures 4(a) and (b), the data points demonstrates the evolution of the system over time, each data point in this figure is the result of an entire simulation for a specific scenario). The figure shows that the Reciprocative decision function using private history in conjunction with selective behavior can scale to large populations.
In Figure 7 we fix the population size and vary the turnover rate.
It demonstrates that while selective behavior is effective for low turnover rates, as turnover gets higher, selective behavior does not scale. This occurs because selection is only effective as long as players from the past stay alive for long enough such that they can be selected for future games.
In order to mitigate asymmetry of interest and scale to higher turnover rate, there is a need in shared history. Shared history means that every peer keeps records about all of the interactions that occur in the system, regardless of whether he was directly involved in them or not. It allows players to leverage off of the experiences of others in cases of few repeat transactions. It only requires that someone has interacted with a particular player for the entire population to observe it, thus scales better to large populations and high turnovers, and also tolerates asymmetry of interest. Some examples of shared history schemes are [20] [23] [28].
Figure 7 shows the effectiveness of shared history under high turnover rates. In this figure, we fix the population size and vary the turnover rate. While selective players with private history can only tolerate a moderate turnover, shared history scales to turnovers of up to approximately 0.1. This means that 10% of the players leave the system at the end of each round. In Figure 6 we fix the turnover and vary the population size. It shows that shared history causes the system to converge to optimal cooperation and performance, regardless of the size of the population.
These results show that shared history addresses all three challenges of large populations, high turnover, and asymmetry of transactions. Nevertheless, shared history has two disadvantages. First, 106 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400 MeanOverallScore/Round NumPlayers Shared Non-Sel Private Non-Sel Private Selective Figure 6: Private vs. Shared History as a function of population size. 0 1 2 3 4 5 6
MeanOverallScore/Round Turnover Shared Non-Sel Private Non-Sel Private Selective Figure 7: Performance of selection mechanism under turnover. The x-axis is the turnover rate. The y-axis is the mean overall per round score. while a decentralized implementation of private history is straightforward, implementation of shared-history requires communication overhead or centralization. A decentralized shared history can be implemented, for example, on top of a DHT, using a peer-to-peer storage system [36] or by disseminating information to other entities in a similar way to routing protocols. Second, and more fundamental, shared history is vulnerable to collusion. In the next section we propose a mechanism that addresses this problem.
Attacks
While shared history is scalable, it is vulnerable to collusion.
Collusion can be either positive (e.g. defecting entities claim that other defecting entities cooperated with them) or negative (e.g. entities claim that other cooperative entities defected on them). Collusion subverts any strategy in which everyone in the system agrees on the reputation of a player (objective reputation). An example of objective reputation is to use the Reciprocative decision function with shared history to count the total number of cooperations a player has given to and received from all entities in the system; another example is the Image strategy [28]. The effect of collusion is magnified in systems with zero-cost identities, where users can create fake identities that report false statements.
Instead, to deal with collusion, entities can compute reputation subjectively, where player A weighs player B"s opinions based on how much player A trusts player B. Our subjective algorithm is based on maxflow [24] [32]. Maxflow is a graph theoretic problem, which given a directed graph with weighted edges asks what is the greatest rate at which material can be shipped from the source to the target without violating any capacity constraints. For example, in figure 8 each edge is labeled with the amount of traffic that can travel on it.
The maxflow algorithm computes the maximum amount of traffic that can go from the source (s) to the target (t) without violating the constraints. In this example, even though there is a loop of high capacity edges, the maxflow between the source and the target is only
in the solution). 100(0) 1(1) 5(1) s t 10(1) 100(1) 1(1) 100(1) 20(0) Figure 8: Each edge in the graph is labeled with its capacity and the actual flow it carries in brackets. The maxflow between the source and the target in the graph is 2.
C C CCCC 100100100100 100 00 0 0 20 20 0 0 A B Figure 9: This graph illustrates the robustness of maxflow in the presence of colluders who report bogus high reputation values.
We apply the maxflow algorithm by constructing a graph whose vertices are entities and the edges are the services that entities have received from each other. This information can be stored using the same methods as the shared history. A maxflow is the greatest level of reputation the source can give to the sink without violating reputation capacity constraints. As a result, nodes who dishonestly report high reputation values will not be able to subvert the reputation system.
Figure 9 illustrates a scenario in which all the colluders (labeled with C) report high reputation values for each other. When node A computes the subjective reputation of B using the maxflow algorithm, it will not be affected by the local false reputation values, rather the maxflow in this case will be 0. This is because no service has been received from any of the colluders. 107 In our algorithm, the benefit that entity i has received (indirectly) from entity j is the maxflow from j to i. Conversely, the benefit that entity i has provided indirectly to j is the maxflow from i to j. The subjective reputation of entity j as perceived by i is: min maxflow(j to i) maxflow(i to j) , 1 (3) 0 1 2 3 4 5 6 0 100 200 300 400 500 600 700 800 900 1000 MeanOverallScore/Round Population Shared Private Subjective Figure 10: Subjective shared history compared to objective shared history and private history in the presence of colluders.
Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time of Maxflow to a constant. method CTMaxflow(self, src, dst) 1: self.surplus ← self.surplus + self.increment {Use the running mean as a prediction.} 2: if random() > (0.5∗self.surplus/self.mean iterations) then 3: return None {Not enough surplus to run.} 4: end if {Get the flow and number of iterations used from the maxflow alg.} 5: flow, iterations ← Maxflow(self.G, src, dst) 6: self.surplus ← self.surplus − iterations {Keep a running mean of the number of iterations used.} 7: self.mean iterations ← self.α ∗ self.mean iterations + (1 − self.α) ∗ iterations 8: return flow The cost of maxflow is its long running time. The standard preflowpush maxflow algorithm has a worst case running time of O(V 3 ).
Instead, we use Algorithm 1 which has a constant mean running time, but sometimes returns no flow even though one exists. The essential idea is to bound the mean number of nodes examined during the maxflow computation. This bounds the overhead, but also bounds the effectiveness. Despite this, the results below show that a maxflow-based Reciprocative decision function scales to higher populations than one using private history.
Figure 10 compares the effectiveness of subjective reputation to objective reputation in the presence of colluders. In these scenarios, defectors collude by claiming that other colluders that they encounter gave them 100 cooperations for that encounter. Also, the parameters for Algorithm 1 are set as follows: increment = 100, α = 0.9.
As in previous sections, Reciprocative with private history results in cooperation up to a point, beyond which it fails. The difference here is that objective shared history fails for all population sizes.
This is because the Reciprocative players cooperate with the colluders because of their high reputations. However, subjective history can reach high levels of cooperation regardless of colluders.
This is because there are no high weight paths in the cooperation graph from colluders to any non-colluders, so the maxflow from a colluder to any non-colluder is 0. Therefore, a subjective Reciprocative player will conclude that that colluder has not provided any service to her and will reject service to the colluder. Thus, the maxflow algorithm enables Reciprocative to maintain the scalability of shared history without being vulnerable to collusion or requiring centralized trust (e.g., trusted peers). Since we bound the running time of the maxflow algorithm, cooperation decreases as the population size increases, but the key point is that the subjective Reciprocative decision function scales to higher populations than one using private history. This advantage only increases over time as CPU power increases and more cycles can be devoted to running the maxflow algorithm (by increasing the increment parameter).
Despite the robustness of the maxflow algorithm to the simple form of collusion described previously, it still has vulnerabilities to more sophisticated attacks. One is for an entity (the mole) to provide service and then lie positively about other colluders. The other colluders can then exploit their reputation to receive service. However, the effectiveness of this attack relies on the amount of service that the mole provides. Since the mole is paying all of the cost of providing service and receiving none of the benefit, she has a strong incentive to stop colluding and try another strategy. This forces the colluders to use mechanisms to maintain cooperation within their group, which may drive the cost of collusion to exceed the benefit.
Another attack is for a defector to lie about receiving or providing service to another entity. There are four possibile actions that can be lied about: providing service, not providing service, receiving service, and not receiving service. Falsely claiming to receive service is the simple collusion attack described above. Falsely claiming not to have provided service provides no benefit to the attacker.
Falsely claiming to have provided service or not to have received it allows an attacker to boost her own reputation and/or lower the reputation of another entity. An entity may want to lower another entity"s reputation in order to discourage others from selecting it and exclusively use its service. These false claims are clearly identifiable in the shared history as inconsistencies where one entity claims a transaction occurred and another claims it did not. To limit this attack, we modify the maxflow algorithm so that an entity always believes the entity that is closer to him in the flow graph. If both entities are equally distant, then the disputed edge in the flow is not critical to the evaluation and is ignored. This modification prevents those cases where the attacker is making false claims about an entity that is closer than her to the evaluating entity, which prevents her from boosting her own reputation. The remaining possibilities are for the attacker to falsely claim to have provided service to or not to have received it from a victim entity that is farther from the evalulator than her. In these cases, an attacker can only lower the reputation of the victim. The effectiveness of doing this is limited by the number of services provided and received by the attacker, which makes executing this attack expensive. 108
History assumes that entities maintain persistent identities.
However, in most P2P systems, identities are zero-cost. This is desirable for network growth as it encourages newcomers to join the system.
However, this also allows misbehaving users to escape the consequences of their actions by switching to new identities (i.e., whitewashing). Whitewashers can cause the system to collapse if they are not punished appropriately. Unfortunately, a player cannot tell if a stranger is a whitewasher or a legitimate newcomer. Always cooperating with strangers encourages newcomers to join, but at the same time encourages whitewashing behavior. Always defecting on strangers prevents whitewashing, but discourages newcomers from joining and may also initiate unfavorable cycles of defection.
This tension suggests that any stranger policy that has a fixed probability of cooperating with strangers will fail by either being too stingy when most strangers are newcomers or too generous when most strangers are whitewashers. Our solution is the Stranger Adaptive stranger policy. The idea is to be generous to strangers when they are being generous and stingy when they are stingy.
Let ps and cs be the number of services that strangers have provided and consumed, respectively. The probability that a player using Stranger Adaptive helps a stranger is ps/cs. However, we do not wish to keep these counts permanently (for reasons described in Section 4.4). Also, players may not know when strangers defect because defections are untraceable (as described in Section 2).
Consequently, instead of keeping ps and cs, we assume that k = ps + cs, where k is a constant and we keep the running ratio r = ps/cs.
When we need to increment ps or cs, we generate the current values of ps and cs from k and r: cs = k/(1 + r) ps = cs ∗ r We then compute the new r as follows: r = (ps + 1)/cs , if the stranger provided service r = ps/(cs + 1) , if the stranger consumed service This method allows us to keep a running ratio that reflects the recent generosity of strangers without knowing when strangers have defected. 0 1 2 3 4 5 6
MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 11: Different stranger policies for Reciprocative with shared history. The x-axis is the turnover rate on a log scale. The y-axis is the mean overall per round score.
Figures 11 and 12 compare the effectiveness of the Reciprocative strategy using different policies toward strangers. Figure 11 0 1 2 3 4 5 6
MeanOverallScore/Round Turnover Stranger Cooperate Stranger Defect Stranger Adaptive Figure 12: Different stranger policies for Reciprocative with private history. The x-axis is the turnover rate on a log scale. The y-axis is the mean overall per round score. compares different stranger policies for Reciprocative with shared history, while Figure 12 is with private history. In both figures, the players using the 100% Defect strategy change their identity (whitewash) after every transaction and are indistinguishable from legitimate newcomers. The Reciprocative players using the Stranger Cooperate policy completely fail to achieve cooperation.
This stranger policy allows whitewashers to maximize their payoff and consequently provides a high incentive for users to switch to whitewashing.
In contrast, Figure 11 shows that the Stranger Defect policy is effective with shared history. This is because whitewashers always appear to be strangers and therefore the Reciprocative players will always defect on them. This is consistent with previous work [13] showing that punishing strangers deals with whitewashers.
However, Figure 12 shows that Stranger Defect is not effective with private history. This is because Reciprocative requires some initial cooperation to bootstrap. In the shared history case, a Reciprocative player can observe that another player has already cooperated with others. With private history, the Reciprocative player only knows about the other players" actions toward her. Therefore, the initial defection dictated by the Stranger Defect policy will lead to later defections, which will prevent Reciprocative players from ever cooperating with each other. In other simulations not shown here, the Stranger Defect stranger policy fails even with shared history when there are no initial 100% Cooperate players.
Figure 11 shows that with shared history, the Stranger Adaptive policy performs as well as Stranger Defect policy until the turnover rate is very high (10% of the population turning over after every transaction). In these scenarios, Stranger Adaptive is using k = 10 and each player keeps a private r. More importantly, it is significantly better than Stranger Defect policy with private history because it can bootstrap cooperation. Although the Stranger Defect policy is marginally more effective than Stranger Adaptive at very high rates of turnover, P2P systems are unlikely to operate there because other services (e.g., routing) also cannot tolerate very high turnover.
We conclude that of the stranger policies that we have explored,
Stranger Adaptive is the most effective. By using Stranger Adaptive, P2P systems with zero-cost identities and a sufficiently low turnover can sustain cooperation without a centralized allocation of identities. 109
Traitors are players who acquire high reputation scores by cooperating for a while, and then traitorously turn into defectors before leaving the system. They model both users who turn deliberately to gain a higher score and cooperators whose identities have been stolen and exploited by defectors. A strategy that maintains longterm history without discriminating between old and recent actions becomes highly vulnerable to exploitation by these traitors.
The top two graphs in Figure 13 demonstrate the effect of traitors on cooperation in a system where players keep long-term history (never clear history). In these simulations, we run for 2000 rounds and allow cooperative players to keep their identities when switching to the 100% Defector strategy. We use the default values for the other parameters. Without traitors, the cooperative strategies thrive.
With traitors, the cooperative strategies thrive until a cooperator turns traitor after 600 rounds. As this cooperator exploits her reputation to achieve a high score, other cooperative players notice this and follow suit via learning. Cooperation eventually collapses. On the other hand, if we maintain short-term history and/or discounting ancient history vis-a-vis recent history, traitors can be quickly detected, and the overall cooperation level stays high, as shown in the bottom two graphs in Figure 13. 0 20 40 60 80 100 1K 2K Long-TermHistory No Traitors Population 0 20 40 60 80 100 1K 2K Traitors Defector Cooperator Recip. Shared 0 20 40 60 80 100 1K 2K Short-TermHistory Time Population 0 20 40 60 80 100 1K 2K Time Figure 13: Keeping long-term vs. short-term history both with and without traitors.
Previous work has examined the incentive problem as applied to societies in general and more recently to Internet applications and peer-to-peer systems in particular. A well-known phenomenon in this context is the tragedy of the commons [18] where resources are under-provisioned due to selfish users who free-ride on the system"s resources, and is especially common in large networks [29] [3].
The problem has been extensively studied adopting a game theoretic approach. The prisoners" dilemma model provides a natural framework to study the effectiveness of different strategies in establishing cooperation among players. In a simulation environment with many repeated games, persistent identities, and no collusion, Axelrod [4] shows that the Tit-for-Tat strategy dominates.
Our model assumes growth follows local learning rather than evolutionary dynamics [14], and also allows for more kinds of attacks.
Nowak and Sigmund [28] introduce the Image strategy and demonstrate its ability to establish cooperation among players despite few repeat transactions by the employment of shared history. Players using Image cooperate with players whose global count of cooperations minus defections exceeds some threshold. As a result, an Image player is either vulnerable to partial defectors (if the threshold is set too low) or does not cooperate with other Image players (if the threshold is set too high).
In recent years, researchers have used economic mechanism design theory to tackle the cooperation problem in Internet applications. Mechanism design is the inverse of game theory. It asks how to design a game in which the behavior of strategic players results in the socially desired outcome. Distributed Algorithmic Mechanism Design seeks solutions within this framework that are both fully distributed and computationally tractable [12]. [10] and [11] are examples of applying DAMD to BGP routing and multicast cost sharing. More recently, DAMD has been also studied in dynamic environments [38]. In this context, demonstrating the superiority of a cooperative strategy (as in the case of our work) is consistent with the objective of incentivizing the desired behavior among selfish players.
The unique challenges imposed by peer-to-peer systems inspired additional body of work [5] [37], mainly in the context of packet forwarding in wireless ad-hoc routing [8] [27] [30] [35], and file sharing [15] [31]. Friedman and Resnick [13] consider the problem of zero-cost identities in online environments and find that in such systems punishing all newcomers is inevitable. Using a theoretical model, they demonstrate that such a system can converge to cooperation only for sufficiently low turnover rates, which our results confirm. [6] and [9] show that whitewashing and collusion can have dire consequences for peer-to-peer systems and are difficult to prevent in a fully decentralized system.
Some commercial file sharing clients [1] [2] provide incentive mechanisms which are enforced by making it difficult for the user to modify the source code. These mechanisms can be circumvented by a skilled user or by a competing company releasing a compatible client without the incentive restrictions. Also, these mechanisms are still vulnerable to zero-cost identities and collusion. BitTorrent [7] uses Tit-for-Tat as a method for resource allocation, where a user"s upload rate dictates his download rate.
In this paper we take a game theoretic approach to the problem of cooperation in peer-to-peer networks. Addressing the challenges imposed by P2P systems, including large populations, high turnover, asymmetry of interest and zero-cost identities, we propose a family of scalable and robust incentive techniques, based upon the Reciprocative decision function, to support cooperative behavior and improve overall system performance.
We find that the adoption of shared history and discriminating server selection techniques can mitigate the challenge of few repeat transactions that arises due to large population size, high turnover and asymmetry of interest. Furthermore, cooperation can be established even in the presence of zero-cost identities through the use of an adaptive policy towards strangers. Finally, colluders and traitors can be kept in check via subjective reputations and short-term history, respectively. 110
We thank Mary Baker, T.J. Giuli, Petros Maniatis, the anonymous reviewer, and our shepherd, Margo Seltzer, for their useful comments that helped improve the paper. This work is supported in part by the National Science Foundation under ITR awards ANI-0085879 and ANI-0331659, and Career award ANI-0133811.
Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NSF, or the U.S. government.
[1] Kazaa. http://www.kazaa.com. [2] Limewire. http://www.limewire.com. [3] ADAR, E., AND HUBERMAN, B. A. Free Riding on Gnutella. First Monday 5, 10 (October 2000). [4] AXELROD, R. The Evolution of Cooperation. Basic Books, 1984. [5] BURAGOHAIN, C., AGRAWAL, D., AND SURI, S. A Game-Theoretic Framework for Incentives in P2P Systems. In International Conference on Peer-to-Peer Computing (Sep 2003). [6] CASTRO, M., DRUSCHEL, P., GANESH, A., ROWSTRON, A., AND WALLACH, D. S. Security for Structured Peer-to-Peer Overlay Networks. In Proceedings of Multimedia Computing and Networking
[7] COHEN, B. Incentives build robustness in bittorrent. In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [8] CROWCROFT, J., GIBBENS, R., KELLY, F., AND ˘ OSTRING, S.
Modeling Incentives for Collaboration in Mobile ad-hoc Networks.
In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [9] DOUCEUR, J. R. The Sybil Attack. In Electronic Proceedings of the International Workshop on Peer-to-Peer Systems (2002). [10] FEIGENBAUM, J., PAPADIMITRIOU, C., SAMI, R., AND SHENKER,
S. A BGP-based Mechanism for Lowest-Cost Routing. In Proceedings of the ACM Symposium on Principles of Distributed Computing (2002). [11] FEIGENBAUM, J., PAPADIMITRIOU, C., AND SHENKER, S. Sharing the Cost of Multicast Transmissions. In Journal of Computer and System Sciences (2001), vol. 63, pp. 21-41. [12] FEIGENBAUM, J., AND SHENKER, S. Distributed Algorithmic Mechanism Design: Recent Results and Future Directions. In Proceedings of the International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications (2002). [13] FRIEDMAN, E., AND RESNICK, P. The Social Cost of Cheap Pseudonyms. Journal of Economics and Management Strategy 10, 2 (1998), 173-199. [14] FUDENBERG, D., AND LEVINE, D. K. The Theory of Learning in Games. The MIT Press, 1999. [15] GOLLE, P., LEYTON-BROWN, K., MIRONOV, I., AND LILLIBRIDGE, M. Incentives For Sharing in Peer-to-Peer Networks.
In Proceedings of the 3rd ACM conference on Electronic Commerce,
October 2001 (2001). [16] GROSS, B., AND ACQUISTI, A. Balances of Power on EBay: Peers or Unquals? In Workshop on economics of peer-to-peer networks (2003). [17] GU, B., AND JARVENPAA, S. Are Contributions to P2P Technical Forums Private or Public Goods? - An Empirical Investigation. In 1st Workshop on Economics of Peer-to-Peer Systems (2003). [18] HARDIN, G. The Tragedy of the Commons. Science 162 (1968), 1243-1248. [19] JOSEF HOFBAUER AND KARL SIGMUND. Evolutionary Games and Population Dynamics. Cambridge University Press, 1998. [20] KAMVAR, S. D., SCHLOSSER, M. T., AND GARCIA-MOLINA, H.
The EigenTrust Algorithm for Reputation Management in P2P Networks. In Proceedings of the Twelfth International World Wide Web Conference (May 2003). [21] KAN, G. Peer-to-Peer: Harnessing the Power of Disruptive Technologies, 1st ed. O"Reilly & Associates, Inc., March 2001, ch. Gnutella, pp. 94-122. [22] KUHN, S. Prisoner"s Dilemma. In The Stanford Encyclopedia of Philosophy, Edward N. Zalta, Ed., Summer ed. 2003. [23] LEE, S., SHERWOOD, R., AND BHATTACHARJEE, B. Cooperative Peer Groups in Nice. In Proceedings of the IEEE INFOCOM (2003). [24] LEVIEN, R., AND AIKEN, A. Attack-Resistant Trust Metrics for Public Key Certification. In Proceedings of the USENIX Security Symposium (1998), pp. 229-242. [25] MANIATIS, P., ROUSSOPOULOS, M., GIULI, T. J., ROSENTHAL,
D. S. H., BAKER, M., AND MULIADI, Y. Preserving Peer Replicas by Rate-Limited Sampled Voting. In ACM Symposium on Operating Systems Principles (2003). [26] MARTI, S., GIULI, T. J., LAI, K., AND BAKER, M. Mitigating Routing Misbehavior in Mobile ad-hoc Networks. In Proceedings of MobiCom (2000), pp. 255-265. [27] MICHIARDI, P., AND MOLVA, R. A Game Theoretical Approach to Evaluate Cooperation Enforcement Mechanisms in Mobile ad-hoc Networks. In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [28] NOWAK, M. A., AND SIGMUND, K. Evolution of Indirect Reciprocity by Image Scoring. Nature 393 (1998), 573-577. [29] OLSON, M. The Logic of Collective Action: Public Goods and the Theory of Groups. Harvard University Press, 1971. [30] RAGHAVAN, B., AND SNOEREN, A. Priority Forwarding in ad-hoc Networks with Self-Ineterested Parties. In Workshop on Economics of Peer-to-Peer Systems (June 2003). [31] RANGANATHAN, K., RIPEANU, M., SARIN, A., AND FOSTER, I.
To Share or Not to Share: An Analysis of Incentives to Contribute in Collaborative File Sharing Environments. In Workshop on Economics of Peer-to-Peer Systems (June 2003). [32] REITER, M. K., AND STUBBLEBINE, S. G. Authentication Metric Analysis and Design. ACM Transactions on Information and System Security 2, 2 (1999), 138-158. [33] SAROIU, S., GUMMADI, P. K., AND GRIBBLE, S. D. A Measurement Study of Peer-to-Peer File Sharing Systems. In Proceedings of Multimedia Computing and Networking 2002 (MMCN "02) (2002). [34] SMITH, J. M. Evolution and the Theory of Games. Cambridge University Press, 1982. [35] URPI, A., BONUCCELLI, M., AND GIORDANO, S. Modeling Cooperation in Mobile ad-hoc Networks: a Formal Description of Selfishness. In Modeling and Optimization in Mobile, ad-hoc and Wireless Networks (2003). [36] VISHNUMURTHY, V., CHANDRAKUMAR, S., AND SIRER, E. G.
KARMA : A Secure Economic Framework for P2P Resource Sharing. In Workshop on Economics of Peer-to-Peer Networks (2003). [37] WANG, W., AND LI, B. To Play or to Control: A Game-based Control-Theoretic Approach to Peer-to-Peer Incentive Engineering.

Diffusion and Networked Coordination Games. A fundamental question in the social sciences is to understand the ways in which new ideas, behaviors, and practices diffuse through populations.
Such issues arise, for example, in the adoption of new technologies, the emergence of new social norms or organizational conventions, or the spread of human languages [2, 14, 15, 16, 17]. An active line of research in economics and mathematical sociology is concerned with modeling these types of diffusion processes as a coordination game played on a social network [1, 5, 7, 13, 19].
We begin by discussing one of the most basic game-theoretic diffusion models, proposed in an influential paper of Morris [13], which will form the starting point for our work here. We describe it in terms of the following technology adoption scenario, though there are many other examples that would serve the same purpose.
Suppose there are two instant messenger (IM) systems A and B, which are not interoperable - users must be on the same system in order to communicate. There is a social network G on the users, indicating who wants to talk to whom, and the endpoints of each edge (v, w) play a coordination game with possible strategies A or B: if v and w each choose IM system B, then they they each receive a payoff of q (since they can talk to each other using system B); if they each choose IM system A, then they they each receive a payoff of 1 − q; and if they choose opposite systems, then they each receive a payoff of 0 (reflecting the lack of interoperability).
Note that A is the better technology if q < 1 2 , in the sense that A-A payoffs would then exceed B-B payoffs, while A is the worse technology if q > 1 2 . 75 A number of qualitative insights can be derived from a diffusion model even at this level of simplicity. Specifically, consider a network G, and let all nodes initially play B. Now suppose a small number of nodes begin adopting strategy A instead. If we apply best-response updates to nodes in the network, then nodes in effect will be repeatedly applying the following simple rule: switch to A if enough of your network neighbors have already adopted A. (E.g. you begin using a particular IM system - or social-networking site, or electronic document format - if enough of your friends are users of it.) As this unfolds, there can be a cascading sequence of nodes switching to A, such that a network-wide equilibrium is reached in the limit: this equilibrium may involve uniformity, with all nodes adopting A; or it may involve coexistence, with the nodes partitioned into a set adopting A and a set adopting B, and edges yielding zero payoff connecting the two sets. Morris [13] provides a set of elegant graph-theoretic characterizations for when these qualitatively different types of equilibria arise, in terms of the underlying network topology and the quality of A relative to B (i.e. the relative sizes of 1 − q and q).
Compatibility, Interoperability, and Bilinguality. In most of the settings that form the motivation for diffusion models, coexistence (however unbalanced) is the typical outcome: for example, human languages and social conventions coexist along geographic boundaries; it is a stable outcome for the financial industry to use Windows while the entertainment industry uses Mac OS. An important piece that is arguably missing from the basic game-theoretic models of diffusion, however, is a more detailed picture of what is happening at the coexistence boundary, where the basic form of the model posits nodes that adopt A linked to nodes that adopt B.
In these motivating settings for the models, of course, one very often sees interface regions in which individuals essentially become bilingual. In the case of human language diffusion, this bilinguality is meant literally: geographic regions where there is substantial interaction with speakers of two different languages tend to have inhabitants who speak both. But bilinguality is also an essential feature of technological interaction: in the end, many people have accounts on multiple IM systems, for example, and more generally many maintain the ability to work within multiple computer systems so as to collaborate with people embedded in each.
Taking this view, it is natural to ask how diffusion models behave when extended so that certain nodes can be bilingual in this very general sense, adopting both strategies at some cost to themselves. What might we learn from such an extension? To begin with, it has the potential to provide a valuable perspective on the question of compatibility and incompatibility that underpins competition among technology companies. There is a large literature on how compatibility among technologies affects competition between firms, and in particular how incompatibility may be a beneficial strategic decision for certain participants in a market [3, 4, 8, 9, 12]. Whinston [18] provides an interesting taxonomy of different kinds of strategic incompatibility; and specific industry case studies (including theoretical perspectives) have recently been carried out for commercial banks [10], copying and imaging technology [11] and instant messenger systems [6].
While these existing models of compatibility capture network effects in the sense that the users in the market prefer to use technology that is more widespread, they do not capture the more finegrained network phenomenon represented by diffusion - that each user is including its local view in the decision, based on what its own social network neighbors are doing. A diffusion model that incorporated such extensions could provide insight into the structure of boundaries in the network between technologies; it could potentially offer a graph-theoretic basis for how incompatibility may benefit an existing technology, by strengthening these boundaries and preventing the incursion of a new, better technology.
The present work: Diffusion with bilingual behavior. In this paper, we develop a set of diffusion models that incorporate notions of compatibility and bilinguality, and we find that some unexpected phenomena emerge even from very simple versions of the models.
We begin with perhaps the simplest way of extending Morris"s model discussed above to incorporate bilingual behavior. Consider again the example of IM systems A and B, with the payoff structure as before, but now suppose that each node can adopt a third strategy, denoted AB, in which it decides to use both A and B. An adopter of AB gets to use, on an edge-by-edge basis, whichever of A or B yields higher payoffs in each interaction, and the payoff structure is defined according to this principle: if an adopter of AB interacts with an adopter of B, both receive q; with an adopter of A, both receive 1 − q; and with another adopter of AB, both receive max(q, 1 − q). Finally, an adopter of AB pays a fixed-cost penalty of c (i.e. −c is added to its total payoff) to represent the cost of having to maintain both technologies.
Thus, in this model, there are two parameters that can be varied: the relative qualities of the two technologies (encoded by q), and the cost of being bilingual, which reflects a type of incompatibility (encoded by c).
Following [13] we assume the underlying graph G is infinite; we further assume that for some natural number Δ, each node has degree Δ.1 We are interested in the question posed at the outset, of whether a new technology A can spread through a network where almost everyone is initially using B. Formally, we say that strategy A can become epidemic if the following holds: starting from a state in which all nodes in a finite set S adopt A, and all other nodes adopt B, a sequence of best-response updates (potentially with tiebreaking) in G − S causes every node to eventually adopt A. We also introduce one additional bit of notation that will be useful in the subsequent sections: we define r = c/Δ, the fixed penalty for adopting AB, scaled so that it is a per-edge cost.
In the Morris model, where the only strategic options are A and B, a key parameter is the contagion threshold of G, denoted q∗ (G): this is the supremum of q for which A can become epidemic in G with parameter q in the payoff structure. A central result of [13] is that 1 2 is the maximum possible contagion threshold for any graph: supG q∗ (G) = 1 2 . Indeed, there exist graphs in which the contagion threshold is as large as 1 2 (including the infinite line - the unique infinite connected 2-regular graph); on the other hand, one can show there is no graph with a contagion threshold greater than 1 2 .
In our model where the bilingual strategy AB is possible, we have a two-dimensional parameter space, so instead of a contagion threshold q∗ (G) we have an epidemic region Ω(G), which is the subset of the (q, r) plane for which A can become epidemic in G. And in place of the maximum possible contagion threshold supG q∗ (G), we must consider the general epidemic region Ω = ∪GΩ(G), where the union is taken over all infinite Δ-regular graphs; this is the set of all (q, r) values for which A can become epidemic in some Δ-regular network. 1 We can obtain strictly analogous results by taking a sequence of finite graphs and expressing results asymptotically, but the use of an infinite bounded-degree graph G makes it conceptually much cleaner to express the results (as it does in Morris"s paper [13]): less intricate quantification is needed to express the diffusion properties, and the qualitative phenomena remain the same. 76 1/20 1 r q 0 1/2 1 Figure 1: The region of the (q, r) plane for which technology A can become epidemic on the infinite line.
Our Results. We find, first of all, that the epidemic region Ω(G) can be unexpectedly complex, even for very simple graphs G.
Figure 1 shows the epidemic region for the infinite line; one observes that neither the region Ω(G) nor its complement is convex in the positive quadrant, due to the triangular cut-out shape. (We find analogous shapes that become even more complex for other simple infinite graph structures; see for example Figures 3 and 4.) In particular, this means that for values of q close to but less than 1 2 , strategy A can become epidemic on the infinite line if r is sufficiently small or sufficiently large, but not if r takes values in some intermediate interval. In other words, strategy B (which represents the worse technology, since q < 1 2 ) will survive if and only if the cost of being bilingual is calibrated to lie in this middle interval.
This is a reflection of limited compatibility - that it may be in the interest of an incumbent technology to make it difficult but not too difficult to use a new technology - and we find it surprising that it should emerge from a basic model on such a simple network structure. It is natural to ask whether there is a qualitative interpretation of how this arises from the model, and in fact it is not hard to give such an interpretation, as follows.
When r is very small, it is cheap for nodes to adopt AB as a strategy, and so AB spreads through the whole network.
Once AB is everywhere, the best-response updates cause all nodes to switch to A, since they get the same interaction benefits without paying the penalty of r.
When r is very large, nodes at the interface, with one A neighbor and one B neighbor, will find it too expensive to choose AB, so they will choose A (the better technology), and hence A will spread step-by-step through the network.
When r takes an intermediate value, a node v at the interface, with one A neighbor and one B neighbor, will find it most beneficial to adopt AB as a strategy. Once this happens, the neighbor of v who is playing B will not have sufficient incentive to switch, and the best-response updates make no further progress. Hence, this intermediate value of r allows a boundary of AB to form between the adopters of A and the adopters of B.
In short, the situation facing B is this: if it is too permissive, it gets invaded by AB followed by A; if it is too inflexible, forcing nodes to choose just one of A or B, it gets destroyed by a cascade of direct conversions to A. But if it has the right balance in the value of r, then the adoptions of A come to a stop at a bilingual boundary where nodes adopt AB.
Moving beyond specific graphs G, we find that this non-convexity holds in a much more general sense as well, by considering the general epidemic region Ω = ∪GΩ(G). For any given value of Δ, the region Ω is a complicated union of bounded and unbounded polygons, and we do not have a simple closed-form description for it.
However, we can show via a potential function argument that no point (q, r) with q > 1 2 belongs to Ω. Moreover, we can show the existence of a point (q, r) ∈ Ω for which q < 1 2 . On the other hand, consideration of the epidemic region for the infinite line shows that (1 2 , r) ∈ Ω for r = 0 and for r sufficiently large. Hence, neither Ω nor its complement is convex in the positive quadrant.
Finally, we also extend a characterization that Morris gave for the contagion threshold [13], producing a somewhat more intricate characterization of the region Ω(G). In Morris"s setting, without an AB strategy, he showed that A cannot become epidemic with parameter q if and only if every cofinite set of nodes contains a subset S that functions as a well-connected community: every node in S has at least a (1 − q) fraction of its neighbors in S. In other words, tightly-knit communities are the natural obstacles to diffusion in his setting. With the AB strategy as a further option, a more complex structure becomes the obstacle: we show that A cannot become epidemic with parameters (q, r) if and only if every cofinite set contains a structure consisting of a tightly-knit community with a particular kind of interface of neighboring nodes. We show that such a structure allows nodes to adopt AB at the interface and B inside the community itself, preventing the further spread of A; and conversely, this is the only way for the spread of A to be blocked.
The analysis underlying the characterization theorem yields a number of other consequences; a basic one is, roughly speaking, that the outcome of best-response updates is independent of the order in which the updates are sequenced (provided only that each node attempts to update itself infinitely often).
Further Extensions. Another way to model compatibility and interoperability in diffusion models is through the off-diagonal terms representing the payoff for interactions between a node adopting A and a node adopting B. Rather than setting these to 0, we can consider setting them to a value x ≤ min(q, 1 − q). We find that for the case of two technologies, the model does not become more general, in that any such instance is equivalent, by a re-scaling of q and r, to one where x = 0. Moreover, using our characterization of the region Ω(G) in terms of communities and interfaces, we show a monotonicty result: if A can become epidemic on a graph G with parameters (q, r, x), and then x is increased, then A can still become epidemic with the new parameters.
We also consider the effect of these off-diagonal terms in an extension to k > 2 competing technologies; for technologies X and Y , let qX denote the payoff from an X-X interaction on an edge and qXY denote the payoff from an X-Y interaction on an edge.
We consider a setting in which two technologies B and C, which initially coexist with qBC = 0, face the introduction of a third, better technology A at a finite set of nodes. We show an example in which B and C both survive in equilibrium if they set qBC in a particular range of values, but not if they set qBC too low or too high to lie in this range. Thus, in even in a basic diffusion model with three technologies, one finds cases in which two firms have an incentive to adopt a limited strategic alliance, partially increasing their interoperability to defend against a new entrant in the market.
We now develop some further notation and definitions that will be useful for expressing the model. Recall that we have an infinite Δ-regular graph G, and strategies A, B, and AB that are used in a coordination game on each edge. For edge (v, w), the payoff 77 to each endpoint is 0 if one of the two nodes chooses strategy A and the other chooses strategy B; 1 − q if one chooses strategy A and the other chooses either A or AB; q if one chooses strategy B and the other chooses either B or AB; and max(q, 1 − q) if both choose strategy AB. The overall payoff of an agent v is the sum of the above values over all neighbors w of v, minus a cost which is 0 if v chooses A or B and c = rΔ if she chooses AB. We refer to the overall game, played by all nodes in G, as a contagion game, and denote it using the tuple (G, q, r).
This game can have many Nash equilibria. In particular, the two states where everybody uses technology A or everybody uses technology B are both equilibria of this game. As discussed in the previous section, we are interested in the dynamics of reaching an equilibrium in this game; in particular, we would like to know whether it is possible to move from an all-B equilibrium to an all-A equilibrium by changing the strategy of a finite number of agents, and following a sequence of best-response moves.
We provide a formal description of this question via the following two definitions.
DEFINITION 2.1. Consider a contagion game (G, q, r). A state in this game is a strategy profile s : V (G) → {A, B, AB}. For two states s and s and a vertex v ∈ V (G), if starting from state s and letting v play her best-response move (breaking ties in favor of A and then AB) we get to the state s , we write s v → s . Similarly, for two states s and s and a finite sequence S = v1, v2, . . . , vk of vertices of G (where vi"s are not necessarily distinct), we say s S → s if there is a sequence of states s1, . . . , sk−1 such that s v1 → s1 v2 → s2 v3 → · · · sk−1 vk → s . For an infinite sequence S = v1, v2, . . . of vertices of G, we denote the subsequence v1, v2, . . . , vk by Sk. We say s S → s for two states s and s if for every vertex v ∈ V (G) there exists a k0(v) such that for every k > k0(v), s Sk → sk for a state sk with sk(v) = s (v).
DEFINITION 2.2. For T ⊆ V (G), we denote by sT the strategy profile that assigns A to every agent in T and B to every agent in V (G) \ T. We say that technology A can become an epidemic in the game (G, q, r) if there is a finite set T of nodes in G (called the seed set) and a sequence S of vertices in V (G) \ T (where each vertex can appear more than once) such that sT S → sV (G), i.e., endowing agents in T with technology A and letting other agents play their best response according to schedule S would lead every agent to eventually adopt strategy A.2 The above definition requires that the all-A equilibrium be reachable from the initial state by at least one schedule S of best-response moves. In fact, we will show in Section 4 that if A can become an epidemic in a game, then for every schedule of best-response moves of the nodes in V (G) \ T in which each node is scheduled an infinite number of times, eventually all nodes adopt strategy A.3
We begin by considering some basic examples that yield epidemic regions with the kinds of non-convexity properties discussed 2 Note that in our definition we assume that agents in T are endowed with the strategy A at the beginning. Alternatively, one can define the notion of epidemic by allowing agents in T to be endowed with any combination of AB and A, or with just AB. However, the difference between these definitions is rather minor and our results carry over with little or no change to these alternative models. 3 Note that we assume agents in the seed set T cannot change their strategy. 0−1 1 2 Figure 2: The thick line graph in Section 1. We first discuss a natural Δ-regular generalization of the infinite line graph, and for this one we work out the complete analysis that describes the region Ω(G), the set of all pairs (q, r) for which the technology A can become an epidemic. We then describe, without the accompanying detailed analysis, the epidemic regions for the infinite Δ-regular tree and for the two-dimensional grid.
The infinite line and the thick line graph. For a given even integer Δ, we define the thick line graph LΔ as follows: the vertex set of this graph is Z × {1, 2, . . . , Δ/2}, where Z is the set of all integers. There is an edge between vertices (x, i) and (x , i ) if and only if |x − x | = 1. For each x ∈ Z, we call the set of vertices {(x, i) : i ∈ {1, . . . , Δ/2} the x"th group of vertices. Figure 2 shows a picture of L6 Now, assume that starting from a position where every node uses the strategy B, we endow all agents in a group (say, group 0) with the strategy A. Consider the decision faced by the agents in group 1, who have their right-hand neighbors using B and their left-hand neighbors using A. For these agents, the payoffs of strategies A,
B, and AB are (1 − q)Δ/2, qΔ/2, and Δ/2 − rΔ, respectively.
Therefore, if q ≤ 1 2 and q ≤ 2r, the best response of such an agent is A. Hence, if the above inequality holds and we let agents in groups 1, −1, 2, −2, . . . play their best response in this order, then A will become an epidemic.
Also, if we have q > 2r and q ≤ 1 − 2r, the best response of an agent with her neighbors on one side playing A and neighbors on the other side playing B is the strategy AB. Therefore, if we let agents in groups 1 and −1 change to their best response, they would switch their strategy to AB. After this, agents in group 2 will see AB on their left and B on their right. For these agents (and similarly for the agents in group −2), the payoff of strategies A, B, and AB are (1−q)Δ/2, qΔ, and (q+max(q, 1−q))Δ/2− rΔ, respectively. Therefore, if max(1, 2q) − 2r ≥ 1 − q and max(1, 2q) − 2r ≥ 2q, or equivalently, if 2r ≤ q and q + r ≤ 1 2 , the best response of such an agent is AB. Hence, if the above inequality holds and we let agents in groups 2, −2, 3, −3 . . . play their best response in this order, then every agent (except for agents in group 0) switches to AB. Next, if we let agents in groups 1, −1, 2, −2, . . . change their strategy again, for q ≤ 1/2, every agent will switch to strategy A, and hence A becomes an epidemic.4 4 Strictly speaking, since we defined a schedule of moves as a single infinite sequence of vertices in V (G) \ T, the order 1, −1, 2, −2, . . . , 1, −1, 2, −2, . . . is not a valid schedule.
However, since vertices of G have finite degree, it is not hard to see that any ordering of a multiset containing any (possibly infinite) 78 1/20 r q 0 1/4 3/16 1/12 1/4 Figure 3: Epidemic regions for the infinite grid 1/20 1/Δ r q 0 1/Δ Figure 4: Epidemic regions for the infinite Δ-regular tree The above argument shows that for any combination of (q, r) parameters in the marked region in Figure 1, technology A can become an epidemic. It is not hard to see that for points outside this region, A cannot become epidemic.
Further examples: trees and grids. Figures 3 and 4 show the epidemic regions for the infinite grid and the infinite Δ-regular tree.
Note they also exhibit non-convexities.
In this section, we characterize equilibrium properties of contagion games. To this end, we must first argue that contagion games in fact have well-defined and stable equilibria. We then discuss some respects in which the equilibrium reached from an initial state is essentially independent of the order in which best-response updates are performed.
We begin with the following lemma, which proves that agents eventually converge to a fixed strategy, and so the final state of a game is well-defined by its initial state and an infinite sequence of moves. Specifically, we prove that once an agent decides to adopt technology A, she never discards it, and once she decides to discard technology B, she never re-adopts it. Thus, after an infinite number of best-response moves, each agent converges to a single strategy.
LEMMA 4.1. Consider a contagion game (G, q, r) and a (possibly infinite) subset T ⊆ V (G) of agents. Let sT be the strategy profile assigning A to every agent in T and B to every agent in V (G) \ T. Let S = v1, v2, . . . be a (possibly infinite) sequence of number of copies of each vertex of V (G) \ T can be turned into an equivalent schedule of moves. For example, the sequence 1, −1, 2, −2, 1, −1, 3, −3, 2, −2, . . . gives the same outcome as 1, −1, 2, −2, . . . , 1, −1, 2, −2, . . . in the thick line example. agents in V (G) \ T and consider the sequence of states s1, s2, . . . obtained by allowing agents to play their best-response in the order defined by S (i.e., s v1 → s1 v2 → s2 v3 → · · · ). Then for every i, one of the following holds: • si(vi+1) = B and si+1(vi+1) = A, • si(vi+1) = B and si+1(vi+1) = AB, • si(vi+1) = AB and si+1(vi+1) = A, • si(vi+1) = si+1(vi+1).
PROOF. Let X >k v Y indicate that agent v (weakly) prefers strategy X to strategy Y in state sk. For any k let zk A, zk B, and zk AB be the number of neighbors of v with strategies A, B, and AB in state sk, respectively. Thus, for agent v in state sk,
v B if (1 − q)(zk A + zk AB) is greater than q(zk B + zk AB),
v AB if (1− q)(zk A + zk AB) is greater than (1− q)zk A + qzk B + max(q, 1 − q)zk AB − Δr,
v B if (1−q)zk A +qzk B +max(q, 1−q)zk AB −Δr is greater than q(zk B + zk AB).
Suppose the lemma is false and consider the smallest i such that the lemma is violated. Let v = vi+1 be the agent who played her best response at time i. Thus, either 1. si(v) = A and si+1(v) = B, or 2. si(v) = A and si+1(v) = AB, or 3. si(v) = AB and si+1(v) = B. We show that in the third case, agent v could not have been playing a best response. The other cases are similar.
In the third case, we have si(v) = AB and si+1(v) = B. As si(v) = AB, there must be a time j < i where sj v → sj+1 and sj+1(v) = AB. Since this was a best-response move for v, inequality 3 implies that (1 − q)zj A + max(0, 1 − 2q)zj AB ≥ Δr.
Furthermore, as i is the earliest time at which the lemma is violated, zi A ≥ zj A and zj AB − zi AB ≤ zi A − zj A. Thus, the change Q in payoff between AB and B (plus Δr) is Q ≡ (1 − q)zi A + max(0, 1 − 2q)zi AB ≥ (1 − q)(zi A − zj A + zj A) + max(0, 1 − 2q)(zj AB − zi A + zj A) = (1 − q)zj A + max(0, 1 − 2q)zj AB + max(q, 1 − q)(zi A − zj A) ≥ (1 − q)zj A + max(0, 1 − 2q)zj AB ≥ Δr, and so, by inequality 3, B can not be a better response than AB for v in state si.
COROLLARY 4.2. For every infinite sequence S of vertices in V (G) \ T, there is a unique state s such that s0 S → s, where s0 denotes the initial state where every vertex in T plays A and every vertex in V (G) \ T plays B.
Such a state s is called the outcome of the game (G, q, r) starting from T and using the schedule S.
Equivalence of best-response schedules. Lemma 4.1 shows that the outcome of a game is well-defined and unique. The following theorems show that the outcome is also invariant to the dynamics, or sequence of best-response moves, under certain mild conditions.
The first theorem states that if the all-A equilibrium is the outcome of a game for some (unconstrained) schedule, then it is the outcome for any schedule in which each vertex is allowed to move infinitely many times. The second theorem states that the outcome of a game is the same for any schedule of moves in which every vertex moves infinitely many times. 79 THEOREM 4.3. Consider a contagion game (G, q, r), a subset T ⊆ V (G), and a schedule S of vertices in V (G) \ T such that the outcome of the game is the all-A equilibrium. Then for any schedule S of vertices in V (G) \ T such that every vertex in this set occurs infinitely many times, the outcome of the game using the schedule S is also the all-A equilibrium.
PROOF. Note that S is a subsequence of S . Let π : S → S be the injection mapping S to its subsequence in S . We show for any vi ∈ S, if vi switches to AB, then π(vi) switches to AB or A, and if vi switches to A, then π(vi) switches to A (here v switches to X means that after the best-response move, the strategy of v is X).
Suppose not and let i be the smallest integer such that the statement doesn"t hold. Let zA, zB, and zAB be the number of neighbors of vi with strategies A, B, and AB in the current state defined by S.
Define zA,zB, and zAB similarly for S . Then, by Lemma 4.1 and the choice of i, zA ≥ zA, zB ≤ zB, zAB − zAB ≤ zB − zB, and zAB − zAB ≤ zA − zA. Now suppose vi switches to AB. Then the same sequence of inequalities as in Lemma 4.1 show that AB is a better response than B for π(vi) (although A might be the best response) and so π(vi) switches to either AB or A. The other case (vi switches to A) is similar.
THEOREM 4.4. Consider a contagion game (G, q, r) and a subset T ⊆ V (G). Then for every two schedules S and S of vertices in V (G)\T such that every vertex in this set occurs infinitely many times in each of these schedules, the outcomes of the game using these schedules are the same.
PROOF. The proof of this theorem is similar to that of theorem 4.3 and is deferred to the full version of the paper.
Blocking structures. Finally, we prove the characterization mentioned in the introduction: A cannot become epidemic if and only if (G, q, r) possesses a certain kind of blocking structure. This result generalizes Morris"s theorem on the contagion threshold for his model; in his case without AB as a possible strategy, a simpler kind of community structure was the obstacle to A becoming epidemic.
We begin by defining the blocking structures.
DEFINITION 4.5. Consider a contagion game (G, q, r). A pair (SAB, SB) of disjoint subsets of V (G) is called a blocking structure for this game if for every vertex v ∈ SAB, degSB (v) > r q Δ, and for every vertex v ∈ SB, (1 − q) degSB (v) + min(q, 1 − q) degSAB (v) > (1 − q − r)Δ, and degSB (v) + q degSAB (v) > (1 − q)Δ, where degS(v) denotes the number of neighbors of v in the set S.
THEOREM 4.6. For every contagion game (G, q, r), technology A cannot become epidemic in this game if and only if every co-finite set of vertices of G contains a blocking structure.
PROOF. We first show that if every co-finite set of vertices of G contains a blocking structure, then technology A cannot become epidemic. Let T be any finite set of vertices endowed with technology A, and let (SAB, SB) be the blocking structure contained in V (G) \ T. We claim that in the outcome of the game for any sequence S of moves, the vertices in SAB have strategy B or AB and the vertices in SB have strategy B. Suppose not and let v be the first vertex in sequence S to violate this (i.e., v ∈ SAB switches to A or v ∈ SB switches to A or AB). Suppose v ∈ SAB (the other cases are similar). Let zA, zB, and zAB denote the number of neighbors of v with strategies A, B, and AB respectively. As v is the first vertex violating the claim, zA ≤ Δ− degSB (v)− degSAB (v) and zB ≥ degSB (v). We show AB is a better strategy than A for v.
To show this, we must prove that (1 − q)zA + qzB + max(q, 1 − q)zAB − Δr > (1 − q)(zA + zAB) or, equivalently, the quantity Q ≡ qzB + max(2q − 1, 0)zAB − Δr > 0: Q = (max(2q − 1, 0) − r)Δ − max(2q − 1, 0)zA +(q − max(2q − 1, 0))zB ≥ (max(2q − 1, 0) − r)Δ + min(q, 1 − q) degSB (v) − max(2q − 1, 0)(Δ − degSB (v) − degSAB (v)) ≥ [min(q, 1 − q) + max(2q − 1, 0)] degSB (v) − rΔ = q degSB (v) − rΔ > 0, where the last inequality holds by the definition of the blocking structure.
We next show that A cannot become epidemic if and only if every co-finite set of vertices contains a blocking structure. To construct a blocking structure for the complement of a finite set T of vertices, endow T with strategy A and consider the outcome of the game for any sequence S which schedules each vertex an infinite number of times. Let SAB be the set of vertices with strategy AB and SB be the set of vertices with strategy B in this outcome. Note for any v ∈ SAB, AB is a best-response and so is strictly better than strategy A, i.e. q degSB (v) + max(q, 1 − q) degSAB −Δr > (1− q) degSAB (v), from where it follows that degSB (v) > (rΔ)/q. The inequalities for the vertices v ∈ SB can be derived in a similar manner.
A corollary to the above theorem is that for every infinite graph G, the epidemic regions in the q-r plane for this graph is a finite union of bounded and unbounded polygons. This is because the inequalities defining blocking structures are linear inequalities in q and r, and the coefficients of these inequalities can take only finitely many values.
GRAPHS The characterization theorem in the previous section provides one way of thinking about the region Ω(G), the set of all (q, r) pairs for which A can become epidemic in the game (G, q, r). We now consider the region Ω = ∪GΩ(G), where the union is taken over all infinite Δ-regular graphs; this is the set of all (q, r) values for which A can become epidemic in some Δ-regular network.
The analysis here uses Lemma 4.1 and an argument based on an appropriately defined potential function.
The first theorem shows that no point (q, r) with q > 1 2 belongs to Ω. Since q > 1 2 implies that the incumbent technology B is superior, it implies that in any network, a superior incumbent will survive for any level of compatibility.
THEOREM 5.1. For every Δ-regular graph G and parameters q and r, the technology A cannot become an epidemic in the game (G, q, r) if q > 1/2.
PROOF. Assume, for contradiction, that there is a Δ-regular graph G and values q > 1/2 and r, a set T of vertices of G that are initially endowed with the strategy A, and a schedule S of moves for vertices in V (G) \ T such that this sequence leads to an all-A equilibrium. We derive a contradiction by defining a non-negative 80 potential function that starts with a finite value and showing that after each best response by some vertex the value of this function decreases by some positive amount bounded away from zero. At any state in the game, let XA,B denote the number of edges in G that have one endpoint using strategy A and the other endpoint using strategy B. Furthermore, let nAB denote the number of agents using the strategy AB. The potential function is the following: qXA,B + cnAB (recall c = Δr is the cost of adopting two technologies). Since G has bounded degree and the initial set T is finite, the initial value of this potential function is finite. We now show that every best response move decreases the value of this function by some positive amount bounded away from zero. By Lemma 4.1, we only need to analyze the effect on the potential function for moves of the sort described by the lemma. Therefore we have three cases: a node u switches from strategy B to AB, a node u switches from strategy AB to A, or a node u switches from strategy B to A. We consider the first case here; the proofs for the other cases are similar.
Suppose a node u with strategy B switches to strategy AB. Let zAB, zA, and zB denote the number of neighbors of u in partition piece AB, A, and B respectively. Thus, recalling that q > 1/2, we see u"s payoff with strategy B is q(zAB + zB) whereas his payoff with strategy AB is q(zAB + zB) + (1 − q)zA − c. In order for this strategic change to improve u"s payoff, it must be the case that (1 − q)zA ≥ c. (1) Now, notice that such a strategic change on the part of u induces a change in the potential function of −qzA + c as zA edges are removed from the XA,B edges between A and B and the size of partition piece AB is increased by one. This change will be negative so long as zA > c/q which holds by inequality 1 as q > (1−q) for q > 1/2. Furthermore, as zA can take only finitely many values (zA ∈ {0, 1, . . . , Δ}), this change is bounded away from zero.
This next theorem shows that for any Δ, there is a point (q, r) ∈ Ω for which q < 1 2 . This means that there is a setting of the parameters q and r for which the new technology A is superior, but for which the incumbent technology is guaranteed to survive regardless of the underlying network.
THEOREM 5.2. There exist q < 1/2 and r such that for every contagion game (G, q, r), A cannot become epidemic.
PROOF. The proof is based on the potential function from Theorem 5.1: qXA,B + cnAB.
We first show that if q is close enough to 1/2 and r is chosen appropriately, this potential function is non-increasing. Specifically, let q = 1 2 − 1 64Δ and c = rΔ = α, where α is any irrational number strictly between 3/64 and q.
Again, there are three cases corresponding to the three possible strategy changes for a node u. Let zAB, zA, and zB denote the number of neighbors of node u in partition piece AB, A, and B respectively.
Case 1: B → AB. Recalling that q < 1/2, we see u"s payoff with strategy B is q(zAB + zB) whereas his payoff with strategy AB is (1 − q)(zAB + zA) + qzB − c. In order for this strategic change to improve u"s payoff, it must be the case that (1 − 2q)zAB + (1 − q)zA ≥ c. (2) Now, notice that such a strategic change on the part of u induces a change in the potential function of −qzA + c as zA edges are removed from the XA,B edges between A and B and the size of partition piece AB is increased by one. This change will be nonpositive so long as zA ≥ c/q. By inequality 2 and the fact that zA is an integer, zA ≥ ‰ c 1 − q − (1 − 2q)zAB 1 − q ı .
Substituting our choice of parameters, (and noting that q ∈ [1/4, 1/2] and zAB ≤ Δ), we see that the term inside the ceiling is less than 1 and at least 3/64 3/4 − 1/32 1/2 > 0. Thus, the ceiling is one, which is larger than c/q.
Case 2: AB → A. Recalling that q < 1/2, we see u"s payoff with strategy AB is (1 − q)(zAB + zA) + qzB − c whereas her payoff with strategy A is (1 − q)(zAB + zA). In order for this strategic change to improve u"s payoff, it must be the case that qzB ≤ c. (3) Such a strategic change on the part of u induces a change in the potential function of qzB −c as zB edges are added to the XA,B edges between A and B and the size of partition piece AB is decreased by one. This change will be non-positive so long as zB ≤ c/q, which holds by inequality 3.
Case 3: B → A. Note u"s payoff with strategy B is q(zAB + zB) whereas his payoff with strategy A is (1 − q)(zAB + zA). In order for this strategic change to improve u"s payoff, it must be the case that (1 − 2q)zAB ≥ qzB − (1 − q)zA. (4) Such a strategic change on the part of u induces a change in the potential function of q(zB − zA) as zA edges are removed and zB edges are added to the XA,B edges between A and B. This change will be negative so long as zB < zA. By inequality 4 and the fact that zA is an integer, zA ≥  qzB 1 − q + (1 − 2q)zAB 1 − q .
Substituting our choice of parameters, it is easy to see that the term inside the floor is at most zB + 1/4, and so the floor is at most zB as zB is an integer. We have shown the potential function is non-increasing for our choice of q and c. This implies the potential function is eventually constant. As c is irrational and the remaining terms are always rational, both nAB and XA,B must remain constant for the potential function as a whole to remain constant.
Suppose A is epidemic in this region. As nAB is constant and A is epidemic, it must be that nAB = 0. Thus, the only moves involve a node u switching from strategy B to strategy A. In order for XA,B to be constant for such moves, it must be that zA (the number of neighbors of u in A) equals zB (the number of neighbors of u in B) and, as nAB = 0, we have that zA = zB = Δ/2. Thus, the payoff of u for strategy A is (1 − q)zA < Δ/4 whereas her payoff for strategy AB is (1−q)zA +qzB −c > Δ/2−q ≥ Δ/4.
This contradicts the assumption that u is playing her best response by switching to A.
We now consider some further ways of modeling compatibility and interoperability. We first consider two technologies, as in the previous sections, and introduce off-diagonal payoffs to capture a positive benefit in direct A-B interactions. We find that this is 81 in fact no more general than the model with zero payoffs for A-B interactions.
We then consider extensions to three technologies, identifying situations in which two coexisting incumbent technologies may or may not want to increases their mutual compatibility in the face of a new, third technology.
Two technologies. A natural relaxation of the two-technology model is to introduce (small) positive payoffs for A-B interaction; that is, cross-technology communication yields some lesser value to both agents. We can model this using a variable xAB representing the payoff gathered by an agent with technology A when her neighbor has technology B, and similarly, a variable xBA representing the payoff gathered by an agent with B when her neighbor has A.
Here we consider the special case in which these off-diagonal entries are symmetric, i.e., xAB = xBA = x. We also assume that x < q ≤ 1 − q.
We first show that the game with off-diagonal entries is equivalent to a game without these entries, under a simple re-scaling of q and r. Note that if we re-scale all payoffs by either an additive or a multiplicative constant, the behavior of the game is unaffected.
Given a game with off-diagonal entries parameterized by q, r and x, consider subtracting x from all payoffs, and scaling up by a factor of 1/(1 − 2x). As can be seen by examining Table 1, the resulting payoffs are exactly those of a game without off-diagonal entries, parameterized by q = (q − x)/(1 − 2x) and r = r/(1 − 2x).
Thus the addition of symmetric off-diagonal entries does not expand the class of games being considered.
Table 1 represents the payoffs in the coordination game in terms of these parameters.
Nevertheless, we can still ask how the addition of an off-diagonal entry might affect the outcome of any particular game. As the following example shows, increasing compatibility between two technologies can allow one technology that was not initially epidemic to become so.
EXAMPLE 6.1. Consider the contagion game played on a thick line graph (see Section 3) with r = 5/32 and q = 3/8. In this case, A is not epidemic, as can be seen by examining Figure 1, since 2r < q and q + r > 1/2. However, if we insert symmetric off-diagonal payoffs x = 1/4, we have a new game, equivalent to a game parameterized by r = 5/16 and q = 1/4. Since q < 1/2 and q < 2r , A is epidemic in this game, and thus also in the game with limited compatibility.
We now show that generally, if A is the superior technology (i.e., q < 1/2), adding a compatibility term x can only help A spread.
THEOREM 6.2. Let G be a game without compatibility, parameterized by r and q on a particular network. Let G be that same game, but with an added symmetric compatibility term x. If A is epidemic for G, then A is epidemic for G .
PROOF. We will show that any blocking structure in G is also a blocking structure in G. By our characterization theorem,
Theorem 4.6, this implies the desired result. We have that G is equivalent to a game without compatibility parameterized by q = (q − x)/(1 − 2x) and r = r/(1 − 2x). Consider a blocking structure (SB, SAB) for G . We know that for any v ∈ SAB, q dSB (v) > r Δ. Thus qdSB (v) > (q − x)dSB (v) = q (1 − 2x)dSB (v) > r (1 − 2x)Δ = rΔ, as required for a blocking structure in G. Similarly, the two blocking structure constraints for v ∈ SB are only strengthened when we move from G to G.
More than two technologies. Given the complex structure inherent in contagion games with two technologies, the understanding of contagion games with three or more technologies is largely open.
Here we indicate some of the technical issues that come up with multiple technologies, through a series of initial results. The basic set-up we study is one in which two incumbent technologies B and C are initially coexisting, and a third technology A, superior to both, is introduced initially at a finite set of nodes.
We first present a theorem stating that for any even Δ, there is a contagion game on a Δ−regular graph in which the two incumbent technologies B and C may find it beneficial to increase their compatibility so as to prevent getting wiped out by the new superior technology A. In particular, we consider a situation in which initially, two technologies B and C with zero compatibility are at a stable state. By a stable state, we mean that no finite perturbation of the current states can lead to an epidemic for either B or C. We also have a technology A that is superior to both B and C, and can become epidemic by forcing a single node to choose A.
However, by increasing their compatibility, B and C can maintain their stability and resist an epidemic from A.
Let qA denote the payoffs to two adjacent nodes that both choose technology A, and define qB and qC analogously. We will assume qA > qB > qC . We also assume that r, the cost of selecting additional technologies, is sufficiently large so as to ensure that nodes never adopt more than one technology. Finally, we consider a compatibility parameter qBC that represents the payoffs to two adjacent nodes when one selects B and the other selects C. Thus our contagion game is now described by five parameters (G, qA, qB, qC , qBC ).
THEOREM 6.3. For any even Δ ≥ 12, there is a Δ-regular graph G, an initial state s, and values qA, qB, qC , and qBC , such that • s is an equilibrium in both (G, qA, qB, qC , 0) and (G, qA, qB, qC , qBC ), • neither B nor C can become epidemic in either (G, qA, qB, qC , 0) or (G, qA, qB, qC , qBC ) starting from state s, • A can become epidemic (G, qA, qB, qC , 0) starting from state s, and • A can not become epidemic in (G, qA, qB, qC , qBC ) starting from state s.
PROOF. (Sketch.) Given Δ, define G by starting with an infinite grid and connecting each node to its nearest Δ − 2 neighbors that are in the same row. The initial state s assigns strategy B to even rows and strategy C to odd rows. Let qA = 4k2 + 4k + 1/2, qB = 2k + 2, qC = 2k + 1, and qBC = 2k + 3/4. The first, third, and fourth claims in the theorem can be verified by checking the corresponding inequalities. The second claim follows from the first and the observation that the alternating rows contain any plausible epidemic from growing vertically.
The above theorem shows that two technologies may both be able to survive the introduction of a new technology by increasing their level of compatibility with each other. As one might expect, 82 A B AB A (1 − q; 1 − q) (x; x) (1 − q; 1 − q − r) B (x; x) (q; q) (q; q − r) AB (1 − q − r; 1 − q) (q − r; q) (max(q, 1 − q) − r; max(q, 1 − q) − r) Table 1: The payoffs in the coordination game. Entry (x, y) in row i, column j indicates that the row player gets a payoff of x and the column player gets a payoff of y when the row player plays strategy i and the column player plays strategy j. there are cases when increased compatibility between two technologies helps one technology at the expense of the other.
Surprisingly, however, there are also instances in which compatibility is in fact harmful to both parties; the next example considers a fixed initial configuration with technologies A, B and C that is at equilibrium when qBC = 0. However, if this compatibility term is increased sufficiently, equilibrium is lost, and A becomes epidemic.
EXAMPLE 6.4. Consider the union of an infinite two-dimensional grid graph with nodes u(x, y) and an infinite line graph with nodes v(y). Add an edge between u(1, y) and v(y) for all y. For this network, we consider the initial configuration in which all v(y) nodes select A, and node u(x, y) selects B if x < 0 and selects C otherwise.
We now define the parameters of this game as follows. Let qA =
for these values, the initial configuration given above is an equilibrium. However, now suppose we increase the coordination term, setting qBC = 0.9. This is not an equilibrium, since each node of the form u(0, y) now has an incentive to switch from C (generating a payoff of 3.9) to B (thereby generating a payoff of 3.95).
However, once these nodes have adopted B, the best-response for each node of the form u(1, y) is A (A generates a payoff of 4 where as B only generates a payoff of 3.95). From here, it is not hard to show that A spreads directly throughout the entire network.

In multiagent settings, often an outcome must be chosen on the basis of the preferences reported by a group of agents. Such outcomes could be potential presidents, joint plans, allocations of goods or resources, etc. The preference aggregator generally does not know the agents" preferences a priori. Rather, the agents report their preferences to the coordinator. Unfortunately, an agent may have an incentive to misreport its preferences in order to mislead the mechanism into selecting an outcome that is more desirable to the agent than the outcome that would be selected if the agent revealed its preferences truthfully. Such manipulation is undesirable because preference aggregation mechanisms are tailored to aggregate preferences in a socially desirable way, and if the agents reveal their preferences insincerely, a socially undesirable outcome may be chosen.
Manipulability is a pervasive problem across preference aggregation mechanisms. A seminal negative result, the Gibbard-Satterthwaite theorem, shows that under any nondictatorial preference aggregation scheme, if there are at least 3 possible outcomes, there are preferences under which an agent is better off reporting untruthfully [10, 23]. (A preference aggregation scheme is called dictatorial if one of the agents dictates the outcome no matter what preferences the other agents report.) What the aggregator would like to do is design a preference aggregation mechanism so that 1) the self-interested agents are motivated to report their preferences truthfully, and 2) the mechanism chooses an outcome that is desirable from the perspective of some objective. This is the classic setting of mechanism design in game theory. In this paper, we study the case where the designer is self-interested, that is, the designer does not directly care about how the out132 come relates to the agents" preferences, but is rather concerned with its own agenda for which outcome should be chosen, and with maximizing payments to itself. This is the mechanism design setting most relevant to electronic commerce.
In the case where the mechanism designer is interested in maximizing some notion of social welfare, the importance of collecting the agents" preferences is clear. It is perhaps less obvious why they should be collected when the designer is self-interested and hence its objective is not directly related to the agents" preferences. The reason for this is that often the agents" preferences impose limits on how the designer chooses the outcome and payments. The most common such constraint is that of individual rationality (IR), which means that the mechanism cannot make any agent worse off than the agent would have been had it not participated in the mechanism. For instance, in the setting of optimal auction design, the designer (auctioneer) is only concerned with how much revenue is collected, and not per se with how well the allocation of the good (or goods) corresponds to the agents" preferences. Nevertheless, the designer cannot force an agent to pay more than its valuation for the bundle of goods allocated to it. Therefore, even a self-interested designer will choose an outcome that makes the agents reasonably well off. On the other hand, the designer will not necessarily choose a social welfare maximizing outcome. For example, if the designer always chooses an outcome that maximizes social welfare with respect to the reported preferences, and forces each agent to pay the difference between the utility it has now and the utility it would have had if it had not participated in the mechanism, it is easy to see that agents may have an incentive to misreport their preferences-and this may actually lead to less revenue being collected. Indeed, one of the counterintuitive results of optimal auction design theory is that sometimes the good is allocated to nobody even when the auctioneer has a reservation price of 0.
Classical mechanism design provides some general mechanisms, which, under certain assumptions, satisfy some notion of nonmanipulability and maximize some objective. The upside of these mechanisms is that they do not rely on (even probabilistic) information about the agents" preferences (e.g., the Vickrey-Clarke-Groves (VCG) mechanism [24, 4, 11]), or they can be easily applied to any probability distribution over the preferences (e.g., the dAGVA mechanism [8, 2], the Myerson auction [18], and the Maskin-Riley multi-unit auction [17]). However, the general mechanisms also have significant downsides: • The most famous and most broadly applicable general mechanisms, VCG and dAGVA, only maximize social welfare. If the designer is self-interested, as is the case in many electronic commerce settings, these mechanisms do not maximize the designer"s objective. • The general mechanisms that do focus on a selfinterested designer are only applicable in very restricted settings-such as Myerson"s expected revenue maximizing auction for selling a single item, and Maskin and Riley"s expected revenue maximizing auction for selling multiple identical units of an item. • Even in the restricted settings in which these mechanisms apply, the mechanisms only allow for payment maximization. In practice, the designer may also be interested in the outcome per se. For example, an auctioneer may care which bidder receives the item. • It is often assumed that side payments can be used to tailor the agents" incentives, but this is not always practical. For example, in barter-based electronic marketplaces-such as Recipco, firstbarter.com,
BarterOne, and Intagio-side payments are not allowed. Furthermore, among software agents, it might be more desirable to construct mechanisms that do not rely on the ability to make payments, because many software agents do not have the infrastructure to make payments.
In contrast, we follow a recent approach where the mechanism is designed automatically for the specific problem at hand. This approach addresses all of the downsides listed above. We formulate the mechanism design problem as an optimization problem. The input is characterized by the number of agents, the agents" possible types (preferences), and the aggregator"s prior distributions over the agents" types. The output is a nonmanipulable mechanism that is optimal with respect to some objective. This approach is called automated mechanism design.
The automated mechanism design approach has four advantages over the classical approach of designing general mechanisms. First, it can be used even in settings that do not satisfy the assumptions of the classical mechanisms (such as availability of side payments or that the objective is social welfare). Second, it may allow one to circumvent impossibility results (such as the Gibbard-Satterthwaite theorem) which state that there is no mechanism that is desirable across all preferences. When the mechanism is designed for the setting at hand, it does not matter that it would not work more generally. Third, it may yield better mechanisms (in terms of stronger nonmanipulability guarantees and/or better outcomes) than classical mechanisms because the mechanism capitalizes on the particulars of the setting (the probabilistic information that the designer has about the agents" types). Given the vast amount of information that parties have about each other today, this approach is likely to lead to tremendous savings over classical mechanisms, which largely ignore that information. For example, imagine a company automatically creating its procurement mechanism based on statistical knowledge about its suppliers, rather than using a classical descending procurement auction. Fourth, the burden of design is shifted from humans to a machine.
However, automated mechanism design requires the mechanism design optimization problem to be solved anew for each setting. Hence its computational complexity becomes a key issue. Previous research has studied this question for benevolent designers-that wish to maximize, for example, social welfare [5, 6]. In this paper we study the computational complexity of automated mechanism design in the case of a self-interested designer. This is an important setting for automated mechanism design due to the shortage of general mechanisms in this area, and the fact that in most e-commerce settings the designer is self-interested. We also show that this problem is closely related to a particular optimal (revenue-maximizing) combinatorial auction design problem. 133 The rest of this paper is organized as follows. In Section 2, we justify the focus on nonmanipulable mechanisms.
In Section 3, we define the problem we study. In Section 4, we show that designing an optimal deterministic mechanism is NP-complete even when the designer only cares about the payments made to it. In Section 5, we show that designing an optimal deterministic mechanism is also NP-complete when payments are not possible and the designer is only interested in the outcome chosen. In Section 6, we show that an optimal randomized mechanism can be designed in polynomial time even in the general case. Finally, in Section 7, we show that for designing optimal combinatorial auctions under best-only preferences, our results on AMD imply that this problem is NP-complete for deterministic auctions, but easy for randomized auctions.
NONMANIPULABLE MECHANISMS Before we define the computational problem of automated mechanism design, we should justify our focus on nonmanipulable mechanisms. After all, it is not immediately obvious that there are no manipulable mechanisms that, even when agents report their types strategically and hence sometimes untruthfully, still reach better outcomes (according to whatever objective we use) than any nonmanipulable mechanism. This does, however, turn out to be the case: given any mechanism, we can construct a nonmanipulable mechanism whose performance is identical, as follows. We build an interface layer between the agents and the original mechanism. The agents report their preferences (or types) to the interface layer; subsequently, the interface layer inputs into the original mechanism the types that the agents would have strategically reported to the original mechanism, if their types were as declared to the interface layer. The resulting outcome is the outcome of the new mechanism. Since the interface layer acts strategically on each agent"s behalf, there is never an incentive to report falsely to the interface layer; and hence, the types reported by the interface layer are the strategic types that would have been reported without the interface layer, so the results are exactly as they would have been with the original mechanism. This argument is known in the mechanism design literature as the revelation principle [16]. (There are computational difficulties with applying the revelation principle in large combinatorial outcome and type spaces [7, 22]. However, because here we focus on flatly represented outcome and type spaces, this is not a concern here.) Given this, we can focus on truthful mechanisms in the rest of the paper.
We now formalize the automated mechanism design setting.
Definition 1. In an automated mechanism design setting, we are given: • a finite set of outcomes O; • a finite set of N agents; • for each agent i,
correlated types, there is a single joint distribution γ over Θ1 × . . . × ΘN ), and
• An objective function whose expectation the designer wishes to maximize.
There are many possible objective functions the designer might have, for example, social welfare (where the designer seeks to maximize the sum of the agents" utilities), or the minimum utility of any agent (where the designer seeks to maximize the worst utility had by any agent). In both of these cases, the designer is benevolent, because the designer, in some sense, is pursuing the agents" collective happiness. However, in this paper, we focus on the case of a self-interested designer. A self-interested designer cares only about the outcome chosen (that is, the designer does not care how the outcome relates to the agents" preferences, but rather has a fixed preference over the outcomes), and about the net payments made by the agents, which flow to the designer.
Definition 2. A self-interested designer has an objective function given by g(o) + N i=1 πi, where g : O → R indicates the designer"s own preference over the outcomes, and πi is the payment made by agent i. In the case where g = 0 everywhere, the designer is said to be payment maximizing.
In the case where payments are not possible, g constitutes the objective function by itself.
We now define the kinds of mechanisms under study. By the revelation principle, we can restrict attention to truthful, direct revelation mechanisms, where agents report their types directly and never have an incentive to misreport them.
Definition 3. We consider the following kinds of mechanism: • A deterministic mechanism without payments consists of an outcome selection function o : Θ1 × Θ2 × . . . × ΘN → O. • A randomized mechanism without payments consists of a distribution selection function p : Θ1 × Θ2 × . . . × ΘN → P(O), where P(O) is the set of probability distributions over O. • A deterministic mechanism with payments consists of an outcome selection function o : Θ1 ×Θ2 ×. . .×ΘN → O and for each agent i, a payment selection function πi : Θ1 × Θ2 × . . . × ΘN → R, where πi(θ1, . . . , θN ) gives the payment made by agent i when the reported types are θ1, . . . , θN . 1 Though this follows standard game theory notation [16], the fact that the agent has both a utility function and a type is perhaps confusing. The types encode the various possible preferences that the agent may turn out to have, and the agent"s type is not known to the aggregator. The utility function is common knowledge, but because the agent"s type is a parameter in the agent"s utility function, the aggregator cannot know what the agent"s utility is without knowing the agent"s type. 134 • A randomized mechanism with payments consists of a distribution selection function p : Θ1 × Θ2 × . . . × ΘN → P(O), and for each agent i, a payment selection function πi : Θ1 × Θ2 × . . . × ΘN → R.2 There are two types of constraint on the designer in building the mechanism.
The first type of constraint is the following. The utility of each agent has to be at least as great as the agent"s fallback utility, that is, the utility that the agent would receive if it did not participate in the mechanism. Otherwise that agent would not participate in the mechanism-and no agent"s participation can ever hurt the mechanism designer"s objective because at worst, the mechanism can ignore an agent by pretending the agent is not there. (Furthermore, if no such constraint applied, the designer could simply make the agents pay an infinite amount.) This type of constraint is called an IR (individual rationality) constraint. There are three different possible IR constraints: ex ante, ex interim, and ex post, depending on what the agent knows about its own type and the others" types when deciding whether to participate in the mechanism. Ex ante IR means that the agent would participate if it knew nothing at all (not even its own type). We will not study this concept in this paper.
Ex interim IR means that the agent would always participate if it knew only its own type, but not those of the others.
Ex post IR means that the agent would always participate even if it knew everybody"s type. We will define the latter two notions of IR formally. First, we need to formalize the concept of the fallback outcome. We assume that each agent"s fallback utility is zero for each one of its types. This is without loss of generality because we can add a constant term to an agent"s utility function (for a given type), without affecting the decision-making behavior of that expected utility maximizing agent [16].
Definition 4. In any automated mechanism design setting with an IR constraint, there is a fallback outcome o0 ∈ O where, for any agent i and any type θi ∈ Θi, we have ui(θi, o0) = 0. (Additionally, in the case of a self-interested designer, g(o0) = 0.) We can now to define the notions of individual rationality.
Definition 5. Individual rationality (IR) is defined by: • A deterministic mechanism is ex interim IR if for any agent i, and any type θi ∈ Θi, we have E(θ1,..,θi−1,θi+1,..,θN )|θi [ui(θi, o(θ1, .., θN ))−πi(θ1, .., θN )] ≥ 0.
A randomized mechanism is ex interim IR if for any agent i, and any type θi ∈ Θi, we have E(θ1,..,θi−1,θi+1,..,θN )|θi Eo|θ1,..,θn [ui(θi, o)−πi(θ1, .., θN )] ≥ 0. • A deterministic mechanism is ex post IR if for any agent i, and any type vector (θ1, . . . , θN ) ∈ Θ1 × . . . × ΘN , we have ui(θi, o(θ1, . . . , θN )) − πi(θ1, . . . , θN ) ≥
2 We do not randomize over payments because as long as the agents and the designer are risk neutral with respect to payments, that is, their utility is linear in payments, there is no reason to randomize over payments.
A randomized mechanism is ex post IR if for any agent i, and any type vector (θ1, . . . , θN ) ∈ Θ1 × . . . × ΘN , we have Eo|θ1,..,θn [ui(θi, o) − πi(θ1, .., θN )] ≥ 0.
The terms involving payments can be left out in the case where payments are not possible.
The second type of constraint says that the agents should never have an incentive to misreport their type (as justified above by the revelation principle). For this type of constraint, the two most common variants (or solution concepts) are implementation in dominant strategies, and implementation in Bayes-Nash equilibrium.
Definition 6. Given an automated mechanism design setting, a mechanism is said to implement its outcome and payment functions in dominant strategies if truthtelling is always optimal even when the types reported by the other agents are already known. Formally, for any agent i, any type vector (θ1, . . . , θi, . . . , θN ) ∈ Θ1 × . . . × Θi × . . . × ΘN , and any alternative type report ˆθi ∈ Θi, in the case of deterministic mechanisms we have ui(θi, o(θ1, . . . , θi, . . . , θN )) − πi(θ1, . . . , θi, . . . , θN ) ≥ ui(θi, o(θ1, . . . , ˆθi, . . . , θN )) − πi(θ1, . . . , ˆθi, . . . , θN ).
In the case of randomized mechanisms we have Eo|θ1,..,θi,..,θn [ui(θi, o) − πi(θ1, . . . , θi, . . . , θN )] ≥ Eo|θ1,.., ˆθi,..,θn [ui(θi, o) − πi(θ1, . . . , ˆθi, . . . , θN )].
The terms involving payments can be left out in the case where payments are not possible.
Thus, in dominant strategies implementation, truthtelling is optimal regardless of what the other agents report. If it is optimal only given that the other agents are truthful, and given that one does not know the other agents" types, we have implementation in Bayes-Nash equilibrium.
Definition 7. Given an automated mechanism design setting, a mechanism is said to implement its outcome and payment functions in Bayes-Nash equilibrium if truthtelling is always optimal to an agent when that agent does not yet know anything about the other agents" types, and the other agents are telling the truth. Formally, for any agent i, any type θi ∈ Θi, and any alternative type report ˆθi ∈ Θi, in the case of deterministic mechanisms we have E(θ1,..,θi−1,θi+1,..,θN )|θi [ui(θi, o(θ1, . . . , θi, . . . , θN ))− πi(θ1, . . . , θi, . . . , θN )] ≥ E(θ1,..,θi−1,θi+1,..,θN )|θi [ui(θi, o(θ1, . . . , ˆθi, . . . , θN ))− πi(θ1, . . . , ˆθi, . . . , θN )].
In the case of randomized mechanisms we have E(θ1,..,θi−1,θi+1,..,θN )|θi Eo|θ1,..,θi,..,θn [ui(θi, o)− πi(θ1, . . . , θi, . . . , θN )] ≥ E(θ1,..,θi−1,θi+1,..,θN )|θi Eo|θ1,.., ˆθi,..,θn [ui(θi, o)− πi(θ1, . . . , ˆθi, . . . , θN )].
The terms involving payments can be left out in the case where payments are not possible. 135
We can now define the computational problem we study.
Definition 8. (AUTOMATED-MECHANISM-DESIGN (AMD)) We are given: • an automated mechanism design setting, • an IR notion (ex interim, ex post, or none), • a solution concept (dominant strategies or Bayes-Nash), • whether payments are possible, • whether randomization is possible, • (in the decision variant of the problem) a target value G.
We are asked whether there exists a mechanism of the specified kind (in terms of payments and randomization) that satisfies both the IR notion and the solution concept, and gives an expected value of at least G for the objective.
An interesting special case is the setting where there is only one agent. In this case, the reporting agent always knows everything there is to know about the other agents" types-because there are no other agents. Since ex post and ex interim IR only differ on what an agent is assumed to know about other agents" types, the two IR concepts coincide here. Also, because implementation in dominant strategies and implementation in Bayes-Nash equilibrium only differ on what an agent is assumed to know about other agents" types, the two solution concepts coincide here. This observation will prove to be a useful tool in proving hardness results: if we prove computational hardness in the singleagent setting, this immediately implies hardness for both IR concepts, for both solution concepts, for any number of agents.
PAYMENT-MAXIMIZINGDETERMINISTIC AMD IS HARD In this section we demonstrate that it is NP-complete to design a deterministic mechanism that maximizes the expected sum of the payments collected from the agents.
We show that this problem is hard even in the single-agent setting, thereby immediately showing it hard for both IR concepts, for both solution concepts. To demonstrate NPhardness, we reduce from the MINSAT problem.
Definition 9 (MINSAT). We are given a formula φ in conjunctive normal form, represented by a set of Boolean variables V and a set of clauses C, and an integer K (K < |C|). We are asked whether there exists an assignment to the variables in V such that at most K clauses in φ are satisfied.
MINSAT was recently shown to be NP-complete [14]. We can now present our result.
Theorem 1. Payment-maximizing deterministic AMD is NP-complete, even for a single agent, even with a uniform distribution over types.
Proof. It is easy to show that the problem is in NP.
To show NP-hardness, we reduce an arbitrary MINSAT instance to the following single-agent payment-maximizing deterministic AMD instance. Let the agent"s type set be Θ = {θc : c ∈ C} ∪ {θv : v ∈ V }, where C is the set of clauses in the MINSAT instance, and V is the set of variables. Let the probability distribution over these types be uniform. Let the outcome set be O = {o0} ∪ {oc : c ∈ C} ∪ {ol : l ∈ L}, where L is the set of literals, that is,
L = {+v : v ∈ V } ∪ {−v : v ∈ V }. Let the notation v(l) = v denote that v is the variable corresponding to the literal l, that is, l ∈ {+v, −v}. Let l ∈ c denote that the literal l occurs in clause c. Then, let the agent"s utility function be given by u(θc, ol) = |Θ| + 1 for all l ∈ L with l ∈ c; u(θc, ol) = 0 for all l ∈ L with l /∈ c; u(θc, oc) = |Θ| + 1; u(θc, oc ) = 0 for all c ∈ C with c = c ; u(θv, ol) = |Θ| for all l ∈ L with v(l) = v; u(θv, ol) = 0 for all l ∈ L with v(l) = v; u(θv, oc) = 0 for all c ∈ C. The goal of the AMD instance is G = |Θ| + |C|−K |Θ| , where K is the goal of the MINSAT instance. We show the instances are equivalent. First, suppose there is a solution to the MINSAT instance. Let the assignment of truth values to the variables in this solution be given by the function f : V → L (where v(f(v)) = v for all v ∈ V ).
Then, for every v ∈ V , let o(θv) = of(v) and π(θv) = |Θ|.
For every c ∈ C, let o(θc) = oc; let π(θc) = |Θ| + 1 if c is not satisfied in the MINSAT solution, and π(θc) = |Θ| if c is satisfied. It is straightforward to check that the IR constraint is satisfied. We now check that the agent has no incentive to misreport. If the agent"s type is some θv, then any other report will give it an outcome that is no better, for a payment that is no less, so it has no incentive to misreport. If the agent"s type is some θc where c is a satisfied clause, again, any other report will give it an outcome that is no better, for a payment that is no less, so it has no incentive to misreport. The final case to check is where the agent"s type is some θc where c is an unsatisfied clause. In this case, we observe that for none of the types, reporting it leads to an outcome ol for a literal l ∈ c, precisely because the clause is not satisfied in the MINSAT instance. Because also, no type besides θc leads to the outcome oc, reporting any other type will give an outcome with utility 0, while still forcing a payment of at least |Θ| from the agent. Clearly the agent is better off reporting truthfully, for a total utility of
misreport. Finally, we show that the goal is reached. If s is the number of satisfied clauses in the MINSAT solution (so that s ≤ K), the expected payment from this mechanism is |V ||Θ|+s|Θ|+(|C|−s)(|Θ|+1) |Θ| ≥ |V ||Θ|+K|Θ|+(|C|−K)(|Θ|+1) |Θ| = |Θ| + |C|−K |Θ| = G. So there is a solution to the AMD instance.
Now suppose there is a solution to the AMD instance, given by an outcome function o and a payment function π. First, suppose there is some v ∈ V such that o(θv) /∈ {o+v, o−v}. Then the utility that the agent derives from the given outcome for this type is 0, and hence, by IR, no payment can be extracted from the agent for this type.
Because, again by IR, the maximum payment that can be extracted for any other type is |Θ| + 1, it follows that the maximum expected payment that could be obtained is at most (|Θ|−1)(|Θ|+1) |Θ| < |Θ| < G, contradicting that this is a solution to the AMD instance. It follows that in the solution to the AMD instance, for every v ∈ V , o(θv) ∈ {o+v, o−v}. 136 We can interpret this as an assignment of truth values to the variables: v is set to true if o(θv) = o+v, and to false if o(θv) = o−v. We claim this assignment is a solution to the MINSAT instance. By the IR constraint, the maximum payment we can extract from any type θv is |Θ|. Because there can be no incentives for the agent to report falsely, for any clause c satisfied by the given assignment, the maximum payment we can extract for the corresponding type θc is |Θ|. (For if we extracted more from this type, the agent"s utility in this case would be less than 1; and if v is the variable satisfying c in the assignment, so that o(θv) = ol where l occurs in c, then the agent would be better off reporting θv instead of the truthful report θc, to get an outcome worth |Θ|+1 to it while having to pay at most |Θ|.) Finally, for any unsatisfied clause c, by the IR constraint, the maximum payment we can extract for the corresponding type θc is |Θ| + 1. It follows that the expected payment from our mechanism is at most V |Θ|+s|Θ|+(|C|−s)(|Θ|+1) Θ , where s is the number of satisfied clauses. Because our mechanism achieves the goal, it follows that V |Θ|+s|Θ|+(|C|−s)(|Θ|+1) Θ ≥ G, which by simple algebraic manipulations is equivalent to s ≤ K. So there is a solution to the MINSAT instance.
Because payment-maximizing AMD is just the special case of AMD for a self-interested designer where the designer has no preferences over the outcome chosen, this immediately implies hardness for the general case of AMD for a selfinterested designer where payments are possible. However, it does not yet imply hardness for the special case where payments are not possible. We will prove hardness in this case in the next section.
AMD WITHOUT PAYMENTS IS HARD In this section we demonstrate that it is NP-complete to design a deterministic mechanism that maximizes the expectation of the designer"s objective when payments are not possible. We show that this problem is hard even in the single-agent setting, thereby immediately showing it hard for both IR concepts, for both solution concepts.
Theorem 2. Without payments, deterministic AMD for a self-interested designer is NP-complete, even for a single agent, even with a uniform distribution over types.
Proof. It is easy to show that the problem is in NP.
To show NP-hardness, we reduce an arbitrary MINSAT instance to the following single-agent self-interested deterministic AMD without payments instance. Let the agent"s type set be Θ = {θc : c ∈ C} ∪ {θv : v ∈ V }, where C is the set of clauses in the MINSAT instance, and V is the set of variables. Let the probability distribution over these types be uniform. Let the outcome set be O = {o0} ∪ {oc : c ∈ C}∪{ol : l ∈ L}∪{o∗ }, where L is the set of literals, that is,
L = {+v : v ∈ V } ∪ {−v : v ∈ V }. Let the notation v(l) = v denote that v is the variable corresponding to the literal l, that is, l ∈ {+v, −v}. Let l ∈ c denote that the literal l occurs in clause c. Then, let the agent"s utility function be given by u(θc, ol) = 2 for all l ∈ L with l ∈ c; u(θc, ol) = −1 for all l ∈ L with l /∈ c; u(θc, oc) = 2; u(θc, oc ) = −1 for all c ∈ C with c = c ; u(θc, o∗ ) = 1; u(θv, ol) = 1 for all l ∈ L with v(l) = v; u(θv, ol) = −1 for all l ∈ L with v(l) = v; u(θv, oc) = −1 for all c ∈ C; u(θv, o∗ ) = −1. Let the designer"s objective function be given by g(o∗ ) = |Θ|+1; g(ol) = |Θ| for all l ∈ L; g(oc) = |Θ| for all c ∈ C. The goal of the AMD instance is G = |Θ| + |C|−K |Θ| , where K is the goal of the MINSAT instance. We show the instances are equivalent. First, suppose there is a solution to the MINSAT instance. Let the assignment of truth values to the variables in this solution be given by the function f : V → L (where v(f(v)) = v for all v ∈ V ). Then, for every v ∈ V , let o(θv) = of(v). For every c ∈ C that is satisfied in the MINSAT solution, let o(θc) = oc; for every unsatisfied c ∈ C, let o(θc) = o∗ . It is straightforward to check that the IR constraint is satisfied. We now check that the agent has no incentive to misreport. If the agent"s type is some θv, it is getting the maximum utility for that type, so it has no incentive to misreport. If the agent"s type is some θc where c is a satisfied clause, again, it is getting the maximum utility for that type, so it has no incentive to misreport. The final case to check is where the agent"s type is some θc where c is an unsatisfied clause. In this case, we observe that for none of the types, reporting it leads to an outcome ol for a literal l ∈ c, precisely because the clause is not satisfied in the MINSAT instance. Because also, no type leads to the outcome oc, there is no outcome that the mechanism ever selects that would give the agent utility greater than 1 for type θc, and hence the agent has no incentive to report falsely. This establishes that the agent never has an incentive to misreport.
Finally, we show that the goal is reached. If s is the number of satisfied clauses in the MINSAT solution (so that s ≤ K), then the expected value of the designer"s objective function is |V ||Θ|+s|Θ|+(|C|−s)(|Θ|+1) |Θ| ≥ |V ||Θ|+K|Θ|+(|C|−K)(|Θ|+1) |Θ| = |Θ| + |C|−K |Θ| = G. So there is a solution to the AMD instance.
Now suppose there is a solution to the AMD instance, given by an outcome function o. First, suppose there is some v ∈ V such that o(θv) /∈ {o+v, o−v}. The only other outcome that the mechanism is allowed to choose under the IR constraint is o0. This has an objective value of 0, and because the highest value the objective function ever takes is |Θ| + 1, it follows that the maximum expected value of the objective function that could be obtained is at most (|Θ|−1)(|Θ|+1) |Θ| < |Θ| < G, contradicting that this is a solution to the AMD instance. It follows that in the solution to the AMD instance, for every v ∈ V , o(θv) ∈ {o+v, o−v}.
We can interpret this as an assignment of truth values to the variables: v is set to true if o(θv) = o+v, and to false if o(θv) = o−v. We claim this assignment is a solution to the MINSAT instance. By the above, for any type θv, the value of the objective function in this mechanism will be |Θ|. For any clause c satisfied by the given assignment, the value of the objective function in the case where the agent reports type θc will be at most |Θ|. (This is because we cannot choose the outcome o∗ for such a type, as in this case the agent would have an incentive to report θv instead, where v is the variable satisfying c in the assignment (so that o(θv) = ol where l occurs in c).) Finally, for any unsatisfied clause c, the maximum value the objective function can take in the case where the agent reports type θc is |Θ| + 1, simply because this is the largest value the function ever takes. It follows that the expected value of the objective function for our mechanism is at most V |Θ|+s|Θ|+(|C|−s)(|Θ|+1) Θ , where s is the number of satisfied 137 clauses. Because our mechanism achieves the goal, it follows that V |Θ|+s|Θ|+(|C|−s)(|Θ|+1) Θ ≥ G, which by simple algebraic manipulations is equivalent to s ≤ K. So there is a solution to the MINSAT instance.
Both of our hardness results relied on the constraint that the mechanism should be deterministic. In the next section, we show that the hardness of design disappears when we allow for randomization in the mechanism.
SELFINTERESTED DESIGNER IS EASY We now show how allowing for randomization over the outcomes makes the problem of self-interested AMD tractable through linear programming, for any constant number of agents.
Theorem 3. Self-interested randomized AMD with a constant number of agents is solvable in polynomial time by linear programming, both with and without payments, both for ex post and ex interim IR, and both for implementation in dominant strategies and for implementation in Bayes-Nash equilibrium-even if the types are correlated.
Proof. Because linear programs can be solved in polynomial time [13], all we need to show is that the number of variables and equations in our program is polynomial for any constant number of agents-that is, exponential only in N.
Throughout, for purposes of determining the size of the linear program, let T = maxi{|Θi|}. The variables of our linear program will be the probabilities (p(θ1, θ2, . . . , θN ))(o) (at most TN |O| variables) and the payments πi(θ1, θ2, . . . , θN ) (at most NTN variables). (We show the linear program for the case where payments are possible; the case without payments is easily obtained from this by simply omitting all the payment variables in the program, or by adding additional constraints forcing the payments to be 0.) First, we show the IR constraints. For ex post IR, we add the following (at most NTN ) constraints to the LP: • For every i ∈ {1, 2, . . . , N}, and for every (θ1, θ2, . . . , θN ) ∈ Θ1 × Θ2 × . . . × ΘN , we add ( o∈O (p(θ1, θ2, . . . , θN ))(o)u(θi, o)) − πi(θ1, θ2, . . . , θN ) ≥ 0.
For ex interim IR, we add the following (at most NT) constraints to the LP: • For every i ∈ {1, 2, . . . , N}, for every θi ∈ Θi, we add θ1,... ,θN γ(θ1, . . . , θN |θi)(( o∈O (p(θ1, θ2, . . . , θN ))(o)u(θi, o))− πi(θ1, θ2, . . . , θN )) ≥ 0.
Now, we show the solution concept constraints. For implementation in dominant strategies, we add the following (at most NTN+1 ) constraints to the LP: • For every i ∈ {1, 2, . . . , N}, for every (θ1, θ2, . . . , θi, . . . , θN ) ∈ Θ1 × Θ2 × . . . × ΘN , and for every alternative type report ˆθi ∈ Θi, we add the constraint ( o∈O (p(θ1, θ2, . . . , θi, . . . , θN ))(o)u(θi, o)) − πi(θ1, θ2, . . . , θi, . . . , θN ) ≥ ( o∈O (p(θ1, θ2, . . . , ˆθi, . . . , θN ))(o)u(θi, o)) − πi(θ1, θ2, . . . , ˆθi, . . . , θN ).
Finally, for implementation in Bayes-Nash equilibrium, we add the following (at most NT2 ) constraints to the LP: • For every i ∈ {1, 2, ..., N}, for every θi ∈ Θi, and for every alternative type report ˆθi ∈ Θi, we add the constraint θ1,...,θN γ(θ1, ..., θN |θi)(( o∈O (p(θ1, θ2, ..., θi, ..., θN ))(o)u(θi, o)) − πi(θ1, θ2, ..., θi, ..., θN )) ≥ θ1,...,θN γ(θ1, ..., θN |θi)(( o∈O (p(θ1, θ2, ..., ˆθi, ..., θN ))(o)u(θi, o)) − πi(θ1, θ2, ..., ˆθi, ..., θN )).
All that is left to do is to give the expression the designer is seeking to maximize, which is: • θ1,...,θN γ(θ1, ..., θN )(( o∈O (p(θ1, θ2, ..., θi, ..., θN ))(o)g(o)) + N i=1 πi(θ1, θ2, ..., θN )).
As we indicated, the number of variables and constraints is exponential only in N, and hence the linear program is of polynomial size for constant numbers of agents. Thus the problem is solvable in polynomial time.
COMBINATORIAL AUCTION DESIGN PROBLEM In this section, we will demonstrate some interesting consequences of the problem of automated mechanism design for a self-interested designer on designing optimal combinatorial auctions.
Consider a combinatorial auction with a set S of items for sale. For any bundle B ⊆ S, let ui(θi, B) be bidder i"s utility for receiving bundle B when the bidder"s type is θi. The optimal auction design problem is to specify the rules of the auction so as to maximize expected revenue to the auctioneer. (By the revelation principle, without loss of generality, we can assume the auction is truthful.) The optimal auction design problem is solved for the case of a single item by the famous Myerson auction [18]. However, designing optimal auctions in combinatorial auctions is a recognized open research problem [3, 25]. The problem is open even if there are only two items for sale. (The twoitem case with a very special form of complementarity and no substitutability has been solved recently [1].) Suppose we have free disposal-items can be thrown away at no cost. Also, suppose that the bidders" preferences have the following structure: whenever a bidder receives a bundle of items, the bidder"s utility for that bundle is determined by the best item in the bundle only. (We emphasize that 138 which item is the best is allowed to depend on the bidder"s type.) Definition 10. Bidder i is said to have best-only preferences over bundles of items if there exists a function vi : Θi × S → R such that for any θi ∈ Θi, for any B ⊆ S, ui(θi, B) = maxs∈B vi(θi, s).
We make the following useful observation in this setting: there is no sense in awarding a bidder more than one item.
The reason is that if the bidder is reporting truthfully, taking all but the highest valued item away from the bidder will not hurt the bidder; and, by free disposal, doing so can only reduce the incentive for this bidder to falsely report this type, when the bidder actually has another type.
We now show that the problem of designing a deterministic optimal auction here is NP-complete, by a reduction from the payment maximizing AMD problem! Theorem 4. Given an optimal combinatorial auction design problem under best-only preferences (given by a set of items S and for each bidder i, a finite type space Θi and a function vi : Θi × S → R such that for any θi ∈ Θi, for any B ⊆ S, ui(θi, B) = maxs∈B vi(θi, s)), designing the optimal deterministic auction is NP-complete, even for a single bidder with a uniform distribution over types.
Proof. The problem is in NP because we can nondeterministically generate an allocation rule, and then set the payments using linear programming.
To show NP-hardness, we reduce an arbitrary paymentmaximizing deterministic AMD instance, with a single agent and a uniform distribution over types, to the following optimal combinatorial auction design problem instance with a single bidder with best-only preferences. For every outcome o ∈ O in the AMD instance (besides the outcome o0), let there be one item so ∈ S. Let the type space be the same, and let v(θi, so) = ui(θi, o) (where u is as specified in the AMD instance). Let the expected revenue target value be the same in both instances. We show the instances are equivalent.
First suppose there exists a solution to the AMD instance, given by an outcome function and a payment function. Then, if the AMD solution chooses outcome o for a type, in the optimal auction solution, allocate {so} to the bidder for this type. (Unless o = o0, in which case we allocate {} to the bidder.) Let the payment functions be the same in both instances. Then, the utility that an agent receives for reporting a type (given the true type) in either solution is the same, so we have incentive compatibility in the optimal auction solution. Moreover, because the type distribution and the payment function are the same, the expected revenue to the auctioneer/designer is the same. It follows that there exists a solution to the optimal auction design instance.
Now suppose there exists a solution to the optimal auction design instance. By the at-most-one-item observation, we can assume without loss of generality that the solution never allocates more than one item. Then, if the optimal auction solution allocates item so to the bidder for a type, in the AMD solution, let the mechanism choose outcome o for that type. If the optimal auction solution allocates nothing to the bidder for a type, in the AMD solution, let the mechanism choose outcome o0 for that type. Let the payment functions be the same. Then, the utility that an agent receives for reporting a type (given the true type) in either solution is the same, so we have incentive compatibility in the AMD solution. Moreover, because the type distribution and the payment function are the same, the expected revenue to the designer/auctioneer is the same. It follows that there exists a solution to the AMD instance.
Fortunately, we can also carry through the easiness result for randomized mechanisms to this combinatorial auction setting-giving us one of the few known polynomial-time algorithms for an optimal combinatorial auction design problem.
Theorem 5. Given an optimal combinatorial auction design problem under best-only preferences (given by a set of items S and for each bidder i, a finite type space Θi and a function vi : Θi × S → R such that for any θi ∈ Θi, for any B ⊆ S, ui(θi, B) = maxs∈B vi(θi, s)), if the number of bidders is a constant k, then the optimal randomized auction can be designed in polynomial time. (For any IC and IR constraints.) Proof. By the at-most-one-item observation, we can without loss of generality restrict ourselves to allocations where each bidder receives at most one item. There are fewer than (|S| + 1)k such allocations-that is, a polynomial number of allocations. Because we can list the outcomes explicitly, we can simply solve this as a payment-maximizing AMD instance, with linear programming.
COMPLEXITY IN MECHANISM DESIGN There has been considerable recent interest in mechanism design in computer science. Some of it has focused on issues of computational complexity, but most of that work has strived toward designing mechanisms that are easy to execute (e.g. [20, 15, 19, 9, 12]), rather than studying the complexity of designing the mechanism. The closest piece of earlier work studied the complexity of automated mechanism design by a benevolent designer [5, 6]. Roughgarden has studied the complexity of designing a good network topology for agents that selfishly choose the links they use [21].
This is related to mechanism design, but differs significantly in that the designer only has restricted control over the rules of the game because there is no party that can impose the outcome (or side payments). Also, there is no explicit reporting of preferences.
RESEARCH Often, an outcome must be chosen on the basis of the preferences reported by a group of agents. The key difficulty is that the agents may report their preferences insincerely to make the chosen outcome more favorable to themselves.
Mechanism design is the art of designing the rules of the game so that the agents are motivated to report their preferences truthfully, and a desirable outcome is chosen. In a recently emerging approach-called automated mechanism design-a mechanism is computed for the specific preference aggregation setting at hand. This has several advantages, 139 but the downside is that the mechanism design optimization problem needs to be solved anew each time. Unlike earlier work on automated mechanism design that studied a benevolent designer, in this paper we studied automated mechanism design problems where the designer is self-interesteda setting much more relevant for electronic commerce. In this setting, the center cares only about which outcome is chosen and what payments are made to it. The reason that the agents" preferences are relevant is that the center is constrained to making each agent at least as well off as the agent would have been had it not participated in the mechanism.
In this setting, we showed that designing an optimal deterministic mechanism is NP-complete in two important special cases: when the center is interested only in the payments made to it, and when payments are not possible and the center is interested only in the outcome chosen. These hardness results imply hardness in all more general automated mechanism design settings with a self-interested designer.
The hardness results apply whether the individual rationality (participation) constraints are applied ex interim or ex post, and whether the solution concept is dominant strategies implementation or Bayes-Nash equilibrium implementation. We then showed that allowing randomization in the mechanism makes the design problem in all these settings computationally easy. Finally, we showed that the paymentmaximizing AMD problem is closely related to an interesting variant of the optimal (revenue-maximizing) combinatorial auction design problem, where the bidders have best-only preferences. We showed that here, too, designing an optimal deterministic mechanism is NP-complete even with one agent, but designing an optimal randomized mechanism is easy.
Future research includes studying automated mechanism design with a self-interested designer in more restricted settings such as auctions (where the designer"s objective may include preferences about which bidder should receive the good-as well as payments). We also want to study the complexity of automated mechanism design in settings where the outcome and type spaces have special structure so they can be represented more concisely. Finally, we plan to assemble a data set of real-world mechanism design problems-both historical and current-and apply automated mechanism design to those problems.
[1] M. Armstrong. Optimal multi-object auctions. Review of Economic Studies, 67:455-481, 2000. [2] K. Arrow. The property rights doctrine and demand revelation under incomplete information. In M. Boskin, editor, Economics and human welfare.
New York Academic Press, 1979. [3] C. Avery and T. Hendershott. Bundling and optimal auctions of multiple products. Review of Economic Studies, 67:483-497, 2000. [4] E. H. Clarke. Multipart pricing of public goods. Public Choice, 11:17-33, 1971. [5] V. Conitzer and T. Sandholm. Complexity of mechanism design. In Proceedings of the 18th Annual Conference on Uncertainty in Artificial Intelligence (UAI-02), pages 103-110, Edmonton, Canada, 2002. [6] V. Conitzer and T. Sandholm. Automated mechanism design: Complexity results stemming from the single-agent setting. In Proceedings of the 5th International Conference on Electronic Commerce (ICEC-03), pages 17-24, Pittsburgh, PA, USA, 2003. [7] V. Conitzer and T. Sandholm. Computational criticisms of the revelation principle. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), New York, NY, 2004. Short paper.
Full-length version appeared in the AAMAS-03 workshop on Agent-Mediated Electronic Commerce (AMEC). [8] C. d"Aspremont and L. A. G´erard-Varet. Incentives and incomplete information. Journal of Public Economics, 11:25-45, 1979. [9] J. Feigenbaum, C. Papadimitriou, and S. Shenker.
Sharing the cost of muliticast transmissions. Journal of Computer and System Sciences, 63:21-41, 2001.
Early version in Proceedings of the Annual ACM Symposium on Theory of Computing (STOC), 2000. [10] A. Gibbard. Manipulation of voting schemes.
Econometrica, 41:587-602, 1973. [11] T. Groves. Incentives in teams. Econometrica, 41:617-631, 1973. [12] J. Hershberger and S. Suri. Vickrey prices and shortest paths: What is an edge worth? In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), 2001. [13] L. Khachiyan. A polynomial algorithm in linear programming. Soviet Math. Doklady, 20:191-194,
[14] R. Kohli, R. Krishnamurthi, and P. Mirchandani. The minimum satisfiability problem. SIAM Journal of Discrete Mathematics, 7(2):275-283, 1994. [15] D. Lehmann, L. I. O"Callaghan, and Y. Shoham.
Truth revelation in rapid, approximately efficient combinatorial auctions. Journal of the ACM, 49(5):577-602, 2002. Early version appeared in Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), 1999. [16] A. Mas-Colell, M. Whinston, and J. R. Green.
Microeconomic Theory. Oxford University Press, 1995. [17] E. S. Maskin and J. Riley. Optimal multi-unit auctions. In F. Hahn, editor, The Economics of Missing Markets, Information, and Games, chapter 14, pages 312-335. Clarendon Press, Oxford,
[18] R. Myerson. Optimal auction design. Mathematics of Operation Research, 6:58-73, 1981. [19] N. Nisan and A. Ronen. Computationally feasible VCG mechanisms. In Proceedings of the ACM Conference on Electronic Commerce (ACM-EC), pages 242-252, Minneapolis, MN, 2000. [20] N. Nisan and A. Ronen. Algorithmic mechanism design. Games and Economic Behavior, 35:166-196,
Symposium on Theory of Computing (STOC), 1999. [21] T. Roughgarden. Designing networks for selfish users is hard. In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS), 2001. [22] T. Sandholm. Issues in computational Vickrey auctions. International Journal of Electronic Commerce, 4(3):107-129, 2000. Special Issue on 140 Applying Intelligent Agents for Electronic Commerce.
A short, early version appeared at the Second International Conference on Multi-Agent Systems (ICMAS), pages 299-306, 1996. [23] M. A. Satterthwaite. Strategy-proofness and Arrow"s conditions: existence and correspondence theorems for voting procedures and social welfare functions.

A wide variety of financial and wagering mechanisms have been developed to support hedging (i.e., insuring) against exposure to uncertain events and/or speculative trading on uncertain events. The dominant mechanism used in financial circles is the continuous double auction (CDA), or in some cases the CDA with market maker (CDAwMM). The primary mechanism used for sports wagering is a bookie or bookmaker, who essentially acts exactly as a market maker.
Horse racing and jai alai wagering traditionally employ the pari-mutuel mechanism. Though there is no formal or logical separation between financial trading and wagering, the two endeavors are socially considered distinct. Recently, there has been a move to employ CDAs or CDAwMMs for all types of wagering, including on sports, horse racing, political events, world news, and many other uncertain events, and a simultaneous and opposite trend to use bookie systems for betting on financial markets. These trends highlight the interchangeable nature of the mechanisms and further blur the line between investing and betting. Some companies at the forefront of these movements are growing exponentially, with some industry observers declaring the onset of a revolution in the wagering business.1 Each mechanism has pros and cons for the market institution and the participating traders. A CDA only matches willing traders, and so poses no risk whatsoever for the market institution. But a CDA can suffer from illiquidity in the form huge bid-ask spreads or even empty bid-ask queues if trading is light and thus markets are thin. A successful CDA must overcome a chicken-and-egg problem: traders are attracted to liquid markets, but liquid markets require a large number of traders. A CDAwMM and the similar bookie mechanism have built-in liquidity, but at a cost: the market maker itself, usually affiliated with the market institution, is exposed to significant risk of large monetary losses. Both the CDA and CDAwMM offer incentives for traders to leverage information continuously as soon as that information becomes available. As a result, prices are known to capture the current state of information exceptionally well.
Pari-mutuel markets effectively have infinite liquidity: anyone can place a bet on any outcome at any time, without the need for a matching offer from another bettor or a market maker. Pari-mutuel markets also involve no risk for the market institution, since they only redistribute money from losing wagers to winning wagers. However, pari-mutuel mar1 http://www.wired.com/news/ebiz/0,1272,61051,00.html 170 kets are not suitable for situations where information arrives over time, since there is a strong disincentive for placing bets until either (1) all information is revealed, or (2) the market is about to close. For this reason, pari-mutuel prices prior to the market"s close cannot be considered a reflection of current information. Pari-mutuel market participants cannot buy low and sell high: they cannot cash out gains (or limit losses) before the event outcome is revealed. Because the process whereby information arrives continuously over time is the rule rather than the exception, the applicability of the standard pari-mutuel mechanism is questionable in a large number of settings.
In this paper, I develop a new mechanism suitable for hedging, speculating, and wagering, called a dynamic parimutuel market (DPM). A DPM can be thought of as a hybrid between a pari-mutuel market and a CDA. A DPM is indeed pari-mutuel in nature, meaning that it acts only to redistribute money from some traders to others, and so exposes the market institution to no volatility (no risk). A constant, pre-determined subsidy is required to start the market. The subsidy can in principle be arbitrarily small and might conceivably come from traders (via antes or transaction fees) rather than the market institution, though a nontrivial outside subsidy may actually encourage trading and information aggregation. A DPM has the infinite liquidity of a pari-mutuel market: traders can always purchase shares in any outcome at any time, at some price automatically set by the market institution. A DPM is also able to react to and incorporate information arriving over time, like a CDA. The market institution changes the price for particular outcomes based on the current state of wagering.
If a particular outcome receives a relatively large number of wagers, its price increases; if an outcome receives relatively few wagers, its price decreases. Prices are computed automatically using a price function, which can differ depending on what properties are desired. The price function determines the instantaneous price per share for an infinitesimal quantity of shares; the total cost for purchasing n shares is computed as the integral of the price function from 0 to n. The complexity of the price function can be hidden from traders by communicating only the ask prices for various lots of shares (e.g., lots of 100 shares), as is common practice in CDAs and CDAwMMs. DPM prices do reflect current information, and traders can cash out in an aftermarket to lock in gains or limit losses before the event outcome is revealed.
While there is always a market maker willing to accept buy orders, there is not a market maker accepting sell orders, and thus no guaranteed liquidity for selling: instead, selling is accomplished via a standard CDA mechanism. Traders can always hedge-sell by purchasing the opposite outcome than they already own.
Pari-mutuel markets are common at horse races [1, 22, 24, 25, 26], dog races, and jai alai games. In a pari-mutuel market people place wagers on which of two or more mutually exclusive and exhaustive outcomes will occur at some time in the future. After the true outcome becomes known, all of the money that is lost by those who bet on the incorrect outcome is redistributed to those who bet on the correct outcome, in direct proportion to the amount they wagered.
More formally, if there are k mutually exclusive and exhaustive outcomes (e.g., k horses, exactly one of which will win), and M1, M2, . . . , Mk dollars are bet on each outcome, and outcome i occurs, then everyone who bet on an outcome j = i loses their wager, while everyone who bet on outcome i receives Pk j=1 Mj/Mi dollars for every dollar they wagered.
That is, every dollar wagered on i receives an equal share of all money wagered. An equivalent way to think about the redistribution rule is that every dollar wagered on i is refunded, then receives an equal share of all remaining money bet on the losing outcomes, or P j=i Mj/Mi dollars.
In practice, the market institution (e.g., the racetrack) first takes a certain percent of the total amount wagered, usually about 20% in the United States, then redistributes whatever money remains to the winners in proportion to their amount bet.
Consider a simple example with two outcomes, A and B.
The outcomes are mutually exclusive and exhaustive, meaning that Pr(A ∧ B) = 0 and Pr(A) + Pr(B) = 1. Suppose $800 is bet on A and $200 on B. Now suppose that A occurs (e.g., horse A wins the race). People who wagered on B lose their money, or $200 in total. People who wagered on A win and each receives a proportional share of the total $1000 wagered (ignoring fees). Specifically, each $1 wager on A entitles its owner a 1/800 share of the $1000, or $1.25.
Every dollar bet in a pari-mutuel market has an equal payoff, regardless of when the wager was placed or how much money was invested in the various outcomes at the time the wager was placed. The only state that matters is the final state: the final amounts wagered on all the outcomes when the market closes, and the identity of the correct outcome.
As a result, there is a disincentive to place a wager early if there is any chance that new information might become available. Moreover, there are no guarantees about the payoff rate of a particular bet, except that it will be nonnegative if the correct outcome is chosen. Payoff rates can fluctuate arbitrarily until the market closes. So a second reason not to bet early is to wait to get a better sense of the final payout rates. This is in contrast to CDAs and CDAwMMs, like the stock market, where incentives exist to invest as soon as new information is revealed.
Pari-mutuel bettors may be allowed to switch their chosen outcome, or even cancel their bet, prior to the market"s close. However, they cannot cash out of the market early, to either lock in gains or limit losses, if new information favors one outcome over another, as is possible in a CDA or a CDAwMM. If bettors can cancel or change their bets, then an aftermarket to sell existing wagers is not sensible: every dollar wagered is worth exactly $1 up until the market"s close-no one would buy at greater than $1 and no one would sell at less than $1. Pari-mutuel bettors must wait until the outcome is revealed to realize any profit or loss.
Unlike a CDA, in a pari-mutuel market, anyone can place a wager of any amount at any time-there is in a sense infinite liquidity for buying. A CDAwMM also has built-in liquidity, but at the cost of significant risk for the market maker. In a pari-mutuel market, since money is only redistributed among bettors, the market institution itself has no risk. The main drawback of a pari-mutuel market is that it is useful only for capturing the value of an uncertain asset at some instant in time. It is ill-suited for situations where information arrives over time, continuously updating the estimated value of the asset-situations common in al171 most all trading and wagering scenarios. There is no notion of buying low and selling high, as occurs in a CDA, where buying when few others are buying (and the price is low) is rewarded more than buying when many others are buying (and the price is high). Perhaps for this reason, in most dynamic environments, financial mechanisms like the CDA that can react in real-time to changing information are more typically employed to facilitate speculating and hedging.
Since a pari-mutuel market can estimate the value of an asset at a single instant in time, a repeated pari-mutuel market, where distinct pari-mutuel markets are run at consecutive intervals, could in principle capture changing information dynamics. But running multiple consecutive markets would likely thin out trading in each individual market.
Also, in each individual pari-mutuel market, the incentives would still be to wait to bet until just before the ending time of that particular market. This last problem might be mitigated by instituting a random stopping rule for each individual pari-mutuel market.
In laboratory experiments, pari-mutuel markets have shown a remarkable ability to aggregate and disseminate information dispersed among traders, at least for a single snapshot in time [17]. A similar ability has been recognized at real racetracks [1, 22, 24, 25, 26].
In the financial world, wagering on the outcomes of uncertain future propositions is also common. The typical market mechanism used is the continuous double auction (CDA).
The term securities market in economics and finance generically encompasses a number of markets where speculating on uncertain events is possible. Examples include stock markets like NASDAQ, options markets like the CBOE [13], futures markets like the CME [21], other derivatives markets, insurance markets, political stock markets [6, 7], idea futures markets [12], decision markets [10] and even market games [3, 15, 16]. Securities markets generally have an economic and social value beyond facilitating speculation or wagering: they allow traders to hedge risk, or to insure against undesirable outcomes. So if a particular outcome has disutility for a trader, he or she can mitigate the risk by wagering for the outcome, to arrange for compensation in case the outcome occurs. In this sense, buying automobile insurance is effectively a bet that an accident or other covered event will occur. Similarly, buying a put option, which is useful as a hedge for a stockholder, is a bet that the underlying stock will go down. In practice, agents engage in a mixture of hedging and speculating, and there is no clear dividing line between the two [14]. Like pari-mutuel markets, often prices in financial markets are excellent information aggregators, yielding very accurate forecasts of future events [5, 18, 19].
A CDA constantly matches orders to buy an asset with orders to sell. If at any time one party is willing to buy one unit of the asset at a bid price of pbid, while another party is willing to sell one unit of the asset at an ask price of pask, and pbid is greater than or equal to pask, then the two parties transact (at some price between pbid and pask). If the highest bid price is less than the lowest ask price, then no transactions occur. In a CDA, the bid and ask prices rapidly change as new information arrives and traders reassess the value of the asset. Since the auctioneer only matches willing bidders, the auctioneer takes on no risk. However, buyers can only buy as many shares as sellers are willing to sell; for any transaction to occur, there must be a counterparty on the other side willing to accept the trade.
As a result, when few traders participate in a CDA, it may become illiquid, meaning that not much trading activity occurs. The spread between the highest bid price and the lowest ask price may be very large, or one or both queues may be completely empty, discouraging trading.2 One way to induce liquidity is to provide a market maker who is willing to accept a large number of buy and sell orders at particular prices. We call this mechanism a CDA with market maker (CDAwMM).3 Conceptually, the market maker is just like any other trader, but typically is willing to accept a much larger volume of trades. The market maker may be a person, or may be an automated algorithm. Adding a market maker to the system increases liquidity, but exposes the market maker to risk. Now, instead of only matching trades, the system actually takes on risk of its own, and depending on what happens in the future, may lose considerable amounts of money.
The typical Las Vegas bookmaker or oddsmaker functions much like a market maker in a CDA. In this case, the market institution (the book or house) sets the odds,4 initially according to expert opinion, and later in response to the relative level of betting on the various outcomes. Unlike in a pari-mutuel environment, whenever a wager is placed with a bookmaker, the odds or terms for that bet are fixed at the time of the bet. The bookmaker profits by offering different odds for the two sides of the bet, essentially defining a bidask spread. While odds may change in response to changing information, any bets made at previously set odds remain in effect according to the odds at the time of the bet; this is precisely in analogy to a CDAwMM. One difference between a bookmaker and a market maker is that the former usually operates in a take it or leave it mode: bettors cannot place their own limit orders on a common queue, they can in effect only place market orders at prices defined by the bookmaker. Still, the bookmaker certainly reacts to bettor demand. Like a market maker, the bookmaker exposes itself to significant risk. Sports betting markets have also been shown to provide high quality aggregate forecasts [4, 9, 23].
Hanson"s [11] market scoring rule (MSR) is a new mechanism for hedging and speculating that shares some properties in common with a DPM. Like a DPM, an MSR can be conceptualized as an automated market maker always willing to accept a trade on any event at some price. An MSR requires a patron to subsidize the market. The patron"s final loss is variable, and thus technically implies a degree of risk, though the maximum loss is bounded. An MSR maintains a probability distribution over all events. At any time any 2 Thin markets do occur often in practice, and can be seen in a variety of the less popular markets available on http://TradeSports.com, or in some financial options markets, for example. 3 A very clear example of a CDAwMM is the interactive betting market on http://WSEX.com. 4 Or, alternatively, the bookmaker sets the game line in order to provide even-money odds. 172 trader who believes the probabilities are wrong can change any part of the distribution by accepting a lottery ticket that pays off according to a scoring rule (e.g., the logarithmic scoring rule) [27], as long as that trader also agrees to pay off the most recent person to change the distribution. In the limit of a single trader, the mechanism behaves like a scoring rule, suitable for polling a single agent for its probability distribution. In the limit of many traders, it produces a combined estimate. Since the market essentially always has a complete set of posted prices for all possible outcomes, the mechanism avoids the problem of thin markets or illiquidity.
An MSR is not pari-mutuel in nature, as the patron in general injects a variable amount of money into the system. An MSR provides a two-sided automated market maker, while a DPM provides a one-sided automated market maker. In an MSR, the vector of payoffs across outcomes is fixed at the time of the trade, while in a DPM, the vector of payoffs across outcomes depends both on the state of wagering at the time of the trade and the state of wagering at the market"s close. While the mechanisms are quite different-and so trader acceptance and incentives may strongly differ-the properties and motivations of DPMs and MSRs are quite similar.
Hanson shows how MSRs are especially well suited for allowing bets on a combinatorial number of outcomes. The patron"s payment for subsidizing trading on all 2n possible combinations of n events is no larger than the sum of subsidizing the n event marginals independently. The mechanism was planned for use in the Policy Analysis Market (PAM), a futures market in Middle East related outcomes and funded by DARPA [20], until a media firestorm killed the project.5 As of this writing, the founders of PAM were considering reopening under private control.6
In contrast to a standard pari-mutuel market, where each dollar always buys an equal share of the payoff, in a DPM each dollar buys a variable share in the payoff depending on the state of wagering at the time of purchase. So a wager on A at a time when most others are wagering on B offers a greater possible profit than a wager on A when most others are also wagering on A.
A natural way to communicate the changing payoff of a bet is to say that, at any given time, a certain amount of money will buy a certain number of shares in one outcome the other. Purchasing a share entitles its owner to an equal stake in the winning pot should the chosen outcome occur.
The payoff is variable, because when few people are betting on an outcome, shares will generally be cheaper than at a time when many people are betting that outcome. There is no pre-determined limit on the number of shares: new shares can be continually generated as trading proceeds.
For simplicity, all analyses in this paper consider the binary outcome case; generalizing to multiple discrete outcomes should be straightforward. Denote the two outcomes A and B. The outcomes are mutually exclusive and ex5 See http://hanson.gmu.edu/policyanalysismarket.html for more information, or http://dpennock.com/pam.html for commentary. 6 http://www.policyanalysismarket.com/ haustive. Denote the instantaneous price per share of A as p1 and the price per share of B as p2. Denote the payoffs per share as P1 and P2, respectively. These four numbers, p1, p2, P1, P2 are the key numbers that traders must track and understand. Note that the price is set at the time of the wager; the payoff per share is finalized only after the event outcome is revealed.
At any time, a trader can purchase an infinitesimal quantity of shares of A at price p1 (and similarly for B). However, since the price changes continuously as shares are purchased, the cost of buying n shares is computed as the integral of a price function from 0 to n. The use of continuous functions and integrals can be hidden from traders by aggregating the automated market maker"s sell orders into discrete lots of, say, 100 shares each. These ask orders can be automatically entered into the system by the market institution, so that traders interact with what looks like a more familiar CDA; we examine this interface issue in more detail below in Section 4.2.
For our analysis, we introduce the following additional notation. Denote M1 as the total amount of money wagered on A, M2 as the total amount of money wagered on B,
T = M1 + M2 as the total amount of money wagered on both sides, N1 as the total number of shares purchased of A, and N2 as the total number of shares purchased of B.
There are many ways to formulate the price function.
Several natural price functions are outlined below; each is motivated as the unique solution to a particular constraint on price dynamics.
To my knowledge, a DPM is the only known mechanism for hedging and speculating that exhibits all three of the following properties: (1) guaranteed liquidity, (2) no risk for the market institution, and (3) continuous incorporation of information. A standard pari-mutuel fails (3). A CDA fails (1). A CDAwMM, the bookmaker mechanism, and an MSR all fail (2). Even though technically an MSR exposes its patron to risk (i.e., a variable future payoff), the patron"s maximum loss is bounded, so the distinction between a DPM and an MSR in terms of these three properties is more technical than practical.
DPM traders can cash out of the market early, just like stock market traders, to lock in a profit or limit a loss, an action that is simply not possible in a standard pari-mutuel.
A DPM also has some drawbacks. The payoff for a wager depends both on the price at the time of the trade, and on the final payoff per share at the market"s close. This contrasts with the CDA variants, where the payoff vector across possible future outcomes is fixed at the time of the trade. So a trader"s strategic optimization problem is complicated by the need to predict the final values of P1 and P2. If P changes according to a random walk, then traders can take the current P as an unbiased estimate of the final P, greatly decreasing the complexity of their optimization. If P does not change according to a random walk, the mechanism still has utility as a mechanism for hedging and speculating, though optimization may be difficult, and determining a measure of the market"s aggregate opinion of the probabilities of A and B may be difficult. We discuss the implications of random walk behavior further below in Section 4.1 in the discussion surrounding Assumption 3.
A second drawback of a DPM is its one-sided nature. 173 While an automated market maker always stands ready to accept buy orders, there is no corresponding market maker to accept sell orders. Traders must sell to each other using a standard CDA mechanism, for example by posting an ask order at a price at or below the market maker"s current ask price. Traders can also always hedge-sell by purchasing shares in the opposite outcome from the market maker, thereby hedging their bet if not fully liquidating it.
In a standard pari-mutuel market, payoffs can be computed in either of two equivalent ways: (1) each winning $1 wager receives a refund of the initial $1 paid, plus an equal share of all losing wagers, or (2) each winning $1 wager receives an equal share of all wagers, winning or losing.
Because each dollar always earns an equal share of the payoff, the two formulations are precisely the same: $1 + Mlose Mwin = Mwin + Mlose Mwin .
In a dynamic pari-mutuel market, because each dollar is not equally weighted, the two formulations are distinct, and lead to significantly different price functions and mechanisms, each with different potentially desirable properties.
We consider each case in turn. The next section analyzes case (1), where only losing money is redistributed. Section 5 examines case (2), where all money is redistributed.
REDISTRIBUTED For the case where the initial payments on winning bets are refunded, and only losing money is redistributed, the respective payoffs per share are simply: P1 = M2 N1 P2 = M1 N2 .
So, if A occurs, shareholders of A receive all of their initial payment back, plus P1 dollars per share owned, while shareholders of B lose all money wagered. Similarly, if B occurs, shareholders of B receive all of their initial payment back, plus P2 dollars per share owned, while shareholders of A lose all money wagered.
Without loss of generality, I will analyze the market from the perspective of A, deriving prices and payoffs for A only.
The equations for B are symmetric.
The trader"s per-share expected value for purchasing an infinitesimal quantity of shares of A is E[ shares] = Pr(A) · E [P1|A] − (1 − Pr(A)) · p1 E[ shares] = Pr(A) · E » M2 N1 ˛ ˛ ˛ ˛ A  − (1 − Pr(A)) · p1 where is an infinitesimal quantity of shares of A, Pr(A) is the trader"s belief in the probability of A, and p1 is the instantaneous price per share of A for an infinitesimal quantity of shares. E[P1|A] is the trader"s expectation of the payoff per share of A after the market closes and given that A occurs. This is a subtle point. The value of P1 does not matter if B occurs, since in this case shares of A are worthless, and the current value of P1 does not necessarily matter as this may change as trading continues. So, in order to determine the expected value of shares of A, the trader must estimate what he or she expects the payoff per share to be in the end (after the market closes) if A occurs.
If E[ shares]/ > 0, a risk-neutral trader should purchase shares of A. How many shares? This depends on the price function determining p1. In general, p1 increases as more shares are purchased. The risk-neutral trader should continue purchasing shares until E[ shares]/ = 0. (A riskaverse trader will generally stop purchasing shares before driving E[ shares]/ all the way to zero.) Assuming riskneutrality, the trader"s optimization problem is to choose a number of shares n ≥ 0 of A to purchase, in order to maximize E[n shares] = Pr(A)·n·E [P1|A]−(1−Pr(A))· Z n 0 p1(n)dn. (1) It"s easy to see that the same value of n can be solved for by finding the number of shares required to drive E[ shares]/ to zero. That is, find n ≥ 0 satisfying
if such a n exists, otherwise n = 0.
As traders who believe that E[ shares of A]/ > 0 purchase shares of A and traders who believe that E[ shares of B]/ > 0 purchase shares of B, the prices p1 and p2 change according to a price function, as prescribed below.
The current prices in a sense reflect the market"s opinion as a whole of the relative probabilities of A and B.
Assuming an efficient marketplace, the market as a whole considers E[ shares]/ = 0, since the mechanisms is a zero sum game. For example, if market participants in aggregate felt that E[ shares]/ > 0, then there would be net demand for A, driving up the price of A until E[ shares]/ = 0. Define MPr(A) to be the market probability of A, or the probability of A inferred by assuming that E[ shares]/ = 0. We can consider MPr(A) to be the aggregate probability of A as judged by the market as a whole. MPr(A) is the solution to
Solving we get MPr(A) = p1 p1 + E[P1|A] . (2) At this point we make a critical assumption in order to greatly simplify the analysis; we assume that E[P1|A] = P1. (3) That is, we assume that the current value for the payoff per share of A is the same as the expected final value of the payoff per share of A given that A occurs. This is certainly true for the last (infinitesimal) wager before the market closes.
It"s not obvious, however, that the assumption is true well before the market"s close. Basically, we are assuming that the value of P1 moves according to an unbiased random walk: the current value of P1 is the best expectation of its future value. I conjecture that there are reasonable market efficiency conditions under which assumption (3) is true, though I have not been able to prove that it arises naturally from rational trading. We examine scenarios below in which 174 assumption (3) seems especially plausible. Nonetheless, the assumption effects our analysis only. Regardless of whether (3) is true, each price function derived below implies a welldefined zero-sum game in which traders can play. If traders can assume that (3) is true, then their optimization problem (1) is greatly simplified; however, optimizing (1) does not depend on the assumption, and traders can still optimize by strategically projecting the final expected payoff in whatever complicated way they desire. So, the utility of DPM for hedging and speculating does not necessarily hinge on the truth of assumption (3). On the other hand, the ability to easily infer an aggregate market consensus probability from market prices does depend on (3).
A variety of price functions seem reasonable, each exhibiting various properties, and implying differing market probabilities.
One natural price function to consider is to set the price per share of A equal to the payoff per share of B, and set the price per share of B equal to the payoff per share of A.
That is, p1 = P2 p2 = P1. (4) Enforcing this relationship reduces the dimensionality of the system from four to two, simplifying the interface: traders need only track two numbers instead of four. The relationship makes sense, since new information supporting A should encourage purchasing of shares A, driving up both the price of A and the payoff of B, and driving down the price of B and the payoff of A. In this setting, assumption (3) seems especially reasonable, since if an efficient market hypothesis leads prices to follow a random walk, than payoffs must also follow a random walk.
The constraints (4) lead to the following derivation of the market probability: MPr(A)P1 = MPr(B)p1 MPr(A)P1 = MPr(B)P2 MPr(A) MPr(B) = P2 P1 MPr(A) MPr(B) = M1 N2 M2 N1 MPr(A) MPr(B) = M1N1 M2N2 MPr(A) = M1N1 M1N1 + M2N2 (5) The constraints (4) specify the instantaneous relationship between payoff and price. From this, we can derive how prices change when (non-infinitesimal) shares are purchased. Let n be the number of shares purchased and let m be the amount of money spent purchasing n shares. Note that p1 = dm/dn, the instantaneous price per share, and m = R n 0 p1(n)dn. Substituting into equation (4), we get: p1 = P2 dm dn = M1 + m N2 dm M1 + m = dn N2 Z dm M1 + m = Z dn N2 ln(M1 + m) = n N2 + C m = M1 h e n N2 − 1 i (6) Equation 6 gives the cost of purchasing n shares. The instantaneous price per share as a function of n is p1(n) = dm dn = M1 N2 e n N2 . (7) Note that p1(0) = M1/N2 = P2 as required. The derivation of the price function p2(n) for B is analogous and the results are symmetric.
The notion of buying infinitesimal shares, or integrating costs over a continuous function, are probably foreign to most traders. A more standard interface can be implemented by discretizing the costs into round lots of shares, for example lots of 100 shares. Then ask orders of 100 shares each at the appropriate price can be automatically placed by the market institution. For example, the market institution can place an ask order for 100 shares at price m(100)/100, another ask order for 100 shares at price (m(200)−m(100))/100, a third ask for 100 shares at (m(300)− m(200))/100, etc. In this way, the market looks more familiar to traders, like a typical CDA with a number of ask orders at various prices automatically available. A trader buying less than 100 shares would pay a bit more than if the true cost were computed using (6), but the discretized interface would probably be more intuitive and transparent to the majority of traders.
The above equations assume that all money that comes in is eventually returned or redistributed. In other words, the mechanism is a zero sum game, and the market institution takes no portion of the money. This could be generalized so that the market institution always takes a certain amount, or a certain percent, or a certain amount per transaction, or a certain percent per transaction, before money in returned or redistributed.
Finally, note that the above price function is undefined when the amount bet or the number of shares are zero. So the system must begin with some positive amount on both sides, and some positive number of shares outstanding on both sides. These initial amounts can be arbitrarily small in principle, but the size of the initial subsidy may affect the incentives of traders to participate. Also, the smaller the initial amounts, the more each new dollar effects the prices.
The initialization amounts could be funded as a subsidy from the market institution or a patron, which I"ll call a seed wager, or from a portion of the fees charged, which I"ll call an ante wager.
money on A A second price function can be derived by requiring the ratio of prices to be equal to the ratio of money wagered. 175 That is, p1 p2 = M1 M2 . (8) In other words, the price of A is proportional to the amount of money wagered on A, and similarly for B. This seems like a particularly natural way to set the price, since the more money that is wagered on one side, the cheaper becomes a share on the other side, in exactly the same proportion.
Using Equation 8, along with (2) and (3), we can derive the implied market probability: M1 M2 = p1 p2 = MPr(A) MPr(B) · M2 N1 MPr(B) MPr(A) · M1 N2 = (MPr(A))2 (MPr(B))2 · M2N2 M1N1 (MPr(A))2 (MPr(B))2 = (M1)2 N1 (M2)2N2 MPr(A) MPr(B) = M1 √ N1 M2 √ N2 MPr(A) = M1 √ N1 M1 √ N1 + M2 √ N2 (9) We can solve for the instantaneous price as follows: p1 = MPr(A) MPr(B) · P1 = M1 √ N1 M2 √ N2 · M2 N1 = M1 √ N1N2 (10) Working from the above instantaneous price, we can derive the implied cost function m as a function of the number n of shares purchased as follows: dm dn = M1 + m √ N1 + n √ N2 Z dm M1 + m = Z dn √ N1 + n √ N2 ln(M1 + m) = 2 N2 [(N1 + n)N2] 1
m = M1 " e 2 r N1+n N2 −2 r N1 N2 − 1 # . (11) From this we get the price function: p1(n) = dm dn = M1 p (N1 + n)N2 e 2 r N1+n N2 −2 r N1 N2 . (12) Note that, as required, p1(0) = M1/ √ N1N2, and p1(0)/p2(0) = M1/M2. If one uses the above price function, then the market dynamics will be such that the ratio of the (instantaneous) prices of A and B always equals the ratio of the amounts wagered on A and B, which seems fairly natural.
Note that, as before, the mechanism can be modified to collect transaction fees of some kind. Also note that seed or ante wagers are required to initialize the system.
Above we examined the policy of refunding winning wagers and redistributing only losing wagers. In this section we consider the second policy mentioned in Section 3.3: all money from all wagers are collected and redistributed to winning wagers.
For the case where all money is redistributed, the respective payoffs per share are: P1 = M1 + M2 N1 = T N1 P2 = M1 + M2 N2 = T N2 , where T = M1 + M2 is the total amount of money wagered on both sides. So, if A occurs, shareholders of A lose their initial price paid, but receive P1 dollars per share owned; shareholders of B simply lose all money wagered. Similarly, if B occurs, shareholders of B lose their initial price paid, but receive P2 dollars per share owned; shareholders of A lose all money wagered.
In this case, the trader"s per-share expected value for purchasing an infinitesimal quantity of shares of A is E[ shares] = Pr(A) · E [P1|A] − p1. (13) A risk-neutral trader optimizes by choosing a number of shares n ≥ 0 of A to purchase, in order to maximize E[n shares] = Pr(A) · n · E [P1|A] − Z n 0 p1(n)dn = Pr(A) · n · E [P1|A] − m (14) The same value of n can be solved for by finding the number of shares required to drive E[ shares]/ to zero. That is, find n ≥ 0 satisfying
if such a n exists, otherwise n = 0.
In this case MPr(A), the aggregate probability of A as judged by the market as a whole, is the solution to
Solving we get MPr(A) = p1 E[P1|A] . (15) As before, we make the simplifying assumption (3) that the expected final payoff per share equals the current payoff per share. The assumption is critical for our analysis, but may not be required for a practical implementation.
For the case where all money is distributed, the constraints (4) that keep the price of A equal to the payoff of B, and vice versa, do not lead to the derivation of a coherent price function.
A reasonable price function can be derived from the constraint (8) employed in Section 4.2.2, where we require that the ratio of prices to be equal to the ratio of money wagered.
That is, p1/p2 = M1/M2. In other words, the price of A is proportional to the amount of money wagered on A, and similarly for B. 176 Using Equations 3, 8, and 15 we can derive the implied market probability: M1 M2 = p1 p2 = MPr(A) MPr(B) · T N1 · N2 T = MPr(A) MPr(B) · N2 N1 MPr(A) MPr(B) = M1N1 M2N2 MPr(A) = M1N1 M1N1 + M2N2 (16) Interestingly, this is the same market probability derived in Section 4.2.1 for the case of losing-money redistribution with the constraints that the price of A equal the payoff of B and vice versa.
The instantaneous price per share for an infinitesimal quantity of shares is: p1 = (M1)2 + M1M2 M1N1 + M2N2 = M1 + M2 N1 + M2 M1 N2 Working from the above instantaneous price, we can derive the number of shares n that can be purchased for m dollars, as follows: dm dn = M1 + M2 + m N1 + n + M2 M1+m N2 dn dm = N1 + n + M2 M1+m N2 M1 + M2 + m (17) · · · n = m(N1 − N2) T + N2(T + m) M2 ln » T(M1 + m) M1(T + m)  .
Note that we solved for n(m) rather than m(n). I could not find a closed-form solution for m(n), as was derived for the two other cases above. Still, n(m) can be used to determine how many shares can be purchased for m dollars, and the inverse function can be approximated to any degree numerically. From n(m) we can also compute the price function: p1(m) = dm dn = (M1 + m)M2T denom , (18) where denom = (M1 + m)M2N1 + (M2 − m)M2N2 +T(M1 + m)N2 ln » T(M1 + m) M1(T + m)  Note that, as required, p1(0)/p2(0) = M1/M2. If one uses the above price function, then the market dynamics will be such that the ratio of the (instantaneous) prices of A and B always equals the ratio of the amounts wagered on A and B.
This price function has another desirable property: it acts such that the expected value of wagering $1 on A and simultaneously wagering $1 on B equals zero, assuming (3). That is, E[$1 of A + $1 of B] = 0. The derivation is omitted.
The main advantage of refunding winning wagers (DPM I) is that every bet on the winning outcome is guaranteed to at least break even. The main disadvantage of refunding winning wagers is that shares are not homogenous: each share of A, for example, is actually composed of two distinct parts: (1) the refund, or a lottery ticket that pays $p if A occurs, where p is the price paid per share, and (2) one share of the final payoff ($P1) if A occurs. This complicates the implementation of an aftermarket to cash out of the market early, which we will examine below in Section 7. When all money is redistributed (DPM II), shares are homogeneous: each share entitles its owner to an equal slice of the final payoff. Because shares are homogenous, the implementation of an aftermarket is straightforward, as we shall see in Section 7. On the other hand, because initial prices paid are not refunded for winning bets, there is a chance that, if prices swing wildly enough, a wager on the correct outcome might actually lose money. Traders must be aware that if they buy in at an excessively high price that later tumbles allowing many others to get in at a much lower price, they may lose money in the end regardless of the outcome. From informal experiments, I don"t believe this eventuality would be common, but nonetheless it requires care in communicating to traders the possible risks. One potential fix would be for the market maker to keep track of when the price is going too low, endangering an investor on the correct outcome. At this point, the market maker could artificially stop lowering the price. Sell orders in the aftermarket might still come in below the market maker"s price, but in this way the system could ensure that every wager on the correct outcome at least breaks even.
A simple ascending price function would set p1 = αM1 and p2 = αM2, where α > 0. In this case, prices would only go up. For the case of all money being redistributed, this would eliminate the possibility of losing money on a wager on the correct outcome. Even though the market maker"s price only rises, the going price may fall well below the market maker"s price, as ask orders are placed in the aftermarket.
I have derived price functions for several other cases, using the same methodology above. Each price function may have its own desirable properties, but it"s not clear which is best, or even that a single best method exists. Further analyses and, more importantly, empirical investigations are required to answer these questions.
A key advantage of DPM over a standard pari-mutuel market is the ability to cash out of the market before it closes, in order to take a profit or limit a loss. This is accomplished by allowing traders to place ask orders on the same queue as the market maker. So traders can sell the shares that they purchased at or below the price set by the market maker. Or traders can place a limit sell order at any price. Buyers will purchase any existing shares for sale at the lower prices first, before purchasing new shares from the market maker.
For the second main case explored above, where all money 177 is redistributed, allowing an aftermarket is simple. In fact, aftermarket may be a poor descriptor: buying and selling are both fully integrated into the same mechanism. Every share is worth precisely the same amount, so traders can simply place ask orders on the same queue as the market maker in order to sell their shares. New buyers will accept the lowest ask price, whether it comes from the market maker or another trader. In this way, traders can cash out early and walk away with their current profit or loss, assuming they can find a willing buyer.
When winning wagers are refunded and only losing wagers are redistributed, each share is potentially worth a different amount, depending on how much was paid for it, so it is not as simple a matter to set up an aftermarket. However, an aftermarket is still possible. In fact, much of the complexity can be hidden from traders, so it looks nearly as simple as placing a sell order on the queue.
In this case shares are not homogenous: each share of A is actually composed of two distinct parts: (1) the refund of p · 1A dollars, and (2) the payoff of P1 · 1A dollars, where p is the per-share price paid and 1A is the indicator function equalling 1 if A occurs, and 0 otherwise. One can imagine running two separate aftermarkets where people can sell these two respective components. However, it is possible to automate the two aftermarkets, by automatically bundling them together in the correct ratio and selling them in the central DPM. In this way, traders can cash out by placing sell orders on the same queue as the DPM market maker, effectively hiding the complexity of explicitly having two separate aftermarkets. The bundling mechanism works as follows. Suppose the current price for 1 share of A is p1. A buyer agrees to purchase the share at p1. The buyer pays p1 dollars and receives p1 · 1A + P1 · 1A dollars. If there is enough inventory in the aftermarkets, the buyer"s share is constructed by bundling together p1 ·1A from the first aftermarket, and P1 ·1A from the second aftermarket. The seller in the first aftermarket receives p1MPr(A) dollars, and the seller in the second aftermarket receives p1MPr(B) dollars.
There is an alternative pseudo aftermarket that"s possible for the case of DPM I that does not require bundling.
Consider a share of A purchased for $5. The share is composed of $5·1A and $P1 ·1A. Now suppose the current price has moved from $5 to $10 per share and the trader wants to cash out at a profit. The trader can sell 1/2 share at market price (1/2 share for $5), receiving all of the initial $5 investment back, and retaining 1/2 share of A. The 1/2 share is worth either some positive amount, or nothing, depending on the outcome and the final payoff. So the trader is left with shares worth a positive expected value and all of his or her initial investment. The trader has essentially cashed out and locked in his or her gains. Now suppose instead that the price moves downward, from $5 to $2 per share. The trader decides to limit his or her loss by selling the share for $2. The buyer gets the 1 share plus $2·1A (the buyer"s price refunded). The trader (seller) gets the $2 plus what remains of the original price refunded, or $3 · 1A. The trader"s loss is now limited to $3 at most instead of $5. If A occurs, the trader breaks even; if B occurs, the trader loses $3.
Also note that-in either DPM formulation-traders can always hedge sell by buying the opposite outcome without the need for any type of aftermarket.
I have presented a new market mechanism for wagering on, or hedging against, a future uncertain event, called a dynamic pari-mutuel market (DPM). The mechanism combines the infinite liquidity and risk-free nature of a parimutuel market with the dynamic nature of a CDA, making it suitable for continuous information aggregation. To my knowledge, all existing mechanisms-including the standard pari-mutuel market, the CDA, the CDAwMM, the bookie mechanism, and the MSR-exhibit at most two of the three properties. An MSR is the closest to a DPM in terms of these properties, if not in terms of mechanics. Given some natural constraints on price dynamics, I have derived in closed form the implied price functions, which encode how prices change continuously as shares are purchased. The interface for traders looks much like the familiar CDA, with the system acting as an automated market maker willing to accept an infinite number of buy orders at some price.
I have explored two main variations of a DPM: one where only losing money is redistributed, and one where all money is redistributed. Each has its own pros and cons, and each supports several reasonable price functions. I have described the workings of an aftermarket, so that traders can cash out of the market early, like in a CDA, to lock in their gains or limit their losses, an operation that is not possible in a standard pari-mutuel setting.
This paper reports the results of an initial investigation of the concept of a dynamic pari-mutuel market. Many avenues for future work present themselves, including the following: • Random walk conjecture. The most important question mark in my mind is whether the random walk assumption (3) can be proven under reasonable market efficiency conditions and, if not, how severely it effects the practicality of the system. • Incentive analysis. Formally, what are the incentives for traders to act on new information and when?
How does the level of initial subsidy effect trader incentives? • Laboratory experiments and field tests. This paper concentrated on the mathematics and algorithmics of the mechanism. However, the true test of the mechanism"s ability to serve as an instrument for hedging, wagering, or information aggregation is to test it with real traders in a realistic environment. In reality, how do people behave when faced with a DPM mechanism? • DPM call market. I have derived the price functions to react to wagers on one outcome at a time. The mechanism could be generalized to accept orders on both sides, then update the prices wholistically, rather than by assuming a particular sequence on the wagers. • Real-valued variables. I believe the mechanisms in this paper can easily be generalized to multiple discrete 178 outcomes, and multiple real-valued outcomes that always sum to some constant value (e.g., multiple percentage values that must sum to 100). However, the generalization to real-valued variables with arbitrary range is less clear, and open for future development. • Compound/combinatorial betting. I believe that DPM may be well suited for compound [8, 11] or combinatorial [2] betting, for many of the same reasons that market scoring rules [11] are well suited for the task. DPM may also have some computational advantages over MSR, though this remains to be seen.
Acknowledgments I thank Dan Fain, Gary Flake, Lance Fortnow, and Robin Hanson.
[1] Mukhtar M. Ali. Probability and utility estimates for racetrack bettors. Journal of Political Economy, 85(4):803-816, 1977. [2] Peter Bossaerts, Leslie Fine, and John Ledyard.
Inducing liquidity in thin financial markets through combined-value trading mechanisms. European Economic Review, 46:1671-1695, 2002. [3] Kay-Yut Chen, Leslie R. Fine, and Bernardo A.
Huberman. Forecasting uncertain events with small groups. In Third ACM Conference on Electronic Commerce (EC"01), pages 58-64, 2001. [4] Sandip Debnath, David M. Pennock, C. Lee Giles, and Steve Lawrence. Information incorporation in online in-game sports betting markets. In Fourth ACM Conference on Electronic Commerce (EC"03), 2003. [5] Robert Forsythe and Russell Lundholm. Information aggregation in an experimental market. Econometrica, 58(2):309-347, 1990. [6] Robert Forsythe, Forrest Nelson, George R. Neumann, and Jack Wright. Anatomy of an experimental political stock market. American Economic Review, 82(5):1142-1161, 1992. [7] Robert Forsythe, Thomas A. Rietz, and Thomas W.
Ross. Wishes, expectations, and actions: A survey on price formation in election stock markets. Journal of Economic Behavior and Organization, 39:83-110,
[8] Lance Fortnow, Joe Kilian, David M. Pennock, and Michael P. Wellman. Betting boolean-style: A framework for trading in securities based on logical formulas. In Proceedings of the Fourth Annual ACM Conference on Electronic Commerce, pages 144-155,
[9] John M. Gandar, William H. Dare, Craig R. Brown, and Richard A. Zuber. Informed traders and price variations in the betting market for professional basketball games. Journal of Finance,
LIII(1):385-401, 1998. [10] Robin Hanson. Decision markets. IEEE Intelligent Systems, 14(3):16-19, 1999. [11] Robin Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1), 2002. [12] Robin D. Hanson. Could gambling save science?
Encouraging an honest consensus. Social Epistemology, 9(1):3-33, 1995. [13] Jens Carsten Jackwerth and Mark Rubinstein.
Recovering probability distributions from options prices. Journal of Finance, 51(5):1611-1631, 1996. [14] Joseph B. Kadane and Robert L. Winkler. Separating probability elicitation from utilities. Journal of the American Statistical Association, 83(402):357-363,
[15] David M. Pennock, Steve Lawrence, C. Lee Giles, and Finn ˚Arup Nielsen. The real power of artificial markets. Science, 291:987-988, February 9 2001. [16] David M. Pennock, Steve Lawrence, Finn ˚Arup Nielsen, and C. Lee Giles. Extracting collective probabilistic forecasts from web games. In Seventh International Conference on Knowledge Discovery and Data Mining, pages 174-183, 2001. [17] C. R. Plott, J. Wit, and W. C. Yang. Parimutuel betting markets as information aggregation devices: Experimental results. Technical Report Social Science Working Paper 986, California Institute of Technology, April 1997. [18] Charles R. Plott. Markets as information gathering tools. Southern Economic Journal, 67(1):1-15, 2000. [19] Charles R. Plott and Shyam Sunder. Rational expectations and the aggregation of diverse information in laboratory security markets.
Econometrica, 56(5):1085-1118, 1988. [20] Charles Polk, Robin Hanson, John Ledyard, and Takashi Ishikida. Policy analysis market: An electronic commerce application of a combinatorial information market. In Proceedings of the Fourth Annual ACM Conference on Electronic Commerce, pages 272-273, 2003. [21] R. Roll. Orange juice and weather. American Economic Review, 74(5):861-880, 1984. [22] Richard N. Rosett. Gambling and rationality. Journal of Political Economy, 73(6):595-607, 1965. [23] Carsten Schmidt and Axel Werwatz. How accurate do markets predict the outcome of an event? The Euro
Report 09-2002, Max Planck Institute for Research into Economic Systems, 2002. [24] Wayne W. Snyder. Horse racing: Testing the efficient markets model. Journal of Finance, 33(4):1109-1118,
[25] Richard H. Thaler and William T. Ziemba. Anomalies: Parimutuel betting markets: Racetracks and lotteries.

In a combinatorial auction, agents may bid on bundles of goods rather than individual goods alone. Since there are an exponential number of bundles (in the number of goods), communicating values over these bundles can be problematic. Communicating valuations in a one-shot fashion can be prohibitively expensive if the number of goods is only moderately large. Furthermore, it might even be hard for agents to determine their valuations for single bundles [14].
It is in the interest of such agents to have auction protocols which require them to bid on as few bundles as possible.
Even if agents can efficiently compute their valuations, they might still be reluctant to reveal them entirely in the course of an auction, because such information may be valuable to their competitors. These considerations motivate the need for auction protocols that minimize the communication and information revelation required to determine an optimal allocation of goods.
There has been recent work exploring the links between the preference elicitation problem in combinatorial auctions and the problem of learning an unknown function from computational learning theory [5, 19]. In learning theory, the goal is to learn a function via various types of queries, such as What is the function"s value on these inputs? In preference elicitation, the goal is to elicit enough partial information about preferences to be able to compute an optimal allocation. Though the goals of learning and preference elicitation differ somewhat, it is clear that these problems share similar structure, and it should come as no surprise that techniques from one field should be relevant to the other.
We show that any exact learning algorithm with membership and equivalence queries can be converted into a preference elicitation algorithm with value and demand queries.
The resulting elicitation algorithm guarantees elicitation in a polynomial number of value and demand queries. Here we mean polynomial in the number of goods, agents, and the sizes of the agents" valuation functions in a given encoding scheme. Preference elicitation schemes have not traditionally considered this last parameter. We argue that complexity guarantees for elicitation schemes should allow dependence on this parameter. Introducing this parameter also allows us to guarantee polynomial worst-case communication, which usually cannot be achieved in the number of goods and agents alone. Finally, we use our conversion procedure to generate combinatorial auction protocols from learning algorithms for polynomials, monotone DNF, and linear-threshold functions.
Of course, a one-shot combinatorial auction where agents provide their entire valuation functions at once would also have polynomial communication in the size of the agents" valuations, and only require one query. The advantage of our scheme is that agents can be viewed as black-boxes that provide incremental information about their valuations.
There is no burden on the agents to formulate their valuations in an encoding scheme of the auctioneer"s choosing.
We expect this to be an important consideration in practice.
Also, with our scheme entire revelation only happens in the worst-case. 180 For now, we leave the issue of incentives aside when deriving elicitation algorithms. Our focus is on the time and communication complexity of preference elicitation regardless of incentive constraints, and on the relationship between the complexities of learning and preference elicitation.
Related work. Zinkevich et al. [19] consider the problem of learning restricted classes of valuation functions which can be represented using read-once formulas and Toolbox DNF.
Read-once formulas can represent certain substitutabilities, but no complementarities, whereas the opposite holds for Toolbox DNF. Since their work is also grounded in learning theory, they allow dependence on the size of the target valuation as we do (though read-once valuations can always be succinctly represented anyway). Their work only makes use of value queries, which are quite limited in power. Because we allow ourselves demand queries, we are able to derive an elicitation scheme for general valuation functions.
Blum et al. [5] provide results relating the complexities of query learning and preference elicitation. They consider models with membership and equivalence queries in query learning, and value and demand queries in preference elicitation. They show that certain classes of functions can be efficiently learned yet not efficiently elicited, and vice-versa.
In contrast, our work shows that given a more general (yet still quite standard) version of demand query than the type they consider, the complexity of preference elicitation is no greater than the complexity of learning. We will show that demand queries can simulate equivalence queries until we have enough information about valuations to imply a solution to the elicitation problem.
Nisan and Segal [12] study the communication complexity of preference elicitation. They show that for many rich classes of valuations, the worst-case communication complexity of computing an optimal allocation is exponential.
Their results apply to the black-box model of computational complexity. In this model algorithms are allowed to ask questions about agent valuations and receive honest responses, without any insight into how the agents internally compute their valuations. This is in fact the basic framework of learning theory. Our work also addresses the issue of communication complexity, and we are able to derive algorithms that provide significant communication guarantees despite Nisan and Segal"s negative results. Their work motivates the need to rely on the sizes of agents" valuation functions in stating worst-case results.
The query learning model we consider here is called exact learning from membership and equivalence queries, introduced by Angluin [2]. In this model the learning algorithm"s objective is to exactly identify an unknown target function f : X → Y via queries to an oracle. The target function is drawn from a function class C that is known to the algorithm. Typically the domain X is some subset of {0, 1}m , and the range Y is either {0, 1} or some subset of the real numbers Ê. As the algorithm progresses, it constructs a manifest hypothesis ˜f which is its current estimate of the target function. Upon termination, the manifest hypothesis of a correct learning algorithm satisfies ˜f(x) = f(x) for all x ∈ X.
It is important to specify the representation that will be used to encode functions from C. For example, consider the following function from {0, 1}m to Ê: f(x) = 2 if x consists of m 1"s, and f(x) = 0 otherwise. This function may simply be represented as a list of 2m values. Or it may be encoded as the polynomial 2x1 · · · xm, which is much more succinct. The choice of encoding may thus have a significant impact on the time and space requirements of the learning algorithm. Let size(f) be the size of the encoding of f with respect to the given representation class. Most representation classes have a natural measure of encoding size. The size of a polynomial can be defined as the number of non-zero coefficients in the polynomial, for example. We will usually only refer to representation classes; the corresponding function classes will be implied. For example, the representation class of monotone DNF formulae implies the function class of monotone Boolean functions.
Two types of queries are commonly used for exact learning: membership and equivalence queries. On a membership query, the learner presents some x ∈ X and the oracle replies with f(x). On an equivalence query, the learner presents its manifest hypothesis ˜f. The oracle either replies ‘YES" if ˜f = f, or returns a counterexample x such that ˜f(x) = f(x).
An equivalence query is proper if size( ˜f) ≤ size(f) at the time the manifest hypothesis is presented.
We are interested in efficient learning algorithms. The following definitions are adapted from Kearns and Vazirani [9]: Definition 1. The representation class C is polynomialquery exactly learnable from membership and equivalence queries if there is a fixed polynomial p(·, ·) and an algorithm L with access to membership and equivalence queries of an oracle such that for any target function f ∈ C, L outputs after at most p(size(f), m) queries a function ˜f ∈ C such that ˜f(x) = f(x) for all instances x.
Similarly, the representation class C is efficiently exactly learnable from membership and equivalence queries if the algorithm L outputs a correct hypothesis in time p(size(f), m), for some fixed polynomial p(·, ·).
Here m is the dimension of the domain. Since the target function must be reconstructed, we also necessarily allow polynomial dependence on size(f).
In a combinatorial auction, a set of goods M is to be allocated among a set of agents N so as to maximize the sum of the agents" valuations. Such an allocation is called efficient in the economics literature, but we will refer to it as optimal and reserve the term efficient to refer to computational efficiency. We let n = |N| and m = |M|. An allocation is a partition of the objects into bundles (S1, . . . , Sn), such that Si ∩ Sj = ∅ for all distinct i, j ∈ N. Let Γ be the set of possible allocations. Each agent i ∈ N has a valuation function vi : 2M → Ê over the space of possible bundles.
Each valuation vi is drawn from a known class of valuations Vi. The valuation classes do not need to coincide.
We will assume that all the valuations considered are normalized, meaning v(∅) = 0, and that there are no externalities, meaning vi(S1, ..., Sn) = vi(Si), for all agents i ∈ N, for any allocation (S1, ..., Sn) ∈ Γ (that is, an agent cares only about the bundle allocated to her). Valuations satisfying these conditions are called general valuations.1 We 1 Often general valuations are made to satisfy the additional 181 also assume that agents have quasi-linear utility functions, meaning that agents" utilities can be divided into monetary and non-monetary components. If an agent i is allocated bundle S at price p, it derives utility ui(S, p) = vi(S) − p.
A valuation function may be viewed as a vector of 2m − 1 non-negative real-values. Of course there may also be more succinct representations for certain valuation classes, and there has been much research into concise bidding languages for various types of valuations [11]. A classic example which we will refer to again later is the XOR bidding language.
In this language, the agent provides a list of atomic bids, which consist of a bundle together with its value. To determine the value of a bundle S given these bids, one searches for the bundle S of highest value listed in the atomic bids such that S ⊆ S. It is then the case that v(S) = v(S ).
As in the learning theory setting, we will usually only refer to bidding languages rather than valuation classes, because the corresponding valuation classes will then be implied. For example, the XOR bidding language implies the class of valuations satisfying free-disposal, which is the condition that A ⊆ B ⇒ v(A) ≤ v(B).
We let size(v1, . . . , vn) = Èn i=1 size(vi). That is, the size of a vector of valuations is the size of the concatenation of the valuations" representations in their respective encoding schemes (bidding languages).
To make an analogy to computational learning theory, we assume that all representation classes considered are polynomially interpretable [11], meaning that the value of a bundle may be computed in polynomial time given the valuation function"s representation. More formally, a representation class (bidding language) C is polynomially interpretable if there exists an algorithm that given as input some v ∈ C and an instance x ∈ X computes the value v(x) in time q(size(v), m), for some fixed polynomial q(·, ·).2 In the intermediate rounds of an (iterative) auction, the auctioneer will have elicited information about the agents" valuation functions via various types of queries. She will thus have constructed a set of manifest valuations, denoted ˜v1, . . . , ˜vn.3 The values of these functions may correspond exactly to the true agent values, or they may for example be upper or lower bounds on the true values, depending on the types of queries made. They may also simply be default or random values if no information has been acquired about certain bundles. The goal in the preference elicitation problem is to construct a set of manifest valuations such that: arg max (S1,...,Sn)∈Γ i∈N ˜vi(Si) ⊆ arg max (S1,...,Sn)∈Γ i∈N vi(Si) That is, the manifest valuations provide enough information to compute an allocation that is optimal with respect to the true valuations. Note that we only require one such optimal allocation. condition of free-disposal (monotonicity), but we do not need it at this point. 2 This excludes OR∗ , assuming P = NP, because interpreting bids from this language is NP-hard by reduction from weighted set-packing, and there is no well-studied representation class in learning theory that is clearly analogous to OR∗ . 3 This view of iterative auctions is meant to parallel the learning setting. In many combinatorial auctions, manifest valuations are not explicitly maintained but rather simply implied by the history of bids.
Two typical queries used in preference elicitation are value and demand queries. On a value query, the auctioneer presents a bundle S ⊆ M and the agent responds with her (exact) value for the bundle v(S) [8]. On a demand query, the auctioneer presents a vector of non-negative prices p ∈ Ê(2m ) over the bundles together with a bundle S. The agent responds ‘YES" if it is the case that S ∈ arg max S ⊆M   v(S ) − p(S ) ¡ or otherwise presents a bundle S such that v(S ) − p(S ) > v(S) − p(S) That is, the agent either confirms that the presented bundle is most preferred at the quoted prices, or indicates a better one [15].4 Note that we include ∅ as a bundle, so the agent will only respond ‘YES" if its utility for the proposed bundle is non-negative. Note also that communicating nonlinear prices does not necessarily entail quoting a price for every possible bundle. There may be more succinct ways of communicating this vector, as we show in section 5.
We make the following definitions to parallel the query learning setting and to simplify the statements of later results: Definition 2. The representation classes V1, . . . , Vn can be polynomial-query elicited from value and demand queries if there is a fixed polynomial p(·, ·) and an algorithm L with access to value and demand queries of the agents such that for any (v1, . . . , vn) ∈ V1 × . . . × Vn, L outputs after at most p(size(v1, . . . , vn), m) queries an allocation (S1, . . . , Sn) ∈ arg max(S1,...,Sn)∈Γ È vi(Si).
Similarly, the representation class C can be efficiently elicited from value and demand queries if the algorithm L outputs an optimal allocation with communication p(size(v1, . . . , vn), m), for some fixed polynomial p(·, ·).
There are some key differences here with the query learning definition. We have dropped the term exactly since the valuation functions need not be determined exactly in order to compute an optimal allocation. Also, an efficient elicitation algorithm is polynomial communication, rather than polynomial time. This reflects the fact that communication rather than runtime is the bottleneck in elicitation.
Computing an optimal allocation of goods even when given the true valuations is NP-hard for a wide range of valuation classes. It is thus unreasonable to require polynomial time in the definition of an efficient preference elicitation algorithm. We are happy to focus on the communication complexity of elicitation because this problem is widely believed to be more significant in practice than that of winner determination [11].5 4 This differs slightly from the definition provided by Blum et al. [5] Their demand queries are restricted to linear prices over the goods, where the price of a bundle is the sum of the prices of its underlying goods. In contrast our demand queries allow for nonlinear prices, i.e. a distinct price for every possible bundle. This is why the lower bound in their Theorem 2 does not contradict our result that follows. 5 Though the winner determination problem is NP-hard for general valuations, there exist many algorithms that solve it efficiently in practice. These range from special purpose algorithms [7, 16] to approaches using off-the-shelf IP solvers [1]. 182 Since the valuations need not be elicited exactly it is initially less clear whether the polynomial dependence on size(v1, . . . , vn) is justified in this setting. Intuitively, this parameter is justified because we must learn valuations exactly when performing elicitation, in the worst-case. We address this in the next section.
AND DEMAND QUERIES We have described the query learning and preference elicitation settings in a manner that highlights their similarities.
Value and membership queries are clear analogs. Slightly less obvious is the fact that equivalence and demand queries are also analogs. To see this, we need the concept of Lindahl prices. Lindahl prices are nonlinear and non-anonymous prices over the bundles. They are nonlinear in the sense that each bundle is assigned a price, and this price is not necessarily the sum of prices over its underlying goods. They are non-anonymous in the sense that two agents may face different prices for the same bundle of goods. Thus Lindahl prices are of the form pi(S), for all S ⊆ M, for all i ∈ N.
Lindahl prices are presented to the agents in demand queries.
When agents have normalized quasi-linear utility functions, Bikhchandani and Ostroy [4] show that there always exist Lindahl prices such that (S1, . . . , Sn) is an optimal allocation if and only if Si ∈ arg max Si   vi(Si) − pi(Si) ¡ ∀i ∈ N (1) (S1, . . . , Sn) ∈ arg max (S1,...,Sn)∈Γ i∈N pi(Si) (2) Condition (1) states that each agent is allocated a bundle that maximizes its utility at the given prices. Condition (2) states that the allocation maximizes the auctioneer"s revenue at the given prices. The scenario in which these conditions hold is called a Lindahl equilibrium, or often a competitive equilibrium. We say that the Lindahl prices support the optimal allocation. It is therefore sufficient to announce supporting Lindahl prices to verify an optimal allocation.
Once we have found an allocation with supporting Lindahl prices, the elicitation problem is solved.
The problem of finding an optimal allocation (with respect to the manifest valuations) can be formulated as a linear program whose solutions are guaranteed to be integral [4].
The dual variables to this linear program are supporting Lindahl prices for the resulting allocation. The objective function to the dual program is: min pi(S) πs + i∈N πi (3) with πi = max S⊆M (˜vi(S) − pi(S)) πs = max (S1,...,Sn)∈Γ i∈N pi(Si) The optimal values of πi and πs correspond to the maximal utility to agent i with respect to its manifest valuation and the maximal revenue to the seller.
There is usually a range of possible Lindahl prices supporting a given optimal allocation. The agent"s manifest valuations are in fact valid Lindahl prices, and we refer to them as maximal Lindahl prices. Out of all possible vectors of Lindahl prices, maximal Lindahl prices maximize the utility of the auctioneer, in fact giving her the entire social welfare. Conversely, prices that maximize the È i∈N πi component of the objective (the sum of the agents" utilities) are minimal Lindahl prices. Any Lindahl prices will do for our results, but some may have better elicitation properties than others. Note that a demand query with maximal Lindahl prices is almost identical to an equivalence query, since in both cases we communicate the manifest valuation to the agent. We leave for future work the question of which Lindahl prices to choose to minimize preference elicitation.
Considering now why demand and equivalence queries are direct analogs, first note that given the πi in some Lindahl equilibrium, setting pi(S) = max{0, ˜vi(S) − πi} (4) for all i ∈ N and S ⊆ M yields valid Lindahl prices. These prices leave every agent indifferent across all bundles with positive price, and satisfy condition (1). Thus demand queries can also implicitly communicate manifest valuations, since Lindahl prices will typically be an additive constant away from these by equality (4). In the following lemma we show how to obtain counterexamples to equivalence queries through demand queries.
Lemma 1. Suppose an agent replies with a preferred bundle S when proposed a bundle S and supporting Lindahl prices p(S) (supporting with respect to the the agent"s manifest valuation). Then either ˜v(S) = v(S) or ˜v(S ) = v(S ).
Proof. We have the following inequalities: ˜v(S) − p(S) ≥ ˜v(S ) − p(S ) ⇒ ˜v(S ) − ˜v(S) ≤ p(S ) − p(S) (5) v(S ) − p(S ) > v(S) − p(S) ⇒ v(S ) − v(S) > p(S ) − p(S) (6) Inequality (5) holds because the prices support the proposed allocation with respect to the manifest valuation. Inequality (6) holds because the agent in fact prefers S to S given the prices, according to its response to the demand query. If it were the case that ˜v(S) = v(S) and ˜v(S ) = v(S ), these inequalities would represent a contradiction. Thus at least one of S and S is a counterexample to the agent"s manifest valuation.
Finally, we justify dependence on size(v1, . . . , vn) in elicitation problems. Nisan and Segal (Proposition 1, [12]) and Parkes (Theorem 1, [13]) show that supporting Lindahl prices must necessarily be revealed in the course of any preference elicitation protocol which terminates with an optimal allocation. Furthermore, Nisan and Segal (Lemma 1, [12]) state that in the worst-case agents" prices must coincide with their valuations (up to a constant), when the valuation class is rich enough to contain dual valuations (as will be the case with most interesting classes). Since revealing Lindahl prices is a necessary condition for establishing an optimal allocation, and since Lindahl prices contain the same information as valuation functions (in the worst-case), allowing for dependence on size(v1, . . . , vn) in elicitation problems is entirely natural. 183
ELICITATION The key to converting a learning algorithm to an elicitation algorithm is to simulate equivalence queries with demand and value queries until an optimal allocation is found.
Because of our Lindahl price construction, when all agents reply ‘YES" to a demand query, we have found an optimal allocation, analogous to the case where an agent replies ‘YES" to an equivalence query when the target function has been exactly learned. Otherwise, we can obtain a counterexample to an equivalence query given an agent"s response to a demand query.
Theorem 1. The representation classes V1, . . . , Vn can be polynomial-query elicited from value and demand queries if they can each be polynomial-query exactly learned from membership and equivalence queries.
Proof. Consider the elicitation algorithm in Figure 1.
Each membership query in step 1 is simulated with a value query since these are in fact identical. Consider step 4. If all agents reply ‘YES", condition (1) holds. Condition (2) holds because the computed allocation is revenue-maximizing for the auctioneer, regardless of the agents" true valuations.
Thus an optimal allocation has been found. Otherwise, at least one of Si or Si is a counterexample to ˜vi, by Lemma 1.
We identify a counterexample by performing value queries on both these bundles, and provide it to Ai as a response to its equivalence query.
This procedure will halt, since in the worst-case all agent valuations will be learned exactly, in which case the optimal allocation and Lindahl prices will be accepted by all agents. The procedure performs a polynomial number of queries, since A1, . . . , An are all polynomial-query learning algorithms.
Note that the conversion procedure results in a preference elicitation algorithm, not a learning algorithm. That is, the resulting algorithm does not simply learn the valuations exactly, then compute an optimal allocation. Rather, it elicits partial information about the valuations through value queries, and periodically tests whether enough information has been gathered by proposing an allocation to the agents through demand queries. It is possible to generate a Lindahl equilibrium for valuations v1, . . . , vn using an allocation and prices derived using manifest valuations ˜v1, . . . , ˜vn, and finding an optimal allocation does not imply that the agents" valuations have been exactly learned. The use of demand queries to simulate equivalence queries enables this early halting. We would not obtain this property with equivalence queries based on manifest valuations.
In this section, we turn to the issue of the communication complexity of elicitation. Nisan and Segal [12] show that for a variety of rich valuation spaces (such as general and submodular valuations), the worst-case communication burden of determining Lindahl prices is exponential in the number of goods, m. The communication burden is measured in terms of the number of bits transmitted between agents and auctioneer in the case of discrete communication, or in terms of the number of real numbers transmitted in the case of continuous communication.
Converting efficient learning algorithms to an elicitation algorithm produces an algorithm whose queries have sizes polynomial in the parameters m and size(v1, . . . , vn).
Theorem 2. The representation classes V1, . . . , Vn can be efficiently elicited from value and demand queries if they can each be efficiently exactly learned from membership and equivalence queries.
Proof. The size of any value query is O(m): the message consists solely of the queried bundle. To communicate Lindahl prices to agent i, it is sufficient to communicate the agent"s manifest valuation function and the value πi, by equality (4). Note that an efficient learning algorithm never builds up a manifest hypothesis of superpolynomial size, because the algorithm"s runtime would then also be superpolynomial, contradicting efficiency. Thus communicating the manifest valuation requires size at most p(size(vi), m), for some polynomial p that upper-bounds the runtime of the efficient learning algorithm. Representing the surplus πi to agent i cannot require space greater than q(size(˜vi), m) for some fixed polynomial q, because we assume that the chosen representation is polynomially interpretable, and thus any value generated will be of polynomial size. We must also communicate to i its allocated bundle, so the total message size for a demand query is at most p(size(vi), m) + q(p(size(vi), m), m)+O(m). Clearly, an agent"s response to a value or demand query has size at most q(size(vi), m) + O(m). Thus the value and demand queries, and the responses to these queries, are always of polynomial size. An efficient learning algorithm performs a polynomial number of queries, so the total communication of the resulting elicitation algorithm is polynomial in the relevant parameters.
There will often be explicit bounds on the number of membership and equivalence queries performed by a learning algorithm, with constants that are not masked by big-O notation. These bounds can be translated to explicit bounds on the number of value and demand queries made by the resulting elicitation algorithm. We upper-bounded the size of the manifest hypothesis with the runtime of the learning algorithm in Theorem 2. We are likely to be able to do much better than this in practice. Recall that an equivalence query is proper if size( ˜f) ≤ size(f) at the time the query is made. If the learning algorithm"s equivalence queries are all proper, it may then also be possible to provide tight bounds on the communication requirements of the resulting elicitation algorithm.
Theorem 2 show that elicitation algorithms that depend on the size(v1, . . . , vn) parameter sidestep Nisan and Segal"s [12] negative results on the worst-case communication complexity of efficient allocation problems. They provide guarantees with respect to the sizes of the instances of valuation functions faced at any run of the algorithm. These algorithms will fare well if the chosen representation class provides succinct representations for the simplest and most common of valuations, and thus the focus moves back to one of compact yet expressive bidding languages. We consider these issues below.
In this section, we demonstrate the application of our methods to particular representation classes for combinatorial valuations. We have shown that the preference elicitation problem for valuation classes V1, . . . , Vn can be reduced 184 Given: exact learning algorithms A1, . . . , An for valuations classes V1, . . . , Vn respectively.
Loop until there is a signal to halt:
equivalence query, or has halted with the agent"s exact valuation.
the manifest valuations ˜v1, . . . , ˜vn determined so far.
has replied with some preferred bundle Si. Perform value queries on Si and Si to find a counterexample to ˜vi, and provide it to Ai.
Figure 1: Converting learning algorithms to an elicitation algorithm. to the problem of finding an efficient learning algorithm for each of these classes separately. This is significant because there already exist learning algorithms for a wealth of function classes, and because it may often be simpler to solve each learning subproblem separately than to attack the preference elicitation problem directly. We can develop an elicitation algorithm that is tailored to each agent"s valuation, with the underlying learning algorithms linked together at the demand query stages in an algorithm-independent way.
We show that existing learning algorithms for polynomials, monotone DNF formulae, and linear-threshold functions can be converted into preference elicitation algorithms for general valuations, valuations with free-disposal, and valuations with substitutabilities, respectively. We focus on representations that are polynomially interpretable, because the computational learning theory literature places a heavy emphasis on computational tractability [18].
In interpreting the methods we emphasize the expressiveness and succinctness of each representation class. The representation class, which in combinatorial auction terms defines a bidding language, must necessarily be expressive enough to represent all possible valuations of interest, and should also succinctly represent the simplest and most common functions in the class.
Schapire and Sellie [17] give a learning algorithm for sparse multivariate polynomials that can be used as the basis for a combinatorial auction protocol. The equivalence queries made by this algorithm are all proper. Specifically, their algorithm learns the representation class of t-sparse multivariate polynomials over the real numbers, where the variables may take on values either 0 or 1. A t-sparse polynomial has at most t terms, where a term is a product of variables, e.g. x1x3x4. A polynomial over the real numbers has coefficients drawn from the real numbers. Polynomials are expressive: every valuation function v : 2M → Ê+ can be uniquely written as a polynomial [17].
To get an idea of the succinctness of polynomials as a bidding language, consider the additive and single-item valuations presented by Nisan [11]. In the additive valuation, the value of a bundle is the number of goods the bundle contains. In the single-item valuation, all bundles have value 1, except ∅ which has value 0 (i.e. the agent is satisfied as soon as it has acquired a single item). It is not hard to show that the single-item valuation requires polynomials of size 2m − 1, while polynomials of size m suffice for the additive valuation. Polynomials are thus appropriate for valuations that are mostly additive, with a few substitutabilities and complementarities that can be introduced by adjusting coefficients.
The learning algorithm for polynomials makes at most mti +2 equivalence queries and at most (mti +1)(t2 i +3ti)/2 membership queries to an agent i, where ti is the sparcity of the polynomial representing vi [17]. We therefore obtain an algorithm that elicits general valuations with a polynomial number of queries and polynomial communication.6
The XOR bidding language is standard in the combinatorial auctions literature. Recall that an XOR bid is characterized by a set of bundles B ⊆ 2M and a value function w : B → Ê+ defined on those bundles, which induces the valuation function: v(B) = max {B ∈B | B ⊆B} w(B ) (7) XOR bids can represent valuations that satisfy free-disposal (and only such valuations), which again is the property that A ⊆ B ⇒ v(A) ≤ v(B).
The XOR bidding language is slightly less expressive than polynomials, because polynomials can represent valuations that do not satisfy free-disposal. However, XOR is as expressive as required in most economic settings. Nisan [11] notes that XOR bids can represent the single-item valuation with m atomic bids, but 2m − 1 atomic bids are needed to represent the additive valuation. Since the opposite holds for polynomials, these two languages are incomparable in succinctness, and somewhat complementary for practical use.
Blum et al. [5] note that monotone DNF formulae are the analogs of XOR bids in the learning theory literature. A monotone DNF formula is a disjunction of conjunctions in which the variables appear unnegated, for example x1x2 ∨ x3 ∨ x2x4x5. Note that such formulae can be represented as XOR bids where each atomic bid has value 1; thus XOR bids generalize monotone DNF formulae from Boolean to real-valued functions. These insights allow us to generalize a classic learning algorithm for monotone DNF ([3] Theorem 6 Note that Theorem 1 applies even if valuations do not satisfy free-disposal. 185 1, [18] Theorem B) to a learning algorithm for XOR bids.7 Lemma 2. An XOR bid containing t atomic bids can be exactly learned with t + 1 equivalence queries and at most tm membership queries.
Proof. The algorithm will identify each atomic bid in the target XOR bid in turn. Initialize the manifest valuation ˜v to the bid that is identically zero on all bundles (this is an XOR bid containing 0 atomic bids). Present ˜v as an equivalence query. If the response is ‘YES", we are done. Otherwise we obtain a bundle S for which v(S) = ˜v(S). Create a bundle T as follows. First initialize T = S. For each item i in T, check via a membership query whether v(T) = v(T − {i}).
If so set T = T − {i}. Otherwise leave T as is and proceed to the next item.
We claim that (T, v(T)) is an atomic bid of the target XOR bid. For each item i in T, we have v(T) = v(T − {i}).
To see this, note that at some point when generating T, we had a ¯T such that T ⊆ ¯T ⊆ S and v( ¯T) > v( ¯T − {i}), so that i was kept in ¯T. Note that v(S) = v( ¯T) = v(T) because the value of the bundle S is maintained throughout the process of deleting items. Now assume v(T) = v(T − {i}). Then v( ¯T) = v(T) = v(T − {i}) > v( ¯T − {i}) which contradicts free-disposal, since T − {i} ⊆ ¯T − {i}.
Thus v(T) > v(T − {i}) for all items i in T. This implies that (T, v(T)) is an atomic bid of v. If this were not the case,
T would take on the maximum value of its strict subsets, by the definition of an XOR bid, and we would have v(T) = max i∈T { max T ⊆T −{i} v(T )} = max i∈T {v(T − {i})} < v(T) which is a contradiction.
We now show that v(T) = ˜v(T), which will imply that (T, v(T)) is not an atomic bid of our manifest hypothesis by induction. Assume that every atomic bid (R, ˜v(R)) identified so far is indeed an atomic bid of v (meaning R is indeed listed in an atomic bid of v as having value v(R) = ˜v(R)).
This assumption holds vacuously when the manifest valuation is initialized. Using the notation from (7), let ( ˜B, ˜w) be our hypothesis, and (B, w) be the target function. We have ˜B ⊆ B, and ˜w(B) = w(B) for B ∈ ˜B by assumption. Thus, ˜v(S) = max {B∈ ˜B | B⊆S} ˜w(B) = max {B∈ ˜B | B⊆S} w(B) ≤ max {B∈B | B⊆S} w(B) = v(S) (8) Now assume v(T) = ˜v(T). Then, ˜v(T) = v(T) = v(S) = ˜v(S) (9) The second equality follows from the fact that the value remains constant when we derive T from S. The last inequality holds because S is a counterexample to the manifest valuation. From equation (9) and free-disposal, we 7 The cited algorithm was also used as the basis for Zinkevich et al."s [19] elicitation algorithm for Toolbox DNF. Recall that Toolbox DNF are polynomials with non-negative coefficients. For these representations, an equivalence query can be simulated with a value query on the bundle containing all goods. have ˜v(T) < ˜v(S). Then again from equation (9) it follows that v(S) < ˜v(S). This contradicts (8), so we in fact have v(T) = ˜v(T). Thus (T, v(T)) is not currently in our hypothesis as an atomic bid, or we would correctly have ˜v(T) = v(T) by the induction hypothesis. We add (T, v(T)) to our hypothesis and repeat the process above, performing additional equivalence queries until all atomic bids have been identified.
After each equivalence query, an atomic bid is identified with at most m membership queries. Each counterexample leads to the discovery of a new atomic bid. Thus we make at most tm membership queries and exactly t + 1 equivalence queries.
The number of time steps required by this algorithm is essentially the same as the number of queries performed, so the algorithm is efficient. Applying Theorem 2, we therefore obtain the following corollary: Theorem 3. The representation class of XOR bids can be efficiently elicited from value and demand queries.
This contrasts with Blum et al."s negative results ([5],
Theorem 2) stating that monotone DNF (and hence XOR bids) cannot be efficiently elicited when the demand queries are restricted to linear and anonymous prices over the goods.
Polynomials, XOR bids, and all languages based on the OR bidding language (such as XOR-of-OR, OR-of-XOR, and OR∗ ) fail to succinctly represent the majority valuation [11]. In this valuation, bundles have value 1 if they contain at least m/2 items, and value 0 otherwise. More generally, consider the r-of-S family of valuations where bundles have value 1 if they contain at least r items from a specified set of items S ⊆ M, and value 0 otherwise. The majority valuation is a special case of the r-of-S valuation with r = m/2 and S = M. These valuations are appropriate for representing substitutabilities: once a required set of items has been obtained, no other items can add value.
Letting k = |S|, such valuations are succinctly represented by r-of-k threshold functions. These functions take the form of linear inequalities: xi1 + . . . + xik ≥ r where the function has value 1 if the inequality holds, and
WINNOW 2 algorithm can learn such functions using equivalence queries only, using at most 8r2 + 5k + 14kr ln m + 1 queries [10]. To provide this guarantee, r must be known to the algorithm, but S (and k) are unknown. The elicitation algorithm that results from WINNOW 2 uses demand queries only (value queries are not necessary here because the values of counterexamples are implied when there are only two possible values).
Note that r-of-k threshold functions can always be succinctly represented in O(m) space. Thus we obtain an algorithm that can elicit such functions with a polynomial number of queries and polynomial communication, in the parameters n and m alone. 186
We have shown that exact learning algorithms with membership and equivalence queries can be used as a basis for preference elicitation algorithms with value and demand queries. At the heart of this result is the fact that demand queries may be viewed as modified equivalence queries, specialized to the problem of preference elicitation. Our result allows us to apply the wealth of available learning algorithms to the problem of preference elicitation.
A learning approach to elicitation also motivates a different approach to designing elicitation algorithms that decomposes neatly across agent types. If the designer knowns beforehand what types of preferences each agent is likely to exhibit (mostly additive, many substitutes, etc...), she can design learning algorithms tailored to each agents" valuations and integrate them into an elicitation scheme. The resulting elicitation algorithm makes a polynomial number of queries, and makes polynomial communication if the original learning algorithms are efficient.
We do not require that agent valuations can be learned with value and demand queries. Equivalence queries can only be, and need only be, simulated up to the point where an optimal allocation has been computed. This is the preference elicitation problem. Theorem 1 implies that elicitation with value and demand queries is no harder than learning with membership and equivalence queries, but it does not provide any asymptotic improvements over the learning algorithms" complexity. It would be interesting to find examples of valuation classes for which elicitation is easier than learning. Blum et al. [5] provide such an example when considering membership/value queries only (Theorem 4).
In future work we plan to address the issue of incentives when converting learning algorithms to elicitation algorithms. In the learning setting, we usually assume that oracles will provide honest responses to queries; in the elicitation setting, agents are usually selfish and will provide possibly dishonest responses so as to maximize their utility.
We also plan to implement the algorithms for learning polynomials and XOR bids as elicitation algorithms, and test their performance against other established combinatorial auction protocols [6, 15]. An interesting question here is: which Lindahl prices in the maximal to minimal range are best to quote in order to minimize information revelation?
We conjecture that information revelation is reduced when moving from maximal to minimal Lindahl prices, namely as we move demand queries further away from equivalence queries. Finally, it would be useful to determine whether the OR∗ bidding language [11] can be efficiently learned (and hence elicited), given this language"s expressiveness and succinctness for a wide variety of valuation classes.
Acknowledgements We would like to thank Debasis Mishra for helpful discussions. This work is supported in part by NSF grant IIS0238147.
[1] A. Andersson, M. Tenhunen, and F. Ygge. Integer programming for combinatorial auction winner determination. In Proceedings of the Fourth International Conference on Multiagent Systems (ICMAS-00), 2000. [2] D. Angluin. Learning regular sets from queries and counterexamples. Information and Computation, 75:87-106, November 1987. [3] D. Angluin. Queries and concept learning. Machine Learning, 2:319-342, 1987. [4] S. Bikhchandani and J. Ostroy. The Package Assignment Model. Journal of Economic Theory, 107(2), December 2002. [5] A. Blum, J. Jackson, T. Sandholm, and M. Zinkevich.
Preference elicitation and query learning. In Proc. 16th Annual Conference on Computational Learning Theory (COLT), Washington DC, 2003. [6] W. Conen and T. Sandholm. Partial-revelation VCG mechanism for combinatorial auctions. In Proc. the 18th National Conference on Artificial Intelligence (AAAI), 2002. [7] Y. Fujishima, K. Leyton-Brown, and Y. Shoham.
Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches. In Proc. the 16th International Joint Conference on Artificial Intelligence (IJCAI), pages 548-553, 1999. [8] B. Hudson and T. Sandholm. Using value queries in combinatorial auctions. In Proc. 4th ACM Conference on Electronic Commerce (ACM-EC), San Diego, CA,
June 2003. [9] M. J. Kearns and U. V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. [10] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.
Machine Learning, 2:285-318, 1988. [11] N. Nisan. Bidding and allocation in combinatorial auctions. In Proc. the ACM Conference on Electronic Commerce, pages 1-12, 2000. [12] N. Nisan and I. Segal. The communication requirements of efficient allocations and supporting Lindahl prices. Working Paper, Hebrew University,
[13] D. C. Parkes. Price-based information certificates for minimal-revelation combinatorial auctions. In Padget et al., editor, Agent-Mediated Electronic Commerce IV,LNAI 2531, pages 103-122.
Springer-Verlag, 2002. [14] D. C. Parkes. Auction design with costly preference elicitation. In Special Issues of Annals of Mathematics and AI on the Foundations of Electronic Commerce,
Forthcoming (2003). [15] D. C. Parkes and L. H. Ungar. Iterative combinatorial auctions: Theory and practice. In Proc. 17th National Conference on Artificial Intelligence (AAAI-00), pages 74-81, 2000. [16] T. Sandholm, S. Suri, A. Gilpin, and D. Levine.
CABOB: A fast optimal algorithm for combinatorial auctions. In Proc. the 17th International Joint Conference on Artificial Intelligence (IJCAI), pages 1102-1108, 2001. [17] R. Schapire and L. Sellie. Learning sparse multivariate polynomials over a field with queries and counterexamples. In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, pages 17-26. ACM Press, 1993. 187 [18] L. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134-1142, Nov. 1984. [19] M. Zinkevich, A. Blum, and T. Sandholm. On polynomial-time preference elicitation with value-queries. In Proc. 4th ACM Conference on Electronic Commerce (ACM-EC), San Diego, CA,

While popular images of Wall Street often depict swashbuckling traders boldly making large gambles on just their market intuitions, the vast majority of trading is actually considerably more technical and constrained. The constraints often derive from a complex combination of business, regulatory and institutional issues, and result in certain kinds of standard trading strategies or criteria that invite algorithmic analysis.
One of the most common activities in modern financial markets is known as Volume Weighted Average Price, or VWAP, trading. Informally, the VWAP of a stock over a specified market period is simply the average price paid per share during that period, so the price of each transaction in the market is weighted by its volume. In VWAP trading, one attempts to buy or sell a fixed number of shares at a price that closely tracks the VWAP.
Very large institutional trades constitute one of the main motivations behind VWAP activity. A typical scenario goes as follows. Suppose a very large mutual fund holds 3% of the outstanding shares of a large, publicly traded company - a huge fraction of the shares - and that this fund"s manager decides he would like to reduce this holding to 2% over a 1-month period. (Such a decision might be forced by the fund"s own regulations or other considerations.) Typically, such a fund manager would be unqualified to sell such a large number of shares in the open market - it requires a professional broker to intelligently break the trade up over time, and possibly over multiple exchanges, in order to minimize the market impact of such a sizable transaction. Thus, the fund manager would approach brokerages for help in selling the 1%.
The brokerage will typically alleviate the fund manager"s problem immediately by simply buying the shares directly from the fund manager, and then selling them off laterbut what price should the brokerage pay the fund manager?
Paying the price on the day of the sale is too risky for the brokerage, as they need to sell the shares themselves over an extended period, and events beyond their control (such as wars) could cause the price to fall dramatically. The usual answer is that the brokerage offers to buy the shares from the fund manager at a per-share price tied to the VWAP over some future period - in our example, the brokerage might offer to buy the 1% at a per-share price of the coming month"s VWAP minus 1 cent. The brokerage now has a very clean challenge: by selling the shares themselves over the next month in a way that exactly matches the VWAP, a penny per share is earned in profits. If they can beat the VWAP by a penny, they make two cents per share. Such small-margin, high-volume profits can be extremely lucrative for a large brokerage. The importance of the VWAP has led to many automated VWAP trading algorithms - indeed, every major brokerage has at least one VWAP box, 189 Price Volume Model Order Book Model Macroscopic Distribution Model OWT Θ(log(R)) (From[3]) O(log(R) log(N)) 2E(Pbins maxprice ) 2(1 + )E(Pbins maxprice ) for -approx of Pbins maxprice Θ(log(Q)) (same as above plus...) VWAP Θ(log(R)) O(log(R) log(N)) (from above) 2E(Pbins vol ) Ω(Q) fixed schedule O(log(Q)) for large N (1 + )2E(Pbins vol ) for -approx. of Pbins vol
Figure 1: The table summarizes the results presented in this paper. The rows represent results for either the OWT or VWAP criterion. The columns represent which model we are working in. The entry in the table is the competitive ratio between our algorithm and an optimal algorithm, and the closer the ratio is to 1 the better. The parameter R represents a bound on the maximum to the minimum price fluctuation and the parameter Q represents a bound on the maximum to minimum volume fluctuation in the respective model. (See Section 4 for a description of the Macroscopic Distribution Model.) All the results for the OWT trading criterion (which is a stronger criterion) directly translate to the VWAP criterion. However, in the VWAP setting, considering a restriction on the maximum to the minimum volume fluctuation Q, leads to an additional class of results which depends on Q. and some small companies focus exclusively on proprietary VWAP trading technology.
In this paper, we provide the first study of VWAP trading algorithms in an online, competitive ratio setting. We first formalize the VWAP trading problem in a basic online model we call the price-volume model, which can be viewed as a generalization of previous theoretical online trading models incorporating market volume information. In this model, we provide VWAP algorithms and competitive ratios, and compare this setting with the one-way trading (OWT) problem studied in [3].
Our most interesting results, however, examine the VWAP trading problem in a new online trading model capturing the important recent phenomenon of limit order books in financial markets. Briefly, a limit buy or sell order specifies both the number of shares and the desired price, and will only be executed if there is a matching party on the opposing side, according to a well-defined matching procedure used by all the major exchanges. While limit order books (the list of limit orders awaiting possible future execution) have existed since the dawn of equity exchanges, only very recently have these books become visible to traders in real time, thus opening the way to trading algorithms of all varieties that attempt to exploit this rich market microstructure data. Such data and algorithms are a topic of great current interest on Wall Street [4].
We thus introduce a new online trading model incorporating limit order books, and examine both the one-way and VWAP trading problems in it. Our results are summarized in Figure 1 (see the caption for a summary).
We now present a trading model which includes both price and volume information about the sequence of trades. While this model is a generalization of previous formalisms for online trading, it makes an infinite liquidity assumption which fails to model the negative market impact that trading a large number of shares typically has. This will be addressed in the order book model studied in the next section.
A note on terminology: throughout the paper (unless otherwise specified), we shall use the term market to describe all activity or orders other than those of the algorithm under consideration. The setting we consider can be viewed as a game between our algorithm and the market.
In the price-volume trading model, we assume that the intraday trading activity in a given stock is summarized by a discrete sequence of price and volume pairs (pt, vt) for t = 1, . . . , T. Here t = 0 corresponds to the day"s market open, and t = T to the close. While there is nothing technically special about the time horizon of a single day, it is particularly consistent with limit order book trading on Wall Street. The pair (pt, vt) represents the fact that a total of vt shares were traded at an (average) price per share pt in the market between time t − 1 and t. Realistically, we should imagine the number of intervals T being reasonably large, so that it is sensible to assign a common approximate price to all shares traded within an interval.
In the price-volume model, we shall make an infinite liquidity assumption for our trading algorithms. More precisely, in this online model, we see the price-volume sequence one pair at a time. Following the observation of (pt, vt), we are permitted to sell any (possibly fractional) number of shares nt at the price pt. Let us assume that our goal is to sell N shares over the course of the day. Hence, at each time, we must select a (possibly fractional) number of shares nt to sell at price pt, subject to the global constraint T t=1 nt = N. It is thus assumed that if we have left over shares to sell after time T − 1, we are forced to sell them at the closing price of the market - that is, nT = N − T −1 t=1 nt is sold at pT . In this way we are certain to sell exactly N shares over the course of the day; the only thing an algorithm must do is determine the schedule of selling based on the incoming market price-volume stream.
Any algorithm which sells fractional volumes can be converted to a randomized algorithm which only sells integral volumes with the same expected number of shares sold. If we keep the hard constraint of selling exactly N shares, we might incur an additional slight loss in the conversion. (Note that we only allow fractional volumes in the price-volume model, where liquidity is not an issue. In the order book model to follow, we do not allow fractional volumes.) In VWAP trading, the goal of an online algorithm A which sells exactly N shares is not to maximize profits per se, but to track the market VWAP. The market VWAP for an intraday trading sequence S = (p1, v1), . . . , (pT , vT ) is simply the average price paid per share over the course of the trading 190 day, ie VWAPM (S) = T t=1 ptvt /V where V is the total daily volume, i.e., V = T t=1 vt. If on the sequence S, the algorithm A sells its N stocks using the volume sequence n1, . . . nT , then we analogously define the VWAP of A on market sequence S by VWAPA(S) = T t=1 ptnt /N .
Note that the market VWAP does not include the shares that the algorithm sells.
The VWAP competitive ratio of A with respect to a set of sequences Σ is then RVWAP(A) = max S∈Σ {VWAPM (S)/VWAPA(S)} In the case that A is randomized, we generalize the definition above by taking an expectation over VWAPA(S) inside the max. We note that unlike on Wall Street, our definition of VWAPM does not take our own trading into account. It is easy to see that this makes it a more challenging criterion to track.
In contrast to the VWAP, another common measure of the performance of an online selling algorithm would be its one-way trading (OWT) competitive ratio [3] with respect to a set of sequences Σ: ROWT(A) = max S∈Σ max 1≤t≤T {pt/VWAPA(S)} where the algorithms performance is compared to the largest individual price appearing in the sequence S.
In both VWAP and OWT, we are comparing the average price per share received by a selling algorithm to some measure of market performance. In the case of OWT, we compare to the rather ambitious benchmark of the high price of the day, ignoring volumes entirely. In VWAP trading, we have the more modest goal of comparing favorably to the overall market average of the day. As we shall see, there are some important commonalities and differences to these two approaches. For now we note one simple fact: on any specific sequence S, VWAPA(S) may be larger that VWAPM (S).
However, RVWAP(A) cannot be smaller than 1, since on any sequence S in which all price pt are identical, it is impossible to get a better average share per price. Thus, for all algorithms A, both RVWAP(A) and ROWT(A) are larger than 1, and the closer to 1 they are, the better A is tracking its respective performance measure.
As in previous work on online trading, it is generally not possible to obtain finite bounds on competitive ratios with absolutely no assumptions on the set of sequences Σbounds on the maximum variation in price or volume are required, depending on the exact setting. We thus introduce the following two assumptions.
Let 0 < Vmin ≤ Vmax be known positive constants, and define Q = Vmax /Vmin . For all intraday trading sequences S ∈ Σ, the total daily volume V ∈ [Vmin , Vmax ].
Let 0 < pmin ≤ pmax be known positive constants, and define R = pmax/pmin. For all intraday trading sequences S ∈ Σ, the prices satisfy pt ∈ [pmin, pmax], for all t = 1, . . . , T.
Competitive ratios are generally taken over all sets Σ consistent with at least one of these assumptions. To gain some intuition consider the two trivial cases of R = 1 and Q = 1.
In the case of R = 1 (where there is no fluctuation in price), any schedule is optimal. In the case of Q = 1 (where the total volume V over the trading period is known), we can gain a competitive ratio of 1 by selling vt V N shares after each time period.
For the OWT problem in the price-volume model, volumes are irrelevant for the performance criterion, but for the VWAP criterion they are central. For the OWT problem under the price variability assumption, the results of [3] established that the optimal competitive ratio was Θ(log(R)).
Our first result establishes that the optimal competitive ratio for VWAP under the volume variability assumption is Θ(log(Q)) and is achieved by an algorithm that ignores the price data.
Theorem 1. In the price-volume model under the volume variability assumption, there exists an online algorithm A for selling N shares achieving competitive ratio RVWAP(A) ≤
the price variability) assumption holds, any online algorithm A for selling N shares has RVWAP(A) = Ω(log(Q)).
Proof. (Sketch) For the upper bound, the idea is similar to the price reservation algorithm of [3] for the OWT problem, and similar in spirit to the general technique of classify and select [1]. Consider algorithms which use a parameter ˆV , which is interpreted as an estimate for the total volume for the day. Then at each time t, if the market price and volume is (pt, vt), the algorithm sells a fraction vt/ ˆV of its shares. We consider a family of log(Q) such algorithms, where algorithm Ai uses ˆV = Vmin 2i−1 . Clearly, one of the Ai has a competitive ratio of 2. We can derive an O(log(Q)) VWAP competitive ratio by running these algorithms in parallel, and letting each algorithm sell N/ log(Q) shares. (Alternatively, we can randomly select one Ai and guarantee the same expected competitive ratio.) We now sketch the proof of the lower bound, which relates performance in the VWAP and OWT problems. Any algorithm that is c-competitive in the VWAP setting (under fixed Q) is 3c-competitive in the OWT setting with R = Q/2. To show this, we take any sequence S of prices for the OWT problem, and convert it into a price-volume sequence for the VWAP problem. The prices in the VWAP sequence are the same as in S. To construct the volumes in the VWAP sequence, we segment the prices in S into log(R) intervals [2i−1 pmin , 2i pmin ). Suppose pt ∈ [2i−1 pmin , 2i pmin ), and this is the first time in S that a price has fallen in this interval. Then in the VWAP sequence we set the volume vt = 2i−1 . If this is not the first visit to the interval containing pt, we set vt = 0. Assume that the maximum price in S is pmax . The VWAP of our sequence is at least pmax /3.
Since we had a c competitive algorithm, its average sell is at least pmax /3c. The lower bound now follows using the lower bound in [3].
An alternative approach to VWAP is to ignore the volumes in favor of prices, and apply an algorithm for the OWT problem. Note that the lower bound in this theorem, unlike in the previous one, only assumes a price variation bound. 191 Theorem 2. In the price-volume model under the price variability assumption, there exists an online algorithm A for selling N shares achieving competitive ratio RVWAP(A) = O(log(R)). In addition, if only the price variability (and not the volume variability) assumption holds, any online A for selling N shares has RVWAP(A) = Ω(log(R)).
Proof. (Sketch) Follows immediately from the results of [3] for OWT: the upper bound from the simple fact that for any sequence S, VWAPA(S) is less than max1≤t≤T {pt}, and the lower bound from a reduction to OWT.
Theorems 1 and 2 demonstrate that one can achieve logarithmic VWAP competitive ratios under the assumption of either bounded variability of total volume or bounded variability of maximum price. If both assumptions hold, it is possible to give an algorithm accomplishing the minimum of log(Q) and log(R). This flexibility of approach derives from the fact that the VWAP is a quantity in which both prices and volumes matter, as opposed to OWT.
All of the VWAP algorithms we have discussed so far make some use of the daily data (pt, vt) as it unfolds, using either the price or volume information. In contrast, a fixed schedule VWAP algorithm has a predetermined distribution {f1, f2, . . . fT }, and simply sells ftN shares at time t, independent of (pt, vt). Fixed schedule VWAP algorithms, or slight variants of them, are surprisingly common on Wall Street, and the schedule is usually derived from historical intraday volume data. Our next result demonstrates that such algorithms can perform considerably worse than dynamically adaptive algorithms in terms of the worst case competitive ratio.
Theorem 3. In the price-volume model under both the volume and price variability assumptions, any fixed schedule VWAP algorithm A for selling N shares has sell VWAP competitive ratio RVWAP(A) = Ω(min(T, R)).
The proofs of all the results in this subsection are in the Appendix.
So far our emphasis has been on VWAP algorithms that must sell exactly N shares. In many realistic circumstances, however, there is actually some flexibility in the precise number of shares to be sold. For instance, this is true at large brokerages, where many separate VWAP trades may be pooled and executed by a common algorithm, and the firm would be quite willing to carry a small position of unsold shares overnight if it resulted in better execution prices.
The following theorem (which interestingly has no analogue for the OWT problem) demonstrates that this trade-off in shares sold and performance can be realized dramatically in our model. It states that if we are willing to let the number of shares sold vary with Q, we can in fact achieve a VWAP competitive ratio of 1.
Theorem 4. In the price-volume model under the volume variability assumption, there exists an algorithm A that always sells between N and QN shares and that the average price per sold share is exactly VWAPM (S).
In many online problems, there is a clear distinction between benefit problems and cost problems [2]. In the VWAP setting, selling shares is a benefit problem, and buying shares is a cost problem. The definitions of the competitive ratios, Rbuy VWAP(A) and Rbuy OWT(A), for algorithms which Figure 2: Sample Island order books for MSFT. buy exactly N shares are maxS∈Σ{VWAPA(S)/VWAPM (S)} and maxS∈Σ maxt{VWAPA(S)/pt} respectively. Eventhough Theorem 4 also holds for buying, in general, the competitive ratio of the buy (cost) problem is much higher, as stated in the following theorem.
Theorem 5. In the price-volume model under the volume and price variability assumptions, there exists an online algorithm A for buying N shares achieving buy VWAP competitive ratio Rbuy VWAP(A) = O(min{Q, √ R}). In addition any online algorithm A for buying N shares has buy VWAP competitive ratio Rbuy VWAP(A) = Ω(min{Q, √ R}).
MODEL Before we can define our online trading model based on limit order books, we give some necessary background on the detailed mechanics of financial markets, which are sometimes referred to as market microstructure. We then provide results and algorithms for both the OWT and VWAP problems. 192
Market Microstructure A fundamental distinction in stock trading is that between a limit order and a market order. Suppose we wish to purchase 1000 shares of Microsoft (MSFT) stock. In a limit order, we specify not only the desired volume (1000 shares), but also the desired price. Suppose that MSFT is currently trading at roughly $24.07 a share (see Figure 2, which shows an actual snapshot of a recent MSFT order book on Island (www.island.com), a well-known electronic exchange for NASDAQ stocks), but we are only willing to buy the
submit a limit order with this specification, and our order will be placed in a queue called the buy order book, which is ordered by price, with the highest offered unexecuted buy price at the top (often referred to as the bid). If there are multiple limit orders at the same price, they are ordered by time of arrival (with older orders higher in the book). In the example provided by Figure 2, our order would be placed immediately after the extant order for 5,503 shares at $24.04; though we offer the same price, this order has arrived before ours. Similarly, a sell order book for sell limit orders (for instance, we might want to sell 500 shares of MSFT at $24.10 or higher) is maintained, this time with the lowest sell price offered (often referred to as the ask).
Thus, the order books are sorted from the most competitive limit orders at the top (high buy prices and low sell prices) down to less competitive limit orders. The bid and ask prices (which again, are simply the prices in the limit orders at the top of the buy and sell books, respectively) together are sometimes referred to as the inside market, and the difference between them as the spread. By definition, the order books always consist exclusively of unexecuted orders - they are queues of orders hopefully waiting for the price to move in their direction.
How then do orders get executed? There are two methods. First, any time a market order arrives, it is immediately matched with the most competitive limit orders on the opposing book. Thus, a market order to buy 2000 shares is matched with enough volume on the sell order book to fill the 2000 shares. For instance, in the example of Figure 2, such an order would be filled by the two limit sell orders for 500 shares at $24.069, the 500 shares at $24.07, the 200 shares at $24.08, and then 300 of the 1981 shares at $24.09.
The remaining 1681 shares of this last limit order would remain as the new top of the sell limit order book. Second, if a buy (sell, respectively) limit order comes in above the ask (below the bid, respectively) price, then the order is matched with orders on the opposing books. It is important to note that the prices of executions are the prices specified in the limit orders already in the books, not the prices of the incoming order that is immediately executed.
Every market or limit order arrives atomically and instantaneously - there is a strict temporal sequence in which orders arrive, and two orders can never arrive simultaneously.
This gives rise to the definition of the last price of the exchange, which is simply the last price at which the exchange executed an order. It is this quantity that is usually meant when people casually refer to the (ticker) price of a stock.
Note that a limit buy (sell, respectively) order with a price of infinity (0, respectively) is effectively a market order. We shall thus assume without loss of generality that all orders are placed as limit order. Although limit orders which are unexecuted may be removed by the party which placed them, for simplicity, we assume that limit orders are never removed from the books.
We refer the reader to [4] for further discussion of modern electronic exchanges and market microstructure.
The online order book trading model is intended to capture the realistic details of market microstructure just discussed in a competitive ratio setting. In this refined model, a day"s market activity is described by a sequence of limit orders (pt, vt, bt). Here bt is a bit indicating whether the order is a buy or sell order, while pt is the limit order price and vt the number of shares desired. Following the arrival of each such limit order, an online trading algorithm is permitted to place its own limit order. These two interleaved sources (market and algorithm) of limit orders are then simply operated on according to the matching process described in Section 3.1. Any limit order that is not immediately executable according to this process is placed in the appropriate (buy or sell) book for possible future execution; arriving orders that can be partially or fully executed are so executed, with any residual shares remaining on the respective book.
The goal of a VWAP or OWT selling algorithm is essentially the same as in the price-volume model, but the context has changed in the following two fundamental ways.
First, the assumption of infinite liquidity in the price-volume model is eliminated entirely. The number of shares available at any given price is restricted to the total volume of limit orders offering that price. Second, all incoming orders, and therefore the complete limit order books, are assumed to be visible to the algorithm. This is consistent with modern electronic financial exchanges, and indeed is the source of much current interest on Wall Street [4].
In general, the definition of competitive ratios in the order book model is complicated by the fact that now our algorithm"s activity influences the sequence of executed prices and volumes. We thus first define the execution sequence determined by a limit order sequence (placed by the market and our algorithm). Let S = (p1, v1, b1), . . . , (pT , vT , bT ) be a limit order sequence placed by the market, and let S = (p1, v1, b1), . . . , (pT , vT , bT ) be a limit order sequence placed by our algorithm (unless otherwise specified, all bt are of the sell type). Let merge(S, S ) be the merged sequence (p1, v1, b1), (p1, v1, b1), . . . , (pT , vT , bT ), (pT , vT , bT ), which is the time sequence of orders placed by the market and algorithm. Note that the algorithm has the option of not placing an order, which we can view as a zero volume order.
If we conducted the order book maintenance and order execution process described in Section 3.1 on the sequence merge(S, S ), at irregular intervals a trade occurs for some number of shares and some price. In each executed trade, the selling party is either the market or the algorithm. Let execM (S, S ) = (q1, w1), . . . , (qT , wT ) be the sequence of executions where the market (that is, a party other than the algorithm) was the selling party, where the qt are the execution prices and wt the execution volumes. Similarly, we define execA(S, S ) = (r1, x1), . . . , (rT , xT ) to be the sequence of executions in which the algorithm was the selling party. Thus, execA(S, S ) ∪ execM (S, S ) is the set of all executions. We generally expect T to be (possibly much) smaller than T .
The revenue of the algorithm and the market are defined 193 as: REVM (S, S ) ≡ T t=1 qtwt , REVA(S, S ) ≡ T t=1 rtxt Note that both these quantities are solely determined by the execution sequences execM (S, S ) and execA(S, S ), respectively.
For an algorithm A which is constrained to sell exactly N shares, we define the OWT competitive ratio of A, ROWT(A), as the maximum ratio (under any S ∈ Σ) of the revenue obtained by A, as compared to the revenue obtained by an optimal oﬄine algorithm A∗ . More formally, for A∗ which is constrained to sell exactly N shares, we define ROWT(A) = max S∈Σ max A∗ REVA∗ (S S∗ ) REVA(S, S ) where S∗ is the limit order sequence placed by A∗ on S. If the algorithm A is randomized then we take the appropriate expectation with respect to S ∼ A.
We define the VWAP competitive ratio, RVWAP(A), as the maximum ratio (under any S ∈ Σ) between the market and algorithm VWAPs. More formally, define VWAPM (S, S ) as REVM (S, S )/ T t=1 wt, where the denominator is just the total executed volume of orders placed by the market. Similarly, we define VWAPA(S, S ) as REVA(S, S )/N, since we assume the algorithm sells no more than N shares (this definition implicitly assumes that A gets a 0 price for unsold shares). The VWAP competitive ratio of A is then: RVWAP(A) = max S∈Σ {VWAPM (S, S )/VWAPA(S, S )} where S is the online sequence of limit orders generated by A in response to the sequence S.
For the OWT problem in the order book model, we introduce a more subtle version of the price variability assumption. This is due to the fact that our algorithm"s trading can impact the high and low prices of the day. For the assumption below, note that execM (S, ∅) is the sequence of executions without the interaction of our algorithm.
Let 0 < pmin ≤ pmax be known positive constants, and define R = pmax/pmin. For all intraday trading sequences S ∈ Σ, the prices pt in the sequence execM (S, ∅) satisfy pt ∈ [pmin, pmax], for all t = 1, . . . , T.
Note that this assumption does not imply that the ratios of high to low prices under the sequences execM (S, S ) or execA(S, S ) are bounded by R. In fact, the ratio in the sequence execA(S, S ) could be infinite if the algorithm ends up selling some stocks at a 0 price.
Theorem 6. In the order book model under the order book price variability assumption, there exists an online algorithm A for selling N shares achieving sell OWT competitive ratio ROWT(A) = 2 log(R) log(N).
Proof. The algorithm A works by guessing a price p in the set {pmin2i : 1 ≤ i ≤ log(R)} and placing a sell limit order for all N shares at the price p at the beginning of the day. (Alternatively, algorithm A can place log(R) sell limit orders, where the i-th one has price 2i pmin and volume N/ log(R).) By placing an order at the beginning of the day, the algorithm undercuts all sell orders that will be placed during the day for a price of p or higher, meaning the algorithm"s N shares must be filled first at this price. Hence, if there were k shares that would have been sold at price p or higher without our activity, then A would sell at least kp shares.
We define {pj} to be the multiset of prices of individual shares that are either executed or are buy limit order shares that remained unexecuted, excluding the activity of our algorithm (that is, assuming our algorithm places no orders). Assume without loss of generality that p1 ≥ p2 ≥ . . ..
Consider guessing the kth highest such price, pk. If an order for N shares is placed at the day"s start at price pk, then we are guaranteed to obtain a return of kpk. Let k∗ = argmaxk{kpk}. We can view our algorithm as attempting to guess pk∗ , and succeeding if the guess p satisfies p ∈ [pk∗ /2, pk∗ ]. Hence, we are 2 log(R) competitive with the quantity max1≤k≤N kpk. Note that ρ ≡ N i=1 pi = N i=1 1 i ipi ≤ max 1≤k≤N kpk N i=1 1 i ≤ log(N) max 1≤k≤N kpk where ρ is defined as the sum of the top N prices pi without A"s involvement.
Similarly, let {pj} be the multiset of prices of individual executed shares, or the prices of unexecuted buy order shares, but now including the orders placed by some selling algorithm A . We now wish to show that for all algorithms A which sell N shares, REVA ≤ N i=1 pi ≤ ρ.
Essentially, this inequality states the intuitive idea that a selling algorithm can only lower executed or unmatched buy order share prices. To prove this, we use induction to show that the removal of the activity of a selling algorithm causes these prices to increase. First, remove the last share in the last sell order placed by either A or the market on an arbitrary sequence merge(S, S ) - by this we mean, take the last sell order placed by A or the market and decrease its volume by one share. After this modification, the top N prices p1 . . . pN will not decrease. This is because either this sell order share was not executed, in which case the claim is trivially true, or, if it was executed, the removal of this sell order share leaves an additional unexecuted buy order share of equal or higher price. For induction, assume that if we remove a share from any sell order that was placed, by A or the market, at or after time t then the top N prices do not decrease. We now show that if we remove a share from the last sell order that was placed by A or the market before time t, then the top N prices do not decrease. If this sell order share was not executed, then the claim is trivially true. Else, if the sell order share was executed, then claim is true because by removing this executed share from the sell order either: i) the corresponding buy order share (of equal or higher value) is unmatched on the remainder of the sequence, in which case the claim is true; or ii) this buy 194 order matches some sell order share at an equal or higher price, which has the effect of removing a share from a sell order on the remainder of the sequence, and, by the inductive assumption, this can only increase prices. Hence, we have proven that for all A which sell N shares REVA ≤ ρ.
We have now established that our revenue satisfies
1≤k≤N {kpk} ≥ ρ/ log(N) ≥ max A {REVA }/ log(N), where A performs an arbitrary sequence of N sell limit orders.
The OWT algorithm from Theorem 6 can be applied to obtain the following VWAP result: Corollary 7. In the order book model under the order book price variability assumption, there exists an online algorithm A for selling N shares achieving sell VWAP competitive ratio RVWAP(A) = O(log(R) log(N)).
We now make a rather different assumption on the sequences S.
Assumption.
The set of sequences Σ satisfies the following two properties. First, we assume that each order placed by the market is of volume less than γ, which we view as a mild assumption since typically single orders on the market are not of high volume (due to liquidity issues). This assumption allows our algorithm to place at least one limit order at a time interleaved with approximately γ market executions. Second, we assume that there is large volume in the sell order books below the price pmax , which means that no orders placed by the market will be executed above the price pmax . The simplest way to instantiate this latter assumption in the order book model is to assume that each sequence S ∈ Σ starts by placing a huge number of sell orders (more than Vmax ) at price pmax .
Although this assumption has a maximum price parameter, it does not imply that the price ratio R is finite, since it does not imply any lower bound on the prices of buy or executed shares (aside from the trivial one of 0).
Theorem 8. Consider the order book model under the bounded order volume and max price assumption. There exists an algorithm A in which after exactly γN market executions have occurred, then A has sold at most N shares and REVA(S, S ) N = VWAPA(S, S ) ≥ (1 − )VWAPM (S, S ) − pmax N where S is a sequence of N sell limit orders generated by A when observing S.
Proof. The algorithm divides the trading day into volume intervals whose real-time duration may vary. For each period i in which γ shares have been executed in the market, the algorithm computes the market VWAP of only those shares traded in period i; let us denote this by VWAPi.
Following this ith volume interval, the algorithm places a limit order to sell exactly one share at a price close to VWAPi.
More precisely, the algorithm only places orders at the discrete prices (1− )pmax , (1− )2 pmax , . . .. Following volume interval i, the algorithm places a limit order to sell one share at the discretized price that is closest to VWAPi, but which is strictly smaller.
For the analysis, we begin by noting that if all of the algorithm"s limit orders are executed during the day, the total revenue received by the algorithm would be at least (1 − )VWAPM (S, S )N. To see this, it suffices to note that VWAPM (S, S ) is a uniform mixture of the VWAPi (since by definition they each cover the same amount of market volume); and if all the algorithm"s limit orders were executed, they each received more than (1 − )VWAPi dollars for the interval i they followed.
We now count the potential lost revenue of the algorithm due to unexecuted limit orders. By the assumption that individual orders are placed with volume less than γ, then our algorithm is able to place a limit order during every block of γ shares have been traded. Hence, after γN market orders have been executed, A has placed N orders in the market.
Note that there can be at most one limit order (and thus, at most one share) left unexecuted at each level of the discretized price ladder defined above. This is because following interval i, the algorithm places its limit order strictly below VWAPi, so if VWAPj ≥ VWAPi for j > i, this limit order must have been executed. Thus unexecuted limit orders bound the VWAPs of the remainder of the day, resulting in at most one unexecuted order per price level.
A bound on the lost revenue is thus the sum of the discretized prices: ∞ i=1(1 − )i pmax ≤ pmax / . Clearly our algorithm has sold at most N shares.
Note that as N becomes large, VWAPA approaches 1 − times the market VWAP. If we knew that the final total volume of the market executions is V , then we can set γ = V/N, assuming that γ >> 1. If we have only an upper and lower bound on V we should be able to guess and incur a logarithmic loss. The following assumption tries to capture the market volume variability.
We now assume that the total volume (which includes the shares executed by both our algorithm and the market) is variable within some known region and that the market volume will be greater than our algorithms volume. More formally, for all S ∈ Σ, assume that the total volume V of shares traded in execM (S, S ), for any sequence S of N sell limit orders, satisfies 2N ≤ Vmin ≤ V ≤ Vmax . Let Q = Vmax /Vmin .
The following corollary is derived using a constant = 1/2 and observing that if we set γ such that V ≤ γN ≤ 2V then our algorithm will place between N and N/2 limit orders.
Corollary 9. In the order book model, if the bounded order volume and max price assumption, and the order book volume variability assumption hold, there exists an online algorithm A for selling at most N shares such that VWAPA(S, S ) ≥ 1
VWAPM (S, S ) − 2pmax N 195
x 10 7 0 20 40 60 80 100 QQQ: log(Q)=4.71, E=3.77
x 10 6 0 20 40 60 80 JNPR: log(Q)=5.66, E=3.97
x 10 6 0 10 20 30 40 50 60 70 MCHP: log(Q)=5.28, E=3.86
x 10 6 0 50 100 150 200 250 CHKP: log(Q)=6.56, E=4.50 Figure 3: Here we present bounds from Section 4 based on the empirical volume distributions for four real stocks: QQQ, MCHP, JNPR, and CHKP. The plots show histograms for the total daily volumes transacted on Island for these stocks, in the last year and a half, along with the corresponding values of log(Q) and E(Pbins vol ) (denoted by "E").
We assume that the minimum and maximum daily volumes in the data correspond to Vmin and Vmax , respectively.
The worst-case competitive ratio bounds (which are twice log(Q)) of our algorithm for those stocks are 9.42, 10.56, 11.32, and 13.20, respectively. The corresponding bounds on the competitive ratio performance of our algorithm under the volume distribution model (which are twice E(Pbins vol )) are better: 7.54, 7.72, 7.94, and 9.00, respectively (a 20−40% relative improvement). Using a finer volume binning along with a slightly more refined bound on the competitive ratio, we can construct algorithms that, using the empirical volume distribution given as correct, guarantee even better competitive ratios of 2.76, 2.73, 2.75, and 3.17, respectively for those stocks (details omitted).
MODELS We conclude our results with a return to the price-volume model, where we shall introduce some refined methods of analysis for online trading algorithms. We leave the generalization of these methods to the order book model for future work.
The competitive ratios defined so far measure performance relative to some baseline criterion in the worst case over all market sequences S ∈ Σ. It has been observed in many online settings that such worst-case metrics can yield pessimistic results, and various relaxations have been considered, such as permitting a probability distribution over the input sequence.
We now consider distributional models that are considerably weaker than assuming a distribution over complete market sequences S ∈ Σ. In the volume distribution model, we assume only that there exists a distribution Pvol over the total volume V traded in the market for the day, and then examine the worst-case competitive ratio over sequences consistent with the randomly chosen volume. More precisely, we define RVWAP(A, Pvol ) = EV ∼Pvol max S∈seq(V ) VWAPM (S) VWAPA(S) .
Here V ∼ Pvol denotes that V is chosen with respect to distribution Pvol , and seq(V ) ⊂ Σ is the set of all market sequences (p1, v1), . . . , (pT , vT ) satisfying T t=1 vt = V .
Similarly, for OWT, we can define ROWT(A, Pmaxprice ) = Ep∼Pmaxprice max S∈seq(p) p VWAPA(S) .
Here Pmaxprice is a distribution over just the maximum price of the day, and we then examine worst-case sequences consistent with this price (seq(p) ⊂ Σ is the set of all market sequences satisfying max1≤t≤T pt = p). Analogous buy-side definitions can be given.
We emphasize that in these models, only the distribution of maximum volume and price is known to the algorithm.
We also note that our probabilistic assumptions on S are considerably weaker than typical statistical finance models, which would posit a detailed stochastic model for the step-by-step evolution of (pt, vt). Here we instead permit only a distribution over crude, macroscopic measures of the entire day"s market activity, such as the total volume and high price, and analyze the worst-case performance consistent with these crude measures. For this reason, we refer to such settings as the macroscopic distribution model.
The work of El-Yaniv et al. [3] examines distributional assumptions similar to ours, but they emphasize the worst196 case choices for the distributions as well, and show that this leads to results no better than the original worst-case analysis over all sequences. In contrast, we feel that the analysis of specific distributions Pvol and Pmaxprice is natural in many financial contexts and our preliminary experimental results show significant improvements when this rather crude distributional information is taken into account (see Figure 3).
Our results in the VWAP setting examine the cases where these distributions are known exactly or only approximately.
Similar results can be obtained for macroscopic distributions of maximum daily price for the one-way trading setting.
Model We begin by noting that the algorithms examined so far work by binning total volumes or maximum prices into bins of exponentially increasing size, and then guessing the index of the bin in which the actual quantity falls. It is thus natural that the macroscopic distribution model performance of such algorithms (which are common in competitive analysis) might depend on the distribution of the true bin index.
In the remaining, we assume that Q is a power of 2 and the base of the logarithm is 2. Let Pvol denote the distribution of total daily market volume. We define the related distribution Pbins vol over bin indices i as follows: for all i = 1, . . . , log(Q) − 1, Pbins vol (i) is equal to the probability, under Pvol , that the daily volume falls in the interval [Vmin 2i−1 , Vmin 2i ), and Pbins vol (log(Q)) is for the last interval [Vmax /2, Vmax ] .
We define E as E(Pbins vol ) ≡ Ei∼P bins vol 1/Pbins vol (i) 2 =   log(Q) i=1 Pbins vol (i)   2 .
Since the support of Pbins vol has only log(Q) elements, E(Pbins vol ) can vary from 1 (for distributions Pvol that place all of their weight in only one of the log(Q) intervals between Vmin , Vmin 2, Vmin 4, . . . , Vmax ) to log(Q) (for distributions Pvol in which the total daily volume is equally likely to fall in any one of these intervals). Note that distributions Pvol of this latter type are far from uniform over the entire range [Vmin , Vmax ].
Theorem 10. In the volume distribution model under the volume variability assumption, there exists an online algorithm A for selling N shares that, using only knowledge of the total volume distribution Pvol , achieves RVWAP(A, Pvol ) ≤ 2E(Pbins vol ).
All proofs in this section are provided in the appendix.
As a concrete example, consider the case in which Pvol is the uniform distribution over [Vmin , Vmax ]. In that case,
Pbins vol is exponentially increasing and peaks at the last bin, which, having the largest width, also has the largest weight.
In this case E(Pbins vol ) is a constant (i.e., independent of Q), leading to a constant competitive ratio. On the other hand, if Pvol is exponential, then Pbins vol is uniform, leading to an O(log(Q)) competitive ratio, just as in the more adversarial price-volume setting discussed earlier. In Figure 3, we provide additional specific bounds obtained for empirical total daily volume distributions computed for some real stocks.
We now examine the setting in which Pvol is unknown, but an approximation ˜Pvol is available. Let us define C(Pbins vol , ˜Pbins vol ) = log(Q) j=1 ˜Pbins vol (j) log(Q) i=1 P bins vol (i) √ ˜P bins vol (i) .
C is minimized at C(Pbins vol , Pbins vol ) = E(Pbins vol ), and C may be infinite if ˜Pbins vol (i) is 0 when Pbins vol (i) > 0.
Theorem 11. In the volume distribution model under the volume variability assumption, there exists an online algorithm A for selling N shares that using only knowledge of an approximation ˜Pvol of Pvol achieves RVWAP(A, Pvol ) ≤ 2C(Pbins vol , ˜Pbins vol ).
As an example of this result, suppose our approximation obeys (1/α)Pbins vol (i) ≤ ˜Pbins vol (i) ≤ αPbins vol (i) for all i, for some α > 1. Thus our estimated bin index probabilities are all within a factor of α of the truth. Then it is easy to show that C(Pbins vol , ˜Pbins vol ) ≤ αE(Pbins vol ), so according to Theorems 10 and 11 our penalty for using the approximate distribution is a factor of α in competitive ratio.
[1] B. Awerbuch, Y. Bartal, A. Fiat, and A. Ros´en.
Competitive non-preemptive call control. In Proc. 5"th ACM-SIAM Symp. on Discrete Algorithms, pages 312-320, 1994. [2] A. Borodin and R. El-Yaniv. Online Computation and Competitive Analysis. Cambridge University Press,
[3] R. El-Yaniv, A. Fiat, R. M. Karp, and G. Turpin.
Optimal search and one-way trading online algorithms.
Algorithmica, 30:101-139, 2001. [4] M. Kearns and L. Ortiz. The Penn-Lehman automated trading project. IEEE Intelligent Systems, 2003. To appear.
Proof. (Sketch of Theorem 3) W.l.o.g., assume that Q =
fixed schedule f sells the least, then ft ≤ N/T. Consider the sequences where at time t we have pt = pmax , vt = V , and for times t = t we have pt = pmin and vt = 0. The VWAP is pmax and the fixed schedule average is (N/T)pmax + (N − N/T)pmin .
Proof. (Sketch of Theorem 4) The algorithm simply sells ut = (vt/Vmin )N shares at time t. The total number of shares sold U is clearly more than N and U = t ut = t (vt/Vmin )N = (V/Vmin )N ≤ QN The average price is V WAPA(S) = ( t ptut)/U = t pt(vt/V ) = V WAPM (S), where we used the fact that ut/U = vt/V . 197 Proof. (of Theorem 5) We start with the proof of the lower bound. Consider the following scenario. For the first T time units we have a price of √ Rpmin , and a total volume of Vmin . We observe how many shares the online algorithm has bought. If it has bought more than half of the shares, the remaining time steps have price pmin and volume Vmax − Vmin . Otherwise, the remaining time steps have price pmax and negligible volume.
In the first case the online has paid at least √ Rpmin /2 while the VWAP is at most √ Rpmin /Q + pmin . Therefore, in this case the competitive ratio is Ω(Q). In the second case the online has to buy at least half of the shares at pmax , so its average cost is at least pmax /2. The market VWAP is√ Rpmin = pmax / √ R, hence the competitive ratio is Ω( √ R).
For the upper bound, we can get a √ R competitive ratio, by buying all the shares once the price drops below √ Rpmin .
The Q upper bound is derive by running an algorithm that assumes the volume is Vmin . The online pays a cost of p, while the VWAP will be at least p/Q.
Proof. (Sketch of Theorem 10) We use the idea of guessing the total volume from Theorem 1, but now allow for the possibility of an arbitrary (but known) distribution over the total volume. In particular, consider constructing a distribution Gbins vol over a set of volume values using Pvol and use it to guess the total volume V . Let the algorithm guess ˆV = Vmin 2i with probability Gbins vol (i). Then note that, for any price-volume sequence S, if V ∈ [Vmin 2i−1 , Vmin 2i ],
VWAPA(S) ≥ Gbins vol (i)VWAPM (S)/2. This implies an upper bound on RVWAP(A, Pvol ) in terms of Gbins vol . We then get that Gbins vol (i) ∝ Pbins vol (i) minimizes the upper bound, which leads to the upper bound stated in the theorem.

Among the types of auctions commonly used in practice, sealed-bid auctions are a good practical choice because they require little communication and can be completed almost instantly. Each bidder simply submits a bid, and the winner is immediately determined. However, sealed-bid auctions do require that the bids be kept private until the auction clears. The increasing popularity of online auctions only makes this disadvantage more troublesome. At an auction house, with all participants present, it is difficult to examine a bid that another bidder gave directly to the auctioneer.
However, in an online auction the auctioneer is often little more than a server with questionable security; and, since all participants are in different locations, one can anonymously attempt to break into the server. In this paper, we present a game theoretic analysis of how bidders should behave when they are aware of the possibility of cheating that is based on knowledge of the bids.
We investigate this type of cheating along two dimensions: whether it is the auctioneer or a bidder who cheats, and which variant (either first or second-price) of the sealed-bid auction is used. Note that two of these cases are trivial.
In our setting, there is no incentive for the seller to submit a shill bid in a first price auction, because doing so would either cancel the auction or not affect the payment of the winning bidder. In a second-price auction, knowing the competing bids does not help a bidder because it is dominant strategy to bid truthfully. This leaves us with two cases that we examine in detail.
A seller can profitably cheat in a second-price auction by looking at the bids before the auction clears and submitting an extra bid. This possibility was pointed out as early as the seminal paper [12] that introduced this type of auction.
For example, if the bidders in an eBay auction each use a proxy bidder (essentially creating a second-price auction), then the seller may be able to break into eBay"s server, observe the maximum price that a bidder is willing to pay, and then extract this price by submitting a shill bid just below it using a false identity. We assume that there is no chance that the seller will be caught when it cheats. However, not all sellers are willing to use this power (or, not all sellers can successfully cheat). We assume that each bidder knows the probability with which the seller will cheat. Possible motivation for this knowledge could be a recently published expos´e on seller cheating in eBay auctions. In this setting, we derive an equilibrium bidding strategy for the case in which each bidder"s value for the good is independently drawn from a common distribution (with no further assumptions except for continuity and differentiability). This result shows how first and second-price auctions can be viewed as the endpoints of a spectrum of auctions.
But why should the seller have all the fun? In a first-price auction, a bidder must bid below his value for the good (also called shaving his bid) in order to have positive utility if he 76 wins. To decide how much to shave his bid, he must trade off the probability of winning the auction against how much he will pay if he does win. Of course, if he could simply examine the other bids before submitting his own, then his problem is solved: bid the minimum necessary to win the auction.
In this setting, our goal is to derive an equilibrium bidding strategy for a non-cheating bidder who is aware of the possibility that he is competing against cheating bidders. When bidder values are drawn from the commonly-analyzed uniform distribution, we show the counterintuitive result that the possibility of other bidders cheating has no effect on the equilibrium strategy of an honest bidder. This result is then extended to show the robustness of the equilibrium of a first-price auction without the possibility of cheating.
We conclude this section by exploring other distributions, including some in which the presence of cheating bidders actually induces an honest bidder to lower its bid.
The rest of the paper is structured as follows. In Section
case of a seller cheating in a second price auction. Section
In Section 4, we quantify the effects that the possibility of cheating has on an honest seller in the two settings. We discuss related work, including other forms of cheating in auctions, in Section 5, before concluding with Section 6. All proofs and derivations are found in the appendix.
CHEATING SELLER In this section, we consider a second-price auction in which the seller may cheat by inserting a shill bid after observing all of the bids. The formulation for this section will be largely reused in the following section on bidders cheating in a first-price auction. While no prior knowledge of game theory or auction theory is assumed, good introductions can be found in [2] and [6], respectively.
The setting consists of N bidders, or agents, (indexed by i = 1, · · · , n) and a seller. Each agent has a type θi ∈ [0, 1], drawn from a continuous range, which represents the agent"s value for the good being auctioned.2 Each agent"s type is independently drawn from a cumulative distribution function (cdf ) F over [0, 1], where F(0) = 0 and F(1) = 1.
We assume that F(·) is strictly increasing and differentiable over the interval [0, 1]. Call the probability density function (pdf ) f(θi) = F (θi), which is the derivative of the cdf.
Each agent knows its own type θi, but only the distribution over the possible types of the other agents. A bidding strategy for an agent bi : [0, 1] → [0, 1] maps its type to its bid.3 Let θ = (θ1, · · · , θn) be the vector of types for all agents, and θ−i = (θ1, · · · , θi−1, θi+1, · · · θn) be the vector of all types except for that of agent i. We can then combine the vectors so that θ = (θi, θ−i). We also define the vector of bids as b(θ) = (b1(θ1), . . . , bn(θn)), and this vector without 2 We can restrict the types to the range [0, 1] without loss of generality because any distribution over a different range can be normalized to this range. 3 We thus limit agents to deterministic bidding strategies, but, because of our continuity assumption, there always exists a pure strategy equilibrium. the bid of agent i as b−i(θ−i). Let b[1](θ) be the value of the highest bid of the vector b(θ), with a corresponding definition for b[1](θ−i).
An agent obviously wins the auction if its bid is greater than all other bids, but ties complicate the formulation.
Fortunately, we can ignore the case of ties in this paper because our continuity assumption will make them a zero probability event in equilibrium. We assume that the seller does not set a reserve price.4 If the seller does not cheat, then the winning agent pays the highest bid by another agent. On the other hand, if the seller does cheat, then the winning agent will pay its bid, since we assume that a cheating seller would take full advantage of its power. Let the indicator variable µc be 1 if the seller cheats, and 0 otherwise. The probability that the seller cheats, Pc , is known by all agents.5 We can then write the payment of the winning agent as follows. pi(b(θ), µc ) = µc · bi(θi) − (1 − µc ) · b[1](θ−i) (1) Let µ(·) be an indicator function that takes an inequality as an argument and returns is 1 if it holds, and 0 otherwise.
The utility for agent i is zero if it does not win the auction, and the difference between its valuation and its price if it does. ui(b(θ), µc , θi) = µ bi(θi) > b[1](θ−i) · θi − pi(b(θ), µc ) (2) We will be concerned with the expected utility of an agent, with the expectation taken over the types of the other agents and over whether or not the seller cheats. By pushing the expectation inward so that it is only over the price (conditioned on the agent winning the auction), we can write the expected utility as: Eθ−i,µc [ui(b(θ), µc , θi)] = Prob bi(θi) > b[1](θ−i) · θi − Eθ−i,µc pi(b(θ), µc ) | bi(θi) > b[1](θ−i) (3) We assume that all agents are rational, expected utility maximizers. Because of the uncertainty over the types of the other agents, we will be looking for a Bayes-Nash equilibrium. A vector of bidding strategies b∗ is a Bayes-Nash equilibrium if for each agent i and each possible type θi, agent i cannot increase its expected utility by using an alternate bidding strategy bi, holding the bidding strategies for all other agents fixed. Formally, b∗ is a Bayes-Nash equilibrium if: ∀i, θi, bi Eθ−i,µc ui b∗ i (θi), b∗ −i(θ−i) , µc , θi ≥ Eθ−i,µc ui bi(θi), b∗ −i(θ−i) , µc , θi (4)
We first present the Bayes-Nash equilibrium for an arbitrary distribution F(·). 4 This simplifies the analysis, but all of our results can be applied to the case in which the seller announces a reserve price before the auction begins. 5 Note that common knowledge is not necessary for the existence of an equilibrium. 77 Theorem 1. In a second-price auction in which the seller cheats with probability Pc , it is a Bayes-Nash equilibrium for each agent to bid according to the following strategy: bi(θi) = θi − θi 0 F( N−1 P c ) (x)dx F( N−1 P c ) (θi) (5) It is useful to consider the extreme points of Pc . Setting Pc = 1 yields the correct result for a first-price auction (see, e.g., [10]). In the case of Pc = 0, this solution is not defined.
However, in the limit, bi(θi) approaches θi as Pc approaches 0, which is what we expect as the auction approaches a standard second-price auction.
The position of Pc is perhaps surprising. For example, the linear combination bi(θi) = θi − Pc · θi
(x)dx F (N−1)(θi) of the equilibrium bidding strategies of first and second-price auctions would have also given us the correct bidding strategies for the cases of Pc = 0 and Pc = 1.
An alternative perspective on the setting is as a continuum between first and second-price auctions. Consider a probabilistic sealed-bid auction in which the seller is honest, but the price paid by the winning agent is determined by a weighted coin flip: with probability Pc it is his bid, and with probability 1 − Pc it is the second-highest bid.
By adjusting Pc , we can smoothly move between a first and second-price auction. Furthermore, the fact that this probabilistic auction satisfies the properties required for the Revenue Equivalence Theorem (see, e.g., [2]) provides a way to verify that the bidding strategy in Equation 5 is the symmetric equilibrium of this auction (see the alternative proof of Theorem 1 in the appendix).
Another way to try to gain insight into Equation 5 is by instantiating the distribution of types. We now consider the often-studied uniform distribution: F(θi) = θi.
Corollary 2. In a second-price auction in which the seller cheats with probability Pc , and F(θi) = θi, it is a Bayes-Nash equilibrium for each agent to bid according to the following strategy: bi(θi) = N − 1 N − 1 + Pc θi (6) This equilibrium bidding strategy, parameterized by Pc , can be viewed as an interpolation between two well-known results. When Pc = 0 the bidding strategy is now welldefined (each agent bids its true type), while when Pc = 1 we get the correct result for a first-price auction: each agent bids according to the strategy bi(θi) = N−1 N θi.
CHEATING AGENTS We now consider the case in which the seller is honest, but there is a chance that agents will cheat and examine the other bids before submitting their own (or, alternatively, they will revise their bid before the auction clears). Since this type of cheating is pointless in a second-price auction, we only analyze the case of a first-price auction. After revising the formulation from the previous section, we present a fixed point equation for the equilibrium strategy for an arbitrary distribution F(·). This equation will be useful for the analysis the uniform distribution, in which we show that the possibility of cheating agents does not change the equilibrium strategy of honest agents. This result has implications for the robustness of the symmetric equilibrium to overbidding in a standard first-price auction. Furthermore, we find that for other distributions overbidding actually induces a competing agent to shave more off of its bid.
It is clear that if a single agent is cheating, he will bid (up to his valuation) the minimum amount necessary to win the auction. It is less obvious, though, what will happen if multiple agents cheat. One could imagine a scenario similar to an English auction, in which all cheating agents keep revising their bids until all but one cheater wants the good at the current winning bid. However, we are only concerned with how an honest agent should bid given that it is aware of the possibility of cheating. Thus, it suffices for an honest agent to know that it will win the auction if and only if its bid exceeds every other honest agent"s bid and every cheating agent"s type.
This intuition can be formalized as the following discriminatory auction. In the first stage, each agent"s payment rule is determined. With probability Pa , the agent will pay the second highest bid if it wins the auction (essentially, he is a cheater), and otherwise it will have to pay its bid.
These selections are recorded by a vector of indicator variables µa = (µa1 , . . . , µan ), where µai = 1 denotes that agent i pays the second highest bid. Each agent knows the probability Pa , but does not know the payment rule for all other agents. Otherwise, this auction is a standard, sealed-bid auction. It is thus a dominant strategy for a cheater to bid its true type, making this formulation strategically equivalent to the setting outlined in the previous paragraph. The expression for the utility of an honest agent in this discriminatory auction is as follows. ui(b(θ), µa , θi) = θi − bi(θ) · j=i µaj · µ bi(θi) > θj + (1 − µaj ) · µ bi(θi) > bj(θj) (7)
Our goal is to find the equilibrium in which all cheating agents use their dominant strategy of bidding truthfully and honest agents bid according to a symmetric bidding strategy.
Since we have left F(·) unspecified, we cannot present a closed form solution for the honest agent"s bidding strategy, and instead give a fixed point equation for it.
Theorem 3. In a first-price auction in which each agent cheats with probability Pa , it is a Bayes-Nash equilibrium for each non-cheating agent i to bid according to the strategy that is a fixed point of the following equation: bi(θi) = θi − θi
(N−1) dx Pa · F(bi(θi)) + (1 − Pa) · F(θi) (N−1) (8) 78
Since we could not solve Equation 8 in the general case, we can only see how the possibility of cheating affects the equilibrium bidding strategy for particular instances of F(·).
A natural place to start is uniform distribution: F(θi) = θi.
Recall the logic behind the symmetric equilibrium strategy in a first-price auction without cheating: bi(θi) = N−1 N θi is the optimal tradeoff between increasing the probability of winning and decreasing the price paid upon winning, given that the other agents are bidding according to the same strategy. Since in the current setting the cheating agents do not shave their bid at all and thus decrease an honest agent"s probability of winning (while obviously not affecting the price that an honest agent pays if he wins), it is natural to expect that an honest agent should compensate by increasing his bid. The idea is that sacrificing some potential profit in order to regain some of the lost probability of winning would bring the two sides of the tradeoff back into balance. However, it turns out that the equilibrium bidding strategy is unchanged.
Corollary 4. In a first-price auction in which each agent cheats with probability Pa , and F(θi) = θi, it is a BayesNash equilibrium for each non-cheating agent to bid according to the strategy bi(θi) = N−1 N θi.
This result suggests that the equilibrium of a first-price auction is particularly robust when types are drawn from the uniform distribution, since the best response is unaffected by deviations of the other agents to the strategy of always bidding their type. In fact, as long as all other agents shave their bid by a fraction (which can differ across the agents) no greater than 1 N , it is still a best response for the remaining agent to bid according to the equilibrium strategy. Note that this result holds even if other agents are shaving their bid by a negative fraction, and are thus irrationally bidding above their type.
Theorem 5. In a first-price auction where F(θi) = θi, if each agent j = i bids according a strategy bj(θj) = N−1+αj N θj, where αj ≥ 0, then it is a best response for the remaining agent i to bid according to the strategy bi(θi) = N−1 N θi.
Obviously, these strategy profiles are not equilibria (unless each αj = 0), because each agent j has an incentive to set αj = 0. The point of this theorem is that a wide range of possible beliefs that an agent can hold about the strategies of the other agents will all lead him to play the equilibrium strategy. This is important because a common (and valid) criticism of equilibrium concepts such as Nash and BayesNash is that they are silent on how the agents converge on a strategy profile from which no one wants to deviate.
However, if the equilibrium strategy is a best response to a large set of strategy profiles that are out of equilibrium, then it seems much more plausible that the agents will indeed converge on this equilibrium.
It is important to note, though, that while this equilibrium is robust against arbitrary deviations to strategies that shave less, it is not robust to even a single agent shaving more off of its bid. In fact, if we take any strategy profile consistent with the conditions of Theorem 5 and change a single agent j"s strategy so that its corresponding αj is negative, then agent i"s best response is to shave more than 1 N off of its bid.
Other Distributions A natural question is whether the best response bidding strategy is similarly robust to overbidding by competing agents for other distributions. It turns out that Theorem
, where k is some positive integer. However, taking a simple linear combination of two such distributions to produce F(θi) = θ2 i +θi 2 yields a distribution in which an agent should actually shave its bid more when other agents shave their bids less. In the example we present for this distribution (with the details in the appendix), there are only two players and the deviation by one agent is to bid his type. However, it can be generalized to a higher number of agents and to other deviations.
Example 1. In a first-price auction where F(θi) = θ2 i +θi 2 and N = 2, if agent 2 always bids its type (b2(θ2) = θ2), then, for all θ1 > 0, agent 1"s best response bidding strategy is strictly less than the bidding strategy of the symmetric equilibrium.
We also note that the same result holds for the normalized exponential distribution (F(θi) = eθi −1 e−1 ).
It is certainly the case that distributions can be found that support the intuition given above that agents should shave their bid less when other agents are doing likewise.
Examples include F(θi) = −1 2 θ2 i + 3 2 θi (the solution to the system of equations: F (θi) = −1, F(0) = 0, and F(1) = 1), and F(θi) = e−e(1−θi) e−1 .
It would be useful to relate the direction of the change in the best response bidding strategy to a general condition on F(·). Unfortunately, we were not able to find such a condition, in part because the integral in the symmetric bidding strategy of a first-price auction cannot be solved without knowing F(·) (or at least some restrictions on it).
We do note, however, that the sign of the second derivative of F(θi)/f(θi) is an accurate predictor for all of the distributions that we considered.
HONEST SELLER In both of the settings we covered, an honest seller suffers a loss in expected revenue due to the possibility of cheating.
The equilibrium bidding strategies that we derived allow us to quantify this loss. Although this is as far as we will take the analysis, it could be applied to more general settings, in which the seller could, for example, choose the market in which he sells his good or pay a trusted third party to oversee the auction.
In a second-price auction in which the seller may cheat, an honest seller suffers due the fact that the agents will shave their bids. For the case in which agent types are drawn from the uniform distribution, every agent will shave its bid by P c N−1+P c , which is thus also the fraction by which an honest seller"s revenue decreases due to the possibility of cheating.
Analysis of the case of a first-price auction in which agents may cheat is not so straightforward. If Pa = 1 (each agent cheats with certainty), then we simply have a second-price auction, and the seller"s expected revenue will be unchanged.
Again considering the uniform distribution for agent types, it is not surprising that Pa = 1 2 causes the seller to lose 79 the most revenue. However, even in this worst case, the percentage of expected revenue lost is significantly less than it is for the second-price auction in which Pc = 1 2 , as shown in Table 1.6 It turns out that setting Pc = 0.2 would make the expected loss of these two settings comparable. While this comparison between the settings is unlikely to be useful for a seller, it is interesting to note that agent suspicions of possible cheating by the seller are in some sense worse than agents actually cheating themselves.
Percentage of Revenue lost for an Honest Seller Agents Second-Price Auction First-Price Auction (Pc = 0.5) (Pa = 0.5)
Table 1: The percentage of expected revenue lost by an honest seller due to the possibility of cheating in the two settings considered in this paper. Agent valuations are drawn from the uniform distribution.
Existing work covers another dimension along which we could analyze cheating: altering the perceived value of N.
In this paper, we have assumed that N is known by all of the bidders. However, in an online setting this assumption is rather tenuous. For example, a bidder"s only source of information about N could be a counter that the seller places on the auction webpage, or a statement by the seller about the number of potential bidders who have indicated that they will participate. In these cases, the seller could arbitrarily manipulate the perceived N. In a first-price auction, the seller obviously has an incentive to increase the perceived value of N in order to induce agents to bid closer to their true valuation. However, if agents are aware that the seller has this power, then any communication about N to the agents is cheap talk, and furthermore is not credible. Thus, in equilibrium the agents would ignore the declared value of N, and bid according to their own prior beliefs about the number of agents. If we make the natural assumption of a common prior, then the setting reduces to the one tackled by [5], which derived the equilibrium bidding strategies of a first-price auction when the number of bidders is drawn from a known distribution but not revealed to any of the bidders. Of course, instead of assuming that the seller can always exploit this power, we could assume that it can only do so with some probability that is known by the agents. The analysis would then proceed in a similar manner as that of our cheating seller model.
The other interesting case of this form of cheating is by bidders in a first-price auction. Bidders would obviously want to decrease the perceived number of agents in order to induce their competition to lower their bids. While it is 6 Note that we have not considered the costs of the seller.
Thus, the expected loss in profit could be much greater than the numbers that appear here. unreasonable for bidders to be able to alter the perceived N arbitrarily, collusion provides an opportunity to decrease the perceived N by having only one of a group of colluding agents participate in the auction. While the non-colluding agents would account for this possibility, as long as they are not certain of the collusion they will still be induced to shave more off of their bids than they would if the collusion did not take place. This issue is tackled in [7].
Other types of collusion are of course related to the general topic of cheating in auctions. Results on collusion in first and second-price auctions can be found in [8] and [3], respectively.
The work most closely related to our first setting is [11], which also presents a model in which the seller may cheat in a second-price auction. In their setting, the seller is a participant in the Bayesian game who decides between running a first-price auction (where profitable cheating is never possible) or second-price auction. The seller makes this choice after observing his type, which is his probability of having the opportunity and willingness to cheat in a second-price auction. The bidders, who know the distribution from which the seller"s type is drawn, then place their bid. It is shown that, in equilibrium, only a seller with the maximum probability of cheating would ever choose to run a second-price auction. Our work differs in that we focus on the agents" strategies in a second-price auction for a given probability of cheating by the seller. An explicit derivation of the equilibrium strategies then allows us relate first and second-price auctions.
An area of related work that can be seen as complementary to ours is that of secure auctions, which takes the point of view of an auction designer. The goals often extend well beyond simply preventing cheating, including properties such as anonymity of the bidders and nonrepudiation of bids. Cryptographic methods are the standard weapon of choice here (see [1, 4, 9]).
In this paper we presented the equilibria of sealed-bid auctions in which cheating is possible. In addition to providing strategy profiles that are stable against deviations, these results give us with insights into both first and second-price auctions. The results for the case of a cheating seller in a second-price auction allow us to relate the two auctions as endpoints along a continuum. The case of agents cheating in a first-price auction showed the robustness of the first-price auction equilibrium when agent types are drawn from the uniform distribution. We also explored the effect of overbidding on the best response bidding strategy for other distributions, and showed that even for relatively simple distributions it can be positive, negative, or neutral. Finally, results from both of our settings allowed us to quantify the expected loss in revenue for a seller due to the possibility of cheating.

Computational game theory has introduced the issue of incentives to many of the classical combinatorial optimization problems. The view that the demand side is many times not under the control of a central authority that optimizes the global performance, but rather under the control of individuals with different incentives, has led already to many important insights.
Consider classical routing and transportation problems such as multicast or multi-commodity problems, which are many times viewed as follows. We are given a graph with edge costs and connectivity demands between nodes, and our goal is to find a minimal cost solution. The classical centralized approach assumes that all the individual demands can both be completely coordinated and have no individual incentives. The game theory point of view would assume that each individual demand is controlled by a player that optimizes its own utility, and the resulting outcome could be far from the optimal solution.
When considering individual incentives one needs to discuss the appropriate solution concept. Much of the research in computational game theory has focused on the classical Nash equilibrium as the primary solution concept. Indeed Nash equilibrium has many benefits, and most importantly it always exists (in mixed strategies). However, the solution concept of Nash equilibrium is resilient only to unilateral deviations, while in reality, players may be able to coordinate their actions.
A strong equilibrium [4] is a state from which no coalition (of any size) can deviate and improve the utility of every member of the coalition (while possibly lowering the utility 84 of players outside the coalition). This resilience to deviations by coalitions of the players is highly attractive, and one can hope that once a strong equilibrium is reached it is highly likely to sustain. From a computational game theory point of view, an additional benefit of a strong equilibrium is that it has a potential to reduce the distance between the optimal solution and the solution obtained as an outcome of selfish behavior. The strong price of anarchy (SPoA), introduced in [1], is the ratio between the cost of the worst strong equilibrium and the cost of an optimal solution.
Obviously, SPoA is meaningful only in those cases where a strong equilibrium exists. A major downside of strong equilibrium is that most games do not admit any strong equilibrium. Even simple classical games like the prisoner"s dilemma do not posses any strong equilibrium (which is also an example of a congestion game that does not posses a strong equilibrium1 ). This unfortunate fact has reduced the concentration in strong equilibrium, despite its highly attractive properties. Yet, [1] have identified two broad families of games, namely job scheduling and network formation, where a strong equilibrium always exists and the SPoA is significantly lower than the price of anarchy (which is the ratio between the worst Nash equilibrium and the optimal solution [15, 18, 5, 6]).
In this work we concentrate on cost sharing connection games, introduced by [3, 2]. In such a game, there is an underlying directed graph with edge costs, and individual users have connectivity demands (between a source and a sink). We consider two models. The fair cost connection model [2] allows each player to select a path from the source to the sink2 . In this game the cost of an edge is shared equally between all the players that selected the edge, and the cost of the player is the sum of its costs on the edges it selected. The general connection game [3] allows each player to offer prices for edges. In this game an edge is bought if the sum of the offers at least covers its cost, and the cost of the player is the sum of its offers on the bought edges (in both games we assume that the player has to guarantee the connectivity between its source and sink).
In this work we focus on two important issues. The first one is identifying under what conditions the existence of a strong equilibrium is guaranteed, and the second one is the quality of the strong equilibria. For the existence part, we identify families of graph topologies that possess some strong equilibrium for any assignment of edge costs. One can view this separation between the graph topology and the edge costs, as a separation between the underlying infrastructure and the costs the players observe to purchase edges. While one expects the infrastructure to be stable over long periods of time, the costs the players observe can be easily modified over short time periods. Such a topological characterization of the underlying infrastructure provides a network designer topological conditions that will ensure stability in his network.
Our results are as follows. For the single commodity case (all the players have the same source and sink), there is a strong equilibrium in any graph (both for fair and general connection games). Moreover, the strong equilibrium is also 1 while any congestion game is known to admit at least one Nash equilibrium in pure strategies [16]. 2 The fair cost sharing scheme is also attractive from a mechanism design point of view, as it is a strategyproof costsharing mechanism [14]. the optimal solution (namely, the players share a shortest path from the common source to the common sink). For the case of a single source and multiple sinks (for example, in a multicast tree), we show that in a fair connection game there is a strong equilibrium if the underlying graph is a series parallel graph, and we show an example of a nonseries parallel graph that does not have a strong equilibrium.
For the case of multi-commodity (multi sources and sinks), we show that in a fair connection game if the graph is an extension parallel graph then there is always a strong equilibrium, and we show an example of a series parallel graph that does not have a strong equilibrium. As far as we know, we are the first to provide a topological characterization for equilibrium existence in multi-commodity and single-source network games.
For any fair connection game we show that if there exists a strong equilibrium it is at most a factor of Θ(log n) from the optimal solution, where n is the number of players. This should be contrasted with the Θ(n) bound that exists for the price of anarchy [2].
For single source general connection games, we show that any series parallel graph possesses a strong equilibrium, and we show an example of a graph that does not have a strong equilibrium. In this case we also show that any strong equilibrium is optimal.
Related work Topological characterizations for single-commodity network games have been recently provided for various equilibrium properties, including equilibrium existence [12, 7, 8], equilibrium uniqueness [10] and equilibrium efficiency [17, 11]. The existence of pure Nash equilibrium in single-commodity network congestion games with player-specific costs or weights was studied in [12]. The existence of strong equilibrium was studied in both utility-decreasing (e.g., routing) and utility-increasing (e.g., fair cost-sharing) congestion games. [7, 8] have provided a full topological characterization for a SE existence in single-commodity utility-decreasing congestion games, and showed that a SE always exists if and only if the underlying graph is extension-parallel. [19] have shown that in single-commodity utility-increasing congestion games, the topological characterization is essentially equivalent to parallel links. In addition, they have shown that these results hold for correlated strong equilibria as well (in contrast to the decreasing setting, where correlated strong equilibria might not exist at all). While the fair cost sharing games we study are utility increasing network congestion games, we derive a different characterization than [19] due to the different assumptions regarding the players" actions.3
A game Λ =< N, (Σi), (ci) > has a finite set N = {1, . . . , n} of players. Player i ∈ N has a set Σi of actions, the joint action set is Σ = Σ1 × · · · × Σn and a joint action S ∈ Σ is also called a profile. The cost function of player i is 3 In [19] they allow to restrict some players from using certain links, even though the links exist in the graph, while we do not allow this, and assume that the available strategies for players are fully represented by the underlying graph. 85 ci : Σ → R+ , which maps the joint action S ∈ Σ to a non-negative real number. Let S = (S1, . . . , Sn) denote the profile of actions taken by the players, and let S−i = (S1, . . . , Si−1, Si+1, . . . , Sn) denote the profile of actions taken by all players other than player i. Note that S = (Si, S−i).
The social cost of a game Λ is the sum of the costs of the players, and we denote by OPT(Λ) the minimal social cost of a game Λ. i.e., OPT(Λ) = minS∈Σ costΛ(S), where costΛ(S) = i∈N ci(S).
A joint action S ∈ Σ is a pure Nash equilibrium if no player i ∈ N can benefit from unilaterally deviating from his action to another action, i.e., ∀i ∈ N ∀Si ∈ Σi : ci(S−i, Si) ≥ ci(S). We denote by NE(Λ) the set of pure Nash equilibria in the game Λ.
Resilience to coalitions: A pure deviation of a set of players Γ ⊂ N (also called coalition) specifies an action for each player in the coalition, i.e., γ ∈ ×i∈ΓΣi. A joint action S ∈ Σ is not resilient to a pure deviation of a coalition Γ if there is a pure joint action γ of Γ such that ci(S−Γ, γ) < ci(S) for every i ∈ Γ (i.e., the players in the coalition can deviate in such a way that each player in the coalition reduces its cost).
A pure Nash equilibrium S ∈ Σ is a k-strong equilibrium, if there is no coalition Γ of size at most k, such that S is not resilient to a pure deviation by Γ. We denote by k-SE(Λ) the set of k-strong equilibria in the game Λ. We denote by SE(Λ) the set of n-strong equilibria, and call S ∈ SE(Λ) a strong equilibrium (SE).
Next we define the Price of Anarchy [9], Price of Stability [2], and their extension to Strong Price of Anarchy and Strong Price of Stability. of anarchy (k-SPoA) for the game Λ. The Price of Anarchy (PoA) is the ratio between the maximal cost of a pure Nash equilibrium (assuming one exists) and the social optimum, i.e., maxS∈NE(Λ) costΛ(S) /OPT(Λ). Similarly, the Price of Stability (PoS) is the ratio between the minimal cost of a pure Nash equilibrium and the social optimum, i.e., minS∈NE(Λ) costΛ(S)/OPT(Λ). The k-Strong Price of Anarchy (k-SPoA) is the ratio between the maximal cost of a k-strong equilibrium (assuming one exists) and the social optimum, i.e., maxS∈k-SE(Λ) costΛ(S) /OPT(Λ). The SPoA is the n-SPoA. Similarly, the Strong Price of Stability (SPoS) is the ratio between the minimal cost of a pure strong equilibrium and the social optimum, i.e., minS∈SE(Λ) costΛ(S)/OPT(Λ). Note that both k-SPoA and SPoS are defined only if some strong equilibrium exists.
A cost sharing connection game has an underlying directed graph G = (V, E) where each edge e ∈ E has an associated cost ce ≥ 04 . In a connection game each player i ∈ N has an associated source si and sink ti.
In a fair connection game the actions Σi of player i include all the paths from si to ti. The cost of each edge is shared equally by the set of all players whose paths contain it. Given a joint action, the cost of a player is the sum of his costs on the edges it selected. More formally, the cost function of each player on an edge e, in a joint action S, is fe(ne(S)) = ce ne(S) , where ne(S) is the number of players that selected a path containing edge e in S. The cost of player i, when selecting path Qi ∈ Σi is ci(S) = e∈Qi fe(ne(S)). 4 In some of the existence proofs, we assume that ce > 0 for simplicity. The full version contains the complete proofs for the case ce ≥ 0.
In a general connection game the actions Σi of player i is a payment vector pi, where pi(e) is how much player i is offering to contribute to the cost of edge e.5 Given a profile p, any edge e such that i pi(e) ≥ ce is considered bought, and Ep denotes the set of bought edges. Let Gp = (V, Ep) denote the graph bought by the players for profile p = (p1, . . . , pn). Clearly, each player tries to minimize his total payment which is ci(p) = e∈Ep pi(e) if si is connected to ti in Gp, and infinity otherwise.6 We denote by c(p) = i ci(p) the total cost under the profile p. For a subgraph H of G we denote the total cost of the edges in H by c(H).
A symmetric connection game implies that the source and sink of all the players are identical. (We also call a symmetric connection game a single source single sink connection game, or a single commodity connection game.) A single source connection game implies that the sources of all the players are identical. Finally, A multi commodity connection game implies that each player has its own source and sink.
Directed Graphs Our directed graphs would be acyclic, and would have a source node (from which all nodes are reachable) and a sink node (which every node can reach). We first define the following actions for composition of directed graphs. • Identification: The identification operation allows to collapse two nodes to one. More formally, given graph G = (V, E) we define the identification of a node v1 ∈ V and v2 ∈ V forming a new node v ∈ V as creating a new graph G = (V , E ), where V = V −{v1, v2}∪{v} and E includes the edges of E where the edges of v1 and v2 are now connected to v. • Parallel composition: Given two directed graphs, G1 = (V1, E1) and G2 = (V2, E2), with sources s1 ∈ V1 and s2 ∈ V2 and sinks t1 ∈ V1 and t2 ∈ V2, respectively, we define a new graph G = G1||G2 as follows. Let G = (V1 ∪ V2, E1 ∪ E2) be the union graph. To create G = G1||G2 we identify the sources s1 and s2, forming a new source node s, and identify the sinks t1 and t2, forming a new sink t. • Series composition: Given two directed graphs, G1 = (V1, E1) and G2 = (V2, E2), with sources s1 ∈ V1 and s2 ∈ V2 and sinks t1 ∈ V1 and t2 ∈ V2, respectively, we define a new graph G = G1 → G2 as follows. Let G = (V1 ∪ V2, E1 ∪ E2) be the union graph. To create G = G1 → G2 we identify the vertices t1 and s2, forming a new vertex u. The graph G has a source s = s1 and a sink t = t2. • Extension composition : A series composition when one of the graphs, G1 or G2, is composed of a single directed edge is an extension composition, and we denote it by G = G1 →e G2.
An extension parallel graph (EPG) is a graph G consisting of either: (1) a single directed edge (s, t), (2) a graph G = G1||G2 or (3) a graph G = G1 →e G2, where G1 and G2 are 5 We limit the players to select a path connecting si to ti and payment only on those edges. 6 This implies that in equilibrium every player has its sink and source connected by a path in Gp. 86 extension parallel graphs (and in the extension composition either G1 or G2 is a single edge.). A series parallel graph (SPG) is a graph G consisting of either: (1) a single directed edge (s, t), (2) a graph G = G1||G2 or (3) a graph G = G1 → G2, where G1 and G2 are series parallel graphs.
Given a path Q and two vertices u, v on Q, we denote the subpath of Q from u to v by Qu,v. The following lemma, whose proof appears in the full version, would be the main topological tool in the case of single source graph.
Lemma 2.1. Let G be an SPG with source s and sink t.
Given a path Q, from s to t, and a vertex t , there exist a vertex y ∈ Q, such that for any path Q from s to t , the path Q contains y and the paths Qy,t and Q are edge disjoint. (We call the vertex y the intersecting vertex of Q and t .)
This section derives our results for fair connection games.
While it is known that every fair connection game possesses a Nash equilibrium in pure strategies [2], this is not necessarily the case for a strong equilibrium. In this section, we study the existence of strong equilibrium in fair connection games. We begin with a simple case, showing that every symmetric fair connection game possesses a strong equilibrium.
Theorem 3.1. In every symmetric fair connection game there exists a strong equilibrium.
Proof. Let s be the source and t be the sink of all the players. We show that a profile S in which all the players choose the same shortest path Q (from the source s to the sink t ) is a strong equilibrium. Suppose by contradiction that S is not a SE. Then there is a coalition Γ that can deviate to a new profile S such that the cost of every player j ∈ Γ decreases. Let Qj be a new path used by player j ∈ Γ.
Since Q is a shortest path, it holds that c(Qj \ (Q ∩ Qj)) ≥ c(Q \ (Q ∩ Qj)), for any path Qj. Therefore for every player j ∈ Γ we have that cj(S ) ≥ cj(S). However, this contradicts the fact that all players in Γ reduce their cost. (In fact, no player in Γ has reduced its cost.) While every symmetric fair connection game admits a SE, it does not hold for every fair connection game. In what follows, we study the network topologies that admit a strong equilibrium for any assignment of edge costs, and give examples of topologies for which a strong equilibrium does not exist. The following lemma, whose proof appears in the full version, plays a major role in our proofs of the existence of SE.
Lemma 3.2. Let Λ be a fair connection game on a series parallel graph G with source s and sink t. Assume that player i has si = s and ti = t and that Λ has some SE. Let S be a SE that minimizes the cost of player i (out of all SE), i.e., ci(S) = minT ∈SE(Λ) ci(T) and let S∗ be the profile that minimizes the cost of player i (out of all possible profiles), i.e., ci(S∗ ) = minT ∈Σ ci(T). Then, ci(S) = ci(S∗ ).
The next lemma considers parallel composition.
Lemma 3.3. Let Λ be a fair connection game on graph G = G1||G2, where G1 and G2 are series parallel graphs. If every fair connection game on the graphs G1 and G2 possesses a strong equilibrium, then the game Λ possesses a strong equilibrium.
Proof. Let G1 = (V1, E1) and G2 = (V2, E2) have sources s1 and s2 and sinks t1 and t2, respectively. Let Ti be the set of players with an endpoint in Vi \ {s, t}, for i ∈ {1, 2}. (An endpoint is either a source or a sink of a player). Let T3 be the set of players j such that sj = s and tj = t. Let Λ1 and Λ2 be the original game on the respective graphs G1 and G2 with players T1 ∪ T3 and T2 ∪ T3, respectively.
Let S and S be the SE in Λ1 and Λ2 that minimizes the cost of players in T3, respectively. Assume w.l.o.g. that ci(S ) ≤ ci(S ) where player i ∈ T3. In addition, let Λ2 be the game on the graph G2 with players T2 and let ¯S be a SE in Λ2.
We will show that the profile S = S ∪ ¯S is a SE in Λ.
Suppose by contradiction that S is not a SE. Then, there is a coalition Γ that can deviate such that the cost of every player j ∈ Γ decreases. By Lemma 3.2 and the assumption that ci(S ) ≤ ci(S ), a player j ∈ T3 cannot improve his cost. Therefore, Γ ⊆ T1 ∪ T2. But this is a contradiction to S being a SE in Λ1 or ¯S being a SE in Λ2.
The following theorem considers the case of single source fair connection games.
Theorem 3.4. Every single source fair connection game on a series-parallel graph possesses a strong equilibrium.
Proof. We prove the theorem by induction on the network size |V |. The claim obviously holds if |V | = 2. We show the claim for a series composition, i.e., G = G1 → G2, and for a parallel composition, i.e., G = G1||G2, where G1 = (V1, E1) and G2 = (V2, E2) are SPG"s with sources s1, s2, and sinks t1, t2, respectively. series composition. Let G = G1 → G2. Let T1 be the set of players j such that tj ∈ V1, and T2 be the set of players j such that tj ∈ V2 \ {s2}.
Let Λ1 and Λ2 be the original game on the respective graphs G1 and G2 with players T1 ∪ T2 and T2, respectively.
For every player i ∈ T2 with action Si in the game Λ let Si ∩E1 be his induced action in the game Λ1, and let Si ∩E2 be his induced action in the game Λ2.
Let S be a SE in Λ1 that minimizes the cost of players in T2 (such a SE exists by the induction hypothesis and Lemma
S = S ∪ S is a SE in the game Λ, i.e., for player j ∈ T2 we use the profile Sj = Sj ∪ Sj .
Suppose by contradiction that S is not a SE. Then, there is a coalition Γ that can deviate such that the cost of every player j ∈ Γ decreases. Now, there are two cases: Case 1: Γ ⊆ T1. This is a contradiction to S being a SE.
Case 2: There exists a player j ∈ Γ ∩ T2. By Lemma 3.2, player j cannot improve his cost in Λ1 so the improvement is due to Λ2. Consider the coalition Γ ∩ T2, it would still improve its cost. However, this contradicts the fact that S is a SE in Λ2. parallel composition. Follows from Lemma 3.3.
While multi-commodity fair connection games on series parallel graphs do not necessarily possess a SE (see Theorem 3.6), fair connection games on extension parallel graphs always possess a strong equilibrium.
Theorem 3.5. Every fair connection game on an extension parallel graph possesses a strong equilibrium. 87 t2 t1 s1 s2 2 2 1 3 3 1 (b)(a) a b e f c d Figure 1: Graph topologies.
Proof. We prove the theorem by induction on the network size |V |. Let Λ be a fair connection game on an EPG G = (V, E). The claim obviously holds if |V | = 2. If the graph G is a parallel composition of two EPG graphs G1 and G2, then the claim follows from Lemma 3.3. It remains to prove the claim for extension composition. Suppose the graph G is an extension composition of the graph G1 consisting of a single edge e = (s1, t1) and an EPG G2 = (V2, E2) with terminals s2, t2, such that s = s1 and t = t2. (The case that G2 is a single edge is similar.) Let T1 be the set of players with source s1 and sink t1 (i.e., their path is in G1). Let T2 be the set of players with source and sink in G2. Let T3 be the set of players with source s1 and sink in V2 \ t1.
Let Λ1 and Λ2 be the original game on the respective graphs G1 and G2 with players T1 ∪ T3 and T2 ∪ T3, respectively. Let S , S be SE in Λ1 and Λ2 respectively. We will show that the profile S = S ∪ S is a SE in the game Λ.
Suppose by contradiction that S is not a SE. Then, there is a coalition Γ of minimal size that can deviate such that the cost of any player j ∈ Γ decreases. Clearly, T1 ∩Γ = φ, since players in T1 have a single strategy. Hence, Γ ⊆ T2 ∪T3. Any player j ∈ T2 ∪T3 cannot improve his cost in Λ1. Therefore, any player j ∈ T2 ∪ T3 improves his cost in Λ2. However, this contradicts the fact that S is a SE in Λ2.
In the following theorem we provide a few examples of topologies in which a strong equilibrium does not exist, showing that our characterization is almost tight.
Theorem 3.6. The following connection games exist: (1) There exists a multi-commodity fair connection game on a series parallel graph that does not possess a strong equilibrium. (2) There exists a single source fair connection game that does not possess a strong equilibrium.
Proof. For claim (1) consider the graph depicted in Figure 1(a). This game has a unique NE where S1 = {e, c},
S2 = {b, f}, and each player has a cost of 5.7 However, consider the following coordinated deviation S . S1 = {a, b, c}, 7 In any NE of the game, player 1 will buy the edge e and player 2 will buy the edge f. This is since the alternate path, in the respective part, will cost the player 2.5. Thus, player 1 (player 2) will buy the edge c (edge b) alone, and each player will have a cost of 5. s 2 + 2 2 1 − 2 1 + 3 1 2 − 3 1 1 1 2 − 3 t1 t2 a c d e f h g b Figure 2: Example of a single source connection game that does not admit SE. and S2 = {b, c, d}. In this profile, each player pays a cost of 4, and thus improves its cost.
For claim (2) consider a single source fair connection game on the graph G depicted in Figure 2. There are two players.
Player i = 1, 2 wishes to connect the source s to its sink ti and the unique NE is S1 = {a, b}, S2 = {a, c}, and each player has a cost of 2. 8 Then, both players can deviate to S1 = {h, f, d} and S2 = {h, f, e}, and decrease their costs to 2 − /2.
Unfortunately, our characterization is not completely tight.
The graph in Figure 1(b) is an example of a non-extension parallel graph which always admits a strong equilibrium.
While the price of anarchy in fair connection games can be as bad as n, the following theorem shows that the strong price of anarchy is bounded by H(n) = n i=1 1 i = Θ(log n).
Theorem 3.7. The strong price of anarchy of a fair connection game with n players is at most H(n).
Proof. Let Λ be a fair connection game on the graph G.
We denote by Λ(Γ) the game played on the graph G by a set of players Γ, where the action of player i ∈ Γ remains Σi (the same as in Λ). Let S = (S1, . . . , Sn) be a profile in the game Λ. We denote by S(Γ) = SΓ the induced profile of players in Γ in the game Λ(Γ). Let ne(S(Γ)) denote the load of edge e under the profile S(Γ) in the game Λ(Γ), i.e., ne(S(Γ)) = |{j|j ∈ Γ, e ∈ Sj}|. Similar to congestion games [16, 13] we denote by Φ(S(Γ)) the potential function of the profile S(Γ) in the game Λ(Γ), where Φ(S(Γ)) = e∈E ne(S(Γ)) j=1 fe(j), and define Φ(S(φ)) = 0. In our case, it holds that Φ(S) = e∈E ce · H(ne(S)). (1) Let S be a SE, and let S∗ be the profile of the optimal solution. We define an order on the players as follows. Let Γn = {1, ..., n} be the set of all the players. For each k = 8 We can show that this is the unique NE by a simple case analysis: (i) If S1 = {h, f, d} and S2 = {h, f, e}, then player 1 can deviate to S1 = {h, g} and decrease his cost. (ii) If S1 = {h, g} and S2 = {h, f, e}, then player 2 can deviate to S2 = {a, c} and decrease his cost. (iii) If S1 = {h, g} and S2 = {a, c}, then player 1 can deviate to S1 = {a, b} and decrease his cost. 88 n, . . . , 1, since S is a SE, there exists a player in Γk, w.l.o.g. call it player k, such that, ck(S) ≤ ck(S−Γk , S∗ Γk ). (2) In this way, Γk is defined recursively, such that for every k = n, . . . , 2 it holds that Γk−1 = Γk \ {k}. (I.e., after the renaming, Γk = {1, . . . , k}.) Let ck(S(Γk)) denote the cost of player k in the game Λ(Γk) under the induced profile S(Γk). It is easy to see that ck(S(Γk)) = Φ(S(Γk)) − Φ(S(Γk−1)).9 Therefore, ck(S) ≤ ck(S−Γk , S∗ Γk ) (3) ≤ ck(S∗ (Γk)) = Φ(S∗ (Γk)) − Φ(S∗ (Γk−1)).
Summing over all players, we obtain: i∈N ci(S) ≤ Φ(S∗ (Γn)) − Φ(S∗ (φ)) = Φ(S∗ (Γn)) = e∈S∗ ce · H(ne(S∗ )) ≤ e∈S∗ ce · H(n) = H(n) · OPT(Λ), where the first inequality follows since the sum of the right hand side of equation (3) telescopes, and the second equality follows from equation (1).
Next we bound the SPoA when coalitions of size at most k are allowed.
Theorem 3.8. The k-SPoA of a fair connection game with n players is at most n k · H(k).
Proof. Let S be a SE of Λ, and S∗ be the profile of the optimal solution of Λ. To simplify the proof, we assume that n/k is an integer. We partition the players to n/k groups T1, . . . , Tn/k each of size k. Let Λj be the game on the graph G played by the set of players Tj. Let S(Tj) denote the profile of the k players in Tj in the game Λj induced by the profile S of the game Λ. By Theorem 3.7, it holds that for each game Λj, j = 1, . . . , n/k, costΛj (S(Tj)) = i∈Tj ci(S(Tj)) ≤ H(k) · OPT(Λj) ≤ H(k) · OPT(Λ).
Summing over all games Λj, j = 1, . . . , n/k, costΛ(S) ≤ n/k j=1 costΛj (S(Tj)) ≤ n k · H(k) · OPT(Λ), where the first inequality follows since for each group Tj and player i ∈ Tj, it holds that ci(S) ≤ ci(S(Tj)).
Next we show an almost matching lower bound. (The lower bound is at most H(n) = O(log n) from the upper bound and both for k = O(1) and k = Ω(n) the difference is only a constant.) Theorem 3.9. For fair connection games with n players, k-SPoA ≥ max{n k , H(n)}. 9 This follows since for any strategy profile S, if a single player k deviates to strategy Sk, then the change in the potential value Φ(S) − Φ(Sk, S−k) is exactly the change in the cost to player k. t2 s t1 tn−2 tn 1 2 t3 tn−1 1 1 3 1 n−2 2 n 1 + 00 0 0 0 00 0 Figure 3: Example of a network topology in which SPoS > PoS.
Proof. For the lower bound of H(n) we observe that in the example presented in [2], the unique Nash equilibrium is also a strong equilibrium, and therefore k-SPoA = H(n) for any 1 ≤ k ≤ n. For the lower bound of n/k, consider a graph composed of two parallel links of costs 1 and n/k.
Consider the profile S in which all n players use the link of cost n/k. The cost of each player is 1/k, while if any coalition of size at most k deviates to the link of cost 1, the cost of each player is at least 1/k. Therefore, the profile S is a k-SE, and k-SPoA = n/k.
The results of Theorems 3.7 and 3.8 can be extended to concave cost functions. Consider the extended fair connection game, where each edge has a cost which depends on the number of players using that edge, ce(ne). We assume that the cost function ce(ne) is a nondecreasing, concave function. Note that the cost of an edge ce(ne) might increase with the number of players using it, but the cost per player fe(ne) = ce(ne)/ne decreases when ce(ne) is concave.
Theorem 3.10. The strong price of anarchy of a fair connection game with nondecreasing concave edge cost functions and n players is at most H(n).
Proof. The proof is analogues to the proof of Theorem 3.7. For the proof we show that cost(S) ≤ Φ(S∗ ) ≤ H(n)·cost(S∗ ). We first show the first inequality. Since the function ce(x) is concave, the cost per player ce(x)/x is a nonincreasing function. Therefore inequality (3) in the proof of Theorem 3.7 holds. Summing inequality (3) over all players we obtain cost(S) = i ci(S) ≤ Φ(S∗ (Γn))−Φ(S∗ (φ)) = Φ(S∗ ). The second inequality follows since ce(x) is nondecreasing and therefore ne x=1(ce(x)/x) ≤ H(ne) · ce(ne).
Using the arguments in the proof of Theorem 3.10 and the proof of Theorem 3.8 we derive,
Theorem 3.11. The k-SPoA of a fair connection game with nondecreasing concave edge cost functions and n players is at most n k · H(k).
Since the set of strong equilibria is contained in the set of Nash equilibria, it must hold that SPoA ≤ PoA, meaning that the SPoA can only be improved compared to the PoA.
However, with respect to the price of stability the opposite direction holds, that is, SPoS ≥ PoS. We next show that there exists a fair connection game in which the inequality is strict. 89 2 − 2 − 2 − 3 s t1 t2 t3 Figure 4: Example of a single source general connection game that does not admit a strong equilibrium. The edges that are not labeled with costs have a cost of zero.
Theorem 3.12. There exists a fair connection game in which SPoS > PoS.
Proof. Consider a single source fair connection game on the graph G depicted in Figure 3.10 Player i = 1, . . . , n wishes to connect the source s to his sink ti. Assume that each player i = 1, . . . , n − 2 has his own path of cost 1/i from s to ti and players i = n − 1, n have a joint path of cost 2/n from s to ti. Additionally, all players can share a common path of cost 1+ for some small > 0. The optimal solution connects all players through the common path of cost 1 + , and this is also a Nash equilibrium with total cost 1 + . It is easy to verify that the solution where each player i = 1, . . . , n−2 uses his own path and users i = n−1, n use their joint path is the unique strong equilibrium of this game with total cost n−2 i=1 1 i + 2 n = Θ(log n) While the example above shows that the SPoS may be greater than the PoS, the upper bound of H(n) = Θ(log n), proven for the PoS [2], serves as an upper bound for the SPoS as well. This is a direct corollary from theorem 3.7, as SPoS ≤ SPoA by definition.
Corollary 3.13. The strong price of stability of a fair connection game with n players is at most H(n) = O(log n).
In this section, we derive our results for general connection games.
We begin with a characterization of the existence of a strong equilibrium in symmetric general connection games.
Similar to Theorem 3.1 (using a similar proof) we establish,
Theorem 4.1. In every symmetric fair connection game there exists a strong equilibrium.
While every single source general connection game possesses a pure Nash equilibrium [3], it does not necessarily admit some strong equilibrium.11 10 This is a variation on the example given in [2]. 11 We thank Elliot Anshelevich, whose similar topology for the fair-connection game inspired this example.
Theorem 4.2. There exists a single source general connection game that does not admit any strong equilibrium.
Proof. Consider single source general connection game with 3 players on the graph depicted in Figure 4. Player i wishes to connect the source s with its sink ti.We need to consider only the NE profiles: (i) if all three players use the link of cost 3, then there must be two agents whose total sum exceeds 2, thus they can both reduce cost by deviating to an edge of cost 2− . (ii) if two of the players use an edge of cost 2− jointly, and the third player uses a different edge of cost 2 − , then, the players with non-zero payments can deviate to the path with the edge of cost 3 and reduce their costs (since before the deviation the total payments of the players is 4 − 2 ). We showed that none of the NE are SE, and thus the game does not possess any SE.
Next we show that for the class of series parallel graphs, there is always a strong equilibrium in the case of a single source.
Theorem 4.3. In every single source general connection game on a series-parallel graph, there exists a strong equilibrium.
Proof. Let Λ be a single source general connection game on a SPG G = (V, E) with source s and sink t. We present an algorithm that constructs a specific SE. We first consider the following partial order between the players. For players i and j, we have that i → j if there is a directed path from ti to tj.
We complete the partial order to a full order (in an arbitrary way), and w.l.o.g. we assume that 1 → 2 → · · · → n.
The algorithm COMPUTE-SE, considers the players in an increasing order, starting with player 1. Each player i will fully buy a subset of the edges, and any player j > i will consider the cost of those (bought) edges as zero. When COMPUTE-SE considers player j, the cost of the edges that players 1 to j−1 have bought is set to zero, and player j fully buys a shortest path Qj from s to tj. Namely, for every edges e ∈ Qj \ ∪i<jQi we have pj(e) = ce and otherwise pj(e) = 0. We next show that the algorithm COMPUTESE computes a SE.
Assume by way of contradiction that the profile p is not a SE. Then, there exists a coalition that can improve the costs of all its players by a deviation. Let Γ be such a coalition of minimal size and let player i = max{j ∈ Γ}. For a player j ∈ Γ let ¯Qj and ¯pj be the path and payment of player j after the deviation, respectively. Let Q be a path from the sink of player i, i.e. ti, to the sink of G, i.e. t. Then Q = ¯Qi ∪ Q is a path from the source s to the sink t. For any player j < i, let yj be the intersecting vertex of Q and tj (by Lemma 2.1 one is guarantee to exist). Let y be the furthest vertex on the path Q such that y = yj for some j < i. The path from the source s to node y was fully paid for by players j < i in p (before the deviation). There are two cases we consider. case a: After the deviation player i does not pay for edges in j∈Γ\{i} ¯Qj . This is a contradiction to the minimality of the coalition Γ size, since the players in Γ \ {i} can form a smaller coalition with payments ¯p. case b: Otherwise, we show that player i cost after the deviation, i.e. ci(¯p), is at least his cost before the deviation, i.e. ci(p), contradicting the fact that player i improved his cost.
Recall that given two vertices u, v on path ¯Q we denote by ¯Qu,v the subpath of ¯Q from u to v. 90 Before the deviation of the coalition Γ, a path from s to y was fully paid for by the players j < i. Next we show that no player k > i pays for any edge on any path from s to ti.
Consider a player k > i and let Qk = Qk ∪ Qk , where Qk is a path connecting tk to t. Let yk be the intersecting vertex of Qk and ti. Since there exists a path from s to yk that was fully paid for by players j < k before the deviation, in particularly the path Qi s,yk , player k will not pay for any edge on any path connecting s and yk. Therefore player i fully pays for all edges on the path ¯Qi y,ti , i.e., ¯pi(e) = ce for all edges e ∈ ¯Qi y,ti . Now consider the algorithm COMPUTESE at the step when player i selects a shortest path from the source s to its sink ti and determines his payment pi. At this point, player i could buy the path ¯Qi y,ti , since a path from s to y was already paid for by players j < i. Hence, ci(¯p) ≥ ci(p). This contradicts the fact that player i improved its cost and therefore not all the players in Γ reduce their cost. This implies that p is a strong equilibrium.
While for every single source general connection game, it holds that PoS = 1 [3], the price of anarchy can be as large as n, even for two parallel edges. Here, we show that any strong equilibrium in single source general connection games yields the optimal cost.
Theorem 4.4. In single source general connection game, if there exists a strong equilibrium, then the strong price of anarchy is 1.
Proof. Let p = (p1, . . . , pn) be a strong equilibrium, and let T∗ be the minimum cost Steiner tree on all players, rooted at the (single) source s. Let T∗ e be the subtree of T∗ disconnected from s when edge e is removed. Let Γ(Te) be the set of players which have sinks in Te. For a set of edges E, let c(E) = e∈E ce. Let P(Te) = i∈Γ(Te) ci(p).
Assume by way of contradiction that c(p) > c(T∗ ). We will show that there exists a sub-tree T of T∗ , that connects a subset of players Γ ⊆ N, and a new set of payments ¯p, such that for each i ∈ Γ, ci(¯p) < ci(p). This will contradict the assumption that p is a strong equilibrium.
First we show how to find a sub-tree T of T∗ , such that for any edge e, the payments of players with sinks in T∗ e is more than the cost of T∗ e ∪ {e}. To build T , define an edge e to be bad if the cost of T∗ e ∪ {e} is at least the payments of the players with sinks in T∗ e , i.e., c(T∗ e ∪ {e}) ≥ P(T∗ e ).
Let B be the set of bad edges. We define T to be T∗ − ∪e∈B(T∗ e ∪ {e}). Note that we can find a subset B of B such that ∪e∈B(T∗ e ∪ {e}) is equal to ∪e∈B (T∗ e ∪ {e}) and for any e1, e2 ∈ B we have T∗ e1 ∩ T∗ e2 = ∅. (The set B will include any edge e ∈ B for which there is no other edge e ∈ B on the path from e to the source s.) Considering the edges in e ∈ B we can see that any subtree T∗ e we delete from T can not decrease the difference between the payments and the cost of the remaining tree. Therefore, in T for every edge e, we have that c(Te ∪ {e}) < P(Te).
Now we have a tree T and our coalition will be Γ(T ).
What remain is to find payments ¯p for the players in Γ(T ) such that they will buy the tree T and every player in Γ(T ) will lower its cost, i.e. ci(p) > ci(¯p) for i ∈ Γ(T ). (Recall that the payments have the restriction that player i can only pay for edges on the path from s to ti.) We will now define the coalition payments ¯p. Let ci(¯p,
Te) = e∈Te ¯pi(e) be the payments of player i for the subtree Te. We will show that for every subtree Te, ci(¯p, Te ∪ {e}) < ci(p), and hence ci(¯p) < ci(p). Consider the following bottom up process that defines ¯p. We assign the payments of edge e in T , after we assign payments to all the edges in Te. This implies that when we assign payments for e, we have that the sum of the payments in Te is equal to c(Te) = i∈Γ(Te) ci(¯p, Te). Since e was not a bad edge, we know that c(Te ∪ {e}) = c(Te) + ce < P(Te). Therefore, we can update the payments ¯p of players i ∈ Γ(Te), by setting ¯pi(e) = ce∆i/( j∈Γ(Te) ∆j), where ∆j = cj(p) − cj(¯p, Te).
After the update we have for player i ∈ Γ(Te), ci(¯p, Te ∪ {e}) = ci(¯p, Te) + ¯pi(e) = ci(¯p, Te) + ∆i ce j∈Γ(Te) ∆j = ci(p) − ∆i(1 − ce P(Γ(Te)) − c(Te) ), where we used the fact that j∈Γ(Te) ∆j = P(Γ(Te))−c(Te).
Since ce < P(Γ(Te)) − c(Te) it follows that ci(¯p, Te ∪ {e}) < ci(p).
[1] N. Andelman, M. Feldman, and Y. Mansour. Strong Price of Anarchy. In SODA"07, 2007. [2] E. Anshelevich, A. Dasgupta, J. M. Kleinberg, ´E. Tardos, T. Wexler, and T. Roughgarden. The price of stability for network design with fair cost allocation. In FOCS, pages 295-304, 2004. [3] E. Anshelevich, A. Dasgupta, E. Tardos, and T. Wexler. Near-Optimal Network Design with Selfish Agents. In STOC"03, 2003. [4] R. Aumann. Acceptable Points in General Cooperative n-Person Games. In Contributions to the Theory of Games, volume 4, 1959. [5] A. Czumaj and B. V¨ocking. Tight bounds for worst-case equilibria. In SODA, pages 413-420, 2002. [6] A. Fabrikant, A. Luthra, E. Maneva,
C. Papadimitriou, and S. Shenker. On a network creation game. In ACM Symposium on Principles of Distriubted Computing (PODC), 2003. [7] R. Holzman and N. Law-Yone. Strong equilibrium in congestion games. Games and Economic Behavior, 21:85-101, 1997. [8] R. Holzman and N. L.-Y. (Lev-tov). Network structure and strong equilibrium in route selection games.
Mathematical Social Sciences, 46:193-205, 2003. [9] E. Koutsoupias and C. H. Papadimitriou. Worst-case equilibria. In STACS, pages 404-413, 1999. [10] I. Milchtaich. Topological conditions for uniqueness of equilibrium in networks. Mathematics of Operations Research, 30:225244, 2005. [11] I. Milchtaich. Network topology and the efficiency of equilibrium. Games and Economic Behavior, 57:321346, 2006. [12] I. Milchtaich. The equilibrium existence problem in finite network congestion games. Forthcoming in Lecture Notes in Computer Science, 2007. [13] D. Monderer and L. S. Shapley. Potential Games.
Games and Economic Behavior, 14:124-143, 1996. [14] H. Moulin and S. Shenker. Strategyproof sharing of 91 submodular costs: Budget balance versus efficiency.
Economic Theory, 18(3):511-533, 2001. [15] C. Papadimitriou. Algorithms, Games, and the Internet. In Proceedings of 33rd STOC, pages 749-753,
[16] R. W. Rosenthal. A class of games possessing pure-strategy Nash equilibria. International Journal of Game Theory, 2:65-67, 1973. [17] T. Roughgarden. The Price of Anarchy is Independent of the Network Topology. In STOC"02, pages 428-437,

The strong form of the efficient markets hypothesis states that market prices nearly instantly incorporate all information available to all traders. As a result, market prices encode the best forecasts of future outcomes given all information, even if that information is distributed across many sources. Supporting evidence can be found in empirical studies of options markets [14], political stock markets [7, 8, 22], sports betting markets [3, 9, 27], horse-racing markets [30], market games [23, 24], and laboratory investigations of experimental markets [6, 25, 26].
The process of information incorporation is, at its essence, a distributed computation. Each trader begins with his or her own information. As trades are made, summary information is revealed through market prices. Traders learn or infer what information others are likely to have by observing prices, then update their own beliefs based on their observations. Over time, if the process works as advertised, all information is revealed, and all traders converge to the same information state. At this point, the market is in what is called a rational expectations equilibrium [11, 16, 19]. All information available to all traders is now reflected in the going prices, and no further trades are desirable until some new information becomes available.
While most markets are not designed with information aggregation as a primary motivation-for example, derivatives 156 markets are intended mainly for risk management and sports betting markets for entertainment-recently, some markets have been created solely for the purpose of aggregating information on a topic of interest. The Iowa Electronic Market1 is a prime example, operated by the University of Iowa Tippie College of Business for the purpose of investigating how information about political elections distributed among traders gets reflected in securities prices whose payoffs are tied to actual election outcomes [7, 8].
In this paper, we investigate the nature of the computational process whereby distributed information is revealed and combined over time into the prices in information markets. To do so, in Section 3, we propose a model of an information market that is tractable for theoretical analysis and, we believe, captures much of the important essence of real information markets. In Section 4, we present our main theoretical results concerning this model. We prove that only Boolean securities whose payoffs can be expressed as threshold functions of the distributed input bits of information are guaranteed to converge as predicted by rational expectations theory. Boolean securities with more complex payoffs may not converge under some prior distributions. We also provide upper and lower bounds on the convergence time for these threshold securities. We show that, for all prior distributions, the price of a threshold security converges to its rational expectations equilibrium price in at most n rounds, where n is the number of bits of distributed information. We show that this worst-case bound is tight within a factor of two by illustrating a situation in which a threshold security requires n/2 rounds to converge.
As mentioned, there is a great deal of documented evidence supporting the notion that markets are able to aggregate information in a number of scenarios using a variety of market mechanisms. The theoretically ideal mechanism requires what is called a complete market. A complete market contains enough linearly independent securities to span the entire state space of interest [1, 31]. That is, the dimensionality of the available securities equals the dimensionality of the event space over which information is to be aggregated.2 In this ideal case, all private information becomes common knowledge in equilibrium, and thus any function of the private information can be directly evaluated by any agent or observer. However, this theoretical ideal is almost never achievable in practice, because it generally requires a number of securities exponential in the number of random variables of interest.
When available securities form an incomplete market [17] in relation to the desired information space-as is usually the case-aggregation may be partial. Not all private information is revealed in equilibrium, and prices may not convey enough information to recover the complete joint probability distribution over all events. Still, it is generally assumed that aggregation does occur along the dimensions represented in the market; that is, prices do reflect a consistent projection of the entire joint distribution onto the smaller-dimensional space spanned by securities. In this pa1 http://www.biz.uiowa.edu/iem/ 2 When we refer to independence or dimensionality of securities, we mean the independence or dimensionality of the random variables on which the security payoffs are based. per, we investigate cases in which even this partial aggregation fails. For example, even though there is enough private information to determine completely the price of a security in the market, the equilibrium price may in fact reveal no information at all! So characterizations of when a rational expectations equilibrium is fully revealing do not immediately apply to our problem. We are not asking whether all possible functions of private information can be evaluated, but whether a particular target function can be evaluated.
We show that properties of the function itself play a major role, not just the relative dimensionalities of the information and security spaces.
Our second main contribution is examining the dynamics of information aggregation before equilibrium, in particular proving upper and lower bounds on the time to convergence in those cases in which aggregation succeeds.
Shoham and Tennenholtz [29] define a rationally computable function as a function of agents" valuations (types) that can be computed by a market, assuming agents follow rational equilibrium strategies. The authors mainly consider auctions of goods as their basic mechanistic unit and examine the communication complexity involved in computing various functions of agents" valuations of goods. For example, they give auction mechanisms that can compute the maximum, minimum, and kth-highest of the agents" valuations of a single good using 1, 1, and n − k + 1 bits of communication, respectively. They also examine the potential tradeoff between communication complexity and revenue.
MARKET To investigate the properties and limitations of the process whereby an information market converges toward its rational-expectations equilibrium, we formulate a representative model of the market. In designing the model, our goals were two-fold: (1) to make the model rich enough to be realistic and (2) to make the model simple enough to admit meaningful analysis. Any modeling decisions must trade off these two generally conflicting goals, and the decision process is as much an art as a science. Nonetheless, we believe that our model captures enough of the essence of real information markets to lend credence to the results that follow. In this section, we present our modeling assumptions and justifications in detail. Section 3.1 describes the initial information state of the system, Section 3.2 covers the market mechanism, and Section 3.3 presents the agents" strategies.
There are n agents (traders) in the system, each of whom is privy to one bit of information, denoted xi. The vector of all n bits is denoted x = (x1, x2, . . . , xn). In the initial state, each agent is aware only of her own bit of information. All agents have a common prior regarding the joint distribution of bits among agents, but none has any specific information about the actual value of bits held by others. Note that this common-prior assumption-typical in the economics literature-does not imply that all agents agree. To the contrary, because each agent has different information, the initial state of the system is in general a state of disagreement. Nearly any disagreement that could be modeled by assuming different priors can instead be mod157 eled by assuming a common prior with different information, and so the common-prior assumption is not as severe as it may seem.
The security being traded by the agents is a financial instrument whose payoff is a function f(x) of the agents" bits.
The form of f (the description of the security) is common knowledge3 among agents. We sometimes refer to the xi as the input bits. At some time in the future after trading is completed, the true value of f(x) is revealed,4 and every owner of the security is paid an amount f(x) in cash per unit owned. If an agent ends with a negative quantity of the security (by selling short), then the agent must pay the amount f(x) in cash per unit. Note that if someone were to have complete knowledge of all input bits x, then that person would know the true value f(x) of the security with certainty, and so would be willing to buy it at any price lower than f(x) and (short) sell it at any price higher than f(x).5 Following Dubey, Geanakoplos, and Shubik [4], and Jackson and Peck [13], we model the market-price formation process as a multiperiod Shapley-Shubik market game [28].
The Shapley-Shubik process operates as follows: The market proceeds in synchronous rounds. In each round, each agent i submits a bid bi and a quantity qi. The semantics are that agent i is supplying a quantity qi of the security and an amount bi of money to be traded in the market. For simplicity, we assume that there are no restrictions on credit or short sales, and so an agent"s trade is not constrained by her possessions. The market clears in each round by settling at a single price that balances the trade in that round: The clearing price is p = i bi/ i qi. At the end of the round, agent i holds a quantity qi proportional to the money she bid: qi = bi/p. In addition, she is left with an amount of money bi that reflects her net trade at price p: bi = bi − p(qi − qi) = pqi. Note that agent i"s net trade in the security is a purchase if p < bi/qi and a sale if p > bi/qi.
After each round, the clearing price p is publicly revealed.
Agents then revise their beliefs according to any information garnered from the new price. The next round proceeds as the previous. The process continues until an equilibrium is reached, meaning that prices and bids do not change from one round to the next.
In this paper, we make a further simplifying restriction on the trading in each round: We assume that qi = 1 for each agent i. This modeling assumption serves two analytical purposes. First, it ensures that there is forced trade in every round. Classic results in economics show that perfectly rational and risk-neutral agents will never trade with each other for purely speculative reasons (even if they have differing information) [20]. There are many factors that can induce rational agents to trade, such as differing degrees of risk aversion, the presence of other traders who are trading for liquidity reasons rather than speculative gain, or a market maker who is pumping money into the market through a subsidy. We sidestep this issue by simply assuming that the 3 Common knowledge is information that all agents know, that all agents know that all agents know, and so on ad infinitum [5]. 4 The values of the input bits themselves may or may not be publicly revealed. 5 Throughout this paper we ignore the time value of money. informed agents will trade (for unspecified reasons).
Second, forcing qi = 1 for all i means that the total volume of trade and the impact of any one trader on the clearing price are common knowledge; the clearing price p is a simple function of the agents" bids, p = i bi/n. We will discuss the implications of alternative market models in Section 5.
In order to draw formal conclusions about the price evolution process, we need to make some assumptions about how agents behave. Essentially we assume that agents are riskneutral, myopic,6 and bid truthfully: Each agent in each round bids his or her current valuation of the security, which is that agent"s estimation of the expected payoff of the security. Expectations are computed according to each agent"s probability distribution, which is updated via Bayes" rule when new information (revealed via the clearing prices) becomes available. We also assume that it is common knowledge that all the agents behave in the specified manner.
Would rational agents actually behave according to this strategy? It"s hard to say. Certainly, we do not claim that this is an equilibrium strategy in the game-theoretic sense.
Furthermore, it is clear that we are ignoring some legitimate tactics, e.g., bidding falsely in one round in order to effect other agents" judgments in the following rounds (nonmyopic reasoning). However, we believe that the strategy outlined is a reasonable starting point for analysis. Solving for a true game-theoretic equilibrium strategy in this setting seems extremely difficult. Our assumptions seem reasonable when there are enough agents in the system such that extremely complex meta-reasoning is not likely to improve upon simply bidding one"s true expected value. In this case, according the the Shapley-Shubik mechanism, if the clearing price is below an agent"s expected value that agent will end up buying (increasing expected profit); otherwise, if the clearing price is above the agent"s expected value, the agent will end up selling (also increasing expected profit).
In this section, we study the computational power of information markets for a very simple class of aggregation functions: Boolean functions of n variables. We characterize the set of Boolean functions that can be computed in our market model for all prior distributions and then prove upper and lower bounds on the worst-case convergence time for these markets.
The information structure we assume is as follows: There are n agents, and each agent i has a single bit of private information xi. We use x to denote the vector (x1, . . . , xn) of inputs. All the agents also have a common prior probability distribution P : {0, 1}n → [0, 1] over the values of x. We define a Boolean aggregate function f(x) : {0, 1}n → {0, 1} that we would like the market to compute. Note that x, and hence f(x), is completely determined by the combination of all the agents" information, but it is not known to any one agent. The agents trade in a Boolean security F, which pays off $1 if f(x) = 1 and $0 if f(x) = 0. So an omniscient 6 Risk neutrality implies that each agent"s utility for the security is linearly related to his or her subjective estimation of the expected payoff of the security. Myopic behavior means that agents treat each round as if it were the final round: They do not reason about how their bids may affect the bids of other agents in future rounds. 158 agent with access to all the agents" bits would know the true value of security F-either exactly $1 or exactly $0. In reality, risk-neutral agents with limited information will value F according to their expectation of its payoff, or Ei[f(x)], where Ei is the expectation operator applied according to agent i"s probability distribution.
For any function f, trading in F may happen to converge to the true value of f(x) by coincidence if the prior probability distribution is sufficiently degenerate. More interestingly, we would like to know for which functions f does the price of the security F always converge to f(x) for all prior probability distributions P.7 In Section 4.2, we prove a necessary and sufficient condition that guarantees convergence.
In Section 4.3, we address the natural follow-up question, by deriving upper and lower bounds on the worst-case number of rounds of trading required for the value of f(x) to be revealed.
Our analysis builds on a characterization of the equilibrium price of F that follows from a powerful result on common knowledge of aggregates due to McKelvey and Page [19], later extended by Nielsen et al. [21].
Information markets aim to aggregate the knowledge of all the agents. Procedurally, this occurs because the agents learn from the markets: The price of the security conveys information to each agent about the knowledge of other agents. We can model the flow of information through prices as follows.
Let Ω = {0, 1}n be the set of possible values of x; we say that Ω denotes the set of possible states of the world. The prior P defines everyone"s initial belief about the likelihood of each state. As trading proceeds, some possible states can be logically ruled out, but the relative likelihoods among the remaining states are fully determined by the prior P. So the common knowledge after any stage is completely described by the set of states that an external observer-with no information beyond the sequence of prices observed-considers possible (along with the prior). Similarly, the knowledge of agent i at any point is also completely described by the set of states she considers possible. We use the notation Sr to denote the common-knowledge possibility set after round r, and Sr i to denote the set of states that agent i considers possible after round r.
Initially, the only common knowledge is that the input vector x is in Ω; in other words, the set of states considered possible by an external observer before trading has occurred is the set S0 = Ω. However, each agent i also knows the value of her bit xi; thus, her knowledge set S0 i is the set {y ∈ Ω|yi = xi}. Agent i"s first-round bid is her conditional expectation of the event f(x) = 1 given that x ∈ S0 i . All the agents" bids are processed, and the clearing price p1 is announced. An external observer could predict agent i"s bid if he knew the value of xi. Thus, if he knew the value of x, he could predict the value of p1 . In other words, the external observer knows the function price1 (x) that relates the first round price to the true state x. Of course, he does not know the value of x; however, he can rule out any vector x that would have resulted in a different clearing price from the observed price p1 . 7 We assume that the common prior is consistent with x in the sense that it assigns a non-zero probability to the actual value of x.
Thus, the common knowledge after round 1 is the set S1 = {y ∈ S0 | price1 (y) = p1 }. Agent i knows the common knowledge and, in addition, knows the value of bit xi.
Hence, after every round r, the knowledge of agent i is given by Sr i = {y ∈ Sr |yi = xi}. Note that, because knowledge can only improve over time, we must always have Sr i ⊆ Sr−1 i and Sr ⊆ Sr−1 . Thus, only a finite number of changes in each agent"s knowledge are possible, and so eventually we must converge to an equilibrium after which no player learns any further information. We use S∞ to denote the common knowledge at this point, and S∞ i to denote agent i"s knowledge at this point. Let p∞ denote the clearing price at equilibrium.
Informally, McKelvey and Page [19] show that, if n people with common priors but different information about the likelihood of some event A agree about a suitable aggregate of their individual conditional probabilities, then their individual conditional probabilities of event A"s occurring must be identical. (The precise definition of suitable is described below.) There is a strong connection to rational expectation equilibria in markets, which was noted in the original McKelvey-Page paper: The market price of a security is common knowledge at the point of equilibrium. Thus, if the price is a suitable aggregate of the conditional expectations of all the agents, then in equilibrium they must have identical conditional expectations of the event that the security will pay off. (Note that their information may still be different.) Definition 1. A function g : n → is called stochastically monotone if it can be written in the form g(x) = i gi(xi), where each function gi : → is strictly increasing.
Bergin and Brandenburger [2] proved that this simple definition of stochastically monotone functions is equivalent to the original definition in McKelvey-Page [19].
Definition 2. A function g : n → is called stochastically regular if it can be written in the form g = h ◦ g , where g is stochastically monotone and h is invertible on the range of g .
We can now state the McKelvey-Page result, as generalized by Nielsen et al. [21]. In our context, the following simple theorem statement suffices; more general versions of this theorem can be found in [19, 21].
Theorem 1. (Nielsen et al. [21]) Suppose that, at equilibrium, the n agents have a common prior, but possibly different information, about the value of a random variable F, as described above. For all i, let p∞ i = E(F|x ∈ S∞ i ). If g is a stochastically regular function and g(p∞ 1 , p∞ 2 , . . . , p∞ n ) is common knowledge, then it must be the case that p∞ 1 = p∞ 2 = · · · = p∞ n = E(F|x ∈ S∞ ) = p∞ In one round of our simplified Shapley-Shubik trading model, the announced price is the mean of the conditional expectations of the n agents. The mean is a stochastically regular function; hence, Theorem 1 shows that, at equilibrium, all agents have identical conditional expectations of the payoff of the security. It follows that the equilibrium 159 price p∞ must be exactly the conditional expectations of all agents at equilibrium.
Theorem 1 does not in itself say how the equilibrium is reached. McKelvey and Page, extending an argument due to Geanakoplos and Polemarchakis [10], show that repeated announcement of the aggregate will eventually result in common knowledge of the aggregate. In our context, this is achieved by announcing the current price at the end of each round; this will ultimately converge to a state in which all agents bid the same price p∞ .
However, reaching an equilibrium price is not sufficient for the purposes of information aggregation. We also want the price to reveal the actual value of f(x). It is possible that the equilibrium price p∞ of the security F will not be either 0 or 1, and so we cannot infer the value of f(x) from it.
Example 1: Consider two agents 1 and 2 with private input bits x1 and x2 respectively. Suppose the prior probability distribution is uniform, i.e., x = (x1, x2) takes the values (0, 0), (0, 1), (1, 0), and (1, 1) each with probability 1 4 . Now, suppose the aggregate function we want to compute is the XOR function, f(x) = x1 ⊕ x2. To this end, we design a market to trade in a Boolean security F, which will eventually payoff $1 iff x1 ⊕ x2 = 1.
If agent 1 observes x1 = 1, she estimates the expected value of F to be the probability that x2 = 0 (given x1 = 1), which is 1 2 . If she observes x1 = 0, her expectation of the value of F is the conditional probability that x2 = 1, which is also 1 2 . Thus, in either case, agent 1 will bid 0.5 for F in the first round. Similarly, agent 2 will also always bid
ends with a clearing price of 0.5. From this, agent 2 can infer that agent 1 bid 0.5, but this gives her no information about the value of x1-it is still equally likely to be 0 or
of trading, and hence neither agent changes her bid in the following rounds. Thus, the market reaches equilibrium at this point. As predicted by Theorem 1, both agents have the same conditional expectation (0.5) at equilibrium. However, the equilibrium price of the security F does not reveal the value of f(x1, x2), even though the combination of agents" information is enough to determine it precisely.
We now give a necessary and sufficient characterization of the class of functions f such that, for any prior distribution on x, the equilibrium price of F will reveal the true value of f. We show that this is exactly the class of weighted threshold functions: Definition 3. A function f : {0, 1}n → {0, 1} is a weighted threshold function iff there are real constants w1, w2, . . . , wn such that f(x) = 1 iff n i=1 wixi ≥ 1 Theorem 2. If f is a weighted threshold function, then, for any prior probability distribution P, the equilibrium price of F is equal to f(x).
Proof: Let S∞ i denote the possibility set of agent i at equilibrium.
As before, we use p∞ to denote the final trading price at this point. Note that, by Theorem 1, p∞ is exactly agent i"s conditional expectation of the value of f(x), given her final possibility set S∞ i .
First, observe that if p∞ is 0 or 1, then we must have f(x) = p∞ , regardless of the form of f. For instance, if p∞ = 1, this means that E(f(y)|y ∈ S∞ ) = 1. As f(·) can only take the values 0 or 1, it follows that P(f(y) = 1|y ∈ S∞ ) = 1. The actual value x is always in the final possibility set S∞ , and, furthermore, it must have non-zero prior probability, because it actually occurred. Hence, it follows that f(x) = 1 in this case. An identical argument shows that if p∞ = 0, f(x) = 0.
Hence, it is enough to show that, if f is a weighted threshold function, then p∞ is either 0 or 1. We prove this by contradiction. Let f(·) be a weighted threshold function corresponding to weights {wi}, and assume that 0 < p∞ < 1. By Theorem 1, we must have: P(f(y) = 1|y ∈ S∞ ) = p∞ (1) ∀i P(f(y) = 1|y ∈ S∞ i ) = p∞ (2) Recall that S∞ i = {y ∈ S∞ |yi = xi}. Thus, Equation (2) can be written as ∀i P(f(y) = 1|y ∈ S∞ , yi = xi) = p∞ (3) Now define J+ i = P(yi = 1|y ∈ S∞ , f(y) = 1) J− i = P(yi = 1|y ∈ S∞ , f(y) = 0) J+ = n i=1 wiJ+ i J− = n i=1 wiJ− i Because by assumption p∞ = 0, 1, both J+ i and J− i are well-defined (for all i): Neither is conditioned on a zeroprobability event.
Claim: Eqs. 1 and 3 imply that J+ i = J− i , for all i.
Proof of claim: We consider the two cases xi = 1 and xi = 0 separately.
Case (i): xi = 1. We can assume that J− i and J+ i are not both 0 (or else, the claim is trivially true). In this case, we have P(f(y) = 1|y ∈ S∞ ) · J+ i P(f(y) = 1|y ∈ S∞) · J+ i + P(f(y) = 0|y ∈ S∞) · J− i = P(f(y) = 1|yi = 1, y ∈ S∞ ) (Bayes" law) p∞ J+ i p∞J+ i + (1 − p∞)J− i = p∞ (by Eqs. 1 and 3) J+ i = p∞ J+ i + (1 − p∞ )J− i =⇒ J+ i = J− i (as p∞ = 1) Case (ii): xi = 0. When xi = 0, observe that the argument of Case (i) can be used to prove that (1 − J+ i ) = (1 − J− i ).
It immediately follows that J+ i = J− i as well. 2 Hence, we must also have J+ = J− . But using linearity of expectation, we can also write J+ as J+ = E n i=1 wiyi y ∈ S∞ , f(y) = 1 , 160 and, because f(y) = 1 only when i wiyi ≥ 1, this gives us J+ ≥ 1. Similarly,
J− = E n i=1 wiyi y ∈ S∞ , f(y) = 0 , and thus J− < 1. This implies J− = J+ , which leads to a contradiction. 2 Perhaps surprisingly, the converse of Theorem 2 also holds: Theorem 3. Suppose f : {0, 1}n → {0, 1} cannot be expressed as a weighted threshold function. Then there exists a prior distribution P for which the price of the security F does not converge to the value of f(x).
Proof: We start from a geometric characterization of weighted threshold functions. Consider the Boolean hypercube {0, 1}n as a set of points in n . It is well known that f is expressible as a weighted threshold function iff there is a hyperplane in n that separates all the points at which f has value 0 from all the points at which f has value 1.
Now, consider the sets H+ = Conv(f−1 (1)) and H− = Conv(f−1 (0)), where Conv(S) denotes the convex hull of S in n . H+ and H− are convex sets in n , and so, if they do not intersect, we can find a separating hyperlane between them. This means that, if f is not expressible as a weighted threshold function,
H+ and H− must intersect. In this case, we show how to construct a prior P for which f(x) is not computed by the market.
Let x∗ ∈ n be a point in H+ ∩ H− . Because x∗ is in H+ , there exists some points z1 , z2 , . . . , zm and constants λ1, λ2, . . . , λm, such that the following constraints are satisfied: ∀k zk ∈ {0, 1}n , and f(zk ) = 1 ∀k 0 < λk ≤ 1 m k=1 λk = 1 m k=1 λkzk = x∗ Similarly, because x∗ ∈ H− , there are points y1 , y2 , . . . , yl and constants µ1, µ2, . . . , µl, such that ∀j yj ∈ {0, 1}n , and f(yj ) = 0 ∀j 0 < µj ≤ 1 l j=1 µj = 1 l j=1 µj yj = x∗ We now define our prior distribution P as follows: P(zk ) = λk 2 for k = 1, 2, . . . , m P(yj ) = µj 2 for j = 1, 2, . . . , l, and all other points are assigned probability 0. It is easy to see that this is a valid probability distribution. Under this distribution P, first observe that P(f(x) = 1) = 1 2 . Further, for any i such that 0 < x∗ i < 1, we have P(f(x) = 1|xi = 1) = P(f(x) = 1 ∧ xi = 1) P(xi = 1) = x∗ i 2 x∗ i = 1 2 and P(f(x) = 1|xi = 0) = P(f(x) = 1 ∧ xi = 0) P(xi = 0) = (1−x∗ i ) 2 (1 − x∗ i ) = 1 2 For indices i such that x∗ i is 0 or 1 exactly, i"s private information reveals no additional information under prior P, and so here too we have P(f(x) = 1|xi = 0) = P(f(x) = 1|xi = 1) = 1 2 .
Hence, regardless of her private bit xi, each agent i will bid 0.5 for security F in the first round. The clearing price of 0.5 also reveals no additional information, and so this is an equilibrium with price p∞ = 0.5 that does not reveal the value of f(x). 2 The XOR function is one example of a function that cannot be expressed as weighted threshold function; Example 1 illustrates Theorem 3 for this function.
We have shown that the class of Boolean functions computable in our model is the class of weighted threshold functions. The next natural question to ask is: How many rounds of trading are necessary before the equilibrium is reached? We analyze this problem using the same simplified Shapley-Shubik model of market clearing in each round.
We first prove that, in the worst case, at most n rounds are required.
The idea of the proof is to consider the sequence of common knowledge sets Ω = S0 , S1 , . . ., and show that, until the market reaches equilibrium, each set has a strictly lower dimension than the previous set.
Definition 4. For a set S ⊆ {0, 1}n , the dimension of set S is the dimension of the smallest linear subspace of n that contains all the points in S; we use the notation dim(S) to denote it.
Lemma 1. If Sr = Sr−1 , then dim(Sr ) < dim(Sr−1 ).
Proof: Let k = dim(Sr−1 ). Consider the bids in round r.
In our model, agent i will bid her current expectation for the value of F, br i = E(f(y) = 1|y ∈ Sr−1 , yi = xi).
Thus, depending on the value of xi, br i will take on one of two values h (0) i or h (1) i . Note that h (0) i and h (1) i depend only on the set Sr−1 , which is common knowledge before round 161 r. Setting di = h (1) i − h (0) i , we can write br i = h (0) i + dixi. It follows that the clearing price in round r is given by pr = 1 n n i=1 (h (0) i + dixi) (4) All the agents already know all the h (0) i and di values, and they observe the price pr at the end of the rth round. Thus, they effectively have a linear equation in x1, x2, . . . , xn that they use to improve their knowledge by ruling out any possibility that would not have resulted in price pr . In other words, after r rounds, the common knowledge set Sr is the intersection of Sr−1 with the hyperplane defined by Equation (4).
It follows that Sr is contained in the intersection of this hyperplane with the k-dimension linear space containing Sr−1 . If Sr is not equal to Sr−1 , this intersection defines a linear subspace of dimension (k − 1) that contains Sr , and hence Sr has dimension at most (k − 1). 2 Theorem 4. Let f be a weighted threshold function, and let P be an arbitrary prior probability distribution. Then, after at most n rounds of trading, the price reaches its equilibrium value p∞ = f(x).
Proof: Consider the sequence of common knowledge sets S0 , S1 , . . ., and let r be the minimum index such that Sr = Sr−1 . Then, the rth round of trading does not improve any agent"s knowledge, and thus we must have S∞ = Sr−1 and p∞ = pr−1 . Observing that dim(S0 ) = n, and applying Lemma 1 to the first r − 1 rounds, we must have (r − 1) ≤ n. Thus, the price reaches its equilibrium value within n rounds. 2 Theorem 4 provides an upper bound of O(n) on the number of rounds required for convergence. We now show that this bound is tight to within a factor of 2 by constructing a threshold function with 2n inputs and a prior distribution for which it takes n rounds to determine the value of f(x) in the worst case.
The functions we use are the carry-bit functions. The function Cn takes 2n inputs; for convenience, we write the inputs as x1, x2 . . . , xn, y1, y2, . . . , yn or as a pair (x, y). The function value is the value of the high-order carry bit when the binary numbers xnxn−1 · · · x1 and ynyn−1 · · · y1 are added together. In weighted threshold form, this can be written as Cn(x, y) = 1 iff n i=1 xi + yi 2n+1−i ≥ 1.
For this proof, let us call the agents A1, A2, . . . , An, B1, B2, . . . , Bn, where Ai holds input bit xi, and Bi holds input bit yi.
We first illustrate our technique by proving that computing C2 requires 2 rounds in the worst case. To do this, we construct a common prior P2 as follows: • The pair (x1, y1) takes on the values (0, 0), (0, 1), (1, 0), (1, 1) uniformly (i.e., with probability 1 4 each). • We extend this to a distribution on (x1, x2, y1, y2) by specifying the conditional distribution of (x2, y2) given (x1, y1): If (x1, y1) = (1, 1), then (x2, y2) takes the values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1 2 , 1 6 , 1 6 , 1 6 respectively. Otherwise, (x2, y2) takes the values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1 6 , 1 6 , 1 6 , 1 2 respectively.
Now, suppose x1 turns out to be 1, and consider agent A1"s bid in the first round. It is given by b1 A1 = P(C2(x1, x2, y1, y2) = 1|x1 = 1)) = P(y1 = 1|x1 = 1) · P((x2, y2) = (0, 0)|x1 = 1, y1 = 1) +P(y1 = 0|x1 = 1) · P((x2, y2) = (1, 1)|x1 = 1, y1 = 0) = 1 2 · 1 2 + 1 2 · 1 2 = 1 2 On the other hand, if x1 turns out to be 0, agent A1"s bid would be given by b1 A1 = P(C2(x1, x2, y1, y2) = 1|x1 = 0)) = P((x2, y2) = (1, 1)|x1 = 0) = 1 2 Thus, irrespective of her bit, A1 will bid 0.5 in the first round. Note that the function and distribution are symmetric between x and y, and so the same argument shows that B1 will also bid 0.5 in the first round. Thus, the price p1 announced at the end of the first round reveals no information about x1 or y1. The reason this occurs is that, under this distribution, the second carry bit C2 is statistically independent of the first carry bit (x1 ∧ y1); we will use this trick again in the general construction.
Now, suppose that (x2, y2) is either (0, 1) or (1, 0). Then, even if x2 and y2 are completely revealed by the first-round price, the value of C2(x1, x2, y1, y2) is not revealed: It will be 1 if x1 = y1 = 1 and 0 otherwise. Thus, we have shown that at least 2 rounds of trading will be required to reveal the function value in this case.
We now extend this construction to show by induction that the function Cn takes n rounds to reach an equilibrium in the worst case.
Theorem 5. There is a function Cn with 2n inputs and a prior distribution Pn such that, in the worst case, the market takes n rounds to reveal the value of Cn(·).
Proof: We prove the theorem by induction on n. The base case for n = 2 has already been shown to be true.
Starting from the distribution P2 described above, we construct the distributions P3, P4, . . . , Pn by inductively applying the following rule: • Let x−n denote the vector (x1, x2, . . . , xn−1), and define y−n similarly. We extend the distribution Pn−1 on (x−n , y−n ) to a distribution Pn on (x, y) by specifying the conditional distribution of (xn, yn) given (x−n , y−n ): If Cn−1(x−n , y−n ) = 1, then (xn, yn) takes the values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1 2 , 1 6 , 1 6 , 1 6 respectively. Otherwise, (xn, yn) takes the values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1 6 , 1 6 , 1 6 , 1 2 respectively.
Claim: Under distribution Pn, for all i < n,
P(Cn(x, y) = 1|xi = 1) = P(Cn(x, y) = 1|xi = 0). 162 Proof of claim: A similar calculation to that used for C2 above shows that the value of Cn(x, y) under this distribution is statistically independent of Cn−1(x−n , y−n ). For i < n, xi can affect the value of Cn only through Cn−1. Also, by contruction of Pn, given the value of Cn−1, the distribution of Cn is independent of xi. It follows that Cn(x, y) is statistically independent of xi as well. Of course, a similar result holds for yi by symmetry.
Thus, in the first round, for all i = 1, 2, . . . , n − 1, the bids of agents Ai and Bi do not reveal anything about their private information. Thus, the first-round price does not reveal any information about the value of (x−n , y−n ).
On the other hand, agents An and Bn do have different expectations of Cn(x) depending on whether their input bit is a 0 or a 1; thus, the first-round price does reveal whether neither, one, or both of xn and yn are 1. Now, consider a situation in which (xn, yn) takes on the value (1, 0) or (0, 1). We show that, in this case, after one round we are left with the residual problem of computing the value of Cn−1(x−n , y−n ) under the prior Pn−1.
Clearly, when xn + yn = 1, Cn(x, y) = Cn−1(x−n , y−n ).
Further, according to the construction of Pn, the event (xn+ yn = 1) has the same probability (1/3) for all values of (x−n , y−n ). Thus, conditioning on this fact does not alter the probability distribution over (x−n , y−n ); it must still be Pn−1.
Finally, the inductive assumption tells us that solving this residual problem will take at least n − 1 more rounds in the worst case and hence that finding the value of Cn(x, y) takes at least n rounds in the worst case. 2
Our results have been derived in a simplified model of an information market. In this section, we discuss the applicability of these results to more general trading models.
Assuming that agents bid truthfully, Theorem 2 holds in any model in which the price is a known stochastically monotone aggregate of agents" bids. While it seems reasonable that the market price satisfies monotonicity properties, the exact form of the aggregate function may not be known if the volume of each user"s trades is not observable; this depends on the details of the market process. Theorem 3 and Theorem 5 hold more generally; they only require that an agent"s strategy depends only on her conditional expectation of the security"s value. Perhaps the most fragile result is Theorem 4, which relies on the linear form of the Shapley-Shubik clearing price (in addition to the conditions for Theorem 2); however, it seems plausible that a similar dimension-based bound will hold for other families of nonlinear clearing prices.
Up to this point, we have described the model with the same number of agents as bits of information. However, all the results hold even if there is competition in the form of a known number of agents who know each bit of information.
Indeed, modeling such competition may help alleviate the strategic problems in our current model.
Another interesting approach to addressing the strategic issue is to consider alternative markets that are at least myopically incentive compatible. One example is a market mechanism called a market scoring rule, suggested by Hanson [12]. These markets have the property that a riskneutral agent"s best myopic strategy is to truthfully bid her current expected value of the security. Additionally, the number of securities involved in each trade is fixed and publicly known. If the market structure is such that, for example, the current scoring rule is posted publicly after each agent"s trade, then in equilibrium there is common knowledge of all agents" expectation, and hence Theorem 2 holds.
Theorem 3 also applies in this case, and hence we have the same characterization for the set of computable Boolean functions. This suggests that the problem of eliciting truthful responses may be orthogonal to the problem of computing the desired aggregate, reminiscent of the revelation principle [18].
In this paper, we have restricted our attention to the simplest possible aggregation problem: computing Boolean functions of Boolean inputs. The proofs of Theorems 3 and 5 also hold if we consider Boolean functions of real inputs, where each agent"s private information is a real number.
Further, Theorem 2 also holds provided the market reaches equilibrium. With real inputs and arbitrary prior distributions, however, it is not clear that the market will reach an equilibrium in a finite number of steps.
We have framed the process of information aggregation in markets as a computation on distributed information. We have developed a simplified model of an information market that we believe captures many of the important aspects of real agent interaction in an information market. Within this model, we prove several results characterizing precisely what the market can compute and how quickly. Specifically, we show that the market is guaranteed to converge to the true rational expectations equilibrium if and only if the security payoff function is a weighted threshold function. We prove that the process whereby agents reveal their information over time and learn from the resulting announced prices takes at most n rounds to converge to the correct full-information price in the worst case. We show that this bound is tight within a factor of two.
We view this paper as a first step towards understanding the computational power of information markets. Some interesting and important next steps include gaining a better understanding of the following: • The effect of price accuracy and precision: We have assumed that the clearing price is known with unlimited precision; in practice, this will not be true. Further, we have neglected influences on the market price other than from rational traders; the market price may also be influenced by other factors such as misinformed or irrational traders. It is interesting to ask what aggregates can be computed even in the presence of noisy prices. • Incremental updates: If the agents have computed the value of the function and a small number of input bits are switched, can the new value of the function be computed incrementally and quickly? • Distributed computation: In our model, distributed information is aggregated through a centralized market 163 computation. In a sense, some of the computation itself is distributed among the participating agents, but can the market computation also be distributed? For example, can we find a good distributed-computational model of a decentralized market? • Agents" computation: We have not accounted for the complexity of the computations that agents must do to accurately update their beliefs after each round. • Strategic market models: For reasons of simplicity and tractability, we have directly assumed that agents bid truthfully. A more satisfying approach would be to assume only rationality and solve for the resulting gametheoretic solution strategy, either in our current computational model or another model of an information market. • The common-prior assumption: Can we say anything about the market behavior when agents" priors are only approximately the same or when they differ greatly? • Average-case analysis: Our negative results (Theorems 3 and 5) examine worst-case scenarios, and thus involve very specific prior probability distributions. It is interesting to ask whether we would get very different results for generic prior distributions. • Information market design: Non-threshold functions can be implemented by layering two or more threshold functions together. What is the minimum number of threshold securities required to implement a given function? This is exactly the problem of minimizing the size of a neural network, a well-studied problem known to be NP-hard [15]. What configuration of securities can best approximate a given function? Are there ways to define and configure securities to speed up convergence to equilibrium? What is the relationship between machine learning (e.g., neural-network learning) and information-market design?
Acknowledgments We thank Joe Kilian for many helpful discussions. We thank Robin Hanson and the anonymous reviewers for useful insights and pointers.

